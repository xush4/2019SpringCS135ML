{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Support Vector Machines and Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a**: Describe a setting of the hyperparameters `gamma` and `C` that is sure to *overfit* to any typical training set and achieve zero training error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overfit, we should set `gamma` very large (e.g. $10^5$ or bigger). Setting `gamma` large makes the decision boundary as flexible as possible. \n",
    "\n",
    "Given a fixed training point $x$, any nearby point $x+ \\Delta$ will be rated as *unsimilar* to $x$ because the kernel value is small. Let $||\\Delta||_2^2 = \\sum_{f} \\Delta_f^2 = \\epsilon$\n",
    "\n",
    "\\begin{align}\n",
    "k(x, x+\\Delta) = e^{-\\gamma ||(x+ \\Delta) - x||_2^2} \n",
    "= e^{-\\gamma \\epsilon} \\quad \\approx 0 ~~\\text{if gamma is large}\n",
    "\\end{align}\n",
    "\n",
    "Thus, if the kernel rates nearby points as dissimilar, the predicted value can vary widely and lead to flexible training boundaries and overfitting.\n",
    "\n",
    "To overfit, we should also set `C` to a large value like $10^5$ or bigger. Remember that there are two terms here:\n",
    "\n",
    "    margin-width regularizer + C * prediction_error\n",
    "    \n",
    "If we set $C$ large, it overwhelms any influence the regularizer has, and can lead to overfitting (minimizing prediction error on training set).\n",
    "\n",
    "See also here: https://scikit-learn.org/0.15/auto_examples/svm/plot_rbf_parameters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1b**: Describe a setting of the hyperparameters `gamma` and `C` that is sure to *underfit* to any typical training set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To underfit, we should set `gamma` very small (like $10^{-6}$ or smaller). \n",
    "\n",
    "To underfit, we should also set $C$ very small, so that the regularization term has as large a possible influence as it can.\n",
    "\n",
    "Why small $\\gamma$? Given a fixed training point $x$, let's look at a nearby point $x+ \\Delta$, where the pertubation between $x$ and the new point has magnitude $||\\Delta||_2^2 = \\sum_{f} \\Delta_f^2 = \\epsilon$\n",
    "\n",
    "\\begin{align}\n",
    "k(x, x+\\Delta) = e^{-\\gamma ||(x+ \\Delta) - x||_2^2} \n",
    "= e^{-\\gamma \\epsilon} \\quad \\approx e^{0} = 1 ~~~ \\text{if gamma is small}\n",
    "\\end{align}\n",
    "\n",
    "This will make nearby points have very similar predicted values, and thus lead to smoother predictions, smoother boundaries, and possible underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1c**: If you were performing grid search to find the SVM that generalized best to unseen data, what range of values for gamma, C would you recommend? Specify your answer with two lines of NumPy code (e.g. using a list like [1, 2, 3] or np.linspace or np.logspace).  Also provide 1-2 sentences of justification.\n",
    "\n",
    "Assume that you can't afford more than 5 distinct values for each variable.\n",
    "\n",
    "```\n",
    "C_grid     = np.logspace(-4, +4, 5)   ### will use 10^-4, 10^-2, 10^0, 10^2, 10^4\n",
    "gamma_grid = np.logspace(-2, +2, 5)    ### will use 10^-2, 10^-1, 10^0, 10^1, 10^2\n",
    "```\n",
    "\n",
    "Especially if the data is already normalized, both C and gamma might typically do well around values of 1 (=$10^0$), but could require logarithmic adjustments that make them both much smaller or much larger to do well (e.g. the RBF kernel may require very short or very long neighborhood lengthscales). \n",
    "\n",
    "There's a lot of wiggle room here, so as long as students tried some reasonable *log*-spaced grid with values above and below 1, most points were awarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1d**: Answer the following True/False questions, providing *one sentence* of explanation for each one\n",
    "\n",
    "#### **1d(i)**: When used for binary classification, Support Vector Machines (SVMs) provide an estimate of the probability that a given feature vector $x_i$ will have a positive label: $p(Y_i = 1 | x_i)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALSE. SVMs produce binary predictions by thresholding a real-valued score: $w^T x_i + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **1d(ii)**: An advantage of an SVM is that the optimal weight vector $w$ (a vector with one entry per feature) will typically be sparse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALSE. SVM weight vectors $w$ are typically not sparse (see slides from lecture), but the learned $\\alpha$ vector (one entry per example) in the kernelized version is often sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1d(iii)**: When choosing a kernel function $k$, it should be the case that for any finite dataset of $N$ distinct examples, the $N \\times N$ kernel matrix is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALSE. \n",
    "\n",
    "Kernel functions are required to be positive *semi-definite*, but that doesn't mean the K matrix is always invertible, even if there are N distinct points.\n",
    "\n",
    "You can see some counter-examples (where N distinct points do NOT have an invertible $K$ matrix) in the attached script `when_is_kernel_matrix_invertible.py`\n",
    "\n",
    "* linear kernel with $F > N$: typically $K$ is invertible\n",
    "* linear kernel with $F < N$: typically $K$ is NOT invertible\n",
    "* rbf kernel: typically invertible ($F >> N$, because this is an infinite feature space)\n",
    "\n",
    "Invertibility would require **positive-definite** kernel function.\n",
    "\n",
    "What does this mean for kernelized linear regression??\n",
    "\n",
    "$$\n",
    "K \\alpha = y \\rightarrow \\alpha = K^{-1} y\n",
    "$$\n",
    "\n",
    "The simplification here is only valid if $K$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2a:** Describe a setting of the hyperparameters `n_estimators`, `max_features`, and `min_samples_leaf` that is sure to *overfit* to any typical training set. You can assume no BAgging (`bootstrap=False`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overfit with just one tree (n_estimators=1). You just let it use all the features (max_features=F) and allow it to grow until each leaf covers just one example (min_samples_leaf=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2b:** Describe a setting of the hyperparameters `n_estimators`, `max_features`, and `min_samples_leaf` that is sure to *underfit* to any typical training set. You can assume no BAgging (`bootstrap=False`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can underfit again with one tree, by just setting the minimum samples per leaf to a large value (perhaps equal to N, the total number of training examples). This will produce the same constant probability prediction for all examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2c:** If you were performing grid search to find the RandomForest that generalized best to unseen data, what range of values for n_estimators, max_features, and min_samples_leaf would you recommend? Should we set bootstrap to True or False? Specify your answer with a few lines of NumPy code (e.g. using a list like [1, 2, 3] or np.linspace or np.logspace to set each variable). Include a sentence or two of justification. Assume that you can't afford more than 5 distinct values for each variable.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Assume we have N=100 and F = 1000\n",
    "\n",
    "n_estimators_grid = [10, 100, 500]\n",
    "max_features_grid = [np.sqrt(F)]\n",
    "min_samples_leaf_grid = [1, 4, 16, 64, N/4]\n",
    "bootstrap_grid = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**: \n",
    "\n",
    "Bootstrap should always be turned on if we want to generalize well (training on separate samples is what helps RF hit the best bias-variance tradeoff).\n",
    "\n",
    "Trying different minimum leaf sizes is likely the best key to managing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2d:** Answer the following True/False questions, providing *one sentence* of explanation for each one:\n",
    "\n",
    "###  **2d(i)**: When used for binary classification, RandomForests (RFs) can provide an estimate of the probability that a given feature vector $x_i$ will have a positive label: $p(Y_i = 1 | x_i)$.\n",
    "\n",
    "TRUE. We can get probabilities from each decision tree, and just average them to get an estimated probability from the ensemble of many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2d(ii)**: With bootstrap aggregating enabled, random forests will almost always severely overfit the training data if the number of trees used (e.g. the `n_estimators`) is very large (say, over 500).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALSE. As the ISL textbook discusses, there isn't an overfitting danger of using too many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2d(iii)**: When fitting random forests, it is generally a good idea to allow each node of each tree to consider as many features as possible (e.g. `max_features` should be large).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALSE. The key idea of random forests is that we *decorrelate* between trees by making splits using different sets of features. Allowing all trees to greedily use the same features leads to little better performance than a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2d(iv)**: Random forests only use randomness when creating many similar datasets via BAgging. No other step of the algorithm uses randomness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALSE. Random forests can also randomly select subsets of features to use at each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3a:** Can we apply ROC curves to binary classifiers that cannot easily produce probabilities $\\hat{p}_i \\in (0,1)$, but produce some real-valued scores $s_i \\in \\mathbb{R}$ for each example? How would we select the range of thresholds to evaluate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the scores $s_n$ for each of the $N$ examples on the training set, we can use as all possible thresholds the unique values in $s = [s_1, \\ldots s_N]$.\n",
    "\n",
    "At each threshold $\\tau$: we can do $\\hat{y}_i = 1$ if $s_i \\geq \\tau$, and $\\hat{y}_i = 0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3b:** Suppose you fit a classifier to data, and you observe a TPR of 0.3 and an FPR of 0.7. Your friend says that this is worse than a random classifier (if plotted on an ROC curve, would fall below the TPR=FPR diagonal line), and so you should throw this result away. Is there anything better you can do? Describe how to tranform the predicted binary labels to reach better performance.  What TPR and FPR would you expect to achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this situation, we could flip the binary value of each prediction.\n",
    "\n",
    "That is, any prediction the original classifier made that was 0 would be flipped to a 1.\n",
    "Any prediction from the original classifier that was a 1 would be a 0.\n",
    "\n",
    "This would yield an FPR of 0.3 and a TPR of 0.7.\n",
    "\n",
    "Think about the extreme case of a classifier with FPR=1.0 and TPR = 0.0. This is classifier that gets everything wrong. We should just make the opposite prediction for every example, and we'd get everything correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
