{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(0,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "#orig_lr2 = LRGD(alpha=10.0, step_size=0.1)\n",
    "#orig_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_Origin=np.asarray(orig_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "#tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "#acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "#print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "#new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "#new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Initializing w_G with 787 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028764  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.908118  avg_L1_norm_grad         0.028900  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.840618  avg_L1_norm_grad         0.021099  w[0]    0.000 bias    0.018\n",
      "iter    3/1000000  loss         0.791049  avg_L1_norm_grad         0.021624  w[0]   -0.000 bias    0.019\n",
      "iter    4/1000000  loss         0.751305  avg_L1_norm_grad         0.015421  w[0]    0.000 bias    0.034\n",
      "iter    5/1000000  loss         0.720199  avg_L1_norm_grad         0.015490  w[0]    0.000 bias    0.036\n",
      "iter    6/1000000  loss         0.694474  avg_L1_norm_grad         0.011770  w[0]    0.000 bias    0.048\n",
      "iter    7/1000000  loss         0.673130  avg_L1_norm_grad         0.011719  w[0]    0.000 bias    0.052\n",
      "iter    8/1000000  loss         0.654765  avg_L1_norm_grad         0.009928  w[0]    0.000 bias    0.061\n",
      "iter    9/1000000  loss         0.638637  avg_L1_norm_grad         0.009702  w[0]    0.000 bias    0.067\n",
      "iter   10/1000000  loss         0.624177  avg_L1_norm_grad         0.008885  w[0]    0.000 bias    0.074\n",
      "iter   11/1000000  loss         0.611030  avg_L1_norm_grad         0.008570  w[0]    0.000 bias    0.080\n",
      "iter   12/1000000  loss         0.598952  avg_L1_norm_grad         0.008117  w[0]    0.000 bias    0.086\n",
      "iter   13/1000000  loss         0.587770  avg_L1_norm_grad         0.007807  w[0]    0.000 bias    0.092\n",
      "iter   14/1000000  loss         0.577357  avg_L1_norm_grad         0.007493  w[0]    0.000 bias    0.098\n",
      "iter   15/1000000  loss         0.567613  avg_L1_norm_grad         0.007226  w[0]    0.000 bias    0.104\n",
      "iter   16/1000000  loss         0.558459  avg_L1_norm_grad         0.006975  w[0]    0.000 bias    0.109\n",
      "iter   17/1000000  loss         0.549831  avg_L1_norm_grad         0.006748  w[0]    0.000 bias    0.115\n",
      "iter   18/1000000  loss         0.541675  avg_L1_norm_grad         0.006538  w[0]    0.000 bias    0.120\n",
      "iter   19/1000000  loss         0.533949  avg_L1_norm_grad         0.006343  w[0]    0.000 bias    0.125\n",
      "iter  100/1000000  loss         0.329775  avg_L1_norm_grad         0.002063  w[0]   -0.009 bias    0.339\n",
      "iter  101/1000000  loss         0.328902  avg_L1_norm_grad         0.002048  w[0]   -0.009 bias    0.340\n",
      "iter  200/1000000  loss         0.278935  avg_L1_norm_grad         0.001282  w[0]   -0.024 bias    0.431\n",
      "iter  201/1000000  loss         0.278635  avg_L1_norm_grad         0.001278  w[0]   -0.024 bias    0.431\n",
      "iter  300/1000000  loss         0.257047  avg_L1_norm_grad         0.000981  w[0]   -0.040 bias    0.472\n",
      "iter  301/1000000  loss         0.256887  avg_L1_norm_grad         0.000979  w[0]   -0.040 bias    0.472\n",
      "iter  400/1000000  loss         0.244193  avg_L1_norm_grad         0.000812  w[0]   -0.054 bias    0.492\n",
      "iter  401/1000000  loss         0.244090  avg_L1_norm_grad         0.000810  w[0]   -0.055 bias    0.492\n",
      "iter  500/1000000  loss         0.235509  avg_L1_norm_grad         0.000699  w[0]   -0.068 bias    0.501\n",
      "iter  501/1000000  loss         0.235436  avg_L1_norm_grad         0.000698  w[0]   -0.069 bias    0.501\n",
      "iter  600/1000000  loss         0.229162  avg_L1_norm_grad         0.000616  w[0]   -0.081 bias    0.505\n",
      "iter  601/1000000  loss         0.229107  avg_L1_norm_grad         0.000615  w[0]   -0.082 bias    0.505\n",
      "iter  700/1000000  loss         0.224285  avg_L1_norm_grad         0.000552  w[0]   -0.094 bias    0.505\n",
      "iter  701/1000000  loss         0.224242  avg_L1_norm_grad         0.000551  w[0]   -0.094 bias    0.505\n",
      "iter  800/1000000  loss         0.220408  avg_L1_norm_grad         0.000499  w[0]   -0.105 bias    0.503\n",
      "iter  801/1000000  loss         0.220373  avg_L1_norm_grad         0.000499  w[0]   -0.105 bias    0.503\n",
      "iter  900/1000000  loss         0.217248  avg_L1_norm_grad         0.000455  w[0]   -0.115 bias    0.500\n",
      "iter  901/1000000  loss         0.217219  avg_L1_norm_grad         0.000455  w[0]   -0.115 bias    0.500\n",
      "iter 1000/1000000  loss         0.214623  avg_L1_norm_grad         0.000418  w[0]   -0.125 bias    0.496\n",
      "iter 1001/1000000  loss         0.214599  avg_L1_norm_grad         0.000418  w[0]   -0.125 bias    0.496\n",
      "iter 1100/1000000  loss         0.212412  avg_L1_norm_grad         0.000386  w[0]   -0.134 bias    0.492\n",
      "iter 1101/1000000  loss         0.212392  avg_L1_norm_grad         0.000386  w[0]   -0.134 bias    0.492\n",
      "iter 1200/1000000  loss         0.210527  avg_L1_norm_grad         0.000358  w[0]   -0.142 bias    0.488\n",
      "iter 1201/1000000  loss         0.210509  avg_L1_norm_grad         0.000358  w[0]   -0.142 bias    0.488\n",
      "iter 1300/1000000  loss         0.208903  avg_L1_norm_grad         0.000333  w[0]   -0.150 bias    0.484\n",
      "iter 1301/1000000  loss         0.208888  avg_L1_norm_grad         0.000333  w[0]   -0.150 bias    0.484\n",
      "iter 1400/1000000  loss         0.207495  avg_L1_norm_grad         0.000311  w[0]   -0.157 bias    0.480\n",
      "iter 1401/1000000  loss         0.207482  avg_L1_norm_grad         0.000311  w[0]   -0.157 bias    0.480\n",
      "iter 1500/1000000  loss         0.206265  avg_L1_norm_grad         0.000292  w[0]   -0.164 bias    0.476\n",
      "iter 1501/1000000  loss         0.206253  avg_L1_norm_grad         0.000292  w[0]   -0.164 bias    0.475\n",
      "iter 1600/1000000  loss         0.205183  avg_L1_norm_grad         0.000274  w[0]   -0.170 bias    0.472\n",
      "iter 1601/1000000  loss         0.205173  avg_L1_norm_grad         0.000274  w[0]   -0.170 bias    0.472\n",
      "iter 1700/1000000  loss         0.204228  avg_L1_norm_grad         0.000258  w[0]   -0.176 bias    0.468\n",
      "iter 1701/1000000  loss         0.204219  avg_L1_norm_grad         0.000258  w[0]   -0.176 bias    0.468\n",
      "iter 1800/1000000  loss         0.203381  avg_L1_norm_grad         0.000243  w[0]   -0.181 bias    0.464\n",
      "iter 1801/1000000  loss         0.203373  avg_L1_norm_grad         0.000243  w[0]   -0.181 bias    0.464\n",
      "iter 1900/1000000  loss         0.202626  avg_L1_norm_grad         0.000230  w[0]   -0.186 bias    0.460\n",
      "iter 1901/1000000  loss         0.202619  avg_L1_norm_grad         0.000230  w[0]   -0.186 bias    0.460\n",
      "iter 2000/1000000  loss         0.201951  avg_L1_norm_grad         0.000217  w[0]   -0.191 bias    0.457\n",
      "iter 2001/1000000  loss         0.201945  avg_L1_norm_grad         0.000217  w[0]   -0.191 bias    0.457\n",
      "iter 2100/1000000  loss         0.201346  avg_L1_norm_grad         0.000206  w[0]   -0.196 bias    0.454\n",
      "iter 2101/1000000  loss         0.201341  avg_L1_norm_grad         0.000206  w[0]   -0.196 bias    0.454\n",
      "iter 2200/1000000  loss         0.200802  avg_L1_norm_grad         0.000195  w[0]   -0.200 bias    0.450\n",
      "iter 2201/1000000  loss         0.200797  avg_L1_norm_grad         0.000195  w[0]   -0.200 bias    0.450\n",
      "iter 2300/1000000  loss         0.200312  avg_L1_norm_grad         0.000186  w[0]   -0.204 bias    0.447\n",
      "iter 2301/1000000  loss         0.200308  avg_L1_norm_grad         0.000186  w[0]   -0.204 bias    0.447\n",
      "iter 2400/1000000  loss         0.199869  avg_L1_norm_grad         0.000177  w[0]   -0.208 bias    0.444\n",
      "iter 2401/1000000  loss         0.199865  avg_L1_norm_grad         0.000177  w[0]   -0.208 bias    0.444\n",
      "iter 2500/1000000  loss         0.199468  avg_L1_norm_grad         0.000168  w[0]   -0.211 bias    0.442\n",
      "iter 2501/1000000  loss         0.199464  avg_L1_norm_grad         0.000168  w[0]   -0.211 bias    0.442\n",
      "iter 2600/1000000  loss         0.199104  avg_L1_norm_grad         0.000160  w[0]   -0.214 bias    0.439\n",
      "iter 2601/1000000  loss         0.199101  avg_L1_norm_grad         0.000160  w[0]   -0.214 bias    0.439\n",
      "iter 2700/1000000  loss         0.198774  avg_L1_norm_grad         0.000153  w[0]   -0.217 bias    0.436\n",
      "iter 2701/1000000  loss         0.198771  avg_L1_norm_grad         0.000153  w[0]   -0.217 bias    0.436\n",
      "iter 2800/1000000  loss         0.198473  avg_L1_norm_grad         0.000146  w[0]   -0.220 bias    0.434\n",
      "iter 2801/1000000  loss         0.198470  avg_L1_norm_grad         0.000146  w[0]   -0.220 bias    0.434\n",
      "iter 2900/1000000  loss         0.198198  avg_L1_norm_grad         0.000139  w[0]   -0.223 bias    0.431\n",
      "iter 2901/1000000  loss         0.198196  avg_L1_norm_grad         0.000139  w[0]   -0.223 bias    0.431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.197947  avg_L1_norm_grad         0.000133  w[0]   -0.226 bias    0.429\n",
      "iter 3001/1000000  loss         0.197945  avg_L1_norm_grad         0.000133  w[0]   -0.226 bias    0.429\n",
      "iter 3100/1000000  loss         0.197718  avg_L1_norm_grad         0.000127  w[0]   -0.228 bias    0.427\n",
      "iter 3101/1000000  loss         0.197716  avg_L1_norm_grad         0.000127  w[0]   -0.228 bias    0.427\n",
      "iter 3200/1000000  loss         0.197509  avg_L1_norm_grad         0.000122  w[0]   -0.230 bias    0.424\n",
      "iter 3201/1000000  loss         0.197507  avg_L1_norm_grad         0.000122  w[0]   -0.230 bias    0.424\n",
      "iter 3300/1000000  loss         0.197316  avg_L1_norm_grad         0.000117  w[0]   -0.232 bias    0.422\n",
      "iter 3301/1000000  loss         0.197314  avg_L1_norm_grad         0.000117  w[0]   -0.232 bias    0.422\n",
      "iter 3400/1000000  loss         0.197140  avg_L1_norm_grad         0.000112  w[0]   -0.235 bias    0.420\n",
      "iter 3401/1000000  loss         0.197138  avg_L1_norm_grad         0.000112  w[0]   -0.235 bias    0.420\n",
      "iter 3500/1000000  loss         0.196978  avg_L1_norm_grad         0.000107  w[0]   -0.236 bias    0.418\n",
      "iter 3501/1000000  loss         0.196976  avg_L1_norm_grad         0.000107  w[0]   -0.236 bias    0.418\n",
      "iter 3600/1000000  loss         0.196829  avg_L1_norm_grad         0.000103  w[0]   -0.238 bias    0.416\n",
      "iter 3601/1000000  loss         0.196827  avg_L1_norm_grad         0.000103  w[0]   -0.238 bias    0.416\n",
      "iter 3700/1000000  loss         0.196692  avg_L1_norm_grad         0.000099  w[0]   -0.240 bias    0.414\n",
      "iter 3701/1000000  loss         0.196690  avg_L1_norm_grad         0.000099  w[0]   -0.240 bias    0.414\n",
      "iter 3800/1000000  loss         0.196565  avg_L1_norm_grad         0.000095  w[0]   -0.242 bias    0.412\n",
      "iter 3801/1000000  loss         0.196564  avg_L1_norm_grad         0.000095  w[0]   -0.242 bias    0.412\n",
      "iter 3900/1000000  loss         0.196449  avg_L1_norm_grad         0.000091  w[0]   -0.243 bias    0.410\n",
      "iter 3901/1000000  loss         0.196448  avg_L1_norm_grad         0.000091  w[0]   -0.243 bias    0.410\n",
      "iter 4000/1000000  loss         0.196342  avg_L1_norm_grad         0.000087  w[0]   -0.245 bias    0.409\n",
      "iter 4001/1000000  loss         0.196341  avg_L1_norm_grad         0.000087  w[0]   -0.245 bias    0.409\n",
      "iter 4100/1000000  loss         0.196243  avg_L1_norm_grad         0.000084  w[0]   -0.246 bias    0.407\n",
      "iter 4101/1000000  loss         0.196242  avg_L1_norm_grad         0.000084  w[0]   -0.246 bias    0.407\n",
      "iter 4200/1000000  loss         0.196151  avg_L1_norm_grad         0.000081  w[0]   -0.247 bias    0.405\n",
      "iter 4201/1000000  loss         0.196150  avg_L1_norm_grad         0.000081  w[0]   -0.248 bias    0.405\n",
      "iter 4300/1000000  loss         0.196066  avg_L1_norm_grad         0.000078  w[0]   -0.249 bias    0.403\n",
      "iter 4301/1000000  loss         0.196066  avg_L1_norm_grad         0.000078  w[0]   -0.249 bias    0.403\n",
      "iter 4400/1000000  loss         0.195988  avg_L1_norm_grad         0.000075  w[0]   -0.250 bias    0.402\n",
      "iter 4401/1000000  loss         0.195987  avg_L1_norm_grad         0.000075  w[0]   -0.250 bias    0.402\n",
      "iter 4500/1000000  loss         0.195915  avg_L1_norm_grad         0.000072  w[0]   -0.251 bias    0.400\n",
      "iter 4501/1000000  loss         0.195915  avg_L1_norm_grad         0.000072  w[0]   -0.251 bias    0.400\n",
      "iter 4600/1000000  loss         0.195848  avg_L1_norm_grad         0.000069  w[0]   -0.252 bias    0.399\n",
      "iter 4601/1000000  loss         0.195848  avg_L1_norm_grad         0.000069  w[0]   -0.252 bias    0.399\n",
      "iter 4700/1000000  loss         0.195786  avg_L1_norm_grad         0.000067  w[0]   -0.253 bias    0.397\n",
      "iter 4701/1000000  loss         0.195785  avg_L1_norm_grad         0.000067  w[0]   -0.253 bias    0.397\n",
      "iter 4800/1000000  loss         0.195728  avg_L1_norm_grad         0.000064  w[0]   -0.254 bias    0.396\n",
      "iter 4801/1000000  loss         0.195728  avg_L1_norm_grad         0.000064  w[0]   -0.254 bias    0.396\n",
      "iter 4900/1000000  loss         0.195675  avg_L1_norm_grad         0.000062  w[0]   -0.255 bias    0.394\n",
      "iter 4901/1000000  loss         0.195674  avg_L1_norm_grad         0.000062  w[0]   -0.255 bias    0.394\n",
      "iter 5000/1000000  loss         0.195625  avg_L1_norm_grad         0.000060  w[0]   -0.256 bias    0.393\n",
      "iter 5001/1000000  loss         0.195625  avg_L1_norm_grad         0.000060  w[0]   -0.256 bias    0.393\n",
      "iter 5100/1000000  loss         0.195579  avg_L1_norm_grad         0.000057  w[0]   -0.257 bias    0.391\n",
      "iter 5101/1000000  loss         0.195578  avg_L1_norm_grad         0.000057  w[0]   -0.257 bias    0.391\n",
      "iter 5200/1000000  loss         0.195536  avg_L1_norm_grad         0.000055  w[0]   -0.258 bias    0.390\n",
      "iter 5201/1000000  loss         0.195536  avg_L1_norm_grad         0.000055  w[0]   -0.258 bias    0.390\n",
      "iter 5300/1000000  loss         0.195496  avg_L1_norm_grad         0.000053  w[0]   -0.258 bias    0.389\n",
      "iter 5301/1000000  loss         0.195496  avg_L1_norm_grad         0.000053  w[0]   -0.258 bias    0.389\n",
      "iter 5400/1000000  loss         0.195459  avg_L1_norm_grad         0.000051  w[0]   -0.259 bias    0.388\n",
      "iter 5401/1000000  loss         0.195459  avg_L1_norm_grad         0.000051  w[0]   -0.259 bias    0.388\n",
      "iter 5500/1000000  loss         0.195424  avg_L1_norm_grad         0.000050  w[0]   -0.260 bias    0.386\n",
      "iter 5501/1000000  loss         0.195424  avg_L1_norm_grad         0.000050  w[0]   -0.260 bias    0.386\n",
      "iter 5600/1000000  loss         0.195392  avg_L1_norm_grad         0.000048  w[0]   -0.261 bias    0.385\n",
      "iter 5601/1000000  loss         0.195392  avg_L1_norm_grad         0.000048  w[0]   -0.261 bias    0.385\n",
      "iter 5700/1000000  loss         0.195362  avg_L1_norm_grad         0.000046  w[0]   -0.261 bias    0.384\n",
      "iter 5701/1000000  loss         0.195362  avg_L1_norm_grad         0.000046  w[0]   -0.261 bias    0.384\n",
      "iter 5800/1000000  loss         0.195335  avg_L1_norm_grad         0.000044  w[0]   -0.262 bias    0.383\n",
      "iter 5801/1000000  loss         0.195334  avg_L1_norm_grad         0.000044  w[0]   -0.262 bias    0.383\n",
      "iter 5900/1000000  loss         0.195309  avg_L1_norm_grad         0.000043  w[0]   -0.262 bias    0.382\n",
      "iter 5901/1000000  loss         0.195308  avg_L1_norm_grad         0.000043  w[0]   -0.262 bias    0.382\n",
      "iter 6000/1000000  loss         0.195284  avg_L1_norm_grad         0.000041  w[0]   -0.263 bias    0.380\n",
      "iter 6001/1000000  loss         0.195284  avg_L1_norm_grad         0.000041  w[0]   -0.263 bias    0.380\n",
      "iter 6100/1000000  loss         0.195262  avg_L1_norm_grad         0.000040  w[0]   -0.263 bias    0.379\n",
      "iter 6101/1000000  loss         0.195262  avg_L1_norm_grad         0.000040  w[0]   -0.263 bias    0.379\n",
      "iter 6200/1000000  loss         0.195241  avg_L1_norm_grad         0.000039  w[0]   -0.264 bias    0.378\n",
      "iter 6201/1000000  loss         0.195241  avg_L1_norm_grad         0.000039  w[0]   -0.264 bias    0.378\n",
      "iter 6300/1000000  loss         0.195221  avg_L1_norm_grad         0.000037  w[0]   -0.264 bias    0.377\n",
      "iter 6301/1000000  loss         0.195221  avg_L1_norm_grad         0.000037  w[0]   -0.264 bias    0.377\n",
      "iter 6400/1000000  loss         0.195203  avg_L1_norm_grad         0.000036  w[0]   -0.265 bias    0.376\n",
      "iter 6401/1000000  loss         0.195203  avg_L1_norm_grad         0.000036  w[0]   -0.265 bias    0.376\n",
      "iter 6500/1000000  loss         0.195186  avg_L1_norm_grad         0.000035  w[0]   -0.265 bias    0.375\n",
      "iter 6501/1000000  loss         0.195185  avg_L1_norm_grad         0.000035  w[0]   -0.265 bias    0.375\n",
      "iter 6600/1000000  loss         0.195170  avg_L1_norm_grad         0.000033  w[0]   -0.266 bias    0.374\n",
      "iter 6601/1000000  loss         0.195169  avg_L1_norm_grad         0.000033  w[0]   -0.266 bias    0.374\n",
      "iter 6700/1000000  loss         0.195155  avg_L1_norm_grad         0.000032  w[0]   -0.266 bias    0.373\n",
      "iter 6701/1000000  loss         0.195154  avg_L1_norm_grad         0.000032  w[0]   -0.266 bias    0.373\n",
      "iter 6800/1000000  loss         0.195141  avg_L1_norm_grad         0.000031  w[0]   -0.266 bias    0.372\n",
      "iter 6801/1000000  loss         0.195141  avg_L1_norm_grad         0.000031  w[0]   -0.266 bias    0.372\n",
      "iter 6900/1000000  loss         0.195128  avg_L1_norm_grad         0.000030  w[0]   -0.267 bias    0.371\n",
      "iter 6901/1000000  loss         0.195127  avg_L1_norm_grad         0.000030  w[0]   -0.267 bias    0.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.195115  avg_L1_norm_grad         0.000029  w[0]   -0.267 bias    0.371\n",
      "iter 7001/1000000  loss         0.195115  avg_L1_norm_grad         0.000029  w[0]   -0.267 bias    0.371\n",
      "iter 7100/1000000  loss         0.195104  avg_L1_norm_grad         0.000028  w[0]   -0.267 bias    0.370\n",
      "iter 7101/1000000  loss         0.195104  avg_L1_norm_grad         0.000028  w[0]   -0.267 bias    0.370\n",
      "iter 7200/1000000  loss         0.195093  avg_L1_norm_grad         0.000027  w[0]   -0.268 bias    0.369\n",
      "iter 7201/1000000  loss         0.195093  avg_L1_norm_grad         0.000027  w[0]   -0.268 bias    0.369\n",
      "iter 7300/1000000  loss         0.195083  avg_L1_norm_grad         0.000026  w[0]   -0.268 bias    0.368\n",
      "iter 7301/1000000  loss         0.195083  avg_L1_norm_grad         0.000026  w[0]   -0.268 bias    0.368\n",
      "iter 7400/1000000  loss         0.195074  avg_L1_norm_grad         0.000025  w[0]   -0.268 bias    0.367\n",
      "iter 7401/1000000  loss         0.195074  avg_L1_norm_grad         0.000025  w[0]   -0.268 bias    0.367\n",
      "iter 7500/1000000  loss         0.195065  avg_L1_norm_grad         0.000025  w[0]   -0.269 bias    0.366\n",
      "iter 7501/1000000  loss         0.195065  avg_L1_norm_grad         0.000025  w[0]   -0.269 bias    0.366\n",
      "iter 7600/1000000  loss         0.195057  avg_L1_norm_grad         0.000024  w[0]   -0.269 bias    0.366\n",
      "iter 7601/1000000  loss         0.195057  avg_L1_norm_grad         0.000024  w[0]   -0.269 bias    0.366\n",
      "iter 7700/1000000  loss         0.195049  avg_L1_norm_grad         0.000023  w[0]   -0.269 bias    0.365\n",
      "iter 7701/1000000  loss         0.195049  avg_L1_norm_grad         0.000023  w[0]   -0.269 bias    0.365\n",
      "iter 7800/1000000  loss         0.195042  avg_L1_norm_grad         0.000022  w[0]   -0.269 bias    0.364\n",
      "iter 7801/1000000  loss         0.195042  avg_L1_norm_grad         0.000022  w[0]   -0.269 bias    0.364\n",
      "iter 7900/1000000  loss         0.195035  avg_L1_norm_grad         0.000021  w[0]   -0.270 bias    0.363\n",
      "iter 7901/1000000  loss         0.195035  avg_L1_norm_grad         0.000021  w[0]   -0.270 bias    0.363\n",
      "iter 8000/1000000  loss         0.195029  avg_L1_norm_grad         0.000021  w[0]   -0.270 bias    0.363\n",
      "iter 8001/1000000  loss         0.195029  avg_L1_norm_grad         0.000021  w[0]   -0.270 bias    0.363\n",
      "iter 8100/1000000  loss         0.195023  avg_L1_norm_grad         0.000020  w[0]   -0.270 bias    0.362\n",
      "iter 8101/1000000  loss         0.195023  avg_L1_norm_grad         0.000020  w[0]   -0.270 bias    0.362\n",
      "iter 8200/1000000  loss         0.195017  avg_L1_norm_grad         0.000019  w[0]   -0.270 bias    0.361\n",
      "iter 8201/1000000  loss         0.195017  avg_L1_norm_grad         0.000019  w[0]   -0.270 bias    0.361\n",
      "iter 8300/1000000  loss         0.195012  avg_L1_norm_grad         0.000019  w[0]   -0.270 bias    0.361\n",
      "iter 8301/1000000  loss         0.195012  avg_L1_norm_grad         0.000019  w[0]   -0.270 bias    0.361\n",
      "iter 8400/1000000  loss         0.195007  avg_L1_norm_grad         0.000018  w[0]   -0.271 bias    0.360\n",
      "iter 8401/1000000  loss         0.195007  avg_L1_norm_grad         0.000018  w[0]   -0.271 bias    0.360\n",
      "iter 8500/1000000  loss         0.195002  avg_L1_norm_grad         0.000018  w[0]   -0.271 bias    0.359\n",
      "iter 8501/1000000  loss         0.195002  avg_L1_norm_grad         0.000018  w[0]   -0.271 bias    0.359\n",
      "iter 8600/1000000  loss         0.194998  avg_L1_norm_grad         0.000017  w[0]   -0.271 bias    0.359\n",
      "iter 8601/1000000  loss         0.194998  avg_L1_norm_grad         0.000017  w[0]   -0.271 bias    0.359\n",
      "iter 8700/1000000  loss         0.194994  avg_L1_norm_grad         0.000017  w[0]   -0.271 bias    0.358\n",
      "iter 8701/1000000  loss         0.194994  avg_L1_norm_grad         0.000017  w[0]   -0.271 bias    0.358\n",
      "iter 8800/1000000  loss         0.194990  avg_L1_norm_grad         0.000016  w[0]   -0.271 bias    0.358\n",
      "iter 8801/1000000  loss         0.194990  avg_L1_norm_grad         0.000016  w[0]   -0.271 bias    0.358\n",
      "iter 8900/1000000  loss         0.194987  avg_L1_norm_grad         0.000015  w[0]   -0.271 bias    0.357\n",
      "iter 8901/1000000  loss         0.194987  avg_L1_norm_grad         0.000015  w[0]   -0.271 bias    0.357\n",
      "iter 9000/1000000  loss         0.194983  avg_L1_norm_grad         0.000015  w[0]   -0.272 bias    0.357\n",
      "iter 9001/1000000  loss         0.194983  avg_L1_norm_grad         0.000015  w[0]   -0.272 bias    0.357\n",
      "iter 9100/1000000  loss         0.194980  avg_L1_norm_grad         0.000014  w[0]   -0.272 bias    0.356\n",
      "iter 9101/1000000  loss         0.194980  avg_L1_norm_grad         0.000014  w[0]   -0.272 bias    0.356\n",
      "iter 9200/1000000  loss         0.194977  avg_L1_norm_grad         0.000014  w[0]   -0.272 bias    0.355\n",
      "iter 9201/1000000  loss         0.194977  avg_L1_norm_grad         0.000014  w[0]   -0.272 bias    0.355\n",
      "iter 9300/1000000  loss         0.194974  avg_L1_norm_grad         0.000014  w[0]   -0.272 bias    0.355\n",
      "iter 9301/1000000  loss         0.194974  avg_L1_norm_grad         0.000014  w[0]   -0.272 bias    0.355\n",
      "iter 9400/1000000  loss         0.194972  avg_L1_norm_grad         0.000013  w[0]   -0.272 bias    0.354\n",
      "iter 9401/1000000  loss         0.194972  avg_L1_norm_grad         0.000013  w[0]   -0.272 bias    0.354\n",
      "iter 9500/1000000  loss         0.194969  avg_L1_norm_grad         0.000013  w[0]   -0.272 bias    0.354\n",
      "iter 9501/1000000  loss         0.194969  avg_L1_norm_grad         0.000013  w[0]   -0.272 bias    0.354\n",
      "iter 9600/1000000  loss         0.194967  avg_L1_norm_grad         0.000012  w[0]   -0.272 bias    0.353\n",
      "iter 9601/1000000  loss         0.194967  avg_L1_norm_grad         0.000012  w[0]   -0.272 bias    0.353\n",
      "iter 9700/1000000  loss         0.194965  avg_L1_norm_grad         0.000012  w[0]   -0.272 bias    0.353\n",
      "iter 9701/1000000  loss         0.194964  avg_L1_norm_grad         0.000012  w[0]   -0.272 bias    0.353\n",
      "iter 9800/1000000  loss         0.194962  avg_L1_norm_grad         0.000012  w[0]   -0.273 bias    0.353\n",
      "iter 9801/1000000  loss         0.194962  avg_L1_norm_grad         0.000012  w[0]   -0.273 bias    0.353\n",
      "iter 9900/1000000  loss         0.194960  avg_L1_norm_grad         0.000011  w[0]   -0.273 bias    0.352\n",
      "iter 9901/1000000  loss         0.194960  avg_L1_norm_grad         0.000011  w[0]   -0.273 bias    0.352\n",
      "iter 10000/1000000  loss         0.194959  avg_L1_norm_grad         0.000011  w[0]   -0.273 bias    0.352\n",
      "iter 10001/1000000  loss         0.194959  avg_L1_norm_grad         0.000011  w[0]   -0.273 bias    0.352\n",
      "iter 10100/1000000  loss         0.194957  avg_L1_norm_grad         0.000011  w[0]   -0.273 bias    0.351\n",
      "iter 10101/1000000  loss         0.194957  avg_L1_norm_grad         0.000011  w[0]   -0.273 bias    0.351\n",
      "iter 10200/1000000  loss         0.194955  avg_L1_norm_grad         0.000010  w[0]   -0.273 bias    0.351\n",
      "iter 10201/1000000  loss         0.194955  avg_L1_norm_grad         0.000010  w[0]   -0.273 bias    0.351\n",
      "iter 10300/1000000  loss         0.194954  avg_L1_norm_grad         0.000010  w[0]   -0.273 bias    0.350\n",
      "iter 10301/1000000  loss         0.194954  avg_L1_norm_grad         0.000010  w[0]   -0.273 bias    0.350\n",
      "iter 10400/1000000  loss         0.194952  avg_L1_norm_grad         0.000010  w[0]   -0.273 bias    0.350\n",
      "iter 10401/1000000  loss         0.194952  avg_L1_norm_grad         0.000010  w[0]   -0.273 bias    0.350\n",
      "iter 10500/1000000  loss         0.194951  avg_L1_norm_grad         0.000009  w[0]   -0.273 bias    0.350\n",
      "iter 10501/1000000  loss         0.194951  avg_L1_norm_grad         0.000009  w[0]   -0.273 bias    0.350\n",
      "iter 10600/1000000  loss         0.194950  avg_L1_norm_grad         0.000009  w[0]   -0.273 bias    0.349\n",
      "iter 10601/1000000  loss         0.194950  avg_L1_norm_grad         0.000009  w[0]   -0.273 bias    0.349\n",
      "iter 10700/1000000  loss         0.194948  avg_L1_norm_grad         0.000009  w[0]   -0.273 bias    0.349\n",
      "iter 10701/1000000  loss         0.194948  avg_L1_norm_grad         0.000009  w[0]   -0.273 bias    0.349\n",
      "iter 10800/1000000  loss         0.194947  avg_L1_norm_grad         0.000008  w[0]   -0.273 bias    0.349\n",
      "iter 10801/1000000  loss         0.194947  avg_L1_norm_grad         0.000008  w[0]   -0.273 bias    0.349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.194946  avg_L1_norm_grad         0.000008  w[0]   -0.273 bias    0.348\n",
      "iter 10901/1000000  loss         0.194946  avg_L1_norm_grad         0.000008  w[0]   -0.273 bias    0.348\n",
      "iter 11000/1000000  loss         0.194945  avg_L1_norm_grad         0.000008  w[0]   -0.273 bias    0.348\n",
      "iter 11001/1000000  loss         0.194945  avg_L1_norm_grad         0.000008  w[0]   -0.273 bias    0.348\n",
      "iter 11100/1000000  loss         0.194944  avg_L1_norm_grad         0.000008  w[0]   -0.274 bias    0.348\n",
      "iter 11101/1000000  loss         0.194944  avg_L1_norm_grad         0.000008  w[0]   -0.274 bias    0.348\n",
      "iter 11200/1000000  loss         0.194943  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.347\n",
      "iter 11201/1000000  loss         0.194943  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.347\n",
      "iter 11300/1000000  loss         0.194942  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.347\n",
      "iter 11301/1000000  loss         0.194942  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.347\n",
      "iter 11400/1000000  loss         0.194942  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.347\n",
      "iter 11401/1000000  loss         0.194942  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.347\n",
      "iter 11500/1000000  loss         0.194941  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.346\n",
      "iter 11501/1000000  loss         0.194941  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.346\n",
      "iter 11600/1000000  loss         0.194940  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.346\n",
      "iter 11601/1000000  loss         0.194940  avg_L1_norm_grad         0.000007  w[0]   -0.274 bias    0.346\n",
      "iter 11700/1000000  loss         0.194939  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.346\n",
      "iter 11701/1000000  loss         0.194939  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.346\n",
      "iter 11800/1000000  loss         0.194939  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 11801/1000000  loss         0.194939  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 11900/1000000  loss         0.194938  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 11901/1000000  loss         0.194938  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 12000/1000000  loss         0.194938  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 12001/1000000  loss         0.194938  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 12100/1000000  loss         0.194937  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 12101/1000000  loss         0.194937  avg_L1_norm_grad         0.000006  w[0]   -0.274 bias    0.345\n",
      "iter 12200/1000000  loss         0.194937  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12201/1000000  loss         0.194937  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12300/1000000  loss         0.194936  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12301/1000000  loss         0.194936  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12400/1000000  loss         0.194936  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12401/1000000  loss         0.194936  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12500/1000000  loss         0.194935  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12501/1000000  loss         0.194935  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.344\n",
      "iter 12600/1000000  loss         0.194935  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.343\n",
      "iter 12601/1000000  loss         0.194935  avg_L1_norm_grad         0.000005  w[0]   -0.274 bias    0.343\n",
      "Done. Converged after 12657 iterations.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028530  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.918327  avg_L1_norm_grad         0.029939  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.856923  avg_L1_norm_grad         0.020526  w[0]    0.000 bias    0.019\n",
      "iter    3/1000000  loss         0.812020  avg_L1_norm_grad         0.021466  w[0]    0.000 bias    0.022\n",
      "iter    4/1000000  loss         0.776490  avg_L1_norm_grad         0.015132  w[0]    0.001 bias    0.037\n",
      "iter    5/1000000  loss         0.748701  avg_L1_norm_grad         0.015598  w[0]    0.000 bias    0.042\n",
      "iter    6/1000000  loss         0.725833  avg_L1_norm_grad         0.012254  w[0]    0.001 bias    0.053\n",
      "iter    7/1000000  loss         0.706653  avg_L1_norm_grad         0.012328  w[0]    0.001 bias    0.060\n",
      "iter    8/1000000  loss         0.690018  avg_L1_norm_grad         0.010754  w[0]    0.001 bias    0.070\n",
      "iter    9/1000000  loss         0.675271  avg_L1_norm_grad         0.010528  w[0]    0.001 bias    0.077\n",
      "iter   10/1000000  loss         0.661954  avg_L1_norm_grad         0.009787  w[0]    0.001 bias    0.085\n",
      "iter   11/1000000  loss         0.649773  avg_L1_norm_grad         0.009446  w[0]    0.001 bias    0.093\n",
      "iter   12/1000000  loss         0.638521  avg_L1_norm_grad         0.009005  w[0]    0.001 bias    0.101\n",
      "iter   13/1000000  loss         0.628049  avg_L1_norm_grad         0.008689  w[0]    0.001 bias    0.108\n",
      "iter   14/1000000  loss         0.618246  avg_L1_norm_grad         0.008374  w[0]    0.001 bias    0.115\n",
      "iter   15/1000000  loss         0.609024  avg_L1_norm_grad         0.008103  w[0]    0.001 bias    0.122\n",
      "iter   16/1000000  loss         0.600316  avg_L1_norm_grad         0.007851  w[0]    0.002 bias    0.129\n",
      "iter   17/1000000  loss         0.592065  avg_L1_norm_grad         0.007624  w[0]    0.002 bias    0.136\n",
      "iter   18/1000000  loss         0.584228  avg_L1_norm_grad         0.007414  w[0]    0.002 bias    0.143\n",
      "iter   19/1000000  loss         0.576765  avg_L1_norm_grad         0.007218  w[0]    0.002 bias    0.150\n",
      "iter  100/1000000  loss         0.364009  avg_L1_norm_grad         0.002532  w[0]   -0.001 bias    0.485\n",
      "iter  101/1000000  loss         0.363027  avg_L1_norm_grad         0.002513  w[0]   -0.001 bias    0.488\n",
      "iter  200/1000000  loss         0.305843  avg_L1_norm_grad         0.001489  w[0]   -0.010 bias    0.698\n",
      "iter  201/1000000  loss         0.305496  avg_L1_norm_grad         0.001483  w[0]   -0.011 bias    0.700\n",
      "iter  300/1000000  loss         0.280452  avg_L1_norm_grad         0.001089  w[0]   -0.021 bias    0.838\n",
      "iter  301/1000000  loss         0.280267  avg_L1_norm_grad         0.001086  w[0]   -0.021 bias    0.839\n",
      "iter  400/1000000  loss         0.265700  avg_L1_norm_grad         0.000873  w[0]   -0.032 bias    0.942\n",
      "iter  401/1000000  loss         0.265583  avg_L1_norm_grad         0.000871  w[0]   -0.032 bias    0.943\n",
      "iter  500/1000000  loss         0.255909  avg_L1_norm_grad         0.000736  w[0]   -0.043 bias    1.025\n",
      "iter  501/1000000  loss         0.255828  avg_L1_norm_grad         0.000734  w[0]   -0.043 bias    1.026\n",
      "iter  600/1000000  loss         0.248883  avg_L1_norm_grad         0.000639  w[0]   -0.054 bias    1.094\n",
      "iter  601/1000000  loss         0.248823  avg_L1_norm_grad         0.000639  w[0]   -0.054 bias    1.094\n",
      "iter  700/1000000  loss         0.243573  avg_L1_norm_grad         0.000566  w[0]   -0.064 bias    1.152\n",
      "iter  701/1000000  loss         0.243527  avg_L1_norm_grad         0.000566  w[0]   -0.064 bias    1.153\n",
      "iter  800/1000000  loss         0.239412  avg_L1_norm_grad         0.000508  w[0]   -0.074 bias    1.203\n",
      "iter  801/1000000  loss         0.239375  avg_L1_norm_grad         0.000508  w[0]   -0.074 bias    1.203\n",
      "iter  900/1000000  loss         0.236061  avg_L1_norm_grad         0.000461  w[0]   -0.083 bias    1.248\n",
      "iter  901/1000000  loss         0.236031  avg_L1_norm_grad         0.000460  w[0]   -0.083 bias    1.248\n",
      "iter 1000/1000000  loss         0.233306  avg_L1_norm_grad         0.000421  w[0]   -0.091 bias    1.288\n",
      "iter 1001/1000000  loss         0.233281  avg_L1_norm_grad         0.000421  w[0]   -0.092 bias    1.289\n",
      "iter 1100/1000000  loss         0.231005  avg_L1_norm_grad         0.000387  w[0]   -0.100 bias    1.325\n",
      "iter 1101/1000000  loss         0.230984  avg_L1_norm_grad         0.000387  w[0]   -0.100 bias    1.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1200/1000000  loss         0.229057  avg_L1_norm_grad         0.000358  w[0]   -0.107 bias    1.358\n",
      "iter 1201/1000000  loss         0.229039  avg_L1_norm_grad         0.000358  w[0]   -0.107 bias    1.359\n",
      "iter 1300/1000000  loss         0.227390  avg_L1_norm_grad         0.000333  w[0]   -0.115 bias    1.389\n",
      "iter 1301/1000000  loss         0.227375  avg_L1_norm_grad         0.000332  w[0]   -0.115 bias    1.389\n",
      "iter 1400/1000000  loss         0.225952  avg_L1_norm_grad         0.000310  w[0]   -0.122 bias    1.418\n",
      "iter 1401/1000000  loss         0.225939  avg_L1_norm_grad         0.000310  w[0]   -0.122 bias    1.418\n",
      "iter 1500/1000000  loss         0.224702  avg_L1_norm_grad         0.000290  w[0]   -0.128 bias    1.444\n",
      "iter 1501/1000000  loss         0.224690  avg_L1_norm_grad         0.000290  w[0]   -0.128 bias    1.444\n",
      "iter 1600/1000000  loss         0.223607  avg_L1_norm_grad         0.000272  w[0]   -0.134 bias    1.469\n",
      "iter 1601/1000000  loss         0.223597  avg_L1_norm_grad         0.000272  w[0]   -0.134 bias    1.469\n",
      "iter 1700/1000000  loss         0.222645  avg_L1_norm_grad         0.000256  w[0]   -0.140 bias    1.492\n",
      "iter 1701/1000000  loss         0.222636  avg_L1_norm_grad         0.000255  w[0]   -0.140 bias    1.493\n",
      "iter 1800/1000000  loss         0.221794  avg_L1_norm_grad         0.000241  w[0]   -0.145 bias    1.514\n",
      "iter 1801/1000000  loss         0.221786  avg_L1_norm_grad         0.000241  w[0]   -0.145 bias    1.515\n",
      "iter 1900/1000000  loss         0.221038  avg_L1_norm_grad         0.000227  w[0]   -0.150 bias    1.535\n",
      "iter 1901/1000000  loss         0.221031  avg_L1_norm_grad         0.000227  w[0]   -0.150 bias    1.535\n",
      "iter 2000/1000000  loss         0.220365  avg_L1_norm_grad         0.000215  w[0]   -0.155 bias    1.555\n",
      "iter 2001/1000000  loss         0.220359  avg_L1_norm_grad         0.000215  w[0]   -0.155 bias    1.555\n",
      "iter 2100/1000000  loss         0.219764  avg_L1_norm_grad         0.000203  w[0]   -0.160 bias    1.574\n",
      "iter 2101/1000000  loss         0.219758  avg_L1_norm_grad         0.000203  w[0]   -0.160 bias    1.574\n",
      "iter 2200/1000000  loss         0.219224  avg_L1_norm_grad         0.000193  w[0]   -0.164 bias    1.591\n",
      "iter 2201/1000000  loss         0.219219  avg_L1_norm_grad         0.000193  w[0]   -0.164 bias    1.592\n",
      "iter 2300/1000000  loss         0.218740  avg_L1_norm_grad         0.000183  w[0]   -0.168 bias    1.608\n",
      "iter 2301/1000000  loss         0.218735  avg_L1_norm_grad         0.000183  w[0]   -0.168 bias    1.608\n",
      "iter 2400/1000000  loss         0.218303  avg_L1_norm_grad         0.000174  w[0]   -0.172 bias    1.624\n",
      "iter 2401/1000000  loss         0.218299  avg_L1_norm_grad         0.000174  w[0]   -0.172 bias    1.625\n",
      "iter 2500/1000000  loss         0.217908  avg_L1_norm_grad         0.000166  w[0]   -0.175 bias    1.640\n",
      "iter 2501/1000000  loss         0.217905  avg_L1_norm_grad         0.000166  w[0]   -0.175 bias    1.640\n",
      "iter 2600/1000000  loss         0.217551  avg_L1_norm_grad         0.000158  w[0]   -0.179 bias    1.655\n",
      "iter 2601/1000000  loss         0.217548  avg_L1_norm_grad         0.000158  w[0]   -0.179 bias    1.655\n",
      "iter 2700/1000000  loss         0.217228  avg_L1_norm_grad         0.000150  w[0]   -0.182 bias    1.669\n",
      "iter 2701/1000000  loss         0.217225  avg_L1_norm_grad         0.000150  w[0]   -0.182 bias    1.669\n",
      "iter 2800/1000000  loss         0.216934  avg_L1_norm_grad         0.000143  w[0]   -0.185 bias    1.682\n",
      "iter 2801/1000000  loss         0.216931  avg_L1_norm_grad         0.000143  w[0]   -0.185 bias    1.682\n",
      "iter 2900/1000000  loss         0.216667  avg_L1_norm_grad         0.000137  w[0]   -0.188 bias    1.695\n",
      "iter 2901/1000000  loss         0.216665  avg_L1_norm_grad         0.000137  w[0]   -0.188 bias    1.695\n",
      "iter 3000/1000000  loss         0.216424  avg_L1_norm_grad         0.000131  w[0]   -0.191 bias    1.707\n",
      "iter 3001/1000000  loss         0.216421  avg_L1_norm_grad         0.000131  w[0]   -0.191 bias    1.707\n",
      "iter 3100/1000000  loss         0.216202  avg_L1_norm_grad         0.000125  w[0]   -0.193 bias    1.719\n",
      "iter 3101/1000000  loss         0.216200  avg_L1_norm_grad         0.000125  w[0]   -0.193 bias    1.719\n",
      "iter 3200/1000000  loss         0.215999  avg_L1_norm_grad         0.000120  w[0]   -0.196 bias    1.731\n",
      "iter 3201/1000000  loss         0.215997  avg_L1_norm_grad         0.000120  w[0]   -0.196 bias    1.731\n",
      "iter 3300/1000000  loss         0.215814  avg_L1_norm_grad         0.000114  w[0]   -0.198 bias    1.741\n",
      "iter 3301/1000000  loss         0.215812  avg_L1_norm_grad         0.000114  w[0]   -0.198 bias    1.742\n",
      "iter 3400/1000000  loss         0.215645  avg_L1_norm_grad         0.000110  w[0]   -0.200 bias    1.752\n",
      "iter 3401/1000000  loss         0.215643  avg_L1_norm_grad         0.000110  w[0]   -0.200 bias    1.752\n",
      "iter 3500/1000000  loss         0.215489  avg_L1_norm_grad         0.000105  w[0]   -0.202 bias    1.762\n",
      "iter 3501/1000000  loss         0.215488  avg_L1_norm_grad         0.000105  w[0]   -0.202 bias    1.762\n",
      "iter 3600/1000000  loss         0.215347  avg_L1_norm_grad         0.000101  w[0]   -0.204 bias    1.772\n",
      "iter 3601/1000000  loss         0.215346  avg_L1_norm_grad         0.000100  w[0]   -0.204 bias    1.772\n",
      "iter 3700/1000000  loss         0.215216  avg_L1_norm_grad         0.000096  w[0]   -0.206 bias    1.781\n",
      "iter 3701/1000000  loss         0.215215  avg_L1_norm_grad         0.000096  w[0]   -0.206 bias    1.781\n",
      "iter 3800/1000000  loss         0.215096  avg_L1_norm_grad         0.000092  w[0]   -0.208 bias    1.790\n",
      "iter 3801/1000000  loss         0.215095  avg_L1_norm_grad         0.000092  w[0]   -0.208 bias    1.790\n",
      "iter 3900/1000000  loss         0.214986  avg_L1_norm_grad         0.000089  w[0]   -0.210 bias    1.799\n",
      "iter 3901/1000000  loss         0.214985  avg_L1_norm_grad         0.000089  w[0]   -0.210 bias    1.799\n",
      "iter 4000/1000000  loss         0.214884  avg_L1_norm_grad         0.000085  w[0]   -0.211 bias    1.807\n",
      "iter 4001/1000000  loss         0.214883  avg_L1_norm_grad         0.000085  w[0]   -0.211 bias    1.807\n",
      "iter 4100/1000000  loss         0.214791  avg_L1_norm_grad         0.000082  w[0]   -0.213 bias    1.815\n",
      "iter 4101/1000000  loss         0.214790  avg_L1_norm_grad         0.000082  w[0]   -0.213 bias    1.815\n",
      "iter 4200/1000000  loss         0.214705  avg_L1_norm_grad         0.000079  w[0]   -0.214 bias    1.823\n",
      "iter 4201/1000000  loss         0.214704  avg_L1_norm_grad         0.000079  w[0]   -0.214 bias    1.823\n",
      "iter 4300/1000000  loss         0.214625  avg_L1_norm_grad         0.000075  w[0]   -0.216 bias    1.830\n",
      "iter 4301/1000000  loss         0.214625  avg_L1_norm_grad         0.000075  w[0]   -0.216 bias    1.830\n",
      "iter 4400/1000000  loss         0.214552  avg_L1_norm_grad         0.000073  w[0]   -0.217 bias    1.837\n",
      "iter 4401/1000000  loss         0.214551  avg_L1_norm_grad         0.000073  w[0]   -0.217 bias    1.837\n",
      "iter 4500/1000000  loss         0.214484  avg_L1_norm_grad         0.000070  w[0]   -0.218 bias    1.844\n",
      "iter 4501/1000000  loss         0.214484  avg_L1_norm_grad         0.000070  w[0]   -0.218 bias    1.844\n",
      "iter 4600/1000000  loss         0.214422  avg_L1_norm_grad         0.000067  w[0]   -0.220 bias    1.851\n",
      "iter 4601/1000000  loss         0.214421  avg_L1_norm_grad         0.000067  w[0]   -0.220 bias    1.851\n",
      "iter 4700/1000000  loss         0.214364  avg_L1_norm_grad         0.000065  w[0]   -0.221 bias    1.857\n",
      "iter 4701/1000000  loss         0.214364  avg_L1_norm_grad         0.000065  w[0]   -0.221 bias    1.857\n",
      "iter 4800/1000000  loss         0.214311  avg_L1_norm_grad         0.000062  w[0]   -0.222 bias    1.863\n",
      "iter 4801/1000000  loss         0.214310  avg_L1_norm_grad         0.000062  w[0]   -0.222 bias    1.863\n",
      "iter 4900/1000000  loss         0.214261  avg_L1_norm_grad         0.000060  w[0]   -0.223 bias    1.869\n",
      "iter 4901/1000000  loss         0.214261  avg_L1_norm_grad         0.000060  w[0]   -0.223 bias    1.869\n",
      "iter 5000/1000000  loss         0.214216  avg_L1_norm_grad         0.000058  w[0]   -0.224 bias    1.875\n",
      "iter 5001/1000000  loss         0.214215  avg_L1_norm_grad         0.000057  w[0]   -0.224 bias    1.875\n",
      "iter 5100/1000000  loss         0.214173  avg_L1_norm_grad         0.000055  w[0]   -0.225 bias    1.881\n",
      "iter 5101/1000000  loss         0.214173  avg_L1_norm_grad         0.000055  w[0]   -0.225 bias    1.881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5200/1000000  loss         0.214134  avg_L1_norm_grad         0.000053  w[0]   -0.226 bias    1.886\n",
      "iter 5201/1000000  loss         0.214134  avg_L1_norm_grad         0.000053  w[0]   -0.226 bias    1.886\n",
      "iter 5300/1000000  loss         0.214098  avg_L1_norm_grad         0.000051  w[0]   -0.227 bias    1.891\n",
      "iter 5301/1000000  loss         0.214097  avg_L1_norm_grad         0.000051  w[0]   -0.227 bias    1.891\n",
      "iter 5400/1000000  loss         0.214064  avg_L1_norm_grad         0.000049  w[0]   -0.228 bias    1.896\n",
      "iter 5401/1000000  loss         0.214064  avg_L1_norm_grad         0.000049  w[0]   -0.228 bias    1.896\n",
      "iter 5500/1000000  loss         0.214033  avg_L1_norm_grad         0.000048  w[0]   -0.228 bias    1.901\n",
      "iter 5501/1000000  loss         0.214032  avg_L1_norm_grad         0.000048  w[0]   -0.228 bias    1.901\n",
      "iter 5600/1000000  loss         0.214004  avg_L1_norm_grad         0.000046  w[0]   -0.229 bias    1.906\n",
      "iter 5601/1000000  loss         0.214003  avg_L1_norm_grad         0.000046  w[0]   -0.229 bias    1.906\n",
      "iter 5700/1000000  loss         0.213977  avg_L1_norm_grad         0.000044  w[0]   -0.230 bias    1.910\n",
      "iter 5701/1000000  loss         0.213976  avg_L1_norm_grad         0.000044  w[0]   -0.230 bias    1.910\n",
      "iter 5800/1000000  loss         0.213952  avg_L1_norm_grad         0.000043  w[0]   -0.230 bias    1.914\n",
      "iter 5801/1000000  loss         0.213951  avg_L1_norm_grad         0.000043  w[0]   -0.230 bias    1.915\n",
      "iter 5900/1000000  loss         0.213928  avg_L1_norm_grad         0.000041  w[0]   -0.231 bias    1.919\n",
      "iter 5901/1000000  loss         0.213928  avg_L1_norm_grad         0.000041  w[0]   -0.231 bias    1.919\n",
      "iter 6000/1000000  loss         0.213907  avg_L1_norm_grad         0.000040  w[0]   -0.232 bias    1.923\n",
      "iter 6001/1000000  loss         0.213907  avg_L1_norm_grad         0.000040  w[0]   -0.232 bias    1.923\n",
      "iter 6100/1000000  loss         0.213887  avg_L1_norm_grad         0.000038  w[0]   -0.232 bias    1.927\n",
      "iter 6101/1000000  loss         0.213887  avg_L1_norm_grad         0.000038  w[0]   -0.232 bias    1.927\n",
      "iter 6200/1000000  loss         0.213868  avg_L1_norm_grad         0.000037  w[0]   -0.233 bias    1.930\n",
      "iter 6201/1000000  loss         0.213868  avg_L1_norm_grad         0.000037  w[0]   -0.233 bias    1.930\n",
      "iter 6300/1000000  loss         0.213851  avg_L1_norm_grad         0.000036  w[0]   -0.234 bias    1.934\n",
      "iter 6301/1000000  loss         0.213851  avg_L1_norm_grad         0.000036  w[0]   -0.234 bias    1.934\n",
      "iter 6400/1000000  loss         0.213835  avg_L1_norm_grad         0.000034  w[0]   -0.234 bias    1.938\n",
      "iter 6401/1000000  loss         0.213834  avg_L1_norm_grad         0.000034  w[0]   -0.234 bias    1.938\n",
      "iter 6500/1000000  loss         0.213819  avg_L1_norm_grad         0.000033  w[0]   -0.235 bias    1.941\n",
      "iter 6501/1000000  loss         0.213819  avg_L1_norm_grad         0.000033  w[0]   -0.235 bias    1.941\n",
      "iter 6600/1000000  loss         0.213805  avg_L1_norm_grad         0.000032  w[0]   -0.235 bias    1.944\n",
      "iter 6601/1000000  loss         0.213805  avg_L1_norm_grad         0.000032  w[0]   -0.235 bias    1.944\n",
      "iter 6700/1000000  loss         0.213792  avg_L1_norm_grad         0.000031  w[0]   -0.236 bias    1.947\n",
      "iter 6701/1000000  loss         0.213792  avg_L1_norm_grad         0.000031  w[0]   -0.236 bias    1.947\n",
      "iter 6800/1000000  loss         0.213780  avg_L1_norm_grad         0.000030  w[0]   -0.236 bias    1.950\n",
      "iter 6801/1000000  loss         0.213780  avg_L1_norm_grad         0.000030  w[0]   -0.236 bias    1.951\n",
      "iter 6900/1000000  loss         0.213769  avg_L1_norm_grad         0.000029  w[0]   -0.236 bias    1.953\n",
      "iter 6901/1000000  loss         0.213769  avg_L1_norm_grad         0.000029  w[0]   -0.236 bias    1.953\n",
      "iter 7000/1000000  loss         0.213758  avg_L1_norm_grad         0.000028  w[0]   -0.237 bias    1.956\n",
      "iter 7001/1000000  loss         0.213758  avg_L1_norm_grad         0.000028  w[0]   -0.237 bias    1.956\n",
      "iter 7100/1000000  loss         0.213749  avg_L1_norm_grad         0.000027  w[0]   -0.237 bias    1.959\n",
      "iter 7101/1000000  loss         0.213749  avg_L1_norm_grad         0.000027  w[0]   -0.237 bias    1.959\n",
      "iter 7200/1000000  loss         0.213739  avg_L1_norm_grad         0.000026  w[0]   -0.238 bias    1.962\n",
      "iter 7201/1000000  loss         0.213739  avg_L1_norm_grad         0.000026  w[0]   -0.238 bias    1.962\n",
      "iter 7300/1000000  loss         0.213731  avg_L1_norm_grad         0.000025  w[0]   -0.238 bias    1.964\n",
      "iter 7301/1000000  loss         0.213731  avg_L1_norm_grad         0.000025  w[0]   -0.238 bias    1.964\n",
      "iter 7400/1000000  loss         0.213723  avg_L1_norm_grad         0.000024  w[0]   -0.238 bias    1.967\n",
      "iter 7401/1000000  loss         0.213723  avg_L1_norm_grad         0.000024  w[0]   -0.238 bias    1.967\n",
      "iter 7500/1000000  loss         0.213716  avg_L1_norm_grad         0.000023  w[0]   -0.239 bias    1.969\n",
      "iter 7501/1000000  loss         0.213715  avg_L1_norm_grad         0.000023  w[0]   -0.239 bias    1.969\n",
      "iter 7600/1000000  loss         0.213709  avg_L1_norm_grad         0.000022  w[0]   -0.239 bias    1.972\n",
      "iter 7601/1000000  loss         0.213709  avg_L1_norm_grad         0.000022  w[0]   -0.239 bias    1.972\n",
      "iter 7700/1000000  loss         0.213702  avg_L1_norm_grad         0.000022  w[0]   -0.239 bias    1.974\n",
      "iter 7701/1000000  loss         0.213702  avg_L1_norm_grad         0.000022  w[0]   -0.239 bias    1.974\n",
      "iter 7800/1000000  loss         0.213696  avg_L1_norm_grad         0.000021  w[0]   -0.240 bias    1.976\n",
      "iter 7801/1000000  loss         0.213696  avg_L1_norm_grad         0.000021  w[0]   -0.240 bias    1.976\n",
      "iter 7900/1000000  loss         0.213690  avg_L1_norm_grad         0.000020  w[0]   -0.240 bias    1.978\n",
      "iter 7901/1000000  loss         0.213690  avg_L1_norm_grad         0.000020  w[0]   -0.240 bias    1.978\n",
      "iter 8000/1000000  loss         0.213685  avg_L1_norm_grad         0.000020  w[0]   -0.240 bias    1.980\n",
      "iter 8001/1000000  loss         0.213685  avg_L1_norm_grad         0.000020  w[0]   -0.240 bias    1.980\n",
      "iter 8100/1000000  loss         0.213680  avg_L1_norm_grad         0.000019  w[0]   -0.240 bias    1.982\n",
      "iter 8101/1000000  loss         0.213680  avg_L1_norm_grad         0.000019  w[0]   -0.240 bias    1.982\n",
      "iter 8200/1000000  loss         0.213676  avg_L1_norm_grad         0.000018  w[0]   -0.241 bias    1.984\n",
      "iter 8201/1000000  loss         0.213676  avg_L1_norm_grad         0.000018  w[0]   -0.241 bias    1.984\n",
      "iter 8300/1000000  loss         0.213671  avg_L1_norm_grad         0.000018  w[0]   -0.241 bias    1.986\n",
      "iter 8301/1000000  loss         0.213671  avg_L1_norm_grad         0.000018  w[0]   -0.241 bias    1.986\n",
      "iter 8400/1000000  loss         0.213667  avg_L1_norm_grad         0.000017  w[0]   -0.241 bias    1.988\n",
      "iter 8401/1000000  loss         0.213667  avg_L1_norm_grad         0.000017  w[0]   -0.241 bias    1.988\n",
      "iter 8500/1000000  loss         0.213664  avg_L1_norm_grad         0.000016  w[0]   -0.241 bias    1.990\n",
      "iter 8501/1000000  loss         0.213664  avg_L1_norm_grad         0.000016  w[0]   -0.241 bias    1.990\n",
      "iter 8600/1000000  loss         0.213660  avg_L1_norm_grad         0.000016  w[0]   -0.242 bias    1.991\n",
      "iter 8601/1000000  loss         0.213660  avg_L1_norm_grad         0.000016  w[0]   -0.242 bias    1.991\n",
      "iter 8700/1000000  loss         0.213657  avg_L1_norm_grad         0.000015  w[0]   -0.242 bias    1.993\n",
      "iter 8701/1000000  loss         0.213657  avg_L1_norm_grad         0.000015  w[0]   -0.242 bias    1.993\n",
      "iter 8800/1000000  loss         0.213654  avg_L1_norm_grad         0.000015  w[0]   -0.242 bias    1.995\n",
      "iter 8801/1000000  loss         0.213654  avg_L1_norm_grad         0.000015  w[0]   -0.242 bias    1.995\n",
      "iter 8900/1000000  loss         0.213651  avg_L1_norm_grad         0.000014  w[0]   -0.242 bias    1.996\n",
      "iter 8901/1000000  loss         0.213651  avg_L1_norm_grad         0.000014  w[0]   -0.242 bias    1.996\n",
      "iter 9000/1000000  loss         0.213648  avg_L1_norm_grad         0.000014  w[0]   -0.242 bias    1.998\n",
      "iter 9001/1000000  loss         0.213648  avg_L1_norm_grad         0.000014  w[0]   -0.242 bias    1.998\n",
      "iter 9100/1000000  loss         0.213646  avg_L1_norm_grad         0.000013  w[0]   -0.243 bias    1.999\n",
      "iter 9101/1000000  loss         0.213646  avg_L1_norm_grad         0.000013  w[0]   -0.243 bias    1.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9200/1000000  loss         0.213643  avg_L1_norm_grad         0.000013  w[0]   -0.243 bias    2.000\n",
      "iter 9201/1000000  loss         0.213643  avg_L1_norm_grad         0.000013  w[0]   -0.243 bias    2.000\n",
      "iter 9300/1000000  loss         0.213641  avg_L1_norm_grad         0.000013  w[0]   -0.243 bias    2.002\n",
      "iter 9301/1000000  loss         0.213641  avg_L1_norm_grad         0.000013  w[0]   -0.243 bias    2.002\n",
      "iter 9400/1000000  loss         0.213639  avg_L1_norm_grad         0.000012  w[0]   -0.243 bias    2.003\n",
      "iter 9401/1000000  loss         0.213639  avg_L1_norm_grad         0.000012  w[0]   -0.243 bias    2.003\n",
      "iter 9500/1000000  loss         0.213637  avg_L1_norm_grad         0.000012  w[0]   -0.243 bias    2.004\n",
      "iter 9501/1000000  loss         0.213637  avg_L1_norm_grad         0.000012  w[0]   -0.243 bias    2.004\n",
      "iter 9600/1000000  loss         0.213635  avg_L1_norm_grad         0.000011  w[0]   -0.243 bias    2.006\n",
      "iter 9601/1000000  loss         0.213635  avg_L1_norm_grad         0.000011  w[0]   -0.243 bias    2.006\n",
      "iter 9700/1000000  loss         0.213634  avg_L1_norm_grad         0.000011  w[0]   -0.244 bias    2.007\n",
      "iter 9701/1000000  loss         0.213634  avg_L1_norm_grad         0.000011  w[0]   -0.244 bias    2.007\n",
      "iter 9800/1000000  loss         0.213632  avg_L1_norm_grad         0.000011  w[0]   -0.244 bias    2.008\n",
      "iter 9801/1000000  loss         0.213632  avg_L1_norm_grad         0.000011  w[0]   -0.244 bias    2.008\n",
      "iter 9900/1000000  loss         0.213631  avg_L1_norm_grad         0.000010  w[0]   -0.244 bias    2.009\n",
      "iter 9901/1000000  loss         0.213631  avg_L1_norm_grad         0.000010  w[0]   -0.244 bias    2.009\n",
      "iter 10000/1000000  loss         0.213629  avg_L1_norm_grad         0.000010  w[0]   -0.244 bias    2.010\n",
      "iter 10001/1000000  loss         0.213629  avg_L1_norm_grad         0.000010  w[0]   -0.244 bias    2.010\n",
      "iter 10100/1000000  loss         0.213628  avg_L1_norm_grad         0.000010  w[0]   -0.244 bias    2.011\n",
      "iter 10101/1000000  loss         0.213628  avg_L1_norm_grad         0.000010  w[0]   -0.244 bias    2.011\n",
      "Done. Converged after 10199 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9363888888888628\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "No Noise New 0.9474999999999737\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.019625  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.954299  avg_L1_norm_grad         0.031159  w[0]   -0.001 bias   -0.000\n",
      "iter    2/1000000  loss         0.929468  avg_L1_norm_grad         0.036089  w[0]    0.001 bias    0.012\n",
      "iter    3/1000000  loss         0.948132  avg_L1_norm_grad         0.073394  w[0]   -0.001 bias    0.002\n",
      "iter    4/1000000  loss         1.039204  avg_L1_norm_grad         0.101841  w[0]    0.004 bias    0.029\n",
      "iter    5/1000000  loss         1.244739  avg_L1_norm_grad         0.140360  w[0]   -0.003 bias   -0.003\n",
      "iter    6/1000000  loss         1.307538  avg_L1_norm_grad         0.150477  w[0]    0.007 bias    0.047\n",
      "iter    7/1000000  loss         1.440737  avg_L1_norm_grad         0.159546  w[0]   -0.004 bias   -0.002\n",
      "iter    8/1000000  loss         1.219946  avg_L1_norm_grad         0.145177  w[0]    0.007 bias    0.054\n",
      "iter    9/1000000  loss         1.315658  avg_L1_norm_grad         0.150910  w[0]   -0.003 bias    0.007\n",
      "iter   10/1000000  loss         1.112610  avg_L1_norm_grad         0.135413  w[0]    0.008 bias    0.061\n",
      "iter   11/1000000  loss         1.181272  avg_L1_norm_grad         0.139175  w[0]   -0.002 bias    0.017\n",
      "iter   12/1000000  loss         1.009012  avg_L1_norm_grad         0.123527  w[0]    0.008 bias    0.066\n",
      "iter   13/1000000  loss         1.052619  avg_L1_norm_grad         0.125426  w[0]   -0.001 bias    0.027\n",
      "iter   14/1000000  loss         0.912538  avg_L1_norm_grad         0.109983  w[0]    0.008 bias    0.071\n",
      "iter   15/1000000  loss         0.935125  avg_L1_norm_grad         0.110322  w[0]    0.001 bias    0.036\n",
      "iter   16/1000000  loss         0.826803  avg_L1_norm_grad         0.095513  w[0]    0.008 bias    0.076\n",
      "iter   17/1000000  loss         0.833558  avg_L1_norm_grad         0.094675  w[0]    0.002 bias    0.045\n",
      "iter   18/1000000  loss         0.753955  avg_L1_norm_grad         0.080868  w[0]    0.008 bias    0.079\n",
      "iter   19/1000000  loss         0.750376  avg_L1_norm_grad         0.079247  w[0]    0.003 bias    0.054\n",
      "iter  100/1000000  loss         0.435653  avg_L1_norm_grad         0.002246  w[0]    0.007 bias    0.190\n",
      "iter  101/1000000  loss         0.434876  avg_L1_norm_grad         0.002229  w[0]    0.007 bias    0.191\n",
      "iter  200/1000000  loss         0.389300  avg_L1_norm_grad         0.001340  w[0]    0.002 bias    0.265\n",
      "iter  201/1000000  loss         0.389022  avg_L1_norm_grad         0.001335  w[0]    0.002 bias    0.266\n",
      "iter  300/1000000  loss         0.369226  avg_L1_norm_grad         0.000982  w[0]   -0.004 bias    0.318\n",
      "iter  301/1000000  loss         0.369082  avg_L1_norm_grad         0.000979  w[0]   -0.004 bias    0.318\n",
      "iter  400/1000000  loss         0.357895  avg_L1_norm_grad         0.000772  w[0]   -0.009 bias    0.362\n",
      "iter  401/1000000  loss         0.357806  avg_L1_norm_grad         0.000770  w[0]   -0.009 bias    0.362\n",
      "iter  500/1000000  loss         0.350680  avg_L1_norm_grad         0.000631  w[0]   -0.013 bias    0.401\n",
      "iter  501/1000000  loss         0.350621  avg_L1_norm_grad         0.000630  w[0]   -0.013 bias    0.401\n",
      "iter  600/1000000  loss         0.345757  avg_L1_norm_grad         0.000528  w[0]   -0.016 bias    0.437\n",
      "iter  601/1000000  loss         0.345716  avg_L1_norm_grad         0.000527  w[0]   -0.016 bias    0.437\n",
      "iter  700/1000000  loss         0.342242  avg_L1_norm_grad         0.000448  w[0]   -0.019 bias    0.470\n",
      "iter  701/1000000  loss         0.342213  avg_L1_norm_grad         0.000448  w[0]   -0.019 bias    0.471\n",
      "iter  800/1000000  loss         0.339650  avg_L1_norm_grad         0.000387  w[0]   -0.021 bias    0.502\n",
      "iter  801/1000000  loss         0.339627  avg_L1_norm_grad         0.000386  w[0]   -0.021 bias    0.502\n",
      "iter  900/1000000  loss         0.337689  avg_L1_norm_grad         0.000337  w[0]   -0.023 bias    0.532\n",
      "iter  901/1000000  loss         0.337672  avg_L1_norm_grad         0.000336  w[0]   -0.023 bias    0.532\n",
      "iter 1000/1000000  loss         0.336176  avg_L1_norm_grad         0.000296  w[0]   -0.025 bias    0.561\n",
      "iter 1001/1000000  loss         0.336163  avg_L1_norm_grad         0.000295  w[0]   -0.025 bias    0.561\n",
      "iter 1100/1000000  loss         0.334990  avg_L1_norm_grad         0.000262  w[0]   -0.026 bias    0.588\n",
      "iter 1101/1000000  loss         0.334979  avg_L1_norm_grad         0.000261  w[0]   -0.026 bias    0.589\n",
      "iter 1200/1000000  loss         0.334047  avg_L1_norm_grad         0.000233  w[0]   -0.027 bias    0.615\n",
      "iter 1201/1000000  loss         0.334038  avg_L1_norm_grad         0.000233  w[0]   -0.027 bias    0.615\n",
      "iter 1300/1000000  loss         0.333288  avg_L1_norm_grad         0.000208  w[0]   -0.028 bias    0.640\n",
      "iter 1301/1000000  loss         0.333282  avg_L1_norm_grad         0.000208  w[0]   -0.028 bias    0.641\n",
      "iter 1400/1000000  loss         0.332672  avg_L1_norm_grad         0.000187  w[0]   -0.029 bias    0.665\n",
      "iter 1401/1000000  loss         0.332667  avg_L1_norm_grad         0.000187  w[0]   -0.029 bias    0.665\n",
      "iter 1500/1000000  loss         0.332167  avg_L1_norm_grad         0.000168  w[0]   -0.030 bias    0.689\n",
      "iter 1501/1000000  loss         0.332162  avg_L1_norm_grad         0.000168  w[0]   -0.030 bias    0.689\n",
      "iter 1600/1000000  loss         0.331749  avg_L1_norm_grad         0.000152  w[0]   -0.030 bias    0.712\n",
      "iter 1601/1000000  loss         0.331746  avg_L1_norm_grad         0.000152  w[0]   -0.030 bias    0.712\n",
      "iter 1700/1000000  loss         0.331401  avg_L1_norm_grad         0.000138  w[0]   -0.031 bias    0.734\n",
      "iter 1701/1000000  loss         0.331398  avg_L1_norm_grad         0.000138  w[0]   -0.031 bias    0.734\n",
      "iter 1800/1000000  loss         0.331110  avg_L1_norm_grad         0.000125  w[0]   -0.031 bias    0.755\n",
      "iter 1801/1000000  loss         0.331107  avg_L1_norm_grad         0.000125  w[0]   -0.031 bias    0.756\n",
      "iter 1900/1000000  loss         0.330863  avg_L1_norm_grad         0.000114  w[0]   -0.032 bias    0.776\n",
      "iter 1901/1000000  loss         0.330861  avg_L1_norm_grad         0.000114  w[0]   -0.032 bias    0.776\n",
      "iter 2000/1000000  loss         0.330654  avg_L1_norm_grad         0.000104  w[0]   -0.032 bias    0.796\n",
      "iter 2001/1000000  loss         0.330652  avg_L1_norm_grad         0.000104  w[0]   -0.032 bias    0.797\n",
      "iter 2100/1000000  loss         0.330476  avg_L1_norm_grad         0.000095  w[0]   -0.032 bias    0.816\n",
      "iter 2101/1000000  loss         0.330474  avg_L1_norm_grad         0.000095  w[0]   -0.032 bias    0.816\n",
      "iter 2200/1000000  loss         0.330322  avg_L1_norm_grad         0.000087  w[0]   -0.032 bias    0.835\n",
      "iter 2201/1000000  loss         0.330321  avg_L1_norm_grad         0.000087  w[0]   -0.032 bias    0.835\n",
      "iter 2300/1000000  loss         0.330190  avg_L1_norm_grad         0.000079  w[0]   -0.033 bias    0.854\n",
      "iter 2301/1000000  loss         0.330189  avg_L1_norm_grad         0.000079  w[0]   -0.033 bias    0.854\n",
      "iter 2400/1000000  loss         0.330075  avg_L1_norm_grad         0.000073  w[0]   -0.033 bias    0.872\n",
      "iter 2401/1000000  loss         0.330074  avg_L1_norm_grad         0.000073  w[0]   -0.033 bias    0.872\n",
      "iter 2500/1000000  loss         0.329975  avg_L1_norm_grad         0.000067  w[0]   -0.033 bias    0.889\n",
      "iter 2501/1000000  loss         0.329975  avg_L1_norm_grad         0.000067  w[0]   -0.033 bias    0.889\n",
      "iter 2600/1000000  loss         0.329888  avg_L1_norm_grad         0.000062  w[0]   -0.033 bias    0.906\n",
      "iter 2601/1000000  loss         0.329887  avg_L1_norm_grad         0.000062  w[0]   -0.033 bias    0.906\n",
      "iter 2700/1000000  loss         0.329811  avg_L1_norm_grad         0.000057  w[0]   -0.033 bias    0.922\n",
      "iter 2701/1000000  loss         0.329810  avg_L1_norm_grad         0.000057  w[0]   -0.033 bias    0.923\n",
      "iter 2800/1000000  loss         0.329743  avg_L1_norm_grad         0.000052  w[0]   -0.033 bias    0.938\n",
      "iter 2801/1000000  loss         0.329742  avg_L1_norm_grad         0.000052  w[0]   -0.033 bias    0.939\n",
      "iter 2900/1000000  loss         0.329683  avg_L1_norm_grad         0.000048  w[0]   -0.033 bias    0.954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.329682  avg_L1_norm_grad         0.000048  w[0]   -0.033 bias    0.954\n",
      "iter 3000/1000000  loss         0.329629  avg_L1_norm_grad         0.000045  w[0]   -0.034 bias    0.969\n",
      "iter 3001/1000000  loss         0.329629  avg_L1_norm_grad         0.000045  w[0]   -0.034 bias    0.969\n",
      "iter 3100/1000000  loss         0.329581  avg_L1_norm_grad         0.000041  w[0]   -0.034 bias    0.984\n",
      "iter 3101/1000000  loss         0.329581  avg_L1_norm_grad         0.000041  w[0]   -0.034 bias    0.984\n",
      "iter 3200/1000000  loss         0.329538  avg_L1_norm_grad         0.000038  w[0]   -0.034 bias    0.998\n",
      "iter 3201/1000000  loss         0.329538  avg_L1_norm_grad         0.000038  w[0]   -0.034 bias    0.998\n",
      "iter 3300/1000000  loss         0.329499  avg_L1_norm_grad         0.000036  w[0]   -0.034 bias    1.012\n",
      "iter 3301/1000000  loss         0.329499  avg_L1_norm_grad         0.000036  w[0]   -0.034 bias    1.012\n",
      "iter 3400/1000000  loss         0.329465  avg_L1_norm_grad         0.000033  w[0]   -0.034 bias    1.026\n",
      "iter 3401/1000000  loss         0.329464  avg_L1_norm_grad         0.000033  w[0]   -0.034 bias    1.026\n",
      "iter 3500/1000000  loss         0.329433  avg_L1_norm_grad         0.000031  w[0]   -0.034 bias    1.039\n",
      "iter 3501/1000000  loss         0.329433  avg_L1_norm_grad         0.000031  w[0]   -0.034 bias    1.039\n",
      "iter 3600/1000000  loss         0.329404  avg_L1_norm_grad         0.000029  w[0]   -0.034 bias    1.052\n",
      "iter 3601/1000000  loss         0.329404  avg_L1_norm_grad         0.000029  w[0]   -0.034 bias    1.052\n",
      "iter 3700/1000000  loss         0.329378  avg_L1_norm_grad         0.000027  w[0]   -0.034 bias    1.065\n",
      "iter 3701/1000000  loss         0.329377  avg_L1_norm_grad         0.000027  w[0]   -0.034 bias    1.065\n",
      "iter 3800/1000000  loss         0.329354  avg_L1_norm_grad         0.000025  w[0]   -0.034 bias    1.077\n",
      "iter 3801/1000000  loss         0.329353  avg_L1_norm_grad         0.000025  w[0]   -0.034 bias    1.077\n",
      "iter 3900/1000000  loss         0.329332  avg_L1_norm_grad         0.000023  w[0]   -0.034 bias    1.089\n",
      "iter 3901/1000000  loss         0.329331  avg_L1_norm_grad         0.000023  w[0]   -0.034 bias    1.089\n",
      "iter 4000/1000000  loss         0.329311  avg_L1_norm_grad         0.000022  w[0]   -0.034 bias    1.100\n",
      "iter 4001/1000000  loss         0.329311  avg_L1_norm_grad         0.000022  w[0]   -0.034 bias    1.100\n",
      "iter 4100/1000000  loss         0.329293  avg_L1_norm_grad         0.000020  w[0]   -0.034 bias    1.112\n",
      "iter 4101/1000000  loss         0.329292  avg_L1_norm_grad         0.000020  w[0]   -0.034 bias    1.112\n",
      "iter 4200/1000000  loss         0.329275  avg_L1_norm_grad         0.000019  w[0]   -0.034 bias    1.123\n",
      "iter 4201/1000000  loss         0.329275  avg_L1_norm_grad         0.000019  w[0]   -0.034 bias    1.123\n",
      "iter 4300/1000000  loss         0.329259  avg_L1_norm_grad         0.000018  w[0]   -0.034 bias    1.133\n",
      "iter 4301/1000000  loss         0.329259  avg_L1_norm_grad         0.000018  w[0]   -0.034 bias    1.134\n",
      "iter 4400/1000000  loss         0.329245  avg_L1_norm_grad         0.000017  w[0]   -0.034 bias    1.144\n",
      "iter 4401/1000000  loss         0.329244  avg_L1_norm_grad         0.000017  w[0]   -0.034 bias    1.144\n",
      "iter 4500/1000000  loss         0.329231  avg_L1_norm_grad         0.000016  w[0]   -0.034 bias    1.154\n",
      "iter 4501/1000000  loss         0.329231  avg_L1_norm_grad         0.000016  w[0]   -0.034 bias    1.154\n",
      "iter 4600/1000000  loss         0.329218  avg_L1_norm_grad         0.000015  w[0]   -0.034 bias    1.164\n",
      "iter 4601/1000000  loss         0.329218  avg_L1_norm_grad         0.000015  w[0]   -0.034 bias    1.164\n",
      "iter 4700/1000000  loss         0.329206  avg_L1_norm_grad         0.000014  w[0]   -0.034 bias    1.174\n",
      "iter 4701/1000000  loss         0.329206  avg_L1_norm_grad         0.000014  w[0]   -0.034 bias    1.174\n",
      "iter 4800/1000000  loss         0.329195  avg_L1_norm_grad         0.000013  w[0]   -0.035 bias    1.183\n",
      "iter 4801/1000000  loss         0.329195  avg_L1_norm_grad         0.000013  w[0]   -0.035 bias    1.183\n",
      "iter 4900/1000000  loss         0.329184  avg_L1_norm_grad         0.000012  w[0]   -0.035 bias    1.192\n",
      "iter 4901/1000000  loss         0.329184  avg_L1_norm_grad         0.000012  w[0]   -0.035 bias    1.192\n",
      "iter 5000/1000000  loss         0.329174  avg_L1_norm_grad         0.000012  w[0]   -0.035 bias    1.201\n",
      "iter 5001/1000000  loss         0.329174  avg_L1_norm_grad         0.000012  w[0]   -0.035 bias    1.201\n",
      "iter 5100/1000000  loss         0.329165  avg_L1_norm_grad         0.000011  w[0]   -0.035 bias    1.210\n",
      "iter 5101/1000000  loss         0.329165  avg_L1_norm_grad         0.000011  w[0]   -0.035 bias    1.210\n",
      "iter 5200/1000000  loss         0.329157  avg_L1_norm_grad         0.000011  w[0]   -0.035 bias    1.219\n",
      "iter 5201/1000000  loss         0.329157  avg_L1_norm_grad         0.000011  w[0]   -0.035 bias    1.219\n",
      "iter 5300/1000000  loss         0.329149  avg_L1_norm_grad         0.000010  w[0]   -0.035 bias    1.227\n",
      "iter 5301/1000000  loss         0.329148  avg_L1_norm_grad         0.000010  w[0]   -0.035 bias    1.227\n",
      "iter 5400/1000000  loss         0.329141  avg_L1_norm_grad         0.000010  w[0]   -0.035 bias    1.235\n",
      "iter 5401/1000000  loss         0.329141  avg_L1_norm_grad         0.000010  w[0]   -0.035 bias    1.235\n",
      "iter 5500/1000000  loss         0.329134  avg_L1_norm_grad         0.000009  w[0]   -0.035 bias    1.243\n",
      "iter 5501/1000000  loss         0.329134  avg_L1_norm_grad         0.000009  w[0]   -0.035 bias    1.243\n",
      "iter 5600/1000000  loss         0.329127  avg_L1_norm_grad         0.000009  w[0]   -0.035 bias    1.251\n",
      "iter 5601/1000000  loss         0.329127  avg_L1_norm_grad         0.000009  w[0]   -0.035 bias    1.251\n",
      "iter 5700/1000000  loss         0.329121  avg_L1_norm_grad         0.000008  w[0]   -0.035 bias    1.258\n",
      "iter 5701/1000000  loss         0.329121  avg_L1_norm_grad         0.000008  w[0]   -0.035 bias    1.258\n",
      "iter 5800/1000000  loss         0.329115  avg_L1_norm_grad         0.000008  w[0]   -0.035 bias    1.265\n",
      "iter 5801/1000000  loss         0.329114  avg_L1_norm_grad         0.000008  w[0]   -0.035 bias    1.265\n",
      "iter 5900/1000000  loss         0.329109  avg_L1_norm_grad         0.000008  w[0]   -0.035 bias    1.272\n",
      "iter 5901/1000000  loss         0.329109  avg_L1_norm_grad         0.000008  w[0]   -0.035 bias    1.272\n",
      "iter 6000/1000000  loss         0.329103  avg_L1_norm_grad         0.000007  w[0]   -0.035 bias    1.279\n",
      "iter 6001/1000000  loss         0.329103  avg_L1_norm_grad         0.000007  w[0]   -0.035 bias    1.279\n",
      "iter 6100/1000000  loss         0.329098  avg_L1_norm_grad         0.000007  w[0]   -0.035 bias    1.286\n",
      "iter 6101/1000000  loss         0.329098  avg_L1_norm_grad         0.000007  w[0]   -0.035 bias    1.286\n",
      "iter 6200/1000000  loss         0.329094  avg_L1_norm_grad         0.000007  w[0]   -0.035 bias    1.293\n",
      "iter 6201/1000000  loss         0.329094  avg_L1_norm_grad         0.000007  w[0]   -0.035 bias    1.293\n",
      "iter 6300/1000000  loss         0.329089  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.299\n",
      "iter 6301/1000000  loss         0.329089  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.299\n",
      "iter 6400/1000000  loss         0.329085  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.305\n",
      "iter 6401/1000000  loss         0.329085  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.305\n",
      "iter 6500/1000000  loss         0.329081  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.312\n",
      "iter 6501/1000000  loss         0.329081  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.312\n",
      "iter 6600/1000000  loss         0.329077  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.318\n",
      "iter 6601/1000000  loss         0.329077  avg_L1_norm_grad         0.000006  w[0]   -0.035 bias    1.318\n",
      "iter 6700/1000000  loss         0.329073  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.323\n",
      "iter 6701/1000000  loss         0.329073  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.323\n",
      "iter 6800/1000000  loss         0.329070  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.329\n",
      "iter 6801/1000000  loss         0.329070  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.329066  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.335\n",
      "iter 6901/1000000  loss         0.329066  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.335\n",
      "iter 7000/1000000  loss         0.329063  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.340\n",
      "iter 7001/1000000  loss         0.329063  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.340\n",
      "iter 7100/1000000  loss         0.329060  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.345\n",
      "iter 7101/1000000  loss         0.329060  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.345\n",
      "iter 7200/1000000  loss         0.329057  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.350\n",
      "iter 7201/1000000  loss         0.329057  avg_L1_norm_grad         0.000005  w[0]   -0.035 bias    1.350\n",
      "iter 7300/1000000  loss         0.329055  avg_L1_norm_grad         0.000004  w[0]   -0.035 bias    1.355\n",
      "iter 7301/1000000  loss         0.329055  avg_L1_norm_grad         0.000004  w[0]   -0.035 bias    1.355\n",
      "iter 7400/1000000  loss         0.329052  avg_L1_norm_grad         0.000004  w[0]   -0.035 bias    1.360\n",
      "iter 7401/1000000  loss         0.329052  avg_L1_norm_grad         0.000004  w[0]   -0.035 bias    1.360\n",
      "iter 7500/1000000  loss         0.329050  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.365\n",
      "iter 7501/1000000  loss         0.329050  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.365\n",
      "iter 7600/1000000  loss         0.329048  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.370\n",
      "iter 7601/1000000  loss         0.329048  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.370\n",
      "iter 7700/1000000  loss         0.329045  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.374\n",
      "iter 7701/1000000  loss         0.329045  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.374\n",
      "iter 7800/1000000  loss         0.329043  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.378\n",
      "iter 7801/1000000  loss         0.329043  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.378\n",
      "iter 7900/1000000  loss         0.329041  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.383\n",
      "iter 7901/1000000  loss         0.329041  avg_L1_norm_grad         0.000004  w[0]   -0.036 bias    1.383\n",
      "iter 8000/1000000  loss         0.329040  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.387\n",
      "iter 8001/1000000  loss         0.329040  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.387\n",
      "iter 8100/1000000  loss         0.329038  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.391\n",
      "iter 8101/1000000  loss         0.329038  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.391\n",
      "iter 8200/1000000  loss         0.329036  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.395\n",
      "iter 8201/1000000  loss         0.329036  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.395\n",
      "iter 8300/1000000  loss         0.329035  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.399\n",
      "iter 8301/1000000  loss         0.329035  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.399\n",
      "iter 8400/1000000  loss         0.329033  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.403\n",
      "iter 8401/1000000  loss         0.329033  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.403\n",
      "iter 8500/1000000  loss         0.329032  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.406\n",
      "iter 8501/1000000  loss         0.329032  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.406\n",
      "iter 8600/1000000  loss         0.329030  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.410\n",
      "iter 8601/1000000  loss         0.329030  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.410\n",
      "iter 8700/1000000  loss         0.329029  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.413\n",
      "iter 8701/1000000  loss         0.329029  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.413\n",
      "iter 8800/1000000  loss         0.329028  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.417\n",
      "iter 8801/1000000  loss         0.329028  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.417\n",
      "iter 8900/1000000  loss         0.329027  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.420\n",
      "iter 8901/1000000  loss         0.329027  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.420\n",
      "iter 9000/1000000  loss         0.329025  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.423\n",
      "iter 9001/1000000  loss         0.329025  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.423\n",
      "iter 9100/1000000  loss         0.329024  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.427\n",
      "iter 9101/1000000  loss         0.329024  avg_L1_norm_grad         0.000003  w[0]   -0.036 bias    1.427\n",
      "iter 9200/1000000  loss         0.329023  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.430\n",
      "iter 9201/1000000  loss         0.329023  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.430\n",
      "iter 9300/1000000  loss         0.329022  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.433\n",
      "iter 9301/1000000  loss         0.329022  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.433\n",
      "iter 9400/1000000  loss         0.329021  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.436\n",
      "iter 9401/1000000  loss         0.329021  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.436\n",
      "iter 9500/1000000  loss         0.329021  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.439\n",
      "iter 9501/1000000  loss         0.329021  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.439\n",
      "iter 9600/1000000  loss         0.329020  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.441\n",
      "iter 9601/1000000  loss         0.329020  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.441\n",
      "iter 9700/1000000  loss         0.329019  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.444\n",
      "iter 9701/1000000  loss         0.329019  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.444\n",
      "iter 9800/1000000  loss         0.329018  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.447\n",
      "iter 9801/1000000  loss         0.329018  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.447\n",
      "iter 9900/1000000  loss         0.329018  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.449\n",
      "iter 9901/1000000  loss         0.329018  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.449\n",
      "iter 10000/1000000  loss         0.329017  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.452\n",
      "iter 10001/1000000  loss         0.329017  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.452\n",
      "iter 10100/1000000  loss         0.329016  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.454\n",
      "iter 10101/1000000  loss         0.329016  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.454\n",
      "iter 10200/1000000  loss         0.329016  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.457\n",
      "iter 10201/1000000  loss         0.329016  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.457\n",
      "iter 10300/1000000  loss         0.329015  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.459\n",
      "iter 10301/1000000  loss         0.329015  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.459\n",
      "iter 10400/1000000  loss         0.329015  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.461\n",
      "iter 10401/1000000  loss         0.329015  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.461\n",
      "iter 10500/1000000  loss         0.329014  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.464\n",
      "iter 10501/1000000  loss         0.329014  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.464\n",
      "iter 10600/1000000  loss         0.329014  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.466\n",
      "iter 10601/1000000  loss         0.329013  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.466\n",
      "iter 10700/1000000  loss         0.329013  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.468\n",
      "iter 10701/1000000  loss         0.329013  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.329013  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.470\n",
      "iter 10801/1000000  loss         0.329013  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.470\n",
      "iter 10900/1000000  loss         0.329012  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.472\n",
      "iter 10901/1000000  loss         0.329012  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.472\n",
      "iter 11000/1000000  loss         0.329012  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.474\n",
      "iter 11001/1000000  loss         0.329012  avg_L1_norm_grad         0.000002  w[0]   -0.036 bias    1.474\n",
      "iter 11100/1000000  loss         0.329011  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.476\n",
      "iter 11101/1000000  loss         0.329011  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.476\n",
      "iter 11200/1000000  loss         0.329011  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.478\n",
      "iter 11201/1000000  loss         0.329011  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.478\n",
      "iter 11300/1000000  loss         0.329011  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.480\n",
      "iter 11301/1000000  loss         0.329011  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.480\n",
      "iter 11400/1000000  loss         0.329010  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.481\n",
      "iter 11401/1000000  loss         0.329010  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.481\n",
      "iter 11500/1000000  loss         0.329010  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.483\n",
      "iter 11501/1000000  loss         0.329010  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.483\n",
      "iter 11600/1000000  loss         0.329010  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.485\n",
      "iter 11601/1000000  loss         0.329010  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.485\n",
      "iter 11700/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.486\n",
      "iter 11701/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.486\n",
      "iter 11800/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.488\n",
      "iter 11801/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.488\n",
      "iter 11900/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.490\n",
      "iter 11901/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.490\n",
      "iter 12000/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.491\n",
      "iter 12001/1000000  loss         0.329009  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.491\n",
      "iter 12100/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.493\n",
      "iter 12101/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.493\n",
      "iter 12200/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.494\n",
      "iter 12201/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.494\n",
      "iter 12300/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.496\n",
      "iter 12301/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.496\n",
      "iter 12400/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.497\n",
      "iter 12401/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.497\n",
      "iter 12500/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.498\n",
      "iter 12501/1000000  loss         0.329008  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.498\n",
      "iter 12600/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.500\n",
      "iter 12601/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.036 bias    1.500\n",
      "iter 12700/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.501\n",
      "iter 12701/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.501\n",
      "iter 12800/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.502\n",
      "iter 12801/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.502\n",
      "iter 12900/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.503\n",
      "iter 12901/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.503\n",
      "iter 13000/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.505\n",
      "iter 13001/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.505\n",
      "iter 13100/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.506\n",
      "iter 13101/1000000  loss         0.329007  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.506\n",
      "iter 13200/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.507\n",
      "iter 13201/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.507\n",
      "iter 13300/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.508\n",
      "iter 13301/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.508\n",
      "iter 13400/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.509\n",
      "iter 13401/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.509\n",
      "iter 13500/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.510\n",
      "iter 13501/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.510\n",
      "iter 13600/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.511\n",
      "iter 13601/1000000  loss         0.329006  avg_L1_norm_grad         0.000001  w[0]   -0.037 bias    1.511\n",
      "Done. Converged after 13656 iterations.\n",
      "Origin Accuracy 0.9134166666666641\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr22 = LRGD(alpha=100.0, step_size=0.1)\n",
    "orig_lr22.fit(x_te, y_te)\n",
    "y_hat_Origin=np.asarray(orig_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Initializing w_G with 787 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.019842  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.945312  avg_L1_norm_grad         0.028224  w[0]   -0.001 bias   -0.000\n",
      "iter    2/1000000  loss         0.916871  avg_L1_norm_grad         0.039286  w[0]    0.001 bias    0.010\n",
      "iter    3/1000000  loss         0.955756  avg_L1_norm_grad         0.081198  w[0]   -0.002 bias   -0.001\n",
      "iter    4/1000000  loss         1.141973  avg_L1_norm_grad         0.128257  w[0]    0.004 bias    0.028\n",
      "iter    5/1000000  loss         1.518469  avg_L1_norm_grad         0.165473  w[0]   -0.005 bias   -0.013\n",
      "iter    6/1000000  loss         1.517064  avg_L1_norm_grad         0.173225  w[0]    0.006 bias    0.045\n",
      "iter    7/1000000  loss         1.652035  avg_L1_norm_grad         0.173784  w[0]   -0.006 bias   -0.011\n",
      "iter    8/1000000  loss         1.333457  avg_L1_norm_grad         0.162135  w[0]    0.006 bias    0.049\n",
      "iter    9/1000000  loss         1.478553  avg_L1_norm_grad         0.164355  w[0]   -0.005 bias   -0.003\n",
      "iter   10/1000000  loss         1.206761  avg_L1_norm_grad         0.152568  w[0]    0.006 bias    0.054\n",
      "iter   11/1000000  loss         1.314635  avg_L1_norm_grad         0.152638  w[0]   -0.004 bias    0.005\n",
      "iter   12/1000000  loss         1.077602  avg_L1_norm_grad         0.139962  w[0]    0.006 bias    0.058\n",
      "iter   13/1000000  loss         1.151764  avg_L1_norm_grad         0.138137  w[0]   -0.003 bias    0.013\n",
      "iter   14/1000000  loss         0.954786  avg_L1_norm_grad         0.124996  w[0]    0.006 bias    0.061\n",
      "iter   15/1000000  loss         0.998919  avg_L1_norm_grad         0.121600  w[0]   -0.002 bias    0.021\n",
      "iter   16/1000000  loss         0.844679  avg_L1_norm_grad         0.108570  w[0]    0.006 bias    0.064\n",
      "iter   17/1000000  loss         0.864597  avg_L1_norm_grad         0.104049  w[0]   -0.002 bias    0.029\n",
      "iter   18/1000000  loss         0.751454  avg_L1_norm_grad         0.091742  w[0]    0.006 bias    0.066\n",
      "iter   19/1000000  loss         0.754477  avg_L1_norm_grad         0.086559  w[0]   -0.001 bias    0.036\n",
      "iter  100/1000000  loss         0.397255  avg_L1_norm_grad         0.001951  w[0]   -0.006 bias    0.110\n",
      "iter  101/1000000  loss         0.396503  avg_L1_norm_grad         0.001940  w[0]   -0.006 bias    0.110\n",
      "iter  200/1000000  loss         0.350776  avg_L1_norm_grad         0.001287  w[0]   -0.020 bias    0.124\n",
      "iter  201/1000000  loss         0.350485  avg_L1_norm_grad         0.001283  w[0]   -0.020 bias    0.124\n",
      "iter  300/1000000  loss         0.329245  avg_L1_norm_grad         0.000978  w[0]   -0.031 bias    0.125\n",
      "iter  301/1000000  loss         0.329085  avg_L1_norm_grad         0.000976  w[0]   -0.031 bias    0.125\n",
      "iter  400/1000000  loss         0.316487  avg_L1_norm_grad         0.000785  w[0]   -0.041 bias    0.121\n",
      "iter  401/1000000  loss         0.316385  avg_L1_norm_grad         0.000784  w[0]   -0.041 bias    0.121\n",
      "iter  500/1000000  loss         0.308027  avg_L1_norm_grad         0.000652  w[0]   -0.049 bias    0.114\n",
      "iter  501/1000000  loss         0.307957  avg_L1_norm_grad         0.000651  w[0]   -0.049 bias    0.114\n",
      "iter  600/1000000  loss         0.302038  avg_L1_norm_grad         0.000554  w[0]   -0.056 bias    0.107\n",
      "iter  601/1000000  loss         0.301988  avg_L1_norm_grad         0.000553  w[0]   -0.056 bias    0.106\n",
      "iter  700/1000000  loss         0.297612  avg_L1_norm_grad         0.000478  w[0]   -0.061 bias    0.098\n",
      "iter  701/1000000  loss         0.297573  avg_L1_norm_grad         0.000477  w[0]   -0.061 bias    0.098\n",
      "iter  800/1000000  loss         0.294236  avg_L1_norm_grad         0.000418  w[0]   -0.066 bias    0.089\n",
      "iter  801/1000000  loss         0.294207  avg_L1_norm_grad         0.000417  w[0]   -0.066 bias    0.089\n",
      "iter  900/1000000  loss         0.291602  avg_L1_norm_grad         0.000369  w[0]   -0.070 bias    0.080\n",
      "iter  901/1000000  loss         0.291578  avg_L1_norm_grad         0.000368  w[0]   -0.071 bias    0.080\n",
      "iter 1000/1000000  loss         0.289506  avg_L1_norm_grad         0.000328  w[0]   -0.074 bias    0.070\n",
      "iter 1001/1000000  loss         0.289488  avg_L1_norm_grad         0.000327  w[0]   -0.074 bias    0.070\n",
      "iter 1100/1000000  loss         0.287815  avg_L1_norm_grad         0.000294  w[0]   -0.078 bias    0.060\n",
      "iter 1101/1000000  loss         0.287800  avg_L1_norm_grad         0.000294  w[0]   -0.078 bias    0.060\n",
      "iter 1200/1000000  loss         0.286434  avg_L1_norm_grad         0.000265  w[0]   -0.080 bias    0.050\n",
      "iter 1201/1000000  loss         0.286421  avg_L1_norm_grad         0.000265  w[0]   -0.080 bias    0.050\n",
      "iter 1300/1000000  loss         0.285293  avg_L1_norm_grad         0.000240  w[0]   -0.083 bias    0.040\n",
      "iter 1301/1000000  loss         0.285283  avg_L1_norm_grad         0.000240  w[0]   -0.083 bias    0.040\n",
      "iter 1400/1000000  loss         0.284343  avg_L1_norm_grad         0.000219  w[0]   -0.085 bias    0.030\n",
      "iter 1401/1000000  loss         0.284334  avg_L1_norm_grad         0.000218  w[0]   -0.085 bias    0.030\n",
      "iter 1500/1000000  loss         0.283546  avg_L1_norm_grad         0.000200  w[0]   -0.088 bias    0.019\n",
      "iter 1501/1000000  loss         0.283539  avg_L1_norm_grad         0.000200  w[0]   -0.088 bias    0.019\n",
      "iter 1600/1000000  loss         0.282872  avg_L1_norm_grad         0.000183  w[0]   -0.090 bias    0.009\n",
      "iter 1601/1000000  loss         0.282866  avg_L1_norm_grad         0.000183  w[0]   -0.090 bias    0.009\n",
      "iter 1700/1000000  loss         0.282300  avg_L1_norm_grad         0.000168  w[0]   -0.091 bias   -0.001\n",
      "iter 1701/1000000  loss         0.282295  avg_L1_norm_grad         0.000168  w[0]   -0.091 bias   -0.001\n",
      "iter 1800/1000000  loss         0.281812  avg_L1_norm_grad         0.000155  w[0]   -0.093 bias   -0.011\n",
      "iter 1801/1000000  loss         0.281807  avg_L1_norm_grad         0.000155  w[0]   -0.093 bias   -0.012\n",
      "iter 1900/1000000  loss         0.281392  avg_L1_norm_grad         0.000143  w[0]   -0.094 bias   -0.022\n",
      "iter 1901/1000000  loss         0.281388  avg_L1_norm_grad         0.000143  w[0]   -0.094 bias   -0.022\n",
      "iter 2000/1000000  loss         0.281031  avg_L1_norm_grad         0.000132  w[0]   -0.096 bias   -0.032\n",
      "iter 2001/1000000  loss         0.281028  avg_L1_norm_grad         0.000132  w[0]   -0.096 bias   -0.032\n",
      "iter 2100/1000000  loss         0.280719  avg_L1_norm_grad         0.000123  w[0]   -0.097 bias   -0.042\n",
      "iter 2101/1000000  loss         0.280716  avg_L1_norm_grad         0.000123  w[0]   -0.097 bias   -0.042\n",
      "iter 2200/1000000  loss         0.280447  avg_L1_norm_grad         0.000114  w[0]   -0.098 bias   -0.052\n",
      "iter 2201/1000000  loss         0.280445  avg_L1_norm_grad         0.000114  w[0]   -0.098 bias   -0.052\n",
      "iter 2300/1000000  loss         0.280211  avg_L1_norm_grad         0.000106  w[0]   -0.099 bias   -0.062\n",
      "iter 2301/1000000  loss         0.280209  avg_L1_norm_grad         0.000106  w[0]   -0.099 bias   -0.062\n",
      "iter 2400/1000000  loss         0.280005  avg_L1_norm_grad         0.000098  w[0]   -0.100 bias   -0.072\n",
      "iter 2401/1000000  loss         0.280003  avg_L1_norm_grad         0.000098  w[0]   -0.100 bias   -0.072\n",
      "iter 2500/1000000  loss         0.279824  avg_L1_norm_grad         0.000092  w[0]   -0.101 bias   -0.082\n",
      "iter 2501/1000000  loss         0.279822  avg_L1_norm_grad         0.000091  w[0]   -0.101 bias   -0.082\n",
      "iter 2600/1000000  loss         0.279665  avg_L1_norm_grad         0.000085  w[0]   -0.102 bias   -0.092\n",
      "iter 2601/1000000  loss         0.279664  avg_L1_norm_grad         0.000085  w[0]   -0.102 bias   -0.092\n",
      "iter 2700/1000000  loss         0.279526  avg_L1_norm_grad         0.000080  w[0]   -0.103 bias   -0.101\n",
      "iter 2701/1000000  loss         0.279524  avg_L1_norm_grad         0.000080  w[0]   -0.103 bias   -0.102\n",
      "iter 2800/1000000  loss         0.279402  avg_L1_norm_grad         0.000074  w[0]   -0.103 bias   -0.111\n",
      "iter 2801/1000000  loss         0.279401  avg_L1_norm_grad         0.000074  w[0]   -0.103 bias   -0.111\n",
      "iter 2900/1000000  loss         0.279293  avg_L1_norm_grad         0.000069  w[0]   -0.104 bias   -0.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.279292  avg_L1_norm_grad         0.000069  w[0]   -0.104 bias   -0.120\n",
      "iter 3000/1000000  loss         0.279196  avg_L1_norm_grad         0.000065  w[0]   -0.105 bias   -0.130\n",
      "iter 3001/1000000  loss         0.279195  avg_L1_norm_grad         0.000065  w[0]   -0.105 bias   -0.130\n",
      "iter 3100/1000000  loss         0.279110  avg_L1_norm_grad         0.000061  w[0]   -0.105 bias   -0.139\n",
      "iter 3101/1000000  loss         0.279109  avg_L1_norm_grad         0.000061  w[0]   -0.105 bias   -0.139\n",
      "iter 3200/1000000  loss         0.279034  avg_L1_norm_grad         0.000057  w[0]   -0.106 bias   -0.148\n",
      "iter 3201/1000000  loss         0.279033  avg_L1_norm_grad         0.000057  w[0]   -0.106 bias   -0.148\n",
      "iter 3300/1000000  loss         0.278965  avg_L1_norm_grad         0.000053  w[0]   -0.106 bias   -0.157\n",
      "iter 3301/1000000  loss         0.278965  avg_L1_norm_grad         0.000053  w[0]   -0.106 bias   -0.157\n",
      "iter 3400/1000000  loss         0.278904  avg_L1_norm_grad         0.000050  w[0]   -0.107 bias   -0.166\n",
      "iter 3401/1000000  loss         0.278904  avg_L1_norm_grad         0.000050  w[0]   -0.107 bias   -0.166\n",
      "iter 3500/1000000  loss         0.278849  avg_L1_norm_grad         0.000047  w[0]   -0.107 bias   -0.174\n",
      "iter 3501/1000000  loss         0.278849  avg_L1_norm_grad         0.000047  w[0]   -0.107 bias   -0.174\n",
      "iter 3600/1000000  loss         0.278800  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias   -0.183\n",
      "iter 3601/1000000  loss         0.278800  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias   -0.183\n",
      "iter 3700/1000000  loss         0.278756  avg_L1_norm_grad         0.000041  w[0]   -0.108 bias   -0.191\n",
      "iter 3701/1000000  loss         0.278756  avg_L1_norm_grad         0.000041  w[0]   -0.108 bias   -0.191\n",
      "iter 3800/1000000  loss         0.278716  avg_L1_norm_grad         0.000039  w[0]   -0.109 bias   -0.200\n",
      "iter 3801/1000000  loss         0.278716  avg_L1_norm_grad         0.000039  w[0]   -0.109 bias   -0.200\n",
      "iter 3900/1000000  loss         0.278680  avg_L1_norm_grad         0.000036  w[0]   -0.109 bias   -0.208\n",
      "iter 3901/1000000  loss         0.278680  avg_L1_norm_grad         0.000036  w[0]   -0.109 bias   -0.208\n",
      "iter 4000/1000000  loss         0.278648  avg_L1_norm_grad         0.000034  w[0]   -0.109 bias   -0.216\n",
      "iter 4001/1000000  loss         0.278647  avg_L1_norm_grad         0.000034  w[0]   -0.109 bias   -0.216\n",
      "iter 4100/1000000  loss         0.278618  avg_L1_norm_grad         0.000032  w[0]   -0.110 bias   -0.224\n",
      "iter 4101/1000000  loss         0.278618  avg_L1_norm_grad         0.000032  w[0]   -0.110 bias   -0.224\n",
      "iter 4200/1000000  loss         0.278592  avg_L1_norm_grad         0.000030  w[0]   -0.110 bias   -0.231\n",
      "iter 4201/1000000  loss         0.278591  avg_L1_norm_grad         0.000030  w[0]   -0.110 bias   -0.232\n",
      "iter 4300/1000000  loss         0.278567  avg_L1_norm_grad         0.000028  w[0]   -0.110 bias   -0.239\n",
      "iter 4301/1000000  loss         0.278567  avg_L1_norm_grad         0.000028  w[0]   -0.110 bias   -0.239\n",
      "iter 4400/1000000  loss         0.278545  avg_L1_norm_grad         0.000027  w[0]   -0.110 bias   -0.247\n",
      "iter 4401/1000000  loss         0.278545  avg_L1_norm_grad         0.000027  w[0]   -0.110 bias   -0.247\n",
      "iter 4500/1000000  loss         0.278525  avg_L1_norm_grad         0.000025  w[0]   -0.111 bias   -0.254\n",
      "iter 4501/1000000  loss         0.278525  avg_L1_norm_grad         0.000025  w[0]   -0.111 bias   -0.254\n",
      "iter 4600/1000000  loss         0.278506  avg_L1_norm_grad         0.000024  w[0]   -0.111 bias   -0.261\n",
      "iter 4601/1000000  loss         0.278506  avg_L1_norm_grad         0.000024  w[0]   -0.111 bias   -0.261\n",
      "iter 4700/1000000  loss         0.278490  avg_L1_norm_grad         0.000022  w[0]   -0.111 bias   -0.268\n",
      "iter 4701/1000000  loss         0.278489  avg_L1_norm_grad         0.000022  w[0]   -0.111 bias   -0.268\n",
      "iter 4800/1000000  loss         0.278474  avg_L1_norm_grad         0.000021  w[0]   -0.111 bias   -0.275\n",
      "iter 4801/1000000  loss         0.278474  avg_L1_norm_grad         0.000021  w[0]   -0.111 bias   -0.275\n",
      "iter 4900/1000000  loss         0.278460  avg_L1_norm_grad         0.000020  w[0]   -0.111 bias   -0.282\n",
      "iter 4901/1000000  loss         0.278460  avg_L1_norm_grad         0.000020  w[0]   -0.111 bias   -0.282\n",
      "iter 5000/1000000  loss         0.278447  avg_L1_norm_grad         0.000019  w[0]   -0.112 bias   -0.289\n",
      "iter 5001/1000000  loss         0.278447  avg_L1_norm_grad         0.000019  w[0]   -0.112 bias   -0.289\n",
      "iter 5100/1000000  loss         0.278435  avg_L1_norm_grad         0.000018  w[0]   -0.112 bias   -0.295\n",
      "iter 5101/1000000  loss         0.278434  avg_L1_norm_grad         0.000018  w[0]   -0.112 bias   -0.296\n",
      "iter 5200/1000000  loss         0.278423  avg_L1_norm_grad         0.000017  w[0]   -0.112 bias   -0.302\n",
      "iter 5201/1000000  loss         0.278423  avg_L1_norm_grad         0.000017  w[0]   -0.112 bias   -0.302\n",
      "iter 5300/1000000  loss         0.278413  avg_L1_norm_grad         0.000016  w[0]   -0.112 bias   -0.308\n",
      "iter 5301/1000000  loss         0.278413  avg_L1_norm_grad         0.000016  w[0]   -0.112 bias   -0.308\n",
      "iter 5400/1000000  loss         0.278404  avg_L1_norm_grad         0.000015  w[0]   -0.112 bias   -0.315\n",
      "iter 5401/1000000  loss         0.278403  avg_L1_norm_grad         0.000015  w[0]   -0.112 bias   -0.315\n",
      "iter 5500/1000000  loss         0.278395  avg_L1_norm_grad         0.000014  w[0]   -0.112 bias   -0.321\n",
      "iter 5501/1000000  loss         0.278395  avg_L1_norm_grad         0.000014  w[0]   -0.112 bias   -0.321\n",
      "iter 5600/1000000  loss         0.278386  avg_L1_norm_grad         0.000013  w[0]   -0.112 bias   -0.327\n",
      "iter 5601/1000000  loss         0.278386  avg_L1_norm_grad         0.000013  w[0]   -0.112 bias   -0.327\n",
      "iter 5700/1000000  loss         0.278379  avg_L1_norm_grad         0.000013  w[0]   -0.112 bias   -0.333\n",
      "iter 5701/1000000  loss         0.278379  avg_L1_norm_grad         0.000013  w[0]   -0.112 bias   -0.333\n",
      "iter 5800/1000000  loss         0.278372  avg_L1_norm_grad         0.000012  w[0]   -0.113 bias   -0.338\n",
      "iter 5801/1000000  loss         0.278372  avg_L1_norm_grad         0.000012  w[0]   -0.113 bias   -0.339\n",
      "iter 5900/1000000  loss         0.278365  avg_L1_norm_grad         0.000011  w[0]   -0.113 bias   -0.344\n",
      "iter 5901/1000000  loss         0.278365  avg_L1_norm_grad         0.000011  w[0]   -0.113 bias   -0.344\n",
      "iter 6000/1000000  loss         0.278359  avg_L1_norm_grad         0.000011  w[0]   -0.113 bias   -0.350\n",
      "iter 6001/1000000  loss         0.278359  avg_L1_norm_grad         0.000011  w[0]   -0.113 bias   -0.350\n",
      "iter 6100/1000000  loss         0.278353  avg_L1_norm_grad         0.000010  w[0]   -0.113 bias   -0.355\n",
      "iter 6101/1000000  loss         0.278353  avg_L1_norm_grad         0.000010  w[0]   -0.113 bias   -0.355\n",
      "iter 6200/1000000  loss         0.278348  avg_L1_norm_grad         0.000010  w[0]   -0.113 bias   -0.361\n",
      "iter 6201/1000000  loss         0.278347  avg_L1_norm_grad         0.000010  w[0]   -0.113 bias   -0.361\n",
      "iter 6300/1000000  loss         0.278342  avg_L1_norm_grad         0.000009  w[0]   -0.113 bias   -0.366\n",
      "iter 6301/1000000  loss         0.278342  avg_L1_norm_grad         0.000009  w[0]   -0.113 bias   -0.366\n",
      "iter 6400/1000000  loss         0.278338  avg_L1_norm_grad         0.000009  w[0]   -0.113 bias   -0.371\n",
      "iter 6401/1000000  loss         0.278338  avg_L1_norm_grad         0.000009  w[0]   -0.113 bias   -0.371\n",
      "iter 6500/1000000  loss         0.278333  avg_L1_norm_grad         0.000008  w[0]   -0.113 bias   -0.376\n",
      "iter 6501/1000000  loss         0.278333  avg_L1_norm_grad         0.000008  w[0]   -0.113 bias   -0.376\n",
      "iter 6600/1000000  loss         0.278329  avg_L1_norm_grad         0.000008  w[0]   -0.113 bias   -0.381\n",
      "iter 6601/1000000  loss         0.278329  avg_L1_norm_grad         0.000008  w[0]   -0.113 bias   -0.381\n",
      "iter 6700/1000000  loss         0.278325  avg_L1_norm_grad         0.000007  w[0]   -0.113 bias   -0.386\n",
      "iter 6701/1000000  loss         0.278325  avg_L1_norm_grad         0.000007  w[0]   -0.113 bias   -0.386\n",
      "iter 6800/1000000  loss         0.278321  avg_L1_norm_grad         0.000007  w[0]   -0.113 bias   -0.391\n",
      "iter 6801/1000000  loss         0.278321  avg_L1_norm_grad         0.000007  w[0]   -0.113 bias   -0.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.278318  avg_L1_norm_grad         0.000007  w[0]   -0.113 bias   -0.395\n",
      "iter 6901/1000000  loss         0.278318  avg_L1_norm_grad         0.000007  w[0]   -0.113 bias   -0.395\n",
      "iter 7000/1000000  loss         0.278314  avg_L1_norm_grad         0.000006  w[0]   -0.113 bias   -0.400\n",
      "iter 7001/1000000  loss         0.278314  avg_L1_norm_grad         0.000006  w[0]   -0.113 bias   -0.400\n",
      "iter 7100/1000000  loss         0.278311  avg_L1_norm_grad         0.000006  w[0]   -0.113 bias   -0.404\n",
      "iter 7101/1000000  loss         0.278311  avg_L1_norm_grad         0.000006  w[0]   -0.113 bias   -0.404\n",
      "iter 7200/1000000  loss         0.278308  avg_L1_norm_grad         0.000006  w[0]   -0.113 bias   -0.409\n",
      "iter 7201/1000000  loss         0.278308  avg_L1_norm_grad         0.000006  w[0]   -0.113 bias   -0.409\n",
      "iter 7300/1000000  loss         0.278305  avg_L1_norm_grad         0.000005  w[0]   -0.113 bias   -0.413\n",
      "iter 7301/1000000  loss         0.278305  avg_L1_norm_grad         0.000005  w[0]   -0.113 bias   -0.413\n",
      "iter 7400/1000000  loss         0.278303  avg_L1_norm_grad         0.000005  w[0]   -0.114 bias   -0.417\n",
      "iter 7401/1000000  loss         0.278303  avg_L1_norm_grad         0.000005  w[0]   -0.114 bias   -0.417\n",
      "iter 7500/1000000  loss         0.278300  avg_L1_norm_grad         0.000005  w[0]   -0.114 bias   -0.421\n",
      "iter 7501/1000000  loss         0.278300  avg_L1_norm_grad         0.000005  w[0]   -0.114 bias   -0.421\n",
      "iter 7600/1000000  loss         0.278298  avg_L1_norm_grad         0.000005  w[0]   -0.114 bias   -0.425\n",
      "iter 7601/1000000  loss         0.278298  avg_L1_norm_grad         0.000005  w[0]   -0.114 bias   -0.425\n",
      "iter 7700/1000000  loss         0.278295  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.429\n",
      "iter 7701/1000000  loss         0.278295  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.429\n",
      "iter 7800/1000000  loss         0.278293  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.433\n",
      "iter 7801/1000000  loss         0.278293  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.433\n",
      "iter 7900/1000000  loss         0.278291  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.437\n",
      "iter 7901/1000000  loss         0.278291  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.437\n",
      "iter 8000/1000000  loss         0.278289  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.441\n",
      "iter 8001/1000000  loss         0.278289  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.441\n",
      "iter 8100/1000000  loss         0.278287  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.444\n",
      "iter 8101/1000000  loss         0.278287  avg_L1_norm_grad         0.000004  w[0]   -0.114 bias   -0.444\n",
      "iter 8200/1000000  loss         0.278286  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.448\n",
      "iter 8201/1000000  loss         0.278286  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.448\n",
      "iter 8300/1000000  loss         0.278284  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.451\n",
      "iter 8301/1000000  loss         0.278284  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.451\n",
      "iter 8400/1000000  loss         0.278282  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.455\n",
      "iter 8401/1000000  loss         0.278282  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.455\n",
      "iter 8500/1000000  loss         0.278281  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.458\n",
      "iter 8501/1000000  loss         0.278281  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.458\n",
      "iter 8600/1000000  loss         0.278279  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.461\n",
      "iter 8601/1000000  loss         0.278279  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.461\n",
      "iter 8700/1000000  loss         0.278278  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.465\n",
      "iter 8701/1000000  loss         0.278278  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.465\n",
      "iter 8800/1000000  loss         0.278277  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.468\n",
      "iter 8801/1000000  loss         0.278277  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.468\n",
      "iter 8900/1000000  loss         0.278276  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.471\n",
      "iter 8901/1000000  loss         0.278276  avg_L1_norm_grad         0.000003  w[0]   -0.114 bias   -0.471\n",
      "iter 9000/1000000  loss         0.278274  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.474\n",
      "iter 9001/1000000  loss         0.278274  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.474\n",
      "iter 9100/1000000  loss         0.278273  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.477\n",
      "iter 9101/1000000  loss         0.278273  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.477\n",
      "iter 9200/1000000  loss         0.278272  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.480\n",
      "iter 9201/1000000  loss         0.278272  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.480\n",
      "iter 9300/1000000  loss         0.278271  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.483\n",
      "iter 9301/1000000  loss         0.278271  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.483\n",
      "iter 9400/1000000  loss         0.278270  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.485\n",
      "iter 9401/1000000  loss         0.278270  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.485\n",
      "iter 9500/1000000  loss         0.278269  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.488\n",
      "iter 9501/1000000  loss         0.278269  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.488\n",
      "iter 9600/1000000  loss         0.278268  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.491\n",
      "iter 9601/1000000  loss         0.278268  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.491\n",
      "iter 9700/1000000  loss         0.278267  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.493\n",
      "iter 9701/1000000  loss         0.278267  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.493\n",
      "iter 9800/1000000  loss         0.278267  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.496\n",
      "iter 9801/1000000  loss         0.278267  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.496\n",
      "iter 9900/1000000  loss         0.278266  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.498\n",
      "iter 9901/1000000  loss         0.278266  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.498\n",
      "iter 10000/1000000  loss         0.278265  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.501\n",
      "iter 10001/1000000  loss         0.278265  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.501\n",
      "iter 10100/1000000  loss         0.278264  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.503\n",
      "iter 10101/1000000  loss         0.278264  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.503\n",
      "iter 10200/1000000  loss         0.278264  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.505\n",
      "iter 10201/1000000  loss         0.278264  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.505\n",
      "iter 10300/1000000  loss         0.278263  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.508\n",
      "iter 10301/1000000  loss         0.278263  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.508\n",
      "iter 10400/1000000  loss         0.278262  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.510\n",
      "iter 10401/1000000  loss         0.278262  avg_L1_norm_grad         0.000002  w[0]   -0.114 bias   -0.510\n",
      "iter 10500/1000000  loss         0.278262  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.512\n",
      "iter 10501/1000000  loss         0.278262  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.512\n",
      "iter 10600/1000000  loss         0.278261  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.514\n",
      "iter 10601/1000000  loss         0.278261  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.514\n",
      "iter 10700/1000000  loss         0.278261  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.516\n",
      "iter 10701/1000000  loss         0.278261  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.278260  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.518\n",
      "iter 10801/1000000  loss         0.278260  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.519\n",
      "iter 10900/1000000  loss         0.278260  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.520\n",
      "iter 10901/1000000  loss         0.278260  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.521\n",
      "iter 11000/1000000  loss         0.278259  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.522\n",
      "iter 11001/1000000  loss         0.278259  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.522\n",
      "iter 11100/1000000  loss         0.278259  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.524\n",
      "iter 11101/1000000  loss         0.278259  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.524\n",
      "iter 11200/1000000  loss         0.278258  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.526\n",
      "iter 11201/1000000  loss         0.278258  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.526\n",
      "iter 11300/1000000  loss         0.278258  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.528\n",
      "iter 11301/1000000  loss         0.278258  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.528\n",
      "iter 11400/1000000  loss         0.278257  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.530\n",
      "iter 11401/1000000  loss         0.278257  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.530\n",
      "iter 11500/1000000  loss         0.278257  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.532\n",
      "iter 11501/1000000  loss         0.278257  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.532\n",
      "iter 11600/1000000  loss         0.278257  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.533\n",
      "iter 11601/1000000  loss         0.278257  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.533\n",
      "iter 11700/1000000  loss         0.278256  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.535\n",
      "iter 11701/1000000  loss         0.278256  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.535\n",
      "iter 11800/1000000  loss         0.278256  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.537\n",
      "iter 11801/1000000  loss         0.278256  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.537\n",
      "iter 11900/1000000  loss         0.278256  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.538\n",
      "iter 11901/1000000  loss         0.278256  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.538\n",
      "iter 12000/1000000  loss         0.278255  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.540\n",
      "iter 12001/1000000  loss         0.278255  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.540\n",
      "iter 12100/1000000  loss         0.278255  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.542\n",
      "iter 12101/1000000  loss         0.278255  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.542\n",
      "iter 12200/1000000  loss         0.278255  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.543\n",
      "iter 12201/1000000  loss         0.278255  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.543\n",
      "iter 12300/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.545\n",
      "iter 12301/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.545\n",
      "iter 12400/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.546\n",
      "iter 12401/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.546\n",
      "iter 12500/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.547\n",
      "iter 12501/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.547\n",
      "iter 12600/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.549\n",
      "iter 12601/1000000  loss         0.278254  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.549\n",
      "iter 12700/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.550\n",
      "iter 12701/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.550\n",
      "iter 12800/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.552\n",
      "iter 12801/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.552\n",
      "iter 12900/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.553\n",
      "iter 12901/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.553\n",
      "iter 13000/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.554\n",
      "iter 13001/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.554\n",
      "iter 13100/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.555\n",
      "iter 13101/1000000  loss         0.278253  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.555\n",
      "iter 13200/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.557\n",
      "iter 13201/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.557\n",
      "iter 13300/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.558\n",
      "iter 13301/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.558\n",
      "iter 13400/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.559\n",
      "iter 13401/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.559\n",
      "iter 13500/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.560\n",
      "iter 13501/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.560\n",
      "iter 13600/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.561\n",
      "iter 13601/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.561\n",
      "iter 13700/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.562\n",
      "iter 13701/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.562\n",
      "iter 13800/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.563\n",
      "iter 13801/1000000  loss         0.278252  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.563\n",
      "iter 13900/1000000  loss         0.278251  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.564\n",
      "iter 13901/1000000  loss         0.278251  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.564\n",
      "iter 14000/1000000  loss         0.278251  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.566\n",
      "iter 14001/1000000  loss         0.278251  avg_L1_norm_grad         0.000001  w[0]   -0.114 bias   -0.566\n",
      "Done. Converged after 14092 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr2 = LRGDF(alpha=100.0, step_size=0.1)\n",
    "new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "New Accuracy 0.9297222222222196\n"
     ]
    }
   ],
   "source": [
    "y_hat_New=np.asarray(new_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "plt.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "plt.plot(fpr3te,tpr3te, label=\"Adding noise, counting bright blocks with 1 bright NB and with 2\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "plt.plot(fpr1Tte,tpr1Tte, label=\"No added Feature No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "plt.plot(fprTte,tprTte, label=\"counting bright blocks with 1 bright NB and with 2\")\n",
    "\n",
    "plt.xlim([-0.0, 1.0]);\n",
    "plt.ylim([-0.0, 1.0]);\n",
    "plt.legend();\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr2.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.157 0.983278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.53951285e-01  5.26440839e-02  8.79076479e-04 -1.02381199e-01\n",
      " -9.10931411e-03 -8.49692821e-02 -2.84764204e-01 -7.12042116e-01\n",
      "  3.18117987e+00 -5.66442689e-01]\n"
     ]
    }
   ],
   "source": [
    "w1=new_lr2.w_G[-10:]\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
