{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(1,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "Ave Loaded\n",
      "Initializing w_G with 1570 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.024436  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.926748  avg_L1_norm_grad         0.040185  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.936689  avg_L1_norm_grad         0.044868  w[0]    0.002 bias    0.028\n",
      "iter    3/1000000  loss         1.057079  avg_L1_norm_grad         0.067814  w[0]    0.000 bias    0.009\n",
      "iter    4/1000000  loss         0.974083  avg_L1_norm_grad         0.061439  w[0]    0.003 bias    0.053\n",
      "iter    5/1000000  loss         1.076026  avg_L1_norm_grad         0.069421  w[0]    0.001 bias    0.024\n",
      "iter    6/1000000  loss         0.826795  avg_L1_norm_grad         0.048592  w[0]    0.004 bias    0.070\n",
      "iter    7/1000000  loss         0.855146  avg_L1_norm_grad         0.052546  w[0]    0.002 bias    0.048\n",
      "iter    8/1000000  loss         0.722699  avg_L1_norm_grad         0.036569  w[0]    0.004 bias    0.084\n",
      "iter    9/1000000  loss         0.719801  avg_L1_norm_grad         0.038740  w[0]    0.003 bias    0.068\n",
      "iter   10/1000000  loss         0.651422  avg_L1_norm_grad         0.026720  w[0]    0.005 bias    0.096\n",
      "iter   11/1000000  loss         0.639015  avg_L1_norm_grad         0.028239  w[0]    0.004 bias    0.086\n",
      "iter   12/1000000  loss         0.602183  avg_L1_norm_grad         0.018996  w[0]    0.005 bias    0.108\n",
      "iter   13/1000000  loss         0.588701  avg_L1_norm_grad         0.020283  w[0]    0.005 bias    0.102\n",
      "iter   14/1000000  loss         0.567335  avg_L1_norm_grad         0.013152  w[0]    0.006 bias    0.119\n",
      "iter   15/1000000  loss         0.555522  avg_L1_norm_grad         0.014255  w[0]    0.006 bias    0.116\n",
      "iter   16/1000000  loss         0.541969  avg_L1_norm_grad         0.009112  w[0]    0.006 bias    0.129\n",
      "iter   17/1000000  loss         0.532318  avg_L1_norm_grad         0.009940  w[0]    0.006 bias    0.129\n",
      "iter   18/1000000  loss         0.522830  avg_L1_norm_grad         0.006601  w[0]    0.007 bias    0.139\n",
      "iter   19/1000000  loss         0.514999  avg_L1_norm_grad         0.007081  w[0]    0.007 bias    0.141\n",
      "iter  100/1000000  loss         0.336210  avg_L1_norm_grad         0.001574  w[0]    0.012 bias    0.383\n",
      "iter  101/1000000  loss         0.335404  avg_L1_norm_grad         0.001563  w[0]    0.012 bias    0.385\n",
      "iter  200/1000000  loss         0.286867  avg_L1_norm_grad         0.000980  w[0]    0.012 bias    0.537\n",
      "iter  201/1000000  loss         0.286559  avg_L1_norm_grad         0.000977  w[0]    0.012 bias    0.538\n",
      "iter  300/1000000  loss         0.263877  avg_L1_norm_grad         0.000743  w[0]    0.012 bias    0.649\n",
      "iter  301/1000000  loss         0.263705  avg_L1_norm_grad         0.000741  w[0]    0.012 bias    0.650\n",
      "iter  400/1000000  loss         0.249908  avg_L1_norm_grad         0.000607  w[0]    0.012 bias    0.743\n",
      "iter  401/1000000  loss         0.249795  avg_L1_norm_grad         0.000606  w[0]    0.012 bias    0.744\n",
      "iter  500/1000000  loss         0.240305  avg_L1_norm_grad         0.000517  w[0]    0.012 bias    0.825\n",
      "iter  501/1000000  loss         0.240224  avg_L1_norm_grad         0.000516  w[0]    0.012 bias    0.826\n",
      "iter  600/1000000  loss         0.233211  avg_L1_norm_grad         0.000452  w[0]    0.013 bias    0.899\n",
      "iter  601/1000000  loss         0.233149  avg_L1_norm_grad         0.000451  w[0]    0.013 bias    0.900\n",
      "iter  700/1000000  loss         0.227720  avg_L1_norm_grad         0.000402  w[0]    0.013 bias    0.967\n",
      "iter  701/1000000  loss         0.227672  avg_L1_norm_grad         0.000401  w[0]    0.013 bias    0.967\n",
      "iter  800/1000000  loss         0.223330  avg_L1_norm_grad         0.000362  w[0]    0.014 bias    1.029\n",
      "iter  801/1000000  loss         0.223290  avg_L1_norm_grad         0.000362  w[0]    0.014 bias    1.030\n",
      "iter  900/1000000  loss         0.219733  avg_L1_norm_grad         0.000330  w[0]    0.015 bias    1.088\n",
      "iter  901/1000000  loss         0.219700  avg_L1_norm_grad         0.000330  w[0]    0.015 bias    1.088\n",
      "iter 1000/1000000  loss         0.216732  avg_L1_norm_grad         0.000302  w[0]    0.016 bias    1.142\n",
      "iter 1001/1000000  loss         0.216705  avg_L1_norm_grad         0.000302  w[0]    0.016 bias    1.143\n",
      "iter 1100/1000000  loss         0.214192  avg_L1_norm_grad         0.000279  w[0]    0.017 bias    1.194\n",
      "iter 1101/1000000  loss         0.214169  avg_L1_norm_grad         0.000279  w[0]    0.017 bias    1.195\n",
      "iter 1200/1000000  loss         0.212015  avg_L1_norm_grad         0.000259  w[0]    0.018 bias    1.243\n",
      "iter 1201/1000000  loss         0.211995  avg_L1_norm_grad         0.000259  w[0]    0.018 bias    1.243\n",
      "iter 1300/1000000  loss         0.210132  avg_L1_norm_grad         0.000241  w[0]    0.019 bias    1.289\n",
      "iter 1301/1000000  loss         0.210115  avg_L1_norm_grad         0.000241  w[0]    0.019 bias    1.289\n",
      "iter 1400/1000000  loss         0.208490  avg_L1_norm_grad         0.000225  w[0]    0.021 bias    1.333\n",
      "iter 1401/1000000  loss         0.208474  avg_L1_norm_grad         0.000225  w[0]    0.021 bias    1.333\n",
      "iter 1500/1000000  loss         0.207046  avg_L1_norm_grad         0.000211  w[0]    0.022 bias    1.374\n",
      "iter 1501/1000000  loss         0.207033  avg_L1_norm_grad         0.000211  w[0]    0.022 bias    1.375\n",
      "iter 1600/1000000  loss         0.205770  avg_L1_norm_grad         0.000198  w[0]    0.023 bias    1.414\n",
      "iter 1601/1000000  loss         0.205758  avg_L1_norm_grad         0.000198  w[0]    0.023 bias    1.415\n",
      "iter 1700/1000000  loss         0.204636  avg_L1_norm_grad         0.000187  w[0]    0.024 bias    1.452\n",
      "iter 1701/1000000  loss         0.204625  avg_L1_norm_grad         0.000187  w[0]    0.024 bias    1.452\n",
      "iter 1800/1000000  loss         0.203623  avg_L1_norm_grad         0.000176  w[0]    0.025 bias    1.488\n",
      "iter 1801/1000000  loss         0.203613  avg_L1_norm_grad         0.000176  w[0]    0.025 bias    1.489\n",
      "iter 1900/1000000  loss         0.202714  avg_L1_norm_grad         0.000167  w[0]    0.026 bias    1.523\n",
      "iter 1901/1000000  loss         0.202705  avg_L1_norm_grad         0.000167  w[0]    0.026 bias    1.523\n",
      "iter 2000/1000000  loss         0.201895  avg_L1_norm_grad         0.000158  w[0]    0.027 bias    1.556\n",
      "iter 2001/1000000  loss         0.201888  avg_L1_norm_grad         0.000158  w[0]    0.027 bias    1.556\n",
      "iter 2100/1000000  loss         0.201156  avg_L1_norm_grad         0.000150  w[0]    0.028 bias    1.588\n",
      "iter 2101/1000000  loss         0.201149  avg_L1_norm_grad         0.000150  w[0]    0.028 bias    1.588\n",
      "iter 2200/1000000  loss         0.200486  avg_L1_norm_grad         0.000143  w[0]    0.029 bias    1.618\n",
      "iter 2201/1000000  loss         0.200479  avg_L1_norm_grad         0.000143  w[0]    0.029 bias    1.618\n",
      "iter 2300/1000000  loss         0.199876  avg_L1_norm_grad         0.000137  w[0]    0.030 bias    1.647\n",
      "iter 2301/1000000  loss         0.199871  avg_L1_norm_grad         0.000136  w[0]    0.030 bias    1.647\n",
      "iter 2400/1000000  loss         0.199321  avg_L1_norm_grad         0.000130  w[0]    0.031 bias    1.675\n",
      "iter 2401/1000000  loss         0.199316  avg_L1_norm_grad         0.000130  w[0]    0.031 bias    1.675\n",
      "iter 2500/1000000  loss         0.198814  avg_L1_norm_grad         0.000124  w[0]    0.032 bias    1.702\n",
      "iter 2501/1000000  loss         0.198809  avg_L1_norm_grad         0.000124  w[0]    0.032 bias    1.702\n",
      "iter 2600/1000000  loss         0.198349  avg_L1_norm_grad         0.000119  w[0]    0.033 bias    1.728\n",
      "iter 2601/1000000  loss         0.198345  avg_L1_norm_grad         0.000119  w[0]    0.033 bias    1.728\n",
      "iter 2700/1000000  loss         0.197923  avg_L1_norm_grad         0.000114  w[0]    0.033 bias    1.753\n",
      "iter 2701/1000000  loss         0.197919  avg_L1_norm_grad         0.000114  w[0]    0.033 bias    1.753\n",
      "iter 2800/1000000  loss         0.197531  avg_L1_norm_grad         0.000109  w[0]    0.034 bias    1.777\n",
      "iter 2801/1000000  loss         0.197527  avg_L1_norm_grad         0.000109  w[0]    0.034 bias    1.777\n",
      "iter 2900/1000000  loss         0.197170  avg_L1_norm_grad         0.000105  w[0]    0.035 bias    1.800\n",
      "iter 2901/1000000  loss         0.197166  avg_L1_norm_grad         0.000105  w[0]    0.035 bias    1.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.196837  avg_L1_norm_grad         0.000101  w[0]    0.036 bias    1.822\n",
      "iter 3001/1000000  loss         0.196834  avg_L1_norm_grad         0.000101  w[0]    0.036 bias    1.822\n",
      "iter 3100/1000000  loss         0.196529  avg_L1_norm_grad         0.000097  w[0]    0.036 bias    1.843\n",
      "iter 3101/1000000  loss         0.196526  avg_L1_norm_grad         0.000097  w[0]    0.036 bias    1.844\n",
      "iter 3200/1000000  loss         0.196245  avg_L1_norm_grad         0.000093  w[0]    0.037 bias    1.864\n",
      "iter 3201/1000000  loss         0.196242  avg_L1_norm_grad         0.000093  w[0]    0.037 bias    1.864\n",
      "iter 3300/1000000  loss         0.195981  avg_L1_norm_grad         0.000089  w[0]    0.037 bias    1.884\n",
      "iter 3301/1000000  loss         0.195979  avg_L1_norm_grad         0.000089  w[0]    0.037 bias    1.884\n",
      "iter 3400/1000000  loss         0.195736  avg_L1_norm_grad         0.000086  w[0]    0.038 bias    1.903\n",
      "iter 3401/1000000  loss         0.195734  avg_L1_norm_grad         0.000086  w[0]    0.038 bias    1.903\n",
      "iter 3500/1000000  loss         0.195509  avg_L1_norm_grad         0.000083  w[0]    0.039 bias    1.921\n",
      "iter 3501/1000000  loss         0.195507  avg_L1_norm_grad         0.000083  w[0]    0.039 bias    1.922\n",
      "iter 3600/1000000  loss         0.195298  avg_L1_norm_grad         0.000080  w[0]    0.039 bias    1.939\n",
      "iter 3601/1000000  loss         0.195296  avg_L1_norm_grad         0.000080  w[0]    0.039 bias    1.939\n",
      "iter 3700/1000000  loss         0.195101  avg_L1_norm_grad         0.000077  w[0]    0.040 bias    1.956\n",
      "iter 3701/1000000  loss         0.195100  avg_L1_norm_grad         0.000077  w[0]    0.040 bias    1.957\n",
      "iter 3800/1000000  loss         0.194918  avg_L1_norm_grad         0.000074  w[0]    0.040 bias    1.973\n",
      "iter 3801/1000000  loss         0.194916  avg_L1_norm_grad         0.000074  w[0]    0.040 bias    1.973\n",
      "iter 3900/1000000  loss         0.194747  avg_L1_norm_grad         0.000071  w[0]    0.040 bias    1.989\n",
      "iter 3901/1000000  loss         0.194746  avg_L1_norm_grad         0.000071  w[0]    0.040 bias    1.989\n",
      "iter 4000/1000000  loss         0.194588  avg_L1_norm_grad         0.000069  w[0]    0.041 bias    2.005\n",
      "iter 4001/1000000  loss         0.194586  avg_L1_norm_grad         0.000069  w[0]    0.041 bias    2.005\n",
      "iter 4100/1000000  loss         0.194439  avg_L1_norm_grad         0.000067  w[0]    0.041 bias    2.020\n",
      "iter 4101/1000000  loss         0.194437  avg_L1_norm_grad         0.000067  w[0]    0.041 bias    2.020\n",
      "iter 4200/1000000  loss         0.194299  avg_L1_norm_grad         0.000064  w[0]    0.042 bias    2.034\n",
      "iter 4201/1000000  loss         0.194298  avg_L1_norm_grad         0.000064  w[0]    0.042 bias    2.034\n",
      "iter 4300/1000000  loss         0.194169  avg_L1_norm_grad         0.000062  w[0]    0.042 bias    2.048\n",
      "iter 4301/1000000  loss         0.194168  avg_L1_norm_grad         0.000062  w[0]    0.042 bias    2.048\n",
      "iter 4400/1000000  loss         0.194047  avg_L1_norm_grad         0.000060  w[0]    0.042 bias    2.061\n",
      "iter 4401/1000000  loss         0.194046  avg_L1_norm_grad         0.000060  w[0]    0.042 bias    2.062\n",
      "iter 4500/1000000  loss         0.193932  avg_L1_norm_grad         0.000058  w[0]    0.043 bias    2.075\n",
      "iter 4501/1000000  loss         0.193931  avg_L1_norm_grad         0.000058  w[0]    0.043 bias    2.075\n",
      "iter 4600/1000000  loss         0.193825  avg_L1_norm_grad         0.000056  w[0]    0.043 bias    2.087\n",
      "iter 4601/1000000  loss         0.193824  avg_L1_norm_grad         0.000056  w[0]    0.043 bias    2.087\n",
      "iter 4700/1000000  loss         0.193724  avg_L1_norm_grad         0.000054  w[0]    0.043 bias    2.099\n",
      "iter 4701/1000000  loss         0.193723  avg_L1_norm_grad         0.000054  w[0]    0.043 bias    2.099\n",
      "iter 4800/1000000  loss         0.193630  avg_L1_norm_grad         0.000053  w[0]    0.043 bias    2.111\n",
      "iter 4801/1000000  loss         0.193629  avg_L1_norm_grad         0.000053  w[0]    0.043 bias    2.111\n",
      "iter 4900/1000000  loss         0.193541  avg_L1_norm_grad         0.000051  w[0]    0.044 bias    2.123\n",
      "iter 4901/1000000  loss         0.193540  avg_L1_norm_grad         0.000051  w[0]    0.044 bias    2.123\n",
      "iter 5000/1000000  loss         0.193457  avg_L1_norm_grad         0.000049  w[0]    0.044 bias    2.134\n",
      "iter 5001/1000000  loss         0.193456  avg_L1_norm_grad         0.000049  w[0]    0.044 bias    2.134\n",
      "iter 5100/1000000  loss         0.193379  avg_L1_norm_grad         0.000048  w[0]    0.044 bias    2.144\n",
      "iter 5101/1000000  loss         0.193378  avg_L1_norm_grad         0.000048  w[0]    0.044 bias    2.144\n",
      "iter 5200/1000000  loss         0.193305  avg_L1_norm_grad         0.000046  w[0]    0.044 bias    2.155\n",
      "iter 5201/1000000  loss         0.193304  avg_L1_norm_grad         0.000046  w[0]    0.044 bias    2.155\n",
      "iter 5300/1000000  loss         0.193235  avg_L1_norm_grad         0.000045  w[0]    0.045 bias    2.165\n",
      "iter 5301/1000000  loss         0.193234  avg_L1_norm_grad         0.000045  w[0]    0.045 bias    2.165\n",
      "iter 5400/1000000  loss         0.193169  avg_L1_norm_grad         0.000044  w[0]    0.045 bias    2.174\n",
      "iter 5401/1000000  loss         0.193169  avg_L1_norm_grad         0.000044  w[0]    0.045 bias    2.174\n",
      "iter 5500/1000000  loss         0.193107  avg_L1_norm_grad         0.000042  w[0]    0.045 bias    2.184\n",
      "iter 5501/1000000  loss         0.193107  avg_L1_norm_grad         0.000042  w[0]    0.045 bias    2.184\n",
      "iter 5600/1000000  loss         0.193049  avg_L1_norm_grad         0.000041  w[0]    0.045 bias    2.193\n",
      "iter 5601/1000000  loss         0.193049  avg_L1_norm_grad         0.000041  w[0]    0.045 bias    2.193\n",
      "iter 5700/1000000  loss         0.192994  avg_L1_norm_grad         0.000040  w[0]    0.045 bias    2.202\n",
      "iter 5701/1000000  loss         0.192994  avg_L1_norm_grad         0.000040  w[0]    0.045 bias    2.202\n",
      "iter 5800/1000000  loss         0.192942  avg_L1_norm_grad         0.000039  w[0]    0.045 bias    2.210\n",
      "iter 5801/1000000  loss         0.192942  avg_L1_norm_grad         0.000039  w[0]    0.045 bias    2.210\n",
      "iter 5900/1000000  loss         0.192893  avg_L1_norm_grad         0.000037  w[0]    0.046 bias    2.218\n",
      "iter 5901/1000000  loss         0.192893  avg_L1_norm_grad         0.000037  w[0]    0.046 bias    2.218\n",
      "iter 6000/1000000  loss         0.192847  avg_L1_norm_grad         0.000036  w[0]    0.046 bias    2.226\n",
      "iter 6001/1000000  loss         0.192846  avg_L1_norm_grad         0.000036  w[0]    0.046 bias    2.226\n",
      "iter 6100/1000000  loss         0.192803  avg_L1_norm_grad         0.000035  w[0]    0.046 bias    2.234\n",
      "iter 6101/1000000  loss         0.192803  avg_L1_norm_grad         0.000035  w[0]    0.046 bias    2.234\n",
      "iter 6200/1000000  loss         0.192762  avg_L1_norm_grad         0.000034  w[0]    0.046 bias    2.242\n",
      "iter 6201/1000000  loss         0.192761  avg_L1_norm_grad         0.000034  w[0]    0.046 bias    2.242\n",
      "iter 6300/1000000  loss         0.192723  avg_L1_norm_grad         0.000033  w[0]    0.046 bias    2.249\n",
      "iter 6301/1000000  loss         0.192722  avg_L1_norm_grad         0.000033  w[0]    0.046 bias    2.249\n",
      "iter 6400/1000000  loss         0.192686  avg_L1_norm_grad         0.000032  w[0]    0.046 bias    2.256\n",
      "iter 6401/1000000  loss         0.192685  avg_L1_norm_grad         0.000032  w[0]    0.046 bias    2.256\n",
      "iter 6500/1000000  loss         0.192650  avg_L1_norm_grad         0.000031  w[0]    0.046 bias    2.263\n",
      "iter 6501/1000000  loss         0.192650  avg_L1_norm_grad         0.000031  w[0]    0.046 bias    2.263\n",
      "iter 6600/1000000  loss         0.192617  avg_L1_norm_grad         0.000031  w[0]    0.046 bias    2.269\n",
      "iter 6601/1000000  loss         0.192617  avg_L1_norm_grad         0.000031  w[0]    0.046 bias    2.269\n",
      "iter 6700/1000000  loss         0.192586  avg_L1_norm_grad         0.000030  w[0]    0.046 bias    2.276\n",
      "iter 6701/1000000  loss         0.192586  avg_L1_norm_grad         0.000030  w[0]    0.046 bias    2.276\n",
      "iter 6800/1000000  loss         0.192556  avg_L1_norm_grad         0.000029  w[0]    0.047 bias    2.282\n",
      "iter 6801/1000000  loss         0.192556  avg_L1_norm_grad         0.000029  w[0]    0.047 bias    2.282\n",
      "iter 6900/1000000  loss         0.192528  avg_L1_norm_grad         0.000028  w[0]    0.047 bias    2.288\n",
      "iter 6901/1000000  loss         0.192528  avg_L1_norm_grad         0.000028  w[0]    0.047 bias    2.288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.192501  avg_L1_norm_grad         0.000027  w[0]    0.047 bias    2.294\n",
      "iter 7001/1000000  loss         0.192501  avg_L1_norm_grad         0.000027  w[0]    0.047 bias    2.294\n",
      "iter 7100/1000000  loss         0.192476  avg_L1_norm_grad         0.000027  w[0]    0.047 bias    2.299\n",
      "iter 7101/1000000  loss         0.192476  avg_L1_norm_grad         0.000027  w[0]    0.047 bias    2.299\n",
      "iter 7200/1000000  loss         0.192452  avg_L1_norm_grad         0.000026  w[0]    0.047 bias    2.305\n",
      "iter 7201/1000000  loss         0.192452  avg_L1_norm_grad         0.000026  w[0]    0.047 bias    2.305\n",
      "iter 7300/1000000  loss         0.192429  avg_L1_norm_grad         0.000025  w[0]    0.047 bias    2.310\n",
      "iter 7301/1000000  loss         0.192429  avg_L1_norm_grad         0.000025  w[0]    0.047 bias    2.310\n",
      "iter 7400/1000000  loss         0.192408  avg_L1_norm_grad         0.000025  w[0]    0.047 bias    2.315\n",
      "iter 7401/1000000  loss         0.192407  avg_L1_norm_grad         0.000025  w[0]    0.047 bias    2.315\n",
      "iter 7500/1000000  loss         0.192387  avg_L1_norm_grad         0.000024  w[0]    0.047 bias    2.320\n",
      "iter 7501/1000000  loss         0.192387  avg_L1_norm_grad         0.000024  w[0]    0.047 bias    2.320\n",
      "iter 7600/1000000  loss         0.192368  avg_L1_norm_grad         0.000023  w[0]    0.047 bias    2.325\n",
      "iter 7601/1000000  loss         0.192368  avg_L1_norm_grad         0.000023  w[0]    0.047 bias    2.325\n",
      "iter 7700/1000000  loss         0.192349  avg_L1_norm_grad         0.000023  w[0]    0.047 bias    2.330\n",
      "iter 7701/1000000  loss         0.192349  avg_L1_norm_grad         0.000023  w[0]    0.047 bias    2.330\n",
      "iter 7800/1000000  loss         0.192332  avg_L1_norm_grad         0.000022  w[0]    0.047 bias    2.334\n",
      "iter 7801/1000000  loss         0.192332  avg_L1_norm_grad         0.000022  w[0]    0.047 bias    2.334\n",
      "iter 7900/1000000  loss         0.192315  avg_L1_norm_grad         0.000022  w[0]    0.047 bias    2.338\n",
      "iter 7901/1000000  loss         0.192315  avg_L1_norm_grad         0.000022  w[0]    0.047 bias    2.339\n",
      "iter 8000/1000000  loss         0.192299  avg_L1_norm_grad         0.000021  w[0]    0.047 bias    2.343\n",
      "iter 8001/1000000  loss         0.192299  avg_L1_norm_grad         0.000021  w[0]    0.047 bias    2.343\n",
      "iter 8100/1000000  loss         0.192284  avg_L1_norm_grad         0.000020  w[0]    0.047 bias    2.347\n",
      "iter 8101/1000000  loss         0.192284  avg_L1_norm_grad         0.000020  w[0]    0.047 bias    2.347\n",
      "iter 8200/1000000  loss         0.192270  avg_L1_norm_grad         0.000020  w[0]    0.047 bias    2.351\n",
      "iter 8201/1000000  loss         0.192270  avg_L1_norm_grad         0.000020  w[0]    0.047 bias    2.351\n",
      "iter 8300/1000000  loss         0.192256  avg_L1_norm_grad         0.000019  w[0]    0.047 bias    2.355\n",
      "iter 8301/1000000  loss         0.192256  avg_L1_norm_grad         0.000019  w[0]    0.047 bias    2.355\n",
      "iter 8400/1000000  loss         0.192243  avg_L1_norm_grad         0.000019  w[0]    0.047 bias    2.359\n",
      "iter 8401/1000000  loss         0.192243  avg_L1_norm_grad         0.000019  w[0]    0.047 bias    2.359\n",
      "iter 8500/1000000  loss         0.192231  avg_L1_norm_grad         0.000019  w[0]    0.047 bias    2.362\n",
      "iter 8501/1000000  loss         0.192231  avg_L1_norm_grad         0.000019  w[0]    0.047 bias    2.362\n",
      "iter 8600/1000000  loss         0.192219  avg_L1_norm_grad         0.000018  w[0]    0.047 bias    2.366\n",
      "iter 8601/1000000  loss         0.192219  avg_L1_norm_grad         0.000018  w[0]    0.047 bias    2.366\n",
      "iter 8700/1000000  loss         0.192208  avg_L1_norm_grad         0.000018  w[0]    0.047 bias    2.369\n",
      "iter 8701/1000000  loss         0.192208  avg_L1_norm_grad         0.000018  w[0]    0.047 bias    2.369\n",
      "iter 8800/1000000  loss         0.192198  avg_L1_norm_grad         0.000017  w[0]    0.047 bias    2.372\n",
      "iter 8801/1000000  loss         0.192197  avg_L1_norm_grad         0.000017  w[0]    0.047 bias    2.372\n",
      "iter 8900/1000000  loss         0.192188  avg_L1_norm_grad         0.000017  w[0]    0.047 bias    2.376\n",
      "iter 8901/1000000  loss         0.192187  avg_L1_norm_grad         0.000017  w[0]    0.047 bias    2.376\n",
      "iter 9000/1000000  loss         0.192178  avg_L1_norm_grad         0.000016  w[0]    0.047 bias    2.379\n",
      "iter 9001/1000000  loss         0.192178  avg_L1_norm_grad         0.000016  w[0]    0.047 bias    2.379\n",
      "iter 9100/1000000  loss         0.192169  avg_L1_norm_grad         0.000016  w[0]    0.047 bias    2.382\n",
      "iter 9101/1000000  loss         0.192169  avg_L1_norm_grad         0.000016  w[0]    0.047 bias    2.382\n",
      "iter 9200/1000000  loss         0.192160  avg_L1_norm_grad         0.000016  w[0]    0.047 bias    2.385\n",
      "iter 9201/1000000  loss         0.192160  avg_L1_norm_grad         0.000016  w[0]    0.047 bias    2.385\n",
      "iter 9300/1000000  loss         0.192152  avg_L1_norm_grad         0.000015  w[0]    0.047 bias    2.388\n",
      "iter 9301/1000000  loss         0.192152  avg_L1_norm_grad         0.000015  w[0]    0.047 bias    2.388\n",
      "iter 9400/1000000  loss         0.192144  avg_L1_norm_grad         0.000015  w[0]    0.047 bias    2.390\n",
      "iter 9401/1000000  loss         0.192144  avg_L1_norm_grad         0.000015  w[0]    0.047 bias    2.390\n",
      "iter 9500/1000000  loss         0.192136  avg_L1_norm_grad         0.000014  w[0]    0.047 bias    2.393\n",
      "iter 9501/1000000  loss         0.192136  avg_L1_norm_grad         0.000014  w[0]    0.047 bias    2.393\n",
      "iter 9600/1000000  loss         0.192129  avg_L1_norm_grad         0.000014  w[0]    0.048 bias    2.396\n",
      "iter 9601/1000000  loss         0.192129  avg_L1_norm_grad         0.000014  w[0]    0.048 bias    2.396\n",
      "iter 9700/1000000  loss         0.192122  avg_L1_norm_grad         0.000014  w[0]    0.048 bias    2.398\n",
      "iter 9701/1000000  loss         0.192122  avg_L1_norm_grad         0.000014  w[0]    0.048 bias    2.398\n",
      "iter 9800/1000000  loss         0.192116  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.401\n",
      "iter 9801/1000000  loss         0.192116  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.401\n",
      "iter 9900/1000000  loss         0.192110  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.403\n",
      "iter 9901/1000000  loss         0.192110  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.403\n",
      "iter 10000/1000000  loss         0.192104  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.405\n",
      "iter 10001/1000000  loss         0.192104  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.405\n",
      "iter 10100/1000000  loss         0.192098  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.408\n",
      "iter 10101/1000000  loss         0.192098  avg_L1_norm_grad         0.000013  w[0]    0.048 bias    2.408\n",
      "iter 10200/1000000  loss         0.192093  avg_L1_norm_grad         0.000012  w[0]    0.048 bias    2.410\n",
      "iter 10201/1000000  loss         0.192093  avg_L1_norm_grad         0.000012  w[0]    0.048 bias    2.410\n",
      "iter 10300/1000000  loss         0.192088  avg_L1_norm_grad         0.000012  w[0]    0.048 bias    2.412\n",
      "iter 10301/1000000  loss         0.192088  avg_L1_norm_grad         0.000012  w[0]    0.048 bias    2.412\n",
      "iter 10400/1000000  loss         0.192083  avg_L1_norm_grad         0.000012  w[0]    0.048 bias    2.414\n",
      "iter 10401/1000000  loss         0.192083  avg_L1_norm_grad         0.000012  w[0]    0.048 bias    2.414\n",
      "iter 10500/1000000  loss         0.192078  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.416\n",
      "iter 10501/1000000  loss         0.192078  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.416\n",
      "iter 10600/1000000  loss         0.192074  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.418\n",
      "iter 10601/1000000  loss         0.192073  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.418\n",
      "iter 10700/1000000  loss         0.192069  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.420\n",
      "iter 10701/1000000  loss         0.192069  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.420\n",
      "iter 10800/1000000  loss         0.192065  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.421\n",
      "iter 10801/1000000  loss         0.192065  avg_L1_norm_grad         0.000011  w[0]    0.048 bias    2.421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.192061  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.423\n",
      "iter 10901/1000000  loss         0.192061  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.423\n",
      "iter 11000/1000000  loss         0.192058  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.425\n",
      "iter 11001/1000000  loss         0.192058  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.425\n",
      "iter 11100/1000000  loss         0.192054  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.427\n",
      "iter 11101/1000000  loss         0.192054  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.427\n",
      "iter 11200/1000000  loss         0.192051  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.428\n",
      "iter 11201/1000000  loss         0.192051  avg_L1_norm_grad         0.000010  w[0]    0.048 bias    2.428\n",
      "iter 11300/1000000  loss         0.192047  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.430\n",
      "iter 11301/1000000  loss         0.192047  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.430\n",
      "iter 11400/1000000  loss         0.192044  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.431\n",
      "iter 11401/1000000  loss         0.192044  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.431\n",
      "iter 11500/1000000  loss         0.192041  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.433\n",
      "iter 11501/1000000  loss         0.192041  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.433\n",
      "iter 11600/1000000  loss         0.192039  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.434\n",
      "iter 11601/1000000  loss         0.192039  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.434\n",
      "iter 11700/1000000  loss         0.192036  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.436\n",
      "iter 11701/1000000  loss         0.192036  avg_L1_norm_grad         0.000009  w[0]    0.048 bias    2.436\n",
      "iter 11800/1000000  loss         0.192033  avg_L1_norm_grad         0.000008  w[0]    0.048 bias    2.437\n",
      "iter 11801/1000000  loss         0.192033  avg_L1_norm_grad         0.000008  w[0]    0.048 bias    2.437\n",
      "iter 11900/1000000  loss         0.192031  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.438\n",
      "iter 11901/1000000  loss         0.192031  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.438\n",
      "iter 12000/1000000  loss         0.192029  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.439\n",
      "iter 12001/1000000  loss         0.192029  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.439\n",
      "iter 12100/1000000  loss         0.192026  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.441\n",
      "iter 12101/1000000  loss         0.192026  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.441\n",
      "iter 12200/1000000  loss         0.192024  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.442\n",
      "iter 12201/1000000  loss         0.192024  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.442\n",
      "iter 12300/1000000  loss         0.192022  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.443\n",
      "iter 12301/1000000  loss         0.192022  avg_L1_norm_grad         0.000008  w[0]    0.047 bias    2.443\n",
      "iter 12400/1000000  loss         0.192020  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.444\n",
      "iter 12401/1000000  loss         0.192020  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.444\n",
      "iter 12500/1000000  loss         0.192018  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.445\n",
      "iter 12501/1000000  loss         0.192018  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.445\n",
      "iter 12600/1000000  loss         0.192017  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.446\n",
      "iter 12601/1000000  loss         0.192017  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.446\n",
      "iter 12700/1000000  loss         0.192015  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.447\n",
      "iter 12701/1000000  loss         0.192015  avg_L1_norm_grad         0.000007  w[0]    0.047 bias    2.447\n",
      "Done. Converged after 12719 iterations.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028507  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.918226  avg_L1_norm_grad         0.029914  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.856696  avg_L1_norm_grad         0.020499  w[0]    0.001 bias    0.019\n",
      "iter    3/1000000  loss         0.811651  avg_L1_norm_grad         0.021428  w[0]    0.001 bias    0.022\n",
      "iter    4/1000000  loss         0.776005  avg_L1_norm_grad         0.015104  w[0]    0.002 bias    0.037\n",
      "iter    5/1000000  loss         0.748100  avg_L1_norm_grad         0.015568  w[0]    0.002 bias    0.042\n",
      "iter    6/1000000  loss         0.725129  avg_L1_norm_grad         0.012255  w[0]    0.003 bias    0.053\n",
      "iter    7/1000000  loss         0.705850  avg_L1_norm_grad         0.012326  w[0]    0.003 bias    0.060\n",
      "iter    8/1000000  loss         0.689126  avg_L1_norm_grad         0.010800  w[0]    0.004 bias    0.069\n",
      "iter    9/1000000  loss         0.674297  avg_L1_norm_grad         0.010550  w[0]    0.004 bias    0.077\n",
      "iter   10/1000000  loss         0.660909  avg_L1_norm_grad         0.009815  w[0]    0.004 bias    0.085\n",
      "iter   11/1000000  loss         0.648664  avg_L1_norm_grad         0.009477  w[0]    0.005 bias    0.093\n",
      "iter   12/1000000  loss         0.637356  avg_L1_norm_grad         0.009038  w[0]    0.005 bias    0.100\n",
      "iter   13/1000000  loss         0.626836  avg_L1_norm_grad         0.008727  w[0]    0.005 bias    0.108\n",
      "iter   14/1000000  loss         0.616990  avg_L1_norm_grad         0.008411  w[0]    0.006 bias    0.115\n",
      "iter   15/1000000  loss         0.607731  avg_L1_norm_grad         0.008142  w[0]    0.006 bias    0.122\n",
      "iter   16/1000000  loss         0.598991  avg_L1_norm_grad         0.007890  w[0]    0.006 bias    0.129\n",
      "iter   17/1000000  loss         0.590713  avg_L1_norm_grad         0.007661  w[0]    0.007 bias    0.136\n",
      "iter   18/1000000  loss         0.582851  avg_L1_norm_grad         0.007448  w[0]    0.007 bias    0.143\n",
      "iter   19/1000000  loss         0.575368  avg_L1_norm_grad         0.007250  w[0]    0.007 bias    0.149\n",
      "iter  100/1000000  loss         0.362752  avg_L1_norm_grad         0.002505  w[0]    0.018 bias    0.483\n",
      "iter  101/1000000  loss         0.361771  avg_L1_norm_grad         0.002485  w[0]    0.018 bias    0.485\n",
      "iter  200/1000000  loss         0.304545  avg_L1_norm_grad         0.001464  w[0]    0.021 bias    0.694\n",
      "iter  201/1000000  loss         0.304196  avg_L1_norm_grad         0.001458  w[0]    0.021 bias    0.695\n",
      "iter  300/1000000  loss         0.279098  avg_L1_norm_grad         0.001070  w[0]    0.021 bias    0.832\n",
      "iter  301/1000000  loss         0.278912  avg_L1_norm_grad         0.001067  w[0]    0.021 bias    0.833\n",
      "iter  400/1000000  loss         0.264323  avg_L1_norm_grad         0.000861  w[0]    0.020 bias    0.935\n",
      "iter  401/1000000  loss         0.264205  avg_L1_norm_grad         0.000859  w[0]    0.020 bias    0.936\n",
      "iter  500/1000000  loss         0.254526  avg_L1_norm_grad         0.000726  w[0]    0.019 bias    1.016\n",
      "iter  501/1000000  loss         0.254444  avg_L1_norm_grad         0.000725  w[0]    0.019 bias    1.017\n",
      "iter  600/1000000  loss         0.247501  avg_L1_norm_grad         0.000631  w[0]    0.018 bias    1.083\n",
      "iter  601/1000000  loss         0.247440  avg_L1_norm_grad         0.000630  w[0]    0.018 bias    1.083\n",
      "iter  700/1000000  loss         0.242196  avg_L1_norm_grad         0.000560  w[0]    0.017 bias    1.139\n",
      "iter  701/1000000  loss         0.242149  avg_L1_norm_grad         0.000559  w[0]    0.017 bias    1.140\n",
      "iter  800/1000000  loss         0.238041  avg_L1_norm_grad         0.000503  w[0]    0.016 bias    1.188\n",
      "iter  801/1000000  loss         0.238004  avg_L1_norm_grad         0.000502  w[0]    0.016 bias    1.188\n",
      "iter  900/1000000  loss         0.234700  avg_L1_norm_grad         0.000457  w[0]    0.016 bias    1.231\n",
      "iter  901/1000000  loss         0.234670  avg_L1_norm_grad         0.000456  w[0]    0.016 bias    1.231\n",
      "iter 1000/1000000  loss         0.231957  avg_L1_norm_grad         0.000418  w[0]    0.015 bias    1.269\n",
      "iter 1001/1000000  loss         0.231932  avg_L1_norm_grad         0.000418  w[0]    0.015 bias    1.269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1100/1000000  loss         0.229669  avg_L1_norm_grad         0.000385  w[0]    0.015 bias    1.303\n",
      "iter 1101/1000000  loss         0.229648  avg_L1_norm_grad         0.000385  w[0]    0.015 bias    1.303\n",
      "iter 1200/1000000  loss         0.227736  avg_L1_norm_grad         0.000356  w[0]    0.015 bias    1.334\n",
      "iter 1201/1000000  loss         0.227718  avg_L1_norm_grad         0.000356  w[0]    0.015 bias    1.334\n",
      "iter 1300/1000000  loss         0.226086  avg_L1_norm_grad         0.000331  w[0]    0.015 bias    1.362\n",
      "iter 1301/1000000  loss         0.226070  avg_L1_norm_grad         0.000331  w[0]    0.015 bias    1.363\n",
      "iter 1400/1000000  loss         0.224665  avg_L1_norm_grad         0.000309  w[0]    0.015 bias    1.388\n",
      "iter 1401/1000000  loss         0.224652  avg_L1_norm_grad         0.000309  w[0]    0.015 bias    1.389\n",
      "iter 1500/1000000  loss         0.223433  avg_L1_norm_grad         0.000289  w[0]    0.016 bias    1.412\n",
      "iter 1501/1000000  loss         0.223421  avg_L1_norm_grad         0.000289  w[0]    0.016 bias    1.412\n",
      "iter 1600/1000000  loss         0.222358  avg_L1_norm_grad         0.000271  w[0]    0.016 bias    1.435\n",
      "iter 1601/1000000  loss         0.222348  avg_L1_norm_grad         0.000271  w[0]    0.016 bias    1.435\n",
      "iter 1700/1000000  loss         0.221415  avg_L1_norm_grad         0.000255  w[0]    0.016 bias    1.455\n",
      "iter 1701/1000000  loss         0.221406  avg_L1_norm_grad         0.000255  w[0]    0.016 bias    1.455\n",
      "iter 1800/1000000  loss         0.220583  avg_L1_norm_grad         0.000240  w[0]    0.017 bias    1.475\n",
      "iter 1801/1000000  loss         0.220575  avg_L1_norm_grad         0.000240  w[0]    0.017 bias    1.475\n",
      "iter 1900/1000000  loss         0.219847  avg_L1_norm_grad         0.000226  w[0]    0.018 bias    1.493\n",
      "iter 1901/1000000  loss         0.219841  avg_L1_norm_grad         0.000226  w[0]    0.018 bias    1.493\n",
      "iter 2000/1000000  loss         0.219194  avg_L1_norm_grad         0.000214  w[0]    0.018 bias    1.510\n",
      "iter 2001/1000000  loss         0.219188  avg_L1_norm_grad         0.000214  w[0]    0.018 bias    1.510\n",
      "iter 2100/1000000  loss         0.218612  avg_L1_norm_grad         0.000202  w[0]    0.019 bias    1.526\n",
      "iter 2101/1000000  loss         0.218606  avg_L1_norm_grad         0.000202  w[0]    0.019 bias    1.526\n",
      "iter 2200/1000000  loss         0.218091  avg_L1_norm_grad         0.000191  w[0]    0.020 bias    1.541\n",
      "iter 2201/1000000  loss         0.218086  avg_L1_norm_grad         0.000191  w[0]    0.020 bias    1.541\n",
      "iter 2300/1000000  loss         0.217625  avg_L1_norm_grad         0.000181  w[0]    0.021 bias    1.556\n",
      "iter 2301/1000000  loss         0.217620  avg_L1_norm_grad         0.000181  w[0]    0.021 bias    1.556\n",
      "iter 2400/1000000  loss         0.217206  avg_L1_norm_grad         0.000172  w[0]    0.022 bias    1.569\n",
      "iter 2401/1000000  loss         0.217202  avg_L1_norm_grad         0.000172  w[0]    0.022 bias    1.569\n",
      "iter 2500/1000000  loss         0.216828  avg_L1_norm_grad         0.000164  w[0]    0.022 bias    1.582\n",
      "iter 2501/1000000  loss         0.216825  avg_L1_norm_grad         0.000163  w[0]    0.022 bias    1.582\n",
      "iter 2600/1000000  loss         0.216488  avg_L1_norm_grad         0.000156  w[0]    0.023 bias    1.594\n",
      "iter 2601/1000000  loss         0.216485  avg_L1_norm_grad         0.000155  w[0]    0.023 bias    1.594\n",
      "iter 2700/1000000  loss         0.216180  avg_L1_norm_grad         0.000148  w[0]    0.024 bias    1.606\n",
      "iter 2701/1000000  loss         0.216177  avg_L1_norm_grad         0.000148  w[0]    0.024 bias    1.606\n",
      "iter 2800/1000000  loss         0.215902  avg_L1_norm_grad         0.000141  w[0]    0.025 bias    1.617\n",
      "iter 2801/1000000  loss         0.215899  avg_L1_norm_grad         0.000141  w[0]    0.025 bias    1.617\n",
      "iter 2900/1000000  loss         0.215649  avg_L1_norm_grad         0.000134  w[0]    0.026 bias    1.627\n",
      "iter 2901/1000000  loss         0.215647  avg_L1_norm_grad         0.000134  w[0]    0.026 bias    1.628\n",
      "iter 3000/1000000  loss         0.215420  avg_L1_norm_grad         0.000128  w[0]    0.027 bias    1.637\n",
      "iter 3001/1000000  loss         0.215417  avg_L1_norm_grad         0.000128  w[0]    0.027 bias    1.638\n",
      "iter 3100/1000000  loss         0.215211  avg_L1_norm_grad         0.000122  w[0]    0.028 bias    1.647\n",
      "iter 3101/1000000  loss         0.215209  avg_L1_norm_grad         0.000122  w[0]    0.028 bias    1.647\n",
      "iter 3200/1000000  loss         0.215021  avg_L1_norm_grad         0.000117  w[0]    0.029 bias    1.656\n",
      "iter 3201/1000000  loss         0.215019  avg_L1_norm_grad         0.000117  w[0]    0.029 bias    1.656\n",
      "iter 3300/1000000  loss         0.214847  avg_L1_norm_grad         0.000112  w[0]    0.030 bias    1.665\n",
      "iter 3301/1000000  loss         0.214846  avg_L1_norm_grad         0.000112  w[0]    0.030 bias    1.665\n",
      "iter 3400/1000000  loss         0.214689  avg_L1_norm_grad         0.000107  w[0]    0.031 bias    1.673\n",
      "iter 3401/1000000  loss         0.214687  avg_L1_norm_grad         0.000107  w[0]    0.031 bias    1.673\n",
      "iter 3500/1000000  loss         0.214544  avg_L1_norm_grad         0.000102  w[0]    0.032 bias    1.681\n",
      "iter 3501/1000000  loss         0.214543  avg_L1_norm_grad         0.000102  w[0]    0.032 bias    1.681\n",
      "iter 3600/1000000  loss         0.214411  avg_L1_norm_grad         0.000098  w[0]    0.032 bias    1.689\n",
      "iter 3601/1000000  loss         0.214410  avg_L1_norm_grad         0.000098  w[0]    0.032 bias    1.689\n",
      "iter 3700/1000000  loss         0.214290  avg_L1_norm_grad         0.000094  w[0]    0.033 bias    1.696\n",
      "iter 3701/1000000  loss         0.214289  avg_L1_norm_grad         0.000094  w[0]    0.033 bias    1.696\n",
      "iter 3800/1000000  loss         0.214179  avg_L1_norm_grad         0.000090  w[0]    0.034 bias    1.703\n",
      "iter 3801/1000000  loss         0.214178  avg_L1_norm_grad         0.000090  w[0]    0.034 bias    1.703\n",
      "iter 3900/1000000  loss         0.214077  avg_L1_norm_grad         0.000086  w[0]    0.035 bias    1.710\n",
      "iter 3901/1000000  loss         0.214076  avg_L1_norm_grad         0.000086  w[0]    0.035 bias    1.710\n",
      "iter 4000/1000000  loss         0.213983  avg_L1_norm_grad         0.000083  w[0]    0.036 bias    1.716\n",
      "iter 4001/1000000  loss         0.213982  avg_L1_norm_grad         0.000083  w[0]    0.036 bias    1.716\n",
      "iter 4100/1000000  loss         0.213896  avg_L1_norm_grad         0.000079  w[0]    0.037 bias    1.722\n",
      "iter 4101/1000000  loss         0.213896  avg_L1_norm_grad         0.000079  w[0]    0.037 bias    1.722\n",
      "iter 4200/1000000  loss         0.213817  avg_L1_norm_grad         0.000076  w[0]    0.038 bias    1.728\n",
      "iter 4201/1000000  loss         0.213816  avg_L1_norm_grad         0.000076  w[0]    0.038 bias    1.728\n",
      "iter 4300/1000000  loss         0.213744  avg_L1_norm_grad         0.000073  w[0]    0.038 bias    1.734\n",
      "iter 4301/1000000  loss         0.213743  avg_L1_norm_grad         0.000073  w[0]    0.038 bias    1.734\n",
      "iter 4400/1000000  loss         0.213677  avg_L1_norm_grad         0.000070  w[0]    0.039 bias    1.739\n",
      "iter 4401/1000000  loss         0.213676  avg_L1_norm_grad         0.000070  w[0]    0.039 bias    1.739\n",
      "iter 4500/1000000  loss         0.213614  avg_L1_norm_grad         0.000067  w[0]    0.040 bias    1.744\n",
      "iter 4501/1000000  loss         0.213614  avg_L1_norm_grad         0.000067  w[0]    0.040 bias    1.744\n",
      "iter 4600/1000000  loss         0.213557  avg_L1_norm_grad         0.000065  w[0]    0.041 bias    1.749\n",
      "iter 4601/1000000  loss         0.213557  avg_L1_norm_grad         0.000065  w[0]    0.041 bias    1.749\n",
      "iter 4700/1000000  loss         0.213504  avg_L1_norm_grad         0.000062  w[0]    0.042 bias    1.754\n",
      "iter 4701/1000000  loss         0.213504  avg_L1_norm_grad         0.000062  w[0]    0.042 bias    1.754\n",
      "iter 4800/1000000  loss         0.213455  avg_L1_norm_grad         0.000060  w[0]    0.042 bias    1.759\n",
      "iter 4801/1000000  loss         0.213455  avg_L1_norm_grad         0.000060  w[0]    0.042 bias    1.759\n",
      "iter 4900/1000000  loss         0.213410  avg_L1_norm_grad         0.000058  w[0]    0.043 bias    1.763\n",
      "iter 4901/1000000  loss         0.213410  avg_L1_norm_grad         0.000058  w[0]    0.043 bias    1.763\n",
      "iter 5000/1000000  loss         0.213368  avg_L1_norm_grad         0.000055  w[0]    0.044 bias    1.767\n",
      "iter 5001/1000000  loss         0.213368  avg_L1_norm_grad         0.000055  w[0]    0.044 bias    1.767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5100/1000000  loss         0.213330  avg_L1_norm_grad         0.000053  w[0]    0.044 bias    1.771\n",
      "iter 5101/1000000  loss         0.213329  avg_L1_norm_grad         0.000053  w[0]    0.044 bias    1.771\n",
      "iter 5200/1000000  loss         0.213294  avg_L1_norm_grad         0.000051  w[0]    0.045 bias    1.775\n",
      "iter 5201/1000000  loss         0.213293  avg_L1_norm_grad         0.000051  w[0]    0.045 bias    1.775\n",
      "iter 5300/1000000  loss         0.213261  avg_L1_norm_grad         0.000049  w[0]    0.046 bias    1.779\n",
      "iter 5301/1000000  loss         0.213260  avg_L1_norm_grad         0.000049  w[0]    0.046 bias    1.779\n",
      "iter 5400/1000000  loss         0.213230  avg_L1_norm_grad         0.000048  w[0]    0.046 bias    1.783\n",
      "iter 5401/1000000  loss         0.213230  avg_L1_norm_grad         0.000048  w[0]    0.046 bias    1.783\n",
      "iter 5500/1000000  loss         0.213201  avg_L1_norm_grad         0.000046  w[0]    0.047 bias    1.786\n",
      "iter 5501/1000000  loss         0.213201  avg_L1_norm_grad         0.000046  w[0]    0.047 bias    1.786\n",
      "iter 5600/1000000  loss         0.213175  avg_L1_norm_grad         0.000044  w[0]    0.048 bias    1.790\n",
      "iter 5601/1000000  loss         0.213175  avg_L1_norm_grad         0.000044  w[0]    0.048 bias    1.790\n",
      "iter 5700/1000000  loss         0.213150  avg_L1_norm_grad         0.000043  w[0]    0.048 bias    1.793\n",
      "iter 5701/1000000  loss         0.213150  avg_L1_norm_grad         0.000043  w[0]    0.048 bias    1.793\n",
      "iter 5800/1000000  loss         0.213128  avg_L1_norm_grad         0.000041  w[0]    0.049 bias    1.796\n",
      "iter 5801/1000000  loss         0.213127  avg_L1_norm_grad         0.000041  w[0]    0.049 bias    1.796\n",
      "iter 5900/1000000  loss         0.213106  avg_L1_norm_grad         0.000040  w[0]    0.049 bias    1.799\n",
      "iter 5901/1000000  loss         0.213106  avg_L1_norm_grad         0.000040  w[0]    0.049 bias    1.799\n",
      "iter 6000/1000000  loss         0.213087  avg_L1_norm_grad         0.000038  w[0]    0.050 bias    1.802\n",
      "iter 6001/1000000  loss         0.213087  avg_L1_norm_grad         0.000038  w[0]    0.050 bias    1.802\n",
      "iter 6100/1000000  loss         0.213068  avg_L1_norm_grad         0.000037  w[0]    0.051 bias    1.805\n",
      "iter 6101/1000000  loss         0.213068  avg_L1_norm_grad         0.000037  w[0]    0.051 bias    1.805\n",
      "iter 6200/1000000  loss         0.213051  avg_L1_norm_grad         0.000036  w[0]    0.051 bias    1.807\n",
      "iter 6201/1000000  loss         0.213051  avg_L1_norm_grad         0.000036  w[0]    0.051 bias    1.807\n",
      "iter 6300/1000000  loss         0.213036  avg_L1_norm_grad         0.000034  w[0]    0.052 bias    1.810\n",
      "iter 6301/1000000  loss         0.213035  avg_L1_norm_grad         0.000034  w[0]    0.052 bias    1.810\n",
      "iter 6400/1000000  loss         0.213021  avg_L1_norm_grad         0.000033  w[0]    0.052 bias    1.812\n",
      "iter 6401/1000000  loss         0.213021  avg_L1_norm_grad         0.000033  w[0]    0.052 bias    1.812\n",
      "iter 6500/1000000  loss         0.213007  avg_L1_norm_grad         0.000032  w[0]    0.053 bias    1.815\n",
      "iter 6501/1000000  loss         0.213007  avg_L1_norm_grad         0.000032  w[0]    0.053 bias    1.815\n",
      "iter 6600/1000000  loss         0.212994  avg_L1_norm_grad         0.000031  w[0]    0.053 bias    1.817\n",
      "iter 6601/1000000  loss         0.212994  avg_L1_norm_grad         0.000031  w[0]    0.053 bias    1.817\n",
      "iter 6700/1000000  loss         0.212982  avg_L1_norm_grad         0.000030  w[0]    0.054 bias    1.819\n",
      "iter 6701/1000000  loss         0.212982  avg_L1_norm_grad         0.000030  w[0]    0.054 bias    1.819\n",
      "iter 6800/1000000  loss         0.212971  avg_L1_norm_grad         0.000029  w[0]    0.054 bias    1.821\n",
      "iter 6801/1000000  loss         0.212971  avg_L1_norm_grad         0.000029  w[0]    0.054 bias    1.821\n",
      "iter 6900/1000000  loss         0.212961  avg_L1_norm_grad         0.000028  w[0]    0.055 bias    1.823\n",
      "iter 6901/1000000  loss         0.212961  avg_L1_norm_grad         0.000028  w[0]    0.055 bias    1.823\n",
      "iter 7000/1000000  loss         0.212951  avg_L1_norm_grad         0.000027  w[0]    0.055 bias    1.825\n",
      "iter 7001/1000000  loss         0.212951  avg_L1_norm_grad         0.000027  w[0]    0.055 bias    1.825\n",
      "iter 7100/1000000  loss         0.212942  avg_L1_norm_grad         0.000026  w[0]    0.055 bias    1.827\n",
      "iter 7101/1000000  loss         0.212942  avg_L1_norm_grad         0.000026  w[0]    0.055 bias    1.827\n",
      "iter 7200/1000000  loss         0.212934  avg_L1_norm_grad         0.000025  w[0]    0.056 bias    1.829\n",
      "iter 7201/1000000  loss         0.212934  avg_L1_norm_grad         0.000025  w[0]    0.056 bias    1.829\n",
      "iter 7300/1000000  loss         0.212926  avg_L1_norm_grad         0.000024  w[0]    0.056 bias    1.831\n",
      "iter 7301/1000000  loss         0.212926  avg_L1_norm_grad         0.000024  w[0]    0.056 bias    1.831\n",
      "iter 7400/1000000  loss         0.212919  avg_L1_norm_grad         0.000023  w[0]    0.057 bias    1.833\n",
      "iter 7401/1000000  loss         0.212919  avg_L1_norm_grad         0.000023  w[0]    0.057 bias    1.833\n",
      "iter 7500/1000000  loss         0.212912  avg_L1_norm_grad         0.000023  w[0]    0.057 bias    1.834\n",
      "iter 7501/1000000  loss         0.212912  avg_L1_norm_grad         0.000023  w[0]    0.057 bias    1.834\n",
      "iter 7600/1000000  loss         0.212906  avg_L1_norm_grad         0.000022  w[0]    0.057 bias    1.836\n",
      "iter 7601/1000000  loss         0.212906  avg_L1_norm_grad         0.000022  w[0]    0.057 bias    1.836\n",
      "iter 7700/1000000  loss         0.212900  avg_L1_norm_grad         0.000021  w[0]    0.058 bias    1.837\n",
      "iter 7701/1000000  loss         0.212900  avg_L1_norm_grad         0.000021  w[0]    0.058 bias    1.837\n",
      "iter 7800/1000000  loss         0.212894  avg_L1_norm_grad         0.000020  w[0]    0.058 bias    1.839\n",
      "iter 7801/1000000  loss         0.212894  avg_L1_norm_grad         0.000020  w[0]    0.058 bias    1.839\n",
      "iter 7900/1000000  loss         0.212889  avg_L1_norm_grad         0.000020  w[0]    0.058 bias    1.840\n",
      "iter 7901/1000000  loss         0.212889  avg_L1_norm_grad         0.000020  w[0]    0.058 bias    1.840\n",
      "iter 8000/1000000  loss         0.212884  avg_L1_norm_grad         0.000019  w[0]    0.059 bias    1.842\n",
      "iter 8001/1000000  loss         0.212884  avg_L1_norm_grad         0.000019  w[0]    0.059 bias    1.842\n",
      "iter 8100/1000000  loss         0.212880  avg_L1_norm_grad         0.000019  w[0]    0.059 bias    1.843\n",
      "iter 8101/1000000  loss         0.212880  avg_L1_norm_grad         0.000019  w[0]    0.059 bias    1.843\n",
      "iter 8200/1000000  loss         0.212875  avg_L1_norm_grad         0.000018  w[0]    0.059 bias    1.844\n",
      "iter 8201/1000000  loss         0.212875  avg_L1_norm_grad         0.000018  w[0]    0.059 bias    1.844\n",
      "iter 8300/1000000  loss         0.212872  avg_L1_norm_grad         0.000017  w[0]    0.060 bias    1.846\n",
      "iter 8301/1000000  loss         0.212871  avg_L1_norm_grad         0.000017  w[0]    0.060 bias    1.846\n",
      "iter 8400/1000000  loss         0.212868  avg_L1_norm_grad         0.000017  w[0]    0.060 bias    1.847\n",
      "iter 8401/1000000  loss         0.212868  avg_L1_norm_grad         0.000017  w[0]    0.060 bias    1.847\n",
      "iter 8500/1000000  loss         0.212864  avg_L1_norm_grad         0.000016  w[0]    0.060 bias    1.848\n",
      "iter 8501/1000000  loss         0.212864  avg_L1_norm_grad         0.000016  w[0]    0.060 bias    1.848\n",
      "iter 8600/1000000  loss         0.212861  avg_L1_norm_grad         0.000016  w[0]    0.061 bias    1.849\n",
      "iter 8601/1000000  loss         0.212861  avg_L1_norm_grad         0.000016  w[0]    0.061 bias    1.849\n",
      "iter 8700/1000000  loss         0.212858  avg_L1_norm_grad         0.000015  w[0]    0.061 bias    1.850\n",
      "iter 8701/1000000  loss         0.212858  avg_L1_norm_grad         0.000015  w[0]    0.061 bias    1.850\n",
      "iter 8800/1000000  loss         0.212855  avg_L1_norm_grad         0.000015  w[0]    0.061 bias    1.851\n",
      "iter 8801/1000000  loss         0.212855  avg_L1_norm_grad         0.000015  w[0]    0.061 bias    1.851\n",
      "iter 8900/1000000  loss         0.212853  avg_L1_norm_grad         0.000014  w[0]    0.061 bias    1.852\n",
      "iter 8901/1000000  loss         0.212852  avg_L1_norm_grad         0.000014  w[0]    0.061 bias    1.852\n",
      "Done. Converged after 8903 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9344444444444184\n",
      "Ave Loaded\n",
      "No Noise New 0.9352777777777518\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030456  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.910560  avg_L1_norm_grad         0.028713  w[0]    0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.846076  avg_L1_norm_grad         0.019682  w[0]    0.001 bias    0.021\n",
      "iter    3/1000000  loss         0.801656  avg_L1_norm_grad         0.018497  w[0]    0.001 bias    0.025\n",
      "iter    4/1000000  loss         0.768070  avg_L1_norm_grad         0.013993  w[0]    0.001 bias    0.040\n",
      "iter    5/1000000  loss         0.742191  avg_L1_norm_grad         0.013388  w[0]    0.002 bias    0.047\n",
      "iter    6/1000000  loss         0.721047  avg_L1_norm_grad         0.011426  w[0]    0.002 bias    0.059\n",
      "iter    7/1000000  loss         0.703121  avg_L1_norm_grad         0.010935  w[0]    0.002 bias    0.067\n",
      "iter    8/1000000  loss         0.687428  avg_L1_norm_grad         0.010022  w[0]    0.002 bias    0.077\n",
      "iter    9/1000000  loss         0.673385  avg_L1_norm_grad         0.009563  w[0]    0.003 bias    0.086\n",
      "iter   10/1000000  loss         0.660617  avg_L1_norm_grad         0.009043  w[0]    0.003 bias    0.095\n",
      "iter   11/1000000  loss         0.648870  avg_L1_norm_grad         0.008654  w[0]    0.003 bias    0.104\n",
      "iter   12/1000000  loss         0.637967  avg_L1_norm_grad         0.008292  w[0]    0.003 bias    0.113\n",
      "iter   13/1000000  loss         0.627776  avg_L1_norm_grad         0.007979  w[0]    0.004 bias    0.122\n",
      "iter   14/1000000  loss         0.618200  avg_L1_norm_grad         0.007698  w[0]    0.004 bias    0.130\n",
      "iter   15/1000000  loss         0.609160  avg_L1_norm_grad         0.007446  w[0]    0.004 bias    0.138\n",
      "iter   16/1000000  loss         0.600598  avg_L1_norm_grad         0.007217  w[0]    0.004 bias    0.146\n",
      "iter   17/1000000  loss         0.592464  avg_L1_norm_grad         0.007008  w[0]    0.005 bias    0.155\n",
      "iter   18/1000000  loss         0.584719  avg_L1_norm_grad         0.006814  w[0]    0.005 bias    0.162\n",
      "iter   19/1000000  loss         0.577328  avg_L1_norm_grad         0.006634  w[0]    0.005 bias    0.170\n",
      "iter  100/1000000  loss         0.360751  avg_L1_norm_grad         0.002316  w[0]    0.012 bias    0.589\n",
      "iter  101/1000000  loss         0.359706  avg_L1_norm_grad         0.002299  w[0]    0.012 bias    0.593\n",
      "iter  200/1000000  loss         0.297322  avg_L1_norm_grad         0.001376  w[0]    0.013 bias    0.887\n",
      "iter  201/1000000  loss         0.296931  avg_L1_norm_grad         0.001370  w[0]    0.013 bias    0.889\n",
      "iter  300/1000000  loss         0.268421  avg_L1_norm_grad         0.001019  w[0]    0.011 bias    1.098\n",
      "iter  301/1000000  loss         0.268206  avg_L1_norm_grad         0.001017  w[0]    0.011 bias    1.100\n",
      "iter  400/1000000  loss         0.251114  avg_L1_norm_grad         0.000830  w[0]    0.009 bias    1.264\n",
      "iter  401/1000000  loss         0.250975  avg_L1_norm_grad         0.000828  w[0]    0.009 bias    1.266\n",
      "iter  500/1000000  loss         0.239307  avg_L1_norm_grad         0.000713  w[0]    0.006 bias    1.401\n",
      "iter  501/1000000  loss         0.239207  avg_L1_norm_grad         0.000712  w[0]    0.006 bias    1.402\n",
      "iter  600/1000000  loss         0.230598  avg_L1_norm_grad         0.000631  w[0]    0.004 bias    1.518\n",
      "iter  601/1000000  loss         0.230522  avg_L1_norm_grad         0.000631  w[0]    0.004 bias    1.519\n",
      "iter  700/1000000  loss         0.223834  avg_L1_norm_grad         0.000570  w[0]    0.001 bias    1.620\n",
      "iter  701/1000000  loss         0.223774  avg_L1_norm_grad         0.000570  w[0]    0.001 bias    1.621\n",
      "iter  800/1000000  loss         0.218382  avg_L1_norm_grad         0.000523  w[0]   -0.001 bias    1.711\n",
      "iter  801/1000000  loss         0.218333  avg_L1_norm_grad         0.000522  w[0]   -0.001 bias    1.712\n",
      "iter  900/1000000  loss         0.213867  avg_L1_norm_grad         0.000484  w[0]   -0.004 bias    1.792\n",
      "iter  901/1000000  loss         0.213825  avg_L1_norm_grad         0.000483  w[0]   -0.004 bias    1.793\n",
      "iter 1000/1000000  loss         0.210047  avg_L1_norm_grad         0.000451  w[0]   -0.006 bias    1.866\n",
      "iter 1001/1000000  loss         0.210012  avg_L1_norm_grad         0.000451  w[0]   -0.006 bias    1.867\n",
      "iter 1100/1000000  loss         0.206762  avg_L1_norm_grad         0.000423  w[0]   -0.008 bias    1.934\n",
      "iter 1101/1000000  loss         0.206732  avg_L1_norm_grad         0.000423  w[0]   -0.008 bias    1.934\n",
      "iter 1200/1000000  loss         0.203899  avg_L1_norm_grad         0.000399  w[0]   -0.009 bias    1.996\n",
      "iter 1201/1000000  loss         0.203872  avg_L1_norm_grad         0.000399  w[0]   -0.009 bias    1.996\n",
      "iter 1300/1000000  loss         0.201376  avg_L1_norm_grad         0.000378  w[0]   -0.011 bias    2.053\n",
      "iter 1301/1000000  loss         0.201352  avg_L1_norm_grad         0.000378  w[0]   -0.011 bias    2.054\n",
      "iter 1400/1000000  loss         0.199132  avg_L1_norm_grad         0.000360  w[0]   -0.012 bias    2.106\n",
      "iter 1401/1000000  loss         0.199110  avg_L1_norm_grad         0.000359  w[0]   -0.012 bias    2.107\n",
      "iter 1500/1000000  loss         0.197120  avg_L1_norm_grad         0.000343  w[0]   -0.014 bias    2.156\n",
      "iter 1501/1000000  loss         0.197101  avg_L1_norm_grad         0.000343  w[0]   -0.014 bias    2.157\n",
      "iter 1600/1000000  loss         0.195305  avg_L1_norm_grad         0.000328  w[0]   -0.015 bias    2.203\n",
      "iter 1601/1000000  loss         0.195288  avg_L1_norm_grad         0.000328  w[0]   -0.015 bias    2.203\n",
      "iter 1700/1000000  loss         0.193658  avg_L1_norm_grad         0.000314  w[0]   -0.016 bias    2.247\n",
      "iter 1701/1000000  loss         0.193642  avg_L1_norm_grad         0.000314  w[0]   -0.016 bias    2.247\n",
      "iter 1800/1000000  loss         0.192155  avg_L1_norm_grad         0.000302  w[0]   -0.017 bias    2.288\n",
      "iter 1801/1000000  loss         0.192141  avg_L1_norm_grad         0.000301  w[0]   -0.017 bias    2.289\n",
      "iter 1900/1000000  loss         0.190778  avg_L1_norm_grad         0.000290  w[0]   -0.018 bias    2.327\n",
      "iter 1901/1000000  loss         0.190765  avg_L1_norm_grad         0.000290  w[0]   -0.018 bias    2.328\n",
      "iter 2000/1000000  loss         0.189512  avg_L1_norm_grad         0.000280  w[0]   -0.018 bias    2.364\n",
      "iter 2001/1000000  loss         0.189499  avg_L1_norm_grad         0.000279  w[0]   -0.018 bias    2.365\n",
      "iter 2100/1000000  loss         0.188342  avg_L1_norm_grad         0.000270  w[0]   -0.019 bias    2.399\n",
      "iter 2101/1000000  loss         0.188331  avg_L1_norm_grad         0.000270  w[0]   -0.019 bias    2.400\n",
      "iter 2200/1000000  loss         0.187258  avg_L1_norm_grad         0.000261  w[0]   -0.019 bias    2.433\n",
      "iter 2201/1000000  loss         0.187248  avg_L1_norm_grad         0.000261  w[0]   -0.019 bias    2.433\n",
      "iter 2300/1000000  loss         0.186252  avg_L1_norm_grad         0.000252  w[0]   -0.020 bias    2.464\n",
      "iter 2301/1000000  loss         0.186242  avg_L1_norm_grad         0.000252  w[0]   -0.020 bias    2.465\n",
      "iter 2400/1000000  loss         0.185314  avg_L1_norm_grad         0.000244  w[0]   -0.020 bias    2.495\n",
      "iter 2401/1000000  loss         0.185305  avg_L1_norm_grad         0.000244  w[0]   -0.020 bias    2.495\n",
      "iter 2500/1000000  loss         0.184438  avg_L1_norm_grad         0.000237  w[0]   -0.020 bias    2.523\n",
      "iter 2501/1000000  loss         0.184430  avg_L1_norm_grad         0.000237  w[0]   -0.020 bias    2.524\n",
      "iter 2600/1000000  loss         0.183619  avg_L1_norm_grad         0.000230  w[0]   -0.021 bias    2.551\n",
      "iter 2601/1000000  loss         0.183611  avg_L1_norm_grad         0.000230  w[0]   -0.021 bias    2.551\n",
      "iter 2700/1000000  loss         0.182850  avg_L1_norm_grad         0.000223  w[0]   -0.021 bias    2.578\n",
      "iter 2701/1000000  loss         0.182843  avg_L1_norm_grad         0.000223  w[0]   -0.021 bias    2.578\n",
      "iter 2800/1000000  loss         0.182128  avg_L1_norm_grad         0.000217  w[0]   -0.021 bias    2.603\n",
      "iter 2801/1000000  loss         0.182121  avg_L1_norm_grad         0.000217  w[0]   -0.021 bias    2.603\n",
      "iter 2900/1000000  loss         0.181447  avg_L1_norm_grad         0.000211  w[0]   -0.021 bias    2.627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.181441  avg_L1_norm_grad         0.000211  w[0]   -0.021 bias    2.627\n",
      "iter 3000/1000000  loss         0.180806  avg_L1_norm_grad         0.000206  w[0]   -0.021 bias    2.651\n",
      "iter 3001/1000000  loss         0.180800  avg_L1_norm_grad         0.000206  w[0]   -0.021 bias    2.651\n",
      "iter 3100/1000000  loss         0.180200  avg_L1_norm_grad         0.000200  w[0]   -0.021 bias    2.673\n",
      "iter 3101/1000000  loss         0.180194  avg_L1_norm_grad         0.000200  w[0]   -0.021 bias    2.673\n",
      "iter 3200/1000000  loss         0.179627  avg_L1_norm_grad         0.000195  w[0]   -0.020 bias    2.695\n",
      "iter 3201/1000000  loss         0.179622  avg_L1_norm_grad         0.000195  w[0]   -0.020 bias    2.695\n",
      "iter 3300/1000000  loss         0.179084  avg_L1_norm_grad         0.000191  w[0]   -0.020 bias    2.715\n",
      "iter 3301/1000000  loss         0.179079  avg_L1_norm_grad         0.000191  w[0]   -0.020 bias    2.716\n",
      "iter 3400/1000000  loss         0.178570  avg_L1_norm_grad         0.000186  w[0]   -0.020 bias    2.736\n",
      "iter 3401/1000000  loss         0.178565  avg_L1_norm_grad         0.000186  w[0]   -0.020 bias    2.736\n",
      "iter 3500/1000000  loss         0.178081  avg_L1_norm_grad         0.000182  w[0]   -0.020 bias    2.755\n",
      "iter 3501/1000000  loss         0.178076  avg_L1_norm_grad         0.000182  w[0]   -0.020 bias    2.755\n",
      "iter 3600/1000000  loss         0.177616  avg_L1_norm_grad         0.000178  w[0]   -0.019 bias    2.774\n",
      "iter 3601/1000000  loss         0.177612  avg_L1_norm_grad         0.000178  w[0]   -0.019 bias    2.774\n",
      "iter 3700/1000000  loss         0.177174  avg_L1_norm_grad         0.000174  w[0]   -0.019 bias    2.792\n",
      "iter 3701/1000000  loss         0.177170  avg_L1_norm_grad         0.000174  w[0]   -0.019 bias    2.792\n",
      "iter 3800/1000000  loss         0.176753  avg_L1_norm_grad         0.000170  w[0]   -0.019 bias    2.809\n",
      "iter 3801/1000000  loss         0.176749  avg_L1_norm_grad         0.000170  w[0]   -0.019 bias    2.809\n",
      "iter 3900/1000000  loss         0.176351  avg_L1_norm_grad         0.000166  w[0]   -0.018 bias    2.826\n",
      "iter 3901/1000000  loss         0.176347  avg_L1_norm_grad         0.000166  w[0]   -0.018 bias    2.826\n",
      "iter 4000/1000000  loss         0.175968  avg_L1_norm_grad         0.000163  w[0]   -0.018 bias    2.842\n",
      "iter 4001/1000000  loss         0.175964  avg_L1_norm_grad         0.000163  w[0]   -0.018 bias    2.842\n",
      "iter 4100/1000000  loss         0.175601  avg_L1_norm_grad         0.000159  w[0]   -0.017 bias    2.858\n",
      "iter 4101/1000000  loss         0.175598  avg_L1_norm_grad         0.000159  w[0]   -0.017 bias    2.858\n",
      "iter 4200/1000000  loss         0.175251  avg_L1_norm_grad         0.000156  w[0]   -0.017 bias    2.873\n",
      "iter 4201/1000000  loss         0.175248  avg_L1_norm_grad         0.000156  w[0]   -0.017 bias    2.874\n",
      "iter 4300/1000000  loss         0.174916  avg_L1_norm_grad         0.000153  w[0]   -0.016 bias    2.888\n",
      "iter 4301/1000000  loss         0.174913  avg_L1_norm_grad         0.000153  w[0]   -0.016 bias    2.888\n",
      "iter 4400/1000000  loss         0.174595  avg_L1_norm_grad         0.000150  w[0]   -0.016 bias    2.903\n",
      "iter 4401/1000000  loss         0.174592  avg_L1_norm_grad         0.000150  w[0]   -0.016 bias    2.903\n",
      "iter 4500/1000000  loss         0.174288  avg_L1_norm_grad         0.000147  w[0]   -0.015 bias    2.917\n",
      "iter 4501/1000000  loss         0.174285  avg_L1_norm_grad         0.000147  w[0]   -0.015 bias    2.917\n",
      "iter 4600/1000000  loss         0.173993  avg_L1_norm_grad         0.000144  w[0]   -0.015 bias    2.930\n",
      "iter 4601/1000000  loss         0.173990  avg_L1_norm_grad         0.000144  w[0]   -0.015 bias    2.930\n",
      "iter 4700/1000000  loss         0.173710  avg_L1_norm_grad         0.000142  w[0]   -0.014 bias    2.943\n",
      "iter 4701/1000000  loss         0.173707  avg_L1_norm_grad         0.000141  w[0]   -0.014 bias    2.944\n",
      "iter 4800/1000000  loss         0.173439  avg_L1_norm_grad         0.000139  w[0]   -0.013 bias    2.956\n",
      "iter 4801/1000000  loss         0.173436  avg_L1_norm_grad         0.000139  w[0]   -0.013 bias    2.956\n",
      "iter 4900/1000000  loss         0.173178  avg_L1_norm_grad         0.000136  w[0]   -0.013 bias    2.969\n",
      "iter 4901/1000000  loss         0.173175  avg_L1_norm_grad         0.000136  w[0]   -0.013 bias    2.969\n",
      "iter 5000/1000000  loss         0.172927  avg_L1_norm_grad         0.000134  w[0]   -0.012 bias    2.981\n",
      "iter 5001/1000000  loss         0.172924  avg_L1_norm_grad         0.000134  w[0]   -0.012 bias    2.981\n",
      "iter 5100/1000000  loss         0.172685  avg_L1_norm_grad         0.000132  w[0]   -0.011 bias    2.993\n",
      "iter 5101/1000000  loss         0.172683  avg_L1_norm_grad         0.000132  w[0]   -0.011 bias    2.993\n",
      "iter 5200/1000000  loss         0.172453  avg_L1_norm_grad         0.000129  w[0]   -0.011 bias    3.004\n",
      "iter 5201/1000000  loss         0.172451  avg_L1_norm_grad         0.000129  w[0]   -0.011 bias    3.004\n",
      "iter 5300/1000000  loss         0.172230  avg_L1_norm_grad         0.000127  w[0]   -0.010 bias    3.015\n",
      "iter 5301/1000000  loss         0.172228  avg_L1_norm_grad         0.000127  w[0]   -0.010 bias    3.015\n",
      "iter 5400/1000000  loss         0.172014  avg_L1_norm_grad         0.000125  w[0]   -0.009 bias    3.026\n",
      "iter 5401/1000000  loss         0.172012  avg_L1_norm_grad         0.000125  w[0]   -0.009 bias    3.026\n",
      "iter 5500/1000000  loss         0.171807  avg_L1_norm_grad         0.000123  w[0]   -0.009 bias    3.037\n",
      "iter 5501/1000000  loss         0.171805  avg_L1_norm_grad         0.000123  w[0]   -0.009 bias    3.037\n",
      "iter 5600/1000000  loss         0.171607  avg_L1_norm_grad         0.000121  w[0]   -0.008 bias    3.047\n",
      "iter 5601/1000000  loss         0.171605  avg_L1_norm_grad         0.000121  w[0]   -0.008 bias    3.047\n",
      "iter 5700/1000000  loss         0.171414  avg_L1_norm_grad         0.000119  w[0]   -0.007 bias    3.057\n",
      "iter 5701/1000000  loss         0.171412  avg_L1_norm_grad         0.000119  w[0]   -0.007 bias    3.057\n",
      "iter 5800/1000000  loss         0.171228  avg_L1_norm_grad         0.000117  w[0]   -0.006 bias    3.067\n",
      "iter 5801/1000000  loss         0.171226  avg_L1_norm_grad         0.000117  w[0]   -0.006 bias    3.067\n",
      "iter 5900/1000000  loss         0.171048  avg_L1_norm_grad         0.000115  w[0]   -0.006 bias    3.077\n",
      "iter 5901/1000000  loss         0.171046  avg_L1_norm_grad         0.000115  w[0]   -0.006 bias    3.077\n",
      "iter 6000/1000000  loss         0.170874  avg_L1_norm_grad         0.000113  w[0]   -0.005 bias    3.086\n",
      "iter 6001/1000000  loss         0.170872  avg_L1_norm_grad         0.000113  w[0]   -0.005 bias    3.086\n",
      "iter 6100/1000000  loss         0.170706  avg_L1_norm_grad         0.000111  w[0]   -0.004 bias    3.095\n",
      "iter 6101/1000000  loss         0.170705  avg_L1_norm_grad         0.000111  w[0]   -0.004 bias    3.095\n",
      "iter 6200/1000000  loss         0.170544  avg_L1_norm_grad         0.000109  w[0]   -0.004 bias    3.104\n",
      "iter 6201/1000000  loss         0.170543  avg_L1_norm_grad         0.000109  w[0]   -0.004 bias    3.104\n",
      "iter 6300/1000000  loss         0.170387  avg_L1_norm_grad         0.000108  w[0]   -0.003 bias    3.113\n",
      "iter 6301/1000000  loss         0.170386  avg_L1_norm_grad         0.000108  w[0]   -0.003 bias    3.113\n",
      "iter 6400/1000000  loss         0.170236  avg_L1_norm_grad         0.000106  w[0]   -0.002 bias    3.121\n",
      "iter 6401/1000000  loss         0.170234  avg_L1_norm_grad         0.000106  w[0]   -0.002 bias    3.121\n",
      "iter 6500/1000000  loss         0.170089  avg_L1_norm_grad         0.000104  w[0]   -0.001 bias    3.130\n",
      "iter 6501/1000000  loss         0.170088  avg_L1_norm_grad         0.000104  w[0]   -0.001 bias    3.130\n",
      "iter 6600/1000000  loss         0.169947  avg_L1_norm_grad         0.000103  w[0]   -0.001 bias    3.138\n",
      "iter 6601/1000000  loss         0.169946  avg_L1_norm_grad         0.000103  w[0]   -0.001 bias    3.138\n",
      "iter 6700/1000000  loss         0.169810  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.146\n",
      "iter 6701/1000000  loss         0.169808  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.146\n",
      "iter 6800/1000000  loss         0.169677  avg_L1_norm_grad         0.000099  w[0]    0.001 bias    3.154\n",
      "iter 6801/1000000  loss         0.169675  avg_L1_norm_grad         0.000099  w[0]    0.001 bias    3.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.169548  avg_L1_norm_grad         0.000098  w[0]    0.002 bias    3.161\n",
      "iter 6901/1000000  loss         0.169546  avg_L1_norm_grad         0.000098  w[0]    0.002 bias    3.161\n",
      "iter 7000/1000000  loss         0.169423  avg_L1_norm_grad         0.000097  w[0]    0.003 bias    3.169\n",
      "iter 7001/1000000  loss         0.169422  avg_L1_norm_grad         0.000097  w[0]    0.003 bias    3.169\n",
      "iter 7100/1000000  loss         0.169302  avg_L1_norm_grad         0.000095  w[0]    0.003 bias    3.176\n",
      "iter 7101/1000000  loss         0.169300  avg_L1_norm_grad         0.000095  w[0]    0.003 bias    3.176\n",
      "iter 7200/1000000  loss         0.169184  avg_L1_norm_grad         0.000094  w[0]    0.004 bias    3.183\n",
      "iter 7201/1000000  loss         0.169183  avg_L1_norm_grad         0.000094  w[0]    0.004 bias    3.183\n",
      "iter 7300/1000000  loss         0.169070  avg_L1_norm_grad         0.000092  w[0]    0.005 bias    3.190\n",
      "iter 7301/1000000  loss         0.169069  avg_L1_norm_grad         0.000092  w[0]    0.005 bias    3.190\n",
      "iter 7400/1000000  loss         0.168960  avg_L1_norm_grad         0.000091  w[0]    0.006 bias    3.197\n",
      "iter 7401/1000000  loss         0.168959  avg_L1_norm_grad         0.000091  w[0]    0.006 bias    3.197\n",
      "iter 7500/1000000  loss         0.168853  avg_L1_norm_grad         0.000090  w[0]    0.006 bias    3.204\n",
      "iter 7501/1000000  loss         0.168852  avg_L1_norm_grad         0.000090  w[0]    0.006 bias    3.204\n",
      "iter 7600/1000000  loss         0.168749  avg_L1_norm_grad         0.000088  w[0]    0.007 bias    3.210\n",
      "iter 7601/1000000  loss         0.168748  avg_L1_norm_grad         0.000088  w[0]    0.007 bias    3.210\n",
      "iter 7700/1000000  loss         0.168648  avg_L1_norm_grad         0.000087  w[0]    0.008 bias    3.217\n",
      "iter 7701/1000000  loss         0.168647  avg_L1_norm_grad         0.000087  w[0]    0.008 bias    3.217\n",
      "iter 7800/1000000  loss         0.168550  avg_L1_norm_grad         0.000086  w[0]    0.009 bias    3.223\n",
      "iter 7801/1000000  loss         0.168549  avg_L1_norm_grad         0.000086  w[0]    0.009 bias    3.223\n",
      "iter 7900/1000000  loss         0.168455  avg_L1_norm_grad         0.000085  w[0]    0.009 bias    3.229\n",
      "iter 7901/1000000  loss         0.168454  avg_L1_norm_grad         0.000085  w[0]    0.009 bias    3.229\n",
      "iter 8000/1000000  loss         0.168363  avg_L1_norm_grad         0.000084  w[0]    0.010 bias    3.235\n",
      "iter 8001/1000000  loss         0.168362  avg_L1_norm_grad         0.000084  w[0]    0.010 bias    3.235\n",
      "iter 8100/1000000  loss         0.168273  avg_L1_norm_grad         0.000082  w[0]    0.011 bias    3.241\n",
      "iter 8101/1000000  loss         0.168272  avg_L1_norm_grad         0.000082  w[0]    0.011 bias    3.241\n",
      "iter 8200/1000000  loss         0.168186  avg_L1_norm_grad         0.000081  w[0]    0.012 bias    3.247\n",
      "iter 8201/1000000  loss         0.168185  avg_L1_norm_grad         0.000081  w[0]    0.012 bias    3.247\n",
      "iter 8300/1000000  loss         0.168101  avg_L1_norm_grad         0.000080  w[0]    0.012 bias    3.253\n",
      "iter 8301/1000000  loss         0.168100  avg_L1_norm_grad         0.000080  w[0]    0.012 bias    3.253\n",
      "iter 8400/1000000  loss         0.168019  avg_L1_norm_grad         0.000079  w[0]    0.013 bias    3.259\n",
      "iter 8401/1000000  loss         0.168018  avg_L1_norm_grad         0.000079  w[0]    0.013 bias    3.259\n",
      "iter 8500/1000000  loss         0.167938  avg_L1_norm_grad         0.000078  w[0]    0.014 bias    3.264\n",
      "iter 8501/1000000  loss         0.167938  avg_L1_norm_grad         0.000078  w[0]    0.014 bias    3.264\n",
      "iter 8600/1000000  loss         0.167861  avg_L1_norm_grad         0.000077  w[0]    0.015 bias    3.270\n",
      "iter 8601/1000000  loss         0.167860  avg_L1_norm_grad         0.000077  w[0]    0.015 bias    3.270\n",
      "iter 8700/1000000  loss         0.167785  avg_L1_norm_grad         0.000076  w[0]    0.015 bias    3.275\n",
      "iter 8701/1000000  loss         0.167784  avg_L1_norm_grad         0.000076  w[0]    0.015 bias    3.275\n",
      "iter 8800/1000000  loss         0.167711  avg_L1_norm_grad         0.000075  w[0]    0.016 bias    3.280\n",
      "iter 8801/1000000  loss         0.167710  avg_L1_norm_grad         0.000075  w[0]    0.016 bias    3.280\n",
      "iter 8900/1000000  loss         0.167639  avg_L1_norm_grad         0.000074  w[0]    0.017 bias    3.285\n",
      "iter 8901/1000000  loss         0.167639  avg_L1_norm_grad         0.000074  w[0]    0.017 bias    3.285\n",
      "iter 9000/1000000  loss         0.167570  avg_L1_norm_grad         0.000073  w[0]    0.018 bias    3.290\n",
      "iter 9001/1000000  loss         0.167569  avg_L1_norm_grad         0.000073  w[0]    0.018 bias    3.290\n",
      "iter 9100/1000000  loss         0.167502  avg_L1_norm_grad         0.000072  w[0]    0.018 bias    3.295\n",
      "iter 9101/1000000  loss         0.167501  avg_L1_norm_grad         0.000072  w[0]    0.018 bias    3.295\n",
      "iter 9200/1000000  loss         0.167436  avg_L1_norm_grad         0.000071  w[0]    0.019 bias    3.300\n",
      "iter 9201/1000000  loss         0.167435  avg_L1_norm_grad         0.000071  w[0]    0.019 bias    3.300\n",
      "iter 9300/1000000  loss         0.167371  avg_L1_norm_grad         0.000070  w[0]    0.020 bias    3.305\n",
      "iter 9301/1000000  loss         0.167371  avg_L1_norm_grad         0.000070  w[0]    0.020 bias    3.305\n",
      "iter 9400/1000000  loss         0.167309  avg_L1_norm_grad         0.000069  w[0]    0.021 bias    3.310\n",
      "iter 9401/1000000  loss         0.167308  avg_L1_norm_grad         0.000069  w[0]    0.021 bias    3.310\n",
      "iter 9500/1000000  loss         0.167247  avg_L1_norm_grad         0.000068  w[0]    0.021 bias    3.314\n",
      "iter 9501/1000000  loss         0.167247  avg_L1_norm_grad         0.000068  w[0]    0.021 bias    3.314\n",
      "iter 9600/1000000  loss         0.167188  avg_L1_norm_grad         0.000067  w[0]    0.022 bias    3.319\n",
      "iter 9601/1000000  loss         0.167187  avg_L1_norm_grad         0.000067  w[0]    0.022 bias    3.319\n",
      "iter 9700/1000000  loss         0.167130  avg_L1_norm_grad         0.000067  w[0]    0.023 bias    3.323\n",
      "iter 9701/1000000  loss         0.167130  avg_L1_norm_grad         0.000067  w[0]    0.023 bias    3.323\n",
      "iter 9800/1000000  loss         0.167074  avg_L1_norm_grad         0.000066  w[0]    0.023 bias    3.328\n",
      "iter 9801/1000000  loss         0.167073  avg_L1_norm_grad         0.000066  w[0]    0.023 bias    3.328\n",
      "iter 9900/1000000  loss         0.167019  avg_L1_norm_grad         0.000065  w[0]    0.024 bias    3.332\n",
      "iter 9901/1000000  loss         0.167018  avg_L1_norm_grad         0.000065  w[0]    0.024 bias    3.332\n",
      "iter 10000/1000000  loss         0.166965  avg_L1_norm_grad         0.000064  w[0]    0.025 bias    3.336\n",
      "iter 10001/1000000  loss         0.166965  avg_L1_norm_grad         0.000064  w[0]    0.025 bias    3.336\n",
      "iter 10100/1000000  loss         0.166913  avg_L1_norm_grad         0.000063  w[0]    0.025 bias    3.341\n",
      "iter 10101/1000000  loss         0.166912  avg_L1_norm_grad         0.000063  w[0]    0.025 bias    3.341\n",
      "iter 10200/1000000  loss         0.166862  avg_L1_norm_grad         0.000063  w[0]    0.026 bias    3.345\n",
      "iter 10201/1000000  loss         0.166861  avg_L1_norm_grad         0.000063  w[0]    0.026 bias    3.345\n",
      "iter 10300/1000000  loss         0.166812  avg_L1_norm_grad         0.000062  w[0]    0.027 bias    3.349\n",
      "iter 10301/1000000  loss         0.166812  avg_L1_norm_grad         0.000062  w[0]    0.027 bias    3.349\n",
      "iter 10400/1000000  loss         0.166764  avg_L1_norm_grad         0.000061  w[0]    0.028 bias    3.353\n",
      "iter 10401/1000000  loss         0.166763  avg_L1_norm_grad         0.000061  w[0]    0.028 bias    3.353\n",
      "iter 10500/1000000  loss         0.166717  avg_L1_norm_grad         0.000060  w[0]    0.028 bias    3.357\n",
      "iter 10501/1000000  loss         0.166716  avg_L1_norm_grad         0.000060  w[0]    0.028 bias    3.357\n",
      "iter 10600/1000000  loss         0.166671  avg_L1_norm_grad         0.000060  w[0]    0.029 bias    3.361\n",
      "iter 10601/1000000  loss         0.166670  avg_L1_norm_grad         0.000060  w[0]    0.029 bias    3.361\n",
      "iter 10700/1000000  loss         0.166626  avg_L1_norm_grad         0.000059  w[0]    0.030 bias    3.364\n",
      "iter 10701/1000000  loss         0.166625  avg_L1_norm_grad         0.000059  w[0]    0.030 bias    3.364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.166582  avg_L1_norm_grad         0.000058  w[0]    0.030 bias    3.368\n",
      "iter 10801/1000000  loss         0.166582  avg_L1_norm_grad         0.000058  w[0]    0.030 bias    3.368\n",
      "iter 10900/1000000  loss         0.166539  avg_L1_norm_grad         0.000057  w[0]    0.031 bias    3.372\n",
      "iter 10901/1000000  loss         0.166539  avg_L1_norm_grad         0.000057  w[0]    0.031 bias    3.372\n",
      "iter 11000/1000000  loss         0.166498  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    3.375\n",
      "iter 11001/1000000  loss         0.166497  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    3.375\n",
      "iter 11100/1000000  loss         0.166457  avg_L1_norm_grad         0.000056  w[0]    0.032 bias    3.379\n",
      "iter 11101/1000000  loss         0.166457  avg_L1_norm_grad         0.000056  w[0]    0.032 bias    3.379\n",
      "iter 11200/1000000  loss         0.166417  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    3.383\n",
      "iter 11201/1000000  loss         0.166417  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    3.383\n",
      "iter 11300/1000000  loss         0.166379  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    3.386\n",
      "iter 11301/1000000  loss         0.166378  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    3.386\n",
      "iter 11400/1000000  loss         0.166341  avg_L1_norm_grad         0.000054  w[0]    0.034 bias    3.389\n",
      "iter 11401/1000000  loss         0.166340  avg_L1_norm_grad         0.000054  w[0]    0.034 bias    3.389\n",
      "iter 11500/1000000  loss         0.166304  avg_L1_norm_grad         0.000053  w[0]    0.035 bias    3.393\n",
      "iter 11501/1000000  loss         0.166303  avg_L1_norm_grad         0.000053  w[0]    0.035 bias    3.393\n",
      "iter 11600/1000000  loss         0.166268  avg_L1_norm_grad         0.000053  w[0]    0.035 bias    3.396\n",
      "iter 11601/1000000  loss         0.166267  avg_L1_norm_grad         0.000053  w[0]    0.035 bias    3.396\n",
      "iter 11700/1000000  loss         0.166233  avg_L1_norm_grad         0.000052  w[0]    0.036 bias    3.399\n",
      "iter 11701/1000000  loss         0.166232  avg_L1_norm_grad         0.000052  w[0]    0.036 bias    3.399\n",
      "iter 11800/1000000  loss         0.166198  avg_L1_norm_grad         0.000052  w[0]    0.037 bias    3.403\n",
      "iter 11801/1000000  loss         0.166198  avg_L1_norm_grad         0.000052  w[0]    0.037 bias    3.403\n",
      "iter 11900/1000000  loss         0.166165  avg_L1_norm_grad         0.000051  w[0]    0.037 bias    3.406\n",
      "iter 11901/1000000  loss         0.166164  avg_L1_norm_grad         0.000051  w[0]    0.037 bias    3.406\n",
      "iter 12000/1000000  loss         0.166132  avg_L1_norm_grad         0.000050  w[0]    0.038 bias    3.409\n",
      "iter 12001/1000000  loss         0.166131  avg_L1_norm_grad         0.000050  w[0]    0.038 bias    3.409\n",
      "iter 12100/1000000  loss         0.166100  avg_L1_norm_grad         0.000050  w[0]    0.038 bias    3.412\n",
      "iter 12101/1000000  loss         0.166099  avg_L1_norm_grad         0.000050  w[0]    0.038 bias    3.412\n",
      "iter 12200/1000000  loss         0.166068  avg_L1_norm_grad         0.000049  w[0]    0.039 bias    3.415\n",
      "iter 12201/1000000  loss         0.166068  avg_L1_norm_grad         0.000049  w[0]    0.039 bias    3.415\n",
      "iter 12300/1000000  loss         0.166038  avg_L1_norm_grad         0.000049  w[0]    0.040 bias    3.418\n",
      "iter 12301/1000000  loss         0.166038  avg_L1_norm_grad         0.000049  w[0]    0.040 bias    3.418\n",
      "iter 12400/1000000  loss         0.166008  avg_L1_norm_grad         0.000048  w[0]    0.040 bias    3.421\n",
      "iter 12401/1000000  loss         0.166008  avg_L1_norm_grad         0.000048  w[0]    0.040 bias    3.421\n",
      "iter 12500/1000000  loss         0.165979  avg_L1_norm_grad         0.000048  w[0]    0.041 bias    3.424\n",
      "iter 12501/1000000  loss         0.165979  avg_L1_norm_grad         0.000048  w[0]    0.041 bias    3.424\n",
      "iter 12600/1000000  loss         0.165950  avg_L1_norm_grad         0.000047  w[0]    0.041 bias    3.427\n",
      "iter 12601/1000000  loss         0.165950  avg_L1_norm_grad         0.000047  w[0]    0.041 bias    3.427\n",
      "iter 12700/1000000  loss         0.165922  avg_L1_norm_grad         0.000047  w[0]    0.042 bias    3.430\n",
      "iter 12701/1000000  loss         0.165922  avg_L1_norm_grad         0.000047  w[0]    0.042 bias    3.430\n",
      "iter 12800/1000000  loss         0.165895  avg_L1_norm_grad         0.000046  w[0]    0.043 bias    3.432\n",
      "iter 12801/1000000  loss         0.165895  avg_L1_norm_grad         0.000046  w[0]    0.043 bias    3.432\n",
      "iter 12900/1000000  loss         0.165869  avg_L1_norm_grad         0.000046  w[0]    0.043 bias    3.435\n",
      "iter 12901/1000000  loss         0.165868  avg_L1_norm_grad         0.000046  w[0]    0.043 bias    3.435\n",
      "iter 13000/1000000  loss         0.165843  avg_L1_norm_grad         0.000045  w[0]    0.044 bias    3.438\n",
      "iter 13001/1000000  loss         0.165842  avg_L1_norm_grad         0.000045  w[0]    0.044 bias    3.438\n",
      "iter 13100/1000000  loss         0.165817  avg_L1_norm_grad         0.000045  w[0]    0.044 bias    3.441\n",
      "iter 13101/1000000  loss         0.165817  avg_L1_norm_grad         0.000045  w[0]    0.044 bias    3.441\n",
      "iter 13200/1000000  loss         0.165792  avg_L1_norm_grad         0.000044  w[0]    0.045 bias    3.443\n",
      "iter 13201/1000000  loss         0.165792  avg_L1_norm_grad         0.000044  w[0]    0.045 bias    3.443\n",
      "iter 13300/1000000  loss         0.165768  avg_L1_norm_grad         0.000044  w[0]    0.045 bias    3.446\n",
      "iter 13301/1000000  loss         0.165768  avg_L1_norm_grad         0.000044  w[0]    0.045 bias    3.446\n",
      "iter 13400/1000000  loss         0.165744  avg_L1_norm_grad         0.000043  w[0]    0.046 bias    3.448\n",
      "iter 13401/1000000  loss         0.165744  avg_L1_norm_grad         0.000043  w[0]    0.046 bias    3.448\n",
      "iter 13500/1000000  loss         0.165721  avg_L1_norm_grad         0.000043  w[0]    0.047 bias    3.451\n",
      "iter 13501/1000000  loss         0.165721  avg_L1_norm_grad         0.000043  w[0]    0.047 bias    3.451\n",
      "iter 13600/1000000  loss         0.165698  avg_L1_norm_grad         0.000042  w[0]    0.047 bias    3.453\n",
      "iter 13601/1000000  loss         0.165698  avg_L1_norm_grad         0.000042  w[0]    0.047 bias    3.453\n",
      "iter 13700/1000000  loss         0.165676  avg_L1_norm_grad         0.000042  w[0]    0.048 bias    3.456\n",
      "iter 13701/1000000  loss         0.165676  avg_L1_norm_grad         0.000042  w[0]    0.048 bias    3.456\n",
      "iter 13800/1000000  loss         0.165654  avg_L1_norm_grad         0.000041  w[0]    0.048 bias    3.458\n",
      "iter 13801/1000000  loss         0.165654  avg_L1_norm_grad         0.000041  w[0]    0.048 bias    3.458\n",
      "iter 13900/1000000  loss         0.165633  avg_L1_norm_grad         0.000041  w[0]    0.049 bias    3.461\n",
      "iter 13901/1000000  loss         0.165633  avg_L1_norm_grad         0.000041  w[0]    0.049 bias    3.461\n",
      "iter 14000/1000000  loss         0.165612  avg_L1_norm_grad         0.000040  w[0]    0.049 bias    3.463\n",
      "iter 14001/1000000  loss         0.165612  avg_L1_norm_grad         0.000040  w[0]    0.049 bias    3.463\n",
      "iter 14100/1000000  loss         0.165592  avg_L1_norm_grad         0.000040  w[0]    0.050 bias    3.466\n",
      "iter 14101/1000000  loss         0.165592  avg_L1_norm_grad         0.000040  w[0]    0.050 bias    3.466\n",
      "iter 14200/1000000  loss         0.165572  avg_L1_norm_grad         0.000039  w[0]    0.050 bias    3.468\n",
      "iter 14201/1000000  loss         0.165572  avg_L1_norm_grad         0.000039  w[0]    0.050 bias    3.468\n",
      "iter 14300/1000000  loss         0.165552  avg_L1_norm_grad         0.000039  w[0]    0.051 bias    3.470\n",
      "iter 14301/1000000  loss         0.165552  avg_L1_norm_grad         0.000039  w[0]    0.051 bias    3.470\n",
      "iter 14400/1000000  loss         0.165533  avg_L1_norm_grad         0.000039  w[0]    0.051 bias    3.472\n",
      "iter 14401/1000000  loss         0.165533  avg_L1_norm_grad         0.000039  w[0]    0.051 bias    3.472\n",
      "iter 14500/1000000  loss         0.165515  avg_L1_norm_grad         0.000038  w[0]    0.052 bias    3.475\n",
      "iter 14501/1000000  loss         0.165514  avg_L1_norm_grad         0.000038  w[0]    0.052 bias    3.475\n",
      "iter 14600/1000000  loss         0.165496  avg_L1_norm_grad         0.000038  w[0]    0.052 bias    3.477\n",
      "iter 14601/1000000  loss         0.165496  avg_L1_norm_grad         0.000038  w[0]    0.052 bias    3.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.165478  avg_L1_norm_grad         0.000037  w[0]    0.053 bias    3.479\n",
      "iter 14701/1000000  loss         0.165478  avg_L1_norm_grad         0.000037  w[0]    0.053 bias    3.479\n",
      "iter 14800/1000000  loss         0.165461  avg_L1_norm_grad         0.000037  w[0]    0.053 bias    3.481\n",
      "iter 14801/1000000  loss         0.165461  avg_L1_norm_grad         0.000037  w[0]    0.053 bias    3.481\n",
      "iter 14900/1000000  loss         0.165444  avg_L1_norm_grad         0.000037  w[0]    0.054 bias    3.483\n",
      "iter 14901/1000000  loss         0.165444  avg_L1_norm_grad         0.000037  w[0]    0.054 bias    3.483\n",
      "iter 15000/1000000  loss         0.165427  avg_L1_norm_grad         0.000036  w[0]    0.054 bias    3.485\n",
      "iter 15001/1000000  loss         0.165427  avg_L1_norm_grad         0.000036  w[0]    0.054 bias    3.485\n",
      "iter 15100/1000000  loss         0.165411  avg_L1_norm_grad         0.000036  w[0]    0.055 bias    3.487\n",
      "iter 15101/1000000  loss         0.165411  avg_L1_norm_grad         0.000036  w[0]    0.055 bias    3.487\n",
      "iter 15200/1000000  loss         0.165395  avg_L1_norm_grad         0.000035  w[0]    0.055 bias    3.489\n",
      "iter 15201/1000000  loss         0.165394  avg_L1_norm_grad         0.000035  w[0]    0.055 bias    3.489\n",
      "iter 15300/1000000  loss         0.165379  avg_L1_norm_grad         0.000035  w[0]    0.056 bias    3.491\n",
      "iter 15301/1000000  loss         0.165379  avg_L1_norm_grad         0.000035  w[0]    0.056 bias    3.491\n",
      "iter 15400/1000000  loss         0.165364  avg_L1_norm_grad         0.000035  w[0]    0.056 bias    3.493\n",
      "iter 15401/1000000  loss         0.165363  avg_L1_norm_grad         0.000035  w[0]    0.056 bias    3.493\n",
      "iter 15500/1000000  loss         0.165348  avg_L1_norm_grad         0.000034  w[0]    0.057 bias    3.495\n",
      "iter 15501/1000000  loss         0.165348  avg_L1_norm_grad         0.000034  w[0]    0.057 bias    3.495\n",
      "iter 15600/1000000  loss         0.165334  avg_L1_norm_grad         0.000034  w[0]    0.057 bias    3.497\n",
      "iter 15601/1000000  loss         0.165334  avg_L1_norm_grad         0.000034  w[0]    0.057 bias    3.497\n",
      "iter 15700/1000000  loss         0.165319  avg_L1_norm_grad         0.000034  w[0]    0.058 bias    3.499\n",
      "iter 15701/1000000  loss         0.165319  avg_L1_norm_grad         0.000034  w[0]    0.058 bias    3.499\n",
      "iter 15800/1000000  loss         0.165305  avg_L1_norm_grad         0.000033  w[0]    0.058 bias    3.501\n",
      "iter 15801/1000000  loss         0.165305  avg_L1_norm_grad         0.000033  w[0]    0.058 bias    3.501\n",
      "iter 15900/1000000  loss         0.165291  avg_L1_norm_grad         0.000033  w[0]    0.059 bias    3.503\n",
      "iter 15901/1000000  loss         0.165291  avg_L1_norm_grad         0.000033  w[0]    0.059 bias    3.503\n",
      "iter 16000/1000000  loss         0.165278  avg_L1_norm_grad         0.000033  w[0]    0.059 bias    3.505\n",
      "iter 16001/1000000  loss         0.165278  avg_L1_norm_grad         0.000033  w[0]    0.059 bias    3.505\n",
      "iter 16100/1000000  loss         0.165264  avg_L1_norm_grad         0.000032  w[0]    0.060 bias    3.507\n",
      "iter 16101/1000000  loss         0.165264  avg_L1_norm_grad         0.000032  w[0]    0.060 bias    3.507\n",
      "iter 16200/1000000  loss         0.165251  avg_L1_norm_grad         0.000032  w[0]    0.060 bias    3.509\n",
      "iter 16201/1000000  loss         0.165251  avg_L1_norm_grad         0.000032  w[0]    0.060 bias    3.509\n",
      "iter 16300/1000000  loss         0.165239  avg_L1_norm_grad         0.000032  w[0]    0.060 bias    3.510\n",
      "iter 16301/1000000  loss         0.165238  avg_L1_norm_grad         0.000032  w[0]    0.060 bias    3.510\n",
      "iter 16400/1000000  loss         0.165226  avg_L1_norm_grad         0.000031  w[0]    0.061 bias    3.512\n",
      "iter 16401/1000000  loss         0.165226  avg_L1_norm_grad         0.000031  w[0]    0.061 bias    3.512\n",
      "iter 16500/1000000  loss         0.165214  avg_L1_norm_grad         0.000031  w[0]    0.061 bias    3.514\n",
      "iter 16501/1000000  loss         0.165214  avg_L1_norm_grad         0.000031  w[0]    0.061 bias    3.514\n",
      "iter 16600/1000000  loss         0.165202  avg_L1_norm_grad         0.000031  w[0]    0.062 bias    3.516\n",
      "iter 16601/1000000  loss         0.165202  avg_L1_norm_grad         0.000031  w[0]    0.062 bias    3.516\n",
      "iter 16700/1000000  loss         0.165190  avg_L1_norm_grad         0.000030  w[0]    0.062 bias    3.517\n",
      "iter 16701/1000000  loss         0.165190  avg_L1_norm_grad         0.000030  w[0]    0.062 bias    3.517\n",
      "iter 16800/1000000  loss         0.165179  avg_L1_norm_grad         0.000030  w[0]    0.063 bias    3.519\n",
      "iter 16801/1000000  loss         0.165179  avg_L1_norm_grad         0.000030  w[0]    0.063 bias    3.519\n",
      "iter 16900/1000000  loss         0.165167  avg_L1_norm_grad         0.000030  w[0]    0.063 bias    3.521\n",
      "iter 16901/1000000  loss         0.165167  avg_L1_norm_grad         0.000030  w[0]    0.063 bias    3.521\n",
      "iter 17000/1000000  loss         0.165156  avg_L1_norm_grad         0.000029  w[0]    0.063 bias    3.522\n",
      "iter 17001/1000000  loss         0.165156  avg_L1_norm_grad         0.000029  w[0]    0.063 bias    3.522\n",
      "iter 17100/1000000  loss         0.165145  avg_L1_norm_grad         0.000029  w[0]    0.064 bias    3.524\n",
      "iter 17101/1000000  loss         0.165145  avg_L1_norm_grad         0.000029  w[0]    0.064 bias    3.524\n",
      "iter 17200/1000000  loss         0.165135  avg_L1_norm_grad         0.000029  w[0]    0.064 bias    3.525\n",
      "iter 17201/1000000  loss         0.165135  avg_L1_norm_grad         0.000029  w[0]    0.064 bias    3.525\n",
      "iter 17300/1000000  loss         0.165124  avg_L1_norm_grad         0.000029  w[0]    0.065 bias    3.527\n",
      "iter 17301/1000000  loss         0.165124  avg_L1_norm_grad         0.000029  w[0]    0.065 bias    3.527\n",
      "iter 17400/1000000  loss         0.165114  avg_L1_norm_grad         0.000028  w[0]    0.065 bias    3.529\n",
      "iter 17401/1000000  loss         0.165114  avg_L1_norm_grad         0.000028  w[0]    0.065 bias    3.529\n",
      "iter 17500/1000000  loss         0.165104  avg_L1_norm_grad         0.000028  w[0]    0.065 bias    3.530\n",
      "iter 17501/1000000  loss         0.165104  avg_L1_norm_grad         0.000028  w[0]    0.065 bias    3.530\n",
      "iter 17600/1000000  loss         0.165095  avg_L1_norm_grad         0.000028  w[0]    0.066 bias    3.532\n",
      "iter 17601/1000000  loss         0.165094  avg_L1_norm_grad         0.000028  w[0]    0.066 bias    3.532\n",
      "iter 17700/1000000  loss         0.165085  avg_L1_norm_grad         0.000027  w[0]    0.066 bias    3.533\n",
      "iter 17701/1000000  loss         0.165085  avg_L1_norm_grad         0.000027  w[0]    0.066 bias    3.533\n",
      "iter 17800/1000000  loss         0.165076  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.535\n",
      "iter 17801/1000000  loss         0.165075  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.535\n",
      "iter 17900/1000000  loss         0.165066  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.536\n",
      "iter 17901/1000000  loss         0.165066  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.536\n",
      "iter 18000/1000000  loss         0.165057  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.538\n",
      "iter 18001/1000000  loss         0.165057  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.538\n",
      "iter 18100/1000000  loss         0.165048  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.539\n",
      "iter 18101/1000000  loss         0.165048  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.539\n",
      "iter 18200/1000000  loss         0.165040  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.540\n",
      "iter 18201/1000000  loss         0.165040  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.540\n",
      "iter 18300/1000000  loss         0.165031  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.542\n",
      "iter 18301/1000000  loss         0.165031  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.542\n",
      "iter 18400/1000000  loss         0.165023  avg_L1_norm_grad         0.000026  w[0]    0.069 bias    3.543\n",
      "iter 18401/1000000  loss         0.165023  avg_L1_norm_grad         0.000026  w[0]    0.069 bias    3.543\n",
      "iter 18500/1000000  loss         0.165015  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.545\n",
      "iter 18501/1000000  loss         0.165015  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18600/1000000  loss         0.165007  avg_L1_norm_grad         0.000025  w[0]    0.070 bias    3.546\n",
      "iter 18601/1000000  loss         0.165007  avg_L1_norm_grad         0.000025  w[0]    0.070 bias    3.546\n",
      "iter 18700/1000000  loss         0.164999  avg_L1_norm_grad         0.000025  w[0]    0.070 bias    3.547\n",
      "iter 18701/1000000  loss         0.164999  avg_L1_norm_grad         0.000025  w[0]    0.070 bias    3.547\n",
      "iter 18800/1000000  loss         0.164991  avg_L1_norm_grad         0.000025  w[0]    0.070 bias    3.549\n",
      "iter 18801/1000000  loss         0.164991  avg_L1_norm_grad         0.000025  w[0]    0.070 bias    3.549\n",
      "iter 18900/1000000  loss         0.164984  avg_L1_norm_grad         0.000024  w[0]    0.071 bias    3.550\n",
      "iter 18901/1000000  loss         0.164983  avg_L1_norm_grad         0.000024  w[0]    0.071 bias    3.550\n",
      "iter 19000/1000000  loss         0.164976  avg_L1_norm_grad         0.000024  w[0]    0.071 bias    3.551\n",
      "iter 19001/1000000  loss         0.164976  avg_L1_norm_grad         0.000024  w[0]    0.071 bias    3.551\n",
      "iter 19100/1000000  loss         0.164969  avg_L1_norm_grad         0.000024  w[0]    0.071 bias    3.553\n",
      "iter 19101/1000000  loss         0.164969  avg_L1_norm_grad         0.000024  w[0]    0.071 bias    3.553\n",
      "iter 19200/1000000  loss         0.164962  avg_L1_norm_grad         0.000024  w[0]    0.072 bias    3.554\n",
      "iter 19201/1000000  loss         0.164962  avg_L1_norm_grad         0.000024  w[0]    0.072 bias    3.554\n",
      "iter 19300/1000000  loss         0.164955  avg_L1_norm_grad         0.000023  w[0]    0.072 bias    3.555\n",
      "iter 19301/1000000  loss         0.164955  avg_L1_norm_grad         0.000023  w[0]    0.072 bias    3.555\n",
      "iter 19400/1000000  loss         0.164948  avg_L1_norm_grad         0.000023  w[0]    0.072 bias    3.556\n",
      "iter 19401/1000000  loss         0.164948  avg_L1_norm_grad         0.000023  w[0]    0.072 bias    3.556\n",
      "iter 19500/1000000  loss         0.164941  avg_L1_norm_grad         0.000023  w[0]    0.073 bias    3.558\n",
      "iter 19501/1000000  loss         0.164941  avg_L1_norm_grad         0.000023  w[0]    0.073 bias    3.558\n",
      "iter 19600/1000000  loss         0.164935  avg_L1_norm_grad         0.000023  w[0]    0.073 bias    3.559\n",
      "iter 19601/1000000  loss         0.164935  avg_L1_norm_grad         0.000023  w[0]    0.073 bias    3.559\n",
      "iter 19700/1000000  loss         0.164928  avg_L1_norm_grad         0.000023  w[0]    0.073 bias    3.560\n",
      "iter 19701/1000000  loss         0.164928  avg_L1_norm_grad         0.000023  w[0]    0.073 bias    3.560\n",
      "iter 19800/1000000  loss         0.164922  avg_L1_norm_grad         0.000022  w[0]    0.074 bias    3.561\n",
      "iter 19801/1000000  loss         0.164922  avg_L1_norm_grad         0.000022  w[0]    0.074 bias    3.561\n",
      "iter 19900/1000000  loss         0.164916  avg_L1_norm_grad         0.000022  w[0]    0.074 bias    3.562\n",
      "iter 19901/1000000  loss         0.164915  avg_L1_norm_grad         0.000022  w[0]    0.074 bias    3.562\n",
      "iter 20000/1000000  loss         0.164909  avg_L1_norm_grad         0.000022  w[0]    0.074 bias    3.564\n",
      "iter 20001/1000000  loss         0.164909  avg_L1_norm_grad         0.000022  w[0]    0.074 bias    3.564\n",
      "iter 20100/1000000  loss         0.164903  avg_L1_norm_grad         0.000022  w[0]    0.075 bias    3.565\n",
      "iter 20101/1000000  loss         0.164903  avg_L1_norm_grad         0.000022  w[0]    0.075 bias    3.565\n",
      "iter 20200/1000000  loss         0.164898  avg_L1_norm_grad         0.000021  w[0]    0.075 bias    3.566\n",
      "iter 20201/1000000  loss         0.164898  avg_L1_norm_grad         0.000021  w[0]    0.075 bias    3.566\n",
      "iter 20300/1000000  loss         0.164892  avg_L1_norm_grad         0.000021  w[0]    0.075 bias    3.567\n",
      "iter 20301/1000000  loss         0.164892  avg_L1_norm_grad         0.000021  w[0]    0.075 bias    3.567\n",
      "iter 20400/1000000  loss         0.164886  avg_L1_norm_grad         0.000021  w[0]    0.076 bias    3.568\n",
      "iter 20401/1000000  loss         0.164886  avg_L1_norm_grad         0.000021  w[0]    0.076 bias    3.568\n",
      "iter 20500/1000000  loss         0.164881  avg_L1_norm_grad         0.000021  w[0]    0.076 bias    3.569\n",
      "iter 20501/1000000  loss         0.164881  avg_L1_norm_grad         0.000021  w[0]    0.076 bias    3.569\n",
      "iter 20600/1000000  loss         0.164875  avg_L1_norm_grad         0.000021  w[0]    0.076 bias    3.570\n",
      "iter 20601/1000000  loss         0.164875  avg_L1_norm_grad         0.000021  w[0]    0.076 bias    3.570\n",
      "iter 20700/1000000  loss         0.164870  avg_L1_norm_grad         0.000020  w[0]    0.076 bias    3.571\n",
      "iter 20701/1000000  loss         0.164870  avg_L1_norm_grad         0.000020  w[0]    0.076 bias    3.571\n",
      "iter 20800/1000000  loss         0.164865  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    3.573\n",
      "iter 20801/1000000  loss         0.164865  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    3.573\n",
      "iter 20900/1000000  loss         0.164859  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    3.574\n",
      "iter 20901/1000000  loss         0.164859  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    3.574\n",
      "iter 21000/1000000  loss         0.164854  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    3.575\n",
      "iter 21001/1000000  loss         0.164854  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    3.575\n",
      "iter 21100/1000000  loss         0.164849  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.576\n",
      "iter 21101/1000000  loss         0.164849  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.576\n",
      "iter 21200/1000000  loss         0.164845  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.577\n",
      "iter 21201/1000000  loss         0.164845  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.577\n",
      "iter 21300/1000000  loss         0.164840  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    3.578\n",
      "iter 21301/1000000  loss         0.164840  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    3.578\n",
      "iter 21400/1000000  loss         0.164835  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.579\n",
      "iter 21401/1000000  loss         0.164835  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.579\n",
      "iter 21500/1000000  loss         0.164831  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.580\n",
      "iter 21501/1000000  loss         0.164830  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.580\n",
      "Done. Converged after 21569 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr3 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr3.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "Ave Loaded\n",
      "Initializing w_G with 1570 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.026284  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.925042  avg_L1_norm_grad         0.038022  w[0]    0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.908797  avg_L1_norm_grad         0.038244  w[0]    0.001 bias    0.030\n",
      "iter    3/1000000  loss         0.968586  avg_L1_norm_grad         0.053261  w[0]    0.001 bias    0.014\n",
      "iter    4/1000000  loss         0.868593  avg_L1_norm_grad         0.043081  w[0]    0.002 bias    0.055\n",
      "iter    5/1000000  loss         0.901792  avg_L1_norm_grad         0.049041  w[0]    0.001 bias    0.035\n",
      "iter    6/1000000  loss         0.763041  avg_L1_norm_grad         0.033555  w[0]    0.002 bias    0.074\n",
      "iter    7/1000000  loss         0.758796  avg_L1_norm_grad         0.035866  w[0]    0.002 bias    0.060\n",
      "iter    8/1000000  loss         0.684525  avg_L1_norm_grad         0.024265  w[0]    0.003 bias    0.090\n",
      "iter    9/1000000  loss         0.669254  avg_L1_norm_grad         0.025178  w[0]    0.002 bias    0.082\n",
      "iter   10/1000000  loss         0.630825  avg_L1_norm_grad         0.017007  w[0]    0.003 bias    0.105\n",
      "iter   11/1000000  loss         0.615226  avg_L1_norm_grad         0.017310  w[0]    0.003 bias    0.102\n",
      "iter   12/1000000  loss         0.593499  avg_L1_norm_grad         0.011747  w[0]    0.003 bias    0.120\n",
      "iter   13/1000000  loss         0.580388  avg_L1_norm_grad         0.011719  w[0]    0.003 bias    0.120\n",
      "iter   14/1000000  loss         0.566543  avg_L1_norm_grad         0.008208  w[0]    0.004 bias    0.134\n",
      "iter   15/1000000  loss         0.556021  avg_L1_norm_grad         0.008035  w[0]    0.004 bias    0.136\n",
      "iter   16/1000000  loss         0.546055  avg_L1_norm_grad         0.006012  w[0]    0.004 bias    0.148\n",
      "iter   17/1000000  loss         0.537487  avg_L1_norm_grad         0.005832  w[0]    0.004 bias    0.152\n",
      "iter   18/1000000  loss         0.529545  avg_L1_norm_grad         0.004861  w[0]    0.004 bias    0.161\n",
      "iter   19/1000000  loss         0.522295  avg_L1_norm_grad         0.004732  w[0]    0.004 bias    0.166\n",
      "iter  100/1000000  loss         0.336934  avg_L1_norm_grad         0.001432  w[0]    0.007 bias    0.509\n",
      "iter  101/1000000  loss         0.336077  avg_L1_norm_grad         0.001422  w[0]    0.007 bias    0.512\n",
      "iter  200/1000000  loss         0.284309  avg_L1_norm_grad         0.000900  w[0]    0.004 bias    0.751\n",
      "iter  201/1000000  loss         0.283978  avg_L1_norm_grad         0.000897  w[0]    0.004 bias    0.753\n",
      "iter  300/1000000  loss         0.259435  avg_L1_norm_grad         0.000692  w[0]    0.002 bias    0.932\n",
      "iter  301/1000000  loss         0.259246  avg_L1_norm_grad         0.000691  w[0]    0.001 bias    0.933\n",
      "iter  400/1000000  loss         0.243955  avg_L1_norm_grad         0.000576  w[0]   -0.001 bias    1.083\n",
      "iter  401/1000000  loss         0.243828  avg_L1_norm_grad         0.000575  w[0]   -0.001 bias    1.085\n",
      "iter  500/1000000  loss         0.232992  avg_L1_norm_grad         0.000501  w[0]   -0.004 bias    1.216\n",
      "iter  501/1000000  loss         0.232898  avg_L1_norm_grad         0.000500  w[0]   -0.004 bias    1.217\n",
      "iter  600/1000000  loss         0.224623  avg_L1_norm_grad         0.000447  w[0]   -0.006 bias    1.336\n",
      "iter  601/1000000  loss         0.224549  avg_L1_norm_grad         0.000446  w[0]   -0.006 bias    1.337\n",
      "iter  700/1000000  loss         0.217915  avg_L1_norm_grad         0.000406  w[0]   -0.007 bias    1.446\n",
      "iter  701/1000000  loss         0.217854  avg_L1_norm_grad         0.000405  w[0]   -0.007 bias    1.447\n",
      "iter  800/1000000  loss         0.212356  avg_L1_norm_grad         0.000373  w[0]   -0.009 bias    1.548\n",
      "iter  801/1000000  loss         0.212305  avg_L1_norm_grad         0.000373  w[0]   -0.009 bias    1.549\n",
      "iter  900/1000000  loss         0.207633  avg_L1_norm_grad         0.000347  w[0]   -0.010 bias    1.644\n",
      "iter  901/1000000  loss         0.207589  avg_L1_norm_grad         0.000346  w[0]   -0.010 bias    1.645\n",
      "iter 1000/1000000  loss         0.203546  avg_L1_norm_grad         0.000324  w[0]   -0.011 bias    1.735\n",
      "iter 1001/1000000  loss         0.203508  avg_L1_norm_grad         0.000324  w[0]   -0.011 bias    1.736\n",
      "iter 1100/1000000  loss         0.199957  avg_L1_norm_grad         0.000305  w[0]   -0.011 bias    1.821\n",
      "iter 1101/1000000  loss         0.199924  avg_L1_norm_grad         0.000305  w[0]   -0.011 bias    1.822\n",
      "iter 1200/1000000  loss         0.196768  avg_L1_norm_grad         0.000289  w[0]   -0.012 bias    1.903\n",
      "iter 1201/1000000  loss         0.196738  avg_L1_norm_grad         0.000288  w[0]   -0.012 bias    1.904\n",
      "iter 1300/1000000  loss         0.193907  avg_L1_norm_grad         0.000274  w[0]   -0.012 bias    1.981\n",
      "iter 1301/1000000  loss         0.193880  avg_L1_norm_grad         0.000274  w[0]   -0.012 bias    1.982\n",
      "iter 1400/1000000  loss         0.191318  avg_L1_norm_grad         0.000261  w[0]   -0.012 bias    2.056\n",
      "iter 1401/1000000  loss         0.191293  avg_L1_norm_grad         0.000261  w[0]   -0.012 bias    2.056\n",
      "iter 1500/1000000  loss         0.188960  avg_L1_norm_grad         0.000250  w[0]   -0.012 bias    2.128\n",
      "iter 1501/1000000  loss         0.188937  avg_L1_norm_grad         0.000250  w[0]   -0.012 bias    2.128\n",
      "iter 1600/1000000  loss         0.186798  avg_L1_norm_grad         0.000240  w[0]   -0.012 bias    2.197\n",
      "iter 1601/1000000  loss         0.186777  avg_L1_norm_grad         0.000240  w[0]   -0.012 bias    2.197\n",
      "iter 1700/1000000  loss         0.184807  avg_L1_norm_grad         0.000231  w[0]   -0.012 bias    2.263\n",
      "iter 1701/1000000  loss         0.184788  avg_L1_norm_grad         0.000230  w[0]   -0.012 bias    2.264\n",
      "iter 1800/1000000  loss         0.182964  avg_L1_norm_grad         0.000222  w[0]   -0.011 bias    2.328\n",
      "iter 1801/1000000  loss         0.182946  avg_L1_norm_grad         0.000222  w[0]   -0.011 bias    2.328\n",
      "iter 1900/1000000  loss         0.181251  avg_L1_norm_grad         0.000215  w[0]   -0.011 bias    2.390\n",
      "iter 1901/1000000  loss         0.181234  avg_L1_norm_grad         0.000215  w[0]   -0.011 bias    2.390\n",
      "iter 2000/1000000  loss         0.179653  avg_L1_norm_grad         0.000208  w[0]   -0.010 bias    2.450\n",
      "iter 2001/1000000  loss         0.179637  avg_L1_norm_grad         0.000208  w[0]   -0.010 bias    2.451\n",
      "iter 2100/1000000  loss         0.178157  avg_L1_norm_grad         0.000201  w[0]   -0.010 bias    2.508\n",
      "iter 2101/1000000  loss         0.178143  avg_L1_norm_grad         0.000201  w[0]   -0.010 bias    2.509\n",
      "iter 2200/1000000  loss         0.176754  avg_L1_norm_grad         0.000195  w[0]   -0.009 bias    2.565\n",
      "iter 2201/1000000  loss         0.176740  avg_L1_norm_grad         0.000195  w[0]   -0.009 bias    2.565\n",
      "iter 2300/1000000  loss         0.175433  avg_L1_norm_grad         0.000190  w[0]   -0.008 bias    2.620\n",
      "iter 2301/1000000  loss         0.175420  avg_L1_norm_grad         0.000190  w[0]   -0.008 bias    2.620\n",
      "iter 2400/1000000  loss         0.174186  avg_L1_norm_grad         0.000185  w[0]   -0.008 bias    2.673\n",
      "iter 2401/1000000  loss         0.174174  avg_L1_norm_grad         0.000184  w[0]   -0.008 bias    2.674\n",
      "iter 2500/1000000  loss         0.173008  avg_L1_norm_grad         0.000180  w[0]   -0.007 bias    2.725\n",
      "iter 2501/1000000  loss         0.172996  avg_L1_norm_grad         0.000180  w[0]   -0.007 bias    2.725\n",
      "iter 2600/1000000  loss         0.171891  avg_L1_norm_grad         0.000175  w[0]   -0.006 bias    2.775\n",
      "iter 2601/1000000  loss         0.171880  avg_L1_norm_grad         0.000175  w[0]   -0.006 bias    2.776\n",
      "iter 2700/1000000  loss         0.170831  avg_L1_norm_grad         0.000171  w[0]   -0.005 bias    2.825\n",
      "iter 2701/1000000  loss         0.170821  avg_L1_norm_grad         0.000171  w[0]   -0.005 bias    2.825\n",
      "iter 2800/1000000  loss         0.169823  avg_L1_norm_grad         0.000167  w[0]   -0.005 bias    2.872\n",
      "iter 2801/1000000  loss         0.169813  avg_L1_norm_grad         0.000167  w[0]   -0.005 bias    2.873\n",
      "iter 2900/1000000  loss         0.168862  avg_L1_norm_grad         0.000163  w[0]   -0.004 bias    2.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.168853  avg_L1_norm_grad         0.000163  w[0]   -0.004 bias    2.920\n",
      "iter 3000/1000000  loss         0.167945  avg_L1_norm_grad         0.000159  w[0]   -0.003 bias    2.965\n",
      "iter 3001/1000000  loss         0.167936  avg_L1_norm_grad         0.000159  w[0]   -0.003 bias    2.965\n",
      "iter 3100/1000000  loss         0.167070  avg_L1_norm_grad         0.000156  w[0]   -0.002 bias    3.009\n",
      "iter 3101/1000000  loss         0.167061  avg_L1_norm_grad         0.000156  w[0]   -0.002 bias    3.010\n",
      "iter 3200/1000000  loss         0.166232  avg_L1_norm_grad         0.000153  w[0]   -0.001 bias    3.053\n",
      "iter 3201/1000000  loss         0.166224  avg_L1_norm_grad         0.000153  w[0]   -0.001 bias    3.053\n",
      "iter 3300/1000000  loss         0.165429  avg_L1_norm_grad         0.000150  w[0]   -0.000 bias    3.095\n",
      "iter 3301/1000000  loss         0.165422  avg_L1_norm_grad         0.000150  w[0]   -0.000 bias    3.095\n",
      "iter 3400/1000000  loss         0.164660  avg_L1_norm_grad         0.000147  w[0]    0.001 bias    3.136\n",
      "iter 3401/1000000  loss         0.164652  avg_L1_norm_grad         0.000147  w[0]    0.001 bias    3.137\n",
      "iter 3500/1000000  loss         0.163921  avg_L1_norm_grad         0.000144  w[0]    0.001 bias    3.177\n",
      "iter 3501/1000000  loss         0.163914  avg_L1_norm_grad         0.000144  w[0]    0.001 bias    3.177\n",
      "iter 3600/1000000  loss         0.163211  avg_L1_norm_grad         0.000141  w[0]    0.002 bias    3.217\n",
      "iter 3601/1000000  loss         0.163204  avg_L1_norm_grad         0.000141  w[0]    0.002 bias    3.217\n",
      "iter 3700/1000000  loss         0.162528  avg_L1_norm_grad         0.000138  w[0]    0.003 bias    3.255\n",
      "iter 3701/1000000  loss         0.162521  avg_L1_norm_grad         0.000138  w[0]    0.003 bias    3.256\n",
      "iter 3800/1000000  loss         0.161870  avg_L1_norm_grad         0.000136  w[0]    0.004 bias    3.293\n",
      "iter 3801/1000000  loss         0.161863  avg_L1_norm_grad         0.000136  w[0]    0.004 bias    3.294\n",
      "iter 3900/1000000  loss         0.161236  avg_L1_norm_grad         0.000133  w[0]    0.005 bias    3.331\n",
      "iter 3901/1000000  loss         0.161229  avg_L1_norm_grad         0.000133  w[0]    0.005 bias    3.331\n",
      "iter 4000/1000000  loss         0.160624  avg_L1_norm_grad         0.000131  w[0]    0.006 bias    3.367\n",
      "iter 4001/1000000  loss         0.160618  avg_L1_norm_grad         0.000131  w[0]    0.006 bias    3.367\n",
      "iter 4100/1000000  loss         0.160034  avg_L1_norm_grad         0.000129  w[0]    0.007 bias    3.403\n",
      "iter 4101/1000000  loss         0.160028  avg_L1_norm_grad         0.000129  w[0]    0.007 bias    3.403\n",
      "iter 4200/1000000  loss         0.159463  avg_L1_norm_grad         0.000127  w[0]    0.008 bias    3.438\n",
      "iter 4201/1000000  loss         0.159458  avg_L1_norm_grad         0.000127  w[0]    0.008 bias    3.438\n",
      "iter 4300/1000000  loss         0.158912  avg_L1_norm_grad         0.000125  w[0]    0.008 bias    3.472\n",
      "iter 4301/1000000  loss         0.158906  avg_L1_norm_grad         0.000125  w[0]    0.008 bias    3.472\n",
      "iter 4400/1000000  loss         0.158378  avg_L1_norm_grad         0.000123  w[0]    0.009 bias    3.505\n",
      "iter 4401/1000000  loss         0.158373  avg_L1_norm_grad         0.000123  w[0]    0.009 bias    3.506\n",
      "iter 4500/1000000  loss         0.157862  avg_L1_norm_grad         0.000121  w[0]    0.010 bias    3.538\n",
      "iter 4501/1000000  loss         0.157857  avg_L1_norm_grad         0.000121  w[0]    0.010 bias    3.539\n",
      "iter 4600/1000000  loss         0.157361  avg_L1_norm_grad         0.000119  w[0]    0.011 bias    3.571\n",
      "iter 4601/1000000  loss         0.157356  avg_L1_norm_grad         0.000119  w[0]    0.011 bias    3.571\n",
      "iter 4700/1000000  loss         0.156876  avg_L1_norm_grad         0.000117  w[0]    0.012 bias    3.602\n",
      "iter 4701/1000000  loss         0.156872  avg_L1_norm_grad         0.000117  w[0]    0.012 bias    3.603\n",
      "iter 4800/1000000  loss         0.156406  avg_L1_norm_grad         0.000115  w[0]    0.013 bias    3.634\n",
      "iter 4801/1000000  loss         0.156401  avg_L1_norm_grad         0.000115  w[0]    0.013 bias    3.634\n",
      "iter 4900/1000000  loss         0.155950  avg_L1_norm_grad         0.000114  w[0]    0.013 bias    3.664\n",
      "iter 4901/1000000  loss         0.155945  avg_L1_norm_grad         0.000114  w[0]    0.013 bias    3.664\n",
      "iter 5000/1000000  loss         0.155506  avg_L1_norm_grad         0.000112  w[0]    0.014 bias    3.694\n",
      "iter 5001/1000000  loss         0.155502  avg_L1_norm_grad         0.000112  w[0]    0.014 bias    3.694\n",
      "iter 5100/1000000  loss         0.155076  avg_L1_norm_grad         0.000111  w[0]    0.015 bias    3.724\n",
      "iter 5101/1000000  loss         0.155072  avg_L1_norm_grad         0.000111  w[0]    0.015 bias    3.724\n",
      "iter 5200/1000000  loss         0.154657  avg_L1_norm_grad         0.000109  w[0]    0.016 bias    3.752\n",
      "iter 5201/1000000  loss         0.154653  avg_L1_norm_grad         0.000109  w[0]    0.016 bias    3.753\n",
      "iter 5300/1000000  loss         0.154250  avg_L1_norm_grad         0.000108  w[0]    0.017 bias    3.781\n",
      "iter 5301/1000000  loss         0.154246  avg_L1_norm_grad         0.000108  w[0]    0.017 bias    3.781\n",
      "iter 5400/1000000  loss         0.153855  avg_L1_norm_grad         0.000106  w[0]    0.017 bias    3.809\n",
      "iter 5401/1000000  loss         0.153851  avg_L1_norm_grad         0.000106  w[0]    0.017 bias    3.809\n",
      "iter 5500/1000000  loss         0.153469  avg_L1_norm_grad         0.000105  w[0]    0.018 bias    3.836\n",
      "iter 5501/1000000  loss         0.153466  avg_L1_norm_grad         0.000105  w[0]    0.018 bias    3.836\n",
      "iter 5600/1000000  loss         0.153094  avg_L1_norm_grad         0.000103  w[0]    0.019 bias    3.863\n",
      "iter 5601/1000000  loss         0.153091  avg_L1_norm_grad         0.000103  w[0]    0.019 bias    3.863\n",
      "iter 5700/1000000  loss         0.152729  avg_L1_norm_grad         0.000102  w[0]    0.020 bias    3.890\n",
      "iter 5701/1000000  loss         0.152725  avg_L1_norm_grad         0.000102  w[0]    0.020 bias    3.890\n",
      "iter 5800/1000000  loss         0.152373  avg_L1_norm_grad         0.000101  w[0]    0.020 bias    3.916\n",
      "iter 5801/1000000  loss         0.152369  avg_L1_norm_grad         0.000101  w[0]    0.020 bias    3.916\n",
      "iter 5900/1000000  loss         0.152026  avg_L1_norm_grad         0.000100  w[0]    0.021 bias    3.941\n",
      "iter 5901/1000000  loss         0.152022  avg_L1_norm_grad         0.000100  w[0]    0.021 bias    3.942\n",
      "iter 6000/1000000  loss         0.151687  avg_L1_norm_grad         0.000098  w[0]    0.022 bias    3.967\n",
      "iter 6001/1000000  loss         0.151684  avg_L1_norm_grad         0.000098  w[0]    0.022 bias    3.967\n",
      "iter 6100/1000000  loss         0.151357  avg_L1_norm_grad         0.000097  w[0]    0.022 bias    3.991\n",
      "iter 6101/1000000  loss         0.151354  avg_L1_norm_grad         0.000097  w[0]    0.022 bias    3.992\n",
      "iter 6200/1000000  loss         0.151035  avg_L1_norm_grad         0.000096  w[0]    0.023 bias    4.016\n",
      "iter 6201/1000000  loss         0.151032  avg_L1_norm_grad         0.000096  w[0]    0.023 bias    4.016\n",
      "iter 6300/1000000  loss         0.150720  avg_L1_norm_grad         0.000095  w[0]    0.024 bias    4.040\n",
      "iter 6301/1000000  loss         0.150717  avg_L1_norm_grad         0.000095  w[0]    0.024 bias    4.040\n",
      "iter 6400/1000000  loss         0.150413  avg_L1_norm_grad         0.000094  w[0]    0.025 bias    4.063\n",
      "iter 6401/1000000  loss         0.150410  avg_L1_norm_grad         0.000094  w[0]    0.025 bias    4.063\n",
      "iter 6500/1000000  loss         0.150113  avg_L1_norm_grad         0.000093  w[0]    0.025 bias    4.086\n",
      "iter 6501/1000000  loss         0.150110  avg_L1_norm_grad         0.000093  w[0]    0.025 bias    4.087\n",
      "iter 6600/1000000  loss         0.149819  avg_L1_norm_grad         0.000092  w[0]    0.026 bias    4.109\n",
      "iter 6601/1000000  loss         0.149816  avg_L1_norm_grad         0.000092  w[0]    0.026 bias    4.109\n",
      "iter 6700/1000000  loss         0.149533  avg_L1_norm_grad         0.000091  w[0]    0.027 bias    4.132\n",
      "iter 6701/1000000  loss         0.149530  avg_L1_norm_grad         0.000091  w[0]    0.027 bias    4.132\n",
      "iter 6800/1000000  loss         0.149252  avg_L1_norm_grad         0.000090  w[0]    0.027 bias    4.154\n",
      "iter 6801/1000000  loss         0.149250  avg_L1_norm_grad         0.000090  w[0]    0.027 bias    4.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.148978  avg_L1_norm_grad         0.000089  w[0]    0.028 bias    4.176\n",
      "iter 6901/1000000  loss         0.148975  avg_L1_norm_grad         0.000089  w[0]    0.028 bias    4.176\n",
      "iter 7000/1000000  loss         0.148710  avg_L1_norm_grad         0.000088  w[0]    0.028 bias    4.197\n",
      "iter 7001/1000000  loss         0.148707  avg_L1_norm_grad         0.000088  w[0]    0.028 bias    4.197\n",
      "iter 7100/1000000  loss         0.148447  avg_L1_norm_grad         0.000087  w[0]    0.029 bias    4.218\n",
      "iter 7101/1000000  loss         0.148445  avg_L1_norm_grad         0.000087  w[0]    0.029 bias    4.218\n",
      "iter 7200/1000000  loss         0.148191  avg_L1_norm_grad         0.000086  w[0]    0.030 bias    4.239\n",
      "iter 7201/1000000  loss         0.148188  avg_L1_norm_grad         0.000086  w[0]    0.030 bias    4.239\n",
      "iter 7300/1000000  loss         0.147939  avg_L1_norm_grad         0.000085  w[0]    0.030 bias    4.259\n",
      "iter 7301/1000000  loss         0.147937  avg_L1_norm_grad         0.000085  w[0]    0.030 bias    4.260\n",
      "iter 7400/1000000  loss         0.147693  avg_L1_norm_grad         0.000084  w[0]    0.031 bias    4.279\n",
      "iter 7401/1000000  loss         0.147690  avg_L1_norm_grad         0.000084  w[0]    0.031 bias    4.280\n",
      "iter 7500/1000000  loss         0.147451  avg_L1_norm_grad         0.000083  w[0]    0.031 bias    4.299\n",
      "iter 7501/1000000  loss         0.147449  avg_L1_norm_grad         0.000083  w[0]    0.031 bias    4.300\n",
      "iter 7600/1000000  loss         0.147215  avg_L1_norm_grad         0.000082  w[0]    0.032 bias    4.319\n",
      "iter 7601/1000000  loss         0.147213  avg_L1_norm_grad         0.000082  w[0]    0.032 bias    4.319\n",
      "iter 7700/1000000  loss         0.146983  avg_L1_norm_grad         0.000082  w[0]    0.033 bias    4.338\n",
      "iter 7701/1000000  loss         0.146981  avg_L1_norm_grad         0.000082  w[0]    0.033 bias    4.338\n",
      "iter 7800/1000000  loss         0.146756  avg_L1_norm_grad         0.000081  w[0]    0.033 bias    4.357\n",
      "iter 7801/1000000  loss         0.146754  avg_L1_norm_grad         0.000081  w[0]    0.033 bias    4.357\n",
      "iter 7900/1000000  loss         0.146533  avg_L1_norm_grad         0.000080  w[0]    0.034 bias    4.376\n",
      "iter 7901/1000000  loss         0.146531  avg_L1_norm_grad         0.000080  w[0]    0.034 bias    4.376\n",
      "iter 8000/1000000  loss         0.146315  avg_L1_norm_grad         0.000079  w[0]    0.034 bias    4.394\n",
      "iter 8001/1000000  loss         0.146313  avg_L1_norm_grad         0.000079  w[0]    0.034 bias    4.394\n",
      "iter 8100/1000000  loss         0.146101  avg_L1_norm_grad         0.000078  w[0]    0.035 bias    4.412\n",
      "iter 8101/1000000  loss         0.146098  avg_L1_norm_grad         0.000078  w[0]    0.035 bias    4.413\n",
      "iter 8200/1000000  loss         0.145890  avg_L1_norm_grad         0.000078  w[0]    0.035 bias    4.430\n",
      "iter 8201/1000000  loss         0.145888  avg_L1_norm_grad         0.000078  w[0]    0.035 bias    4.430\n",
      "iter 8300/1000000  loss         0.145684  avg_L1_norm_grad         0.000077  w[0]    0.036 bias    4.448\n",
      "iter 8301/1000000  loss         0.145682  avg_L1_norm_grad         0.000077  w[0]    0.036 bias    4.448\n",
      "iter 8400/1000000  loss         0.145482  avg_L1_norm_grad         0.000076  w[0]    0.036 bias    4.465\n",
      "iter 8401/1000000  loss         0.145480  avg_L1_norm_grad         0.000076  w[0]    0.036 bias    4.465\n",
      "iter 8500/1000000  loss         0.145283  avg_L1_norm_grad         0.000076  w[0]    0.037 bias    4.482\n",
      "iter 8501/1000000  loss         0.145281  avg_L1_norm_grad         0.000076  w[0]    0.037 bias    4.483\n",
      "iter 8600/1000000  loss         0.145088  avg_L1_norm_grad         0.000075  w[0]    0.037 bias    4.499\n",
      "iter 8601/1000000  loss         0.145086  avg_L1_norm_grad         0.000075  w[0]    0.037 bias    4.500\n",
      "iter 8700/1000000  loss         0.144896  avg_L1_norm_grad         0.000074  w[0]    0.038 bias    4.516\n",
      "iter 8701/1000000  loss         0.144894  avg_L1_norm_grad         0.000074  w[0]    0.038 bias    4.516\n",
      "iter 8800/1000000  loss         0.144708  avg_L1_norm_grad         0.000074  w[0]    0.038 bias    4.532\n",
      "iter 8801/1000000  loss         0.144706  avg_L1_norm_grad         0.000074  w[0]    0.038 bias    4.533\n",
      "iter 8900/1000000  loss         0.144523  avg_L1_norm_grad         0.000073  w[0]    0.039 bias    4.549\n",
      "iter 8901/1000000  loss         0.144521  avg_L1_norm_grad         0.000073  w[0]    0.039 bias    4.549\n",
      "iter 9000/1000000  loss         0.144342  avg_L1_norm_grad         0.000072  w[0]    0.039 bias    4.565\n",
      "iter 9001/1000000  loss         0.144340  avg_L1_norm_grad         0.000072  w[0]    0.039 bias    4.565\n",
      "iter 9100/1000000  loss         0.144163  avg_L1_norm_grad         0.000072  w[0]    0.040 bias    4.580\n",
      "iter 9101/1000000  loss         0.144161  avg_L1_norm_grad         0.000072  w[0]    0.040 bias    4.580\n",
      "iter 9200/1000000  loss         0.143988  avg_L1_norm_grad         0.000071  w[0]    0.040 bias    4.596\n",
      "iter 9201/1000000  loss         0.143986  avg_L1_norm_grad         0.000071  w[0]    0.040 bias    4.596\n",
      "iter 9300/1000000  loss         0.143815  avg_L1_norm_grad         0.000071  w[0]    0.041 bias    4.611\n",
      "iter 9301/1000000  loss         0.143814  avg_L1_norm_grad         0.000070  w[0]    0.041 bias    4.611\n",
      "iter 9400/1000000  loss         0.143646  avg_L1_norm_grad         0.000070  w[0]    0.041 bias    4.626\n",
      "iter 9401/1000000  loss         0.143644  avg_L1_norm_grad         0.000070  w[0]    0.041 bias    4.626\n",
      "iter 9500/1000000  loss         0.143479  avg_L1_norm_grad         0.000069  w[0]    0.041 bias    4.641\n",
      "iter 9501/1000000  loss         0.143477  avg_L1_norm_grad         0.000069  w[0]    0.041 bias    4.641\n",
      "iter 9600/1000000  loss         0.143315  avg_L1_norm_grad         0.000069  w[0]    0.042 bias    4.656\n",
      "iter 9601/1000000  loss         0.143313  avg_L1_norm_grad         0.000069  w[0]    0.042 bias    4.656\n",
      "iter 9700/1000000  loss         0.143154  avg_L1_norm_grad         0.000068  w[0]    0.042 bias    4.670\n",
      "iter 9701/1000000  loss         0.143152  avg_L1_norm_grad         0.000068  w[0]    0.042 bias    4.670\n",
      "iter 9800/1000000  loss         0.142995  avg_L1_norm_grad         0.000068  w[0]    0.043 bias    4.685\n",
      "iter 9801/1000000  loss         0.142994  avg_L1_norm_grad         0.000068  w[0]    0.043 bias    4.685\n",
      "iter 9900/1000000  loss         0.142839  avg_L1_norm_grad         0.000067  w[0]    0.043 bias    4.699\n",
      "iter 9901/1000000  loss         0.142837  avg_L1_norm_grad         0.000067  w[0]    0.043 bias    4.699\n",
      "iter 10000/1000000  loss         0.142685  avg_L1_norm_grad         0.000067  w[0]    0.044 bias    4.713\n",
      "iter 10001/1000000  loss         0.142684  avg_L1_norm_grad         0.000067  w[0]    0.044 bias    4.713\n",
      "iter 10100/1000000  loss         0.142534  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    4.726\n",
      "iter 10101/1000000  loss         0.142533  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    4.726\n",
      "iter 10200/1000000  loss         0.142385  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    4.740\n",
      "iter 10201/1000000  loss         0.142384  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    4.740\n",
      "iter 10300/1000000  loss         0.142239  avg_L1_norm_grad         0.000065  w[0]    0.045 bias    4.753\n",
      "iter 10301/1000000  loss         0.142238  avg_L1_norm_grad         0.000065  w[0]    0.045 bias    4.753\n",
      "iter 10400/1000000  loss         0.142095  avg_L1_norm_grad         0.000065  w[0]    0.045 bias    4.766\n",
      "iter 10401/1000000  loss         0.142093  avg_L1_norm_grad         0.000065  w[0]    0.045 bias    4.767\n",
      "iter 10500/1000000  loss         0.141953  avg_L1_norm_grad         0.000064  w[0]    0.045 bias    4.779\n",
      "iter 10501/1000000  loss         0.141951  avg_L1_norm_grad         0.000064  w[0]    0.045 bias    4.780\n",
      "iter 10600/1000000  loss         0.141813  avg_L1_norm_grad         0.000064  w[0]    0.046 bias    4.792\n",
      "iter 10601/1000000  loss         0.141812  avg_L1_norm_grad         0.000064  w[0]    0.046 bias    4.792\n",
      "iter 10700/1000000  loss         0.141675  avg_L1_norm_grad         0.000063  w[0]    0.046 bias    4.805\n",
      "iter 10701/1000000  loss         0.141674  avg_L1_norm_grad         0.000063  w[0]    0.046 bias    4.805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.141540  avg_L1_norm_grad         0.000063  w[0]    0.047 bias    4.818\n",
      "iter 10801/1000000  loss         0.141538  avg_L1_norm_grad         0.000063  w[0]    0.047 bias    4.818\n",
      "iter 10900/1000000  loss         0.141406  avg_L1_norm_grad         0.000062  w[0]    0.047 bias    4.830\n",
      "iter 10901/1000000  loss         0.141405  avg_L1_norm_grad         0.000062  w[0]    0.047 bias    4.830\n",
      "iter 11000/1000000  loss         0.141274  avg_L1_norm_grad         0.000062  w[0]    0.047 bias    4.842\n",
      "iter 11001/1000000  loss         0.141273  avg_L1_norm_grad         0.000062  w[0]    0.047 bias    4.842\n",
      "iter 11100/1000000  loss         0.141144  avg_L1_norm_grad         0.000061  w[0]    0.048 bias    4.854\n",
      "iter 11101/1000000  loss         0.141143  avg_L1_norm_grad         0.000061  w[0]    0.048 bias    4.854\n",
      "iter 11200/1000000  loss         0.141017  avg_L1_norm_grad         0.000061  w[0]    0.048 bias    4.866\n",
      "iter 11201/1000000  loss         0.141015  avg_L1_norm_grad         0.000061  w[0]    0.048 bias    4.866\n",
      "iter 11300/1000000  loss         0.140891  avg_L1_norm_grad         0.000060  w[0]    0.048 bias    4.878\n",
      "iter 11301/1000000  loss         0.140889  avg_L1_norm_grad         0.000060  w[0]    0.048 bias    4.878\n",
      "iter 11400/1000000  loss         0.140766  avg_L1_norm_grad         0.000060  w[0]    0.049 bias    4.889\n",
      "iter 11401/1000000  loss         0.140765  avg_L1_norm_grad         0.000060  w[0]    0.049 bias    4.889\n",
      "iter 11500/1000000  loss         0.140644  avg_L1_norm_grad         0.000060  w[0]    0.049 bias    4.901\n",
      "iter 11501/1000000  loss         0.140643  avg_L1_norm_grad         0.000060  w[0]    0.049 bias    4.901\n",
      "iter 11600/1000000  loss         0.140523  avg_L1_norm_grad         0.000059  w[0]    0.049 bias    4.912\n",
      "iter 11601/1000000  loss         0.140522  avg_L1_norm_grad         0.000059  w[0]    0.049 bias    4.912\n",
      "iter 11700/1000000  loss         0.140404  avg_L1_norm_grad         0.000059  w[0]    0.050 bias    4.923\n",
      "iter 11701/1000000  loss         0.140403  avg_L1_norm_grad         0.000059  w[0]    0.050 bias    4.923\n",
      "iter 11800/1000000  loss         0.140287  avg_L1_norm_grad         0.000058  w[0]    0.050 bias    4.934\n",
      "iter 11801/1000000  loss         0.140286  avg_L1_norm_grad         0.000058  w[0]    0.050 bias    4.934\n",
      "iter 11900/1000000  loss         0.140171  avg_L1_norm_grad         0.000058  w[0]    0.050 bias    4.945\n",
      "iter 11901/1000000  loss         0.140170  avg_L1_norm_grad         0.000058  w[0]    0.050 bias    4.945\n",
      "iter 12000/1000000  loss         0.140057  avg_L1_norm_grad         0.000058  w[0]    0.051 bias    4.956\n",
      "iter 12001/1000000  loss         0.140056  avg_L1_norm_grad         0.000058  w[0]    0.051 bias    4.956\n",
      "iter 12100/1000000  loss         0.139944  avg_L1_norm_grad         0.000057  w[0]    0.051 bias    4.966\n",
      "iter 12101/1000000  loss         0.139943  avg_L1_norm_grad         0.000057  w[0]    0.051 bias    4.966\n",
      "iter 12200/1000000  loss         0.139833  avg_L1_norm_grad         0.000057  w[0]    0.051 bias    4.977\n",
      "iter 12201/1000000  loss         0.139832  avg_L1_norm_grad         0.000057  w[0]    0.051 bias    4.977\n",
      "iter 12300/1000000  loss         0.139724  avg_L1_norm_grad         0.000056  w[0]    0.052 bias    4.987\n",
      "iter 12301/1000000  loss         0.139723  avg_L1_norm_grad         0.000056  w[0]    0.052 bias    4.987\n",
      "iter 12400/1000000  loss         0.139616  avg_L1_norm_grad         0.000056  w[0]    0.052 bias    4.997\n",
      "iter 12401/1000000  loss         0.139615  avg_L1_norm_grad         0.000056  w[0]    0.052 bias    4.997\n",
      "iter 12500/1000000  loss         0.139509  avg_L1_norm_grad         0.000056  w[0]    0.052 bias    5.007\n",
      "iter 12501/1000000  loss         0.139508  avg_L1_norm_grad         0.000056  w[0]    0.052 bias    5.007\n",
      "iter 12600/1000000  loss         0.139404  avg_L1_norm_grad         0.000055  w[0]    0.052 bias    5.017\n",
      "iter 12601/1000000  loss         0.139403  avg_L1_norm_grad         0.000055  w[0]    0.052 bias    5.017\n",
      "iter 12700/1000000  loss         0.139300  avg_L1_norm_grad         0.000055  w[0]    0.053 bias    5.027\n",
      "iter 12701/1000000  loss         0.139299  avg_L1_norm_grad         0.000055  w[0]    0.053 bias    5.027\n",
      "iter 12800/1000000  loss         0.139197  avg_L1_norm_grad         0.000055  w[0]    0.053 bias    5.037\n",
      "iter 12801/1000000  loss         0.139196  avg_L1_norm_grad         0.000055  w[0]    0.053 bias    5.037\n",
      "iter 12900/1000000  loss         0.139096  avg_L1_norm_grad         0.000054  w[0]    0.053 bias    5.046\n",
      "iter 12901/1000000  loss         0.139095  avg_L1_norm_grad         0.000054  w[0]    0.053 bias    5.047\n",
      "iter 13000/1000000  loss         0.138996  avg_L1_norm_grad         0.000054  w[0]    0.054 bias    5.056\n",
      "iter 13001/1000000  loss         0.138995  avg_L1_norm_grad         0.000054  w[0]    0.054 bias    5.056\n",
      "iter 13100/1000000  loss         0.138898  avg_L1_norm_grad         0.000054  w[0]    0.054 bias    5.065\n",
      "iter 13101/1000000  loss         0.138897  avg_L1_norm_grad         0.000054  w[0]    0.054 bias    5.065\n",
      "iter 13200/1000000  loss         0.138800  avg_L1_norm_grad         0.000053  w[0]    0.054 bias    5.075\n",
      "iter 13201/1000000  loss         0.138799  avg_L1_norm_grad         0.000053  w[0]    0.054 bias    5.075\n",
      "iter 13300/1000000  loss         0.138704  avg_L1_norm_grad         0.000053  w[0]    0.054 bias    5.084\n",
      "iter 13301/1000000  loss         0.138703  avg_L1_norm_grad         0.000053  w[0]    0.054 bias    5.084\n",
      "iter 13400/1000000  loss         0.138609  avg_L1_norm_grad         0.000053  w[0]    0.055 bias    5.093\n",
      "iter 13401/1000000  loss         0.138608  avg_L1_norm_grad         0.000053  w[0]    0.055 bias    5.093\n",
      "iter 13500/1000000  loss         0.138515  avg_L1_norm_grad         0.000052  w[0]    0.055 bias    5.102\n",
      "iter 13501/1000000  loss         0.138514  avg_L1_norm_grad         0.000052  w[0]    0.055 bias    5.102\n",
      "iter 13600/1000000  loss         0.138423  avg_L1_norm_grad         0.000052  w[0]    0.055 bias    5.110\n",
      "iter 13601/1000000  loss         0.138422  avg_L1_norm_grad         0.000052  w[0]    0.055 bias    5.111\n",
      "iter 13700/1000000  loss         0.138331  avg_L1_norm_grad         0.000052  w[0]    0.055 bias    5.119\n",
      "iter 13701/1000000  loss         0.138330  avg_L1_norm_grad         0.000052  w[0]    0.055 bias    5.119\n",
      "iter 13800/1000000  loss         0.138241  avg_L1_norm_grad         0.000051  w[0]    0.056 bias    5.128\n",
      "iter 13801/1000000  loss         0.138240  avg_L1_norm_grad         0.000051  w[0]    0.056 bias    5.128\n",
      "iter 13900/1000000  loss         0.138152  avg_L1_norm_grad         0.000051  w[0]    0.056 bias    5.136\n",
      "iter 13901/1000000  loss         0.138151  avg_L1_norm_grad         0.000051  w[0]    0.056 bias    5.136\n",
      "iter 14000/1000000  loss         0.138064  avg_L1_norm_grad         0.000051  w[0]    0.056 bias    5.145\n",
      "iter 14001/1000000  loss         0.138063  avg_L1_norm_grad         0.000051  w[0]    0.056 bias    5.145\n",
      "iter 14100/1000000  loss         0.137977  avg_L1_norm_grad         0.000050  w[0]    0.056 bias    5.153\n",
      "iter 14101/1000000  loss         0.137976  avg_L1_norm_grad         0.000050  w[0]    0.056 bias    5.153\n",
      "iter 14200/1000000  loss         0.137891  avg_L1_norm_grad         0.000050  w[0]    0.057 bias    5.161\n",
      "iter 14201/1000000  loss         0.137890  avg_L1_norm_grad         0.000050  w[0]    0.057 bias    5.161\n",
      "iter 14300/1000000  loss         0.137806  avg_L1_norm_grad         0.000050  w[0]    0.057 bias    5.169\n",
      "iter 14301/1000000  loss         0.137805  avg_L1_norm_grad         0.000050  w[0]    0.057 bias    5.169\n",
      "iter 14400/1000000  loss         0.137722  avg_L1_norm_grad         0.000050  w[0]    0.057 bias    5.177\n",
      "iter 14401/1000000  loss         0.137721  avg_L1_norm_grad         0.000050  w[0]    0.057 bias    5.177\n",
      "iter 14500/1000000  loss         0.137639  avg_L1_norm_grad         0.000049  w[0]    0.057 bias    5.185\n",
      "iter 14501/1000000  loss         0.137638  avg_L1_norm_grad         0.000049  w[0]    0.057 bias    5.185\n",
      "iter 14600/1000000  loss         0.137557  avg_L1_norm_grad         0.000049  w[0]    0.057 bias    5.193\n",
      "iter 14601/1000000  loss         0.137556  avg_L1_norm_grad         0.000049  w[0]    0.057 bias    5.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.137476  avg_L1_norm_grad         0.000049  w[0]    0.058 bias    5.201\n",
      "iter 14701/1000000  loss         0.137475  avg_L1_norm_grad         0.000049  w[0]    0.058 bias    5.201\n",
      "iter 14800/1000000  loss         0.137396  avg_L1_norm_grad         0.000048  w[0]    0.058 bias    5.208\n",
      "iter 14801/1000000  loss         0.137395  avg_L1_norm_grad         0.000048  w[0]    0.058 bias    5.208\n",
      "iter 14900/1000000  loss         0.137316  avg_L1_norm_grad         0.000048  w[0]    0.058 bias    5.216\n",
      "iter 14901/1000000  loss         0.137316  avg_L1_norm_grad         0.000048  w[0]    0.058 bias    5.216\n",
      "iter 15000/1000000  loss         0.137238  avg_L1_norm_grad         0.000048  w[0]    0.058 bias    5.223\n",
      "iter 15001/1000000  loss         0.137237  avg_L1_norm_grad         0.000048  w[0]    0.058 bias    5.223\n",
      "iter 15100/1000000  loss         0.137161  avg_L1_norm_grad         0.000048  w[0]    0.059 bias    5.231\n",
      "iter 15101/1000000  loss         0.137160  avg_L1_norm_grad         0.000048  w[0]    0.059 bias    5.231\n",
      "iter 15200/1000000  loss         0.137084  avg_L1_norm_grad         0.000047  w[0]    0.059 bias    5.238\n",
      "iter 15201/1000000  loss         0.137083  avg_L1_norm_grad         0.000047  w[0]    0.059 bias    5.238\n",
      "iter 15300/1000000  loss         0.137009  avg_L1_norm_grad         0.000047  w[0]    0.059 bias    5.245\n",
      "iter 15301/1000000  loss         0.137008  avg_L1_norm_grad         0.000047  w[0]    0.059 bias    5.245\n",
      "iter 15400/1000000  loss         0.136934  avg_L1_norm_grad         0.000047  w[0]    0.059 bias    5.252\n",
      "iter 15401/1000000  loss         0.136933  avg_L1_norm_grad         0.000047  w[0]    0.059 bias    5.252\n",
      "iter 15500/1000000  loss         0.136860  avg_L1_norm_grad         0.000046  w[0]    0.059 bias    5.259\n",
      "iter 15501/1000000  loss         0.136859  avg_L1_norm_grad         0.000046  w[0]    0.059 bias    5.259\n",
      "iter 15600/1000000  loss         0.136787  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    5.266\n",
      "iter 15601/1000000  loss         0.136786  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    5.266\n",
      "iter 15700/1000000  loss         0.136714  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    5.273\n",
      "iter 15701/1000000  loss         0.136714  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    5.273\n",
      "iter 15800/1000000  loss         0.136643  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    5.280\n",
      "iter 15801/1000000  loss         0.136642  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    5.280\n",
      "iter 15900/1000000  loss         0.136572  avg_L1_norm_grad         0.000045  w[0]    0.060 bias    5.287\n",
      "iter 15901/1000000  loss         0.136571  avg_L1_norm_grad         0.000045  w[0]    0.060 bias    5.287\n",
      "iter 16000/1000000  loss         0.136502  avg_L1_norm_grad         0.000045  w[0]    0.060 bias    5.293\n",
      "iter 16001/1000000  loss         0.136501  avg_L1_norm_grad         0.000045  w[0]    0.060 bias    5.293\n",
      "iter 16100/1000000  loss         0.136433  avg_L1_norm_grad         0.000045  w[0]    0.061 bias    5.300\n",
      "iter 16101/1000000  loss         0.136432  avg_L1_norm_grad         0.000045  w[0]    0.061 bias    5.300\n",
      "iter 16200/1000000  loss         0.136365  avg_L1_norm_grad         0.000045  w[0]    0.061 bias    5.306\n",
      "iter 16201/1000000  loss         0.136364  avg_L1_norm_grad         0.000045  w[0]    0.061 bias    5.306\n",
      "iter 16300/1000000  loss         0.136297  avg_L1_norm_grad         0.000045  w[0]    0.061 bias    5.313\n",
      "iter 16301/1000000  loss         0.136296  avg_L1_norm_grad         0.000045  w[0]    0.061 bias    5.313\n",
      "iter 16400/1000000  loss         0.136230  avg_L1_norm_grad         0.000044  w[0]    0.061 bias    5.319\n",
      "iter 16401/1000000  loss         0.136229  avg_L1_norm_grad         0.000044  w[0]    0.061 bias    5.319\n",
      "iter 16500/1000000  loss         0.136164  avg_L1_norm_grad         0.000044  w[0]    0.061 bias    5.325\n",
      "iter 16501/1000000  loss         0.136163  avg_L1_norm_grad         0.000044  w[0]    0.061 bias    5.325\n",
      "iter 16600/1000000  loss         0.136098  avg_L1_norm_grad         0.000044  w[0]    0.062 bias    5.331\n",
      "iter 16601/1000000  loss         0.136097  avg_L1_norm_grad         0.000044  w[0]    0.062 bias    5.332\n",
      "iter 16700/1000000  loss         0.136033  avg_L1_norm_grad         0.000044  w[0]    0.062 bias    5.338\n",
      "iter 16701/1000000  loss         0.136033  avg_L1_norm_grad         0.000044  w[0]    0.062 bias    5.338\n",
      "iter 16800/1000000  loss         0.135969  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.344\n",
      "iter 16801/1000000  loss         0.135968  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.344\n",
      "iter 16900/1000000  loss         0.135906  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.350\n",
      "iter 16901/1000000  loss         0.135905  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.350\n",
      "iter 17000/1000000  loss         0.135843  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.356\n",
      "iter 17001/1000000  loss         0.135842  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.356\n",
      "iter 17100/1000000  loss         0.135781  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.361\n",
      "iter 17101/1000000  loss         0.135780  avg_L1_norm_grad         0.000043  w[0]    0.062 bias    5.361\n",
      "iter 17200/1000000  loss         0.135719  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.367\n",
      "iter 17201/1000000  loss         0.135718  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.367\n",
      "iter 17300/1000000  loss         0.135658  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.373\n",
      "iter 17301/1000000  loss         0.135658  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.373\n",
      "iter 17400/1000000  loss         0.135598  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.379\n",
      "iter 17401/1000000  loss         0.135597  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.379\n",
      "iter 17500/1000000  loss         0.135538  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.384\n",
      "iter 17501/1000000  loss         0.135538  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.384\n",
      "iter 17600/1000000  loss         0.135479  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.390\n",
      "iter 17601/1000000  loss         0.135479  avg_L1_norm_grad         0.000042  w[0]    0.063 bias    5.390\n",
      "iter 17700/1000000  loss         0.135421  avg_L1_norm_grad         0.000041  w[0]    0.063 bias    5.395\n",
      "iter 17701/1000000  loss         0.135420  avg_L1_norm_grad         0.000041  w[0]    0.063 bias    5.395\n",
      "iter 17800/1000000  loss         0.135363  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.401\n",
      "iter 17801/1000000  loss         0.135362  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.401\n",
      "iter 17900/1000000  loss         0.135306  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.406\n",
      "iter 17901/1000000  loss         0.135305  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.406\n",
      "iter 18000/1000000  loss         0.135249  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.411\n",
      "iter 18001/1000000  loss         0.135249  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.411\n",
      "iter 18100/1000000  loss         0.135193  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.416\n",
      "iter 18101/1000000  loss         0.135192  avg_L1_norm_grad         0.000041  w[0]    0.064 bias    5.417\n",
      "iter 18200/1000000  loss         0.135137  avg_L1_norm_grad         0.000040  w[0]    0.064 bias    5.422\n",
      "iter 18201/1000000  loss         0.135137  avg_L1_norm_grad         0.000040  w[0]    0.064 bias    5.422\n",
      "iter 18300/1000000  loss         0.135082  avg_L1_norm_grad         0.000040  w[0]    0.064 bias    5.427\n",
      "iter 18301/1000000  loss         0.135082  avg_L1_norm_grad         0.000040  w[0]    0.064 bias    5.427\n",
      "iter 18400/1000000  loss         0.135028  avg_L1_norm_grad         0.000040  w[0]    0.064 bias    5.432\n",
      "iter 18401/1000000  loss         0.135027  avg_L1_norm_grad         0.000040  w[0]    0.064 bias    5.432\n",
      "iter 18500/1000000  loss         0.134974  avg_L1_norm_grad         0.000040  w[0]    0.065 bias    5.437\n",
      "iter 18501/1000000  loss         0.134973  avg_L1_norm_grad         0.000040  w[0]    0.065 bias    5.437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18600/1000000  loss         0.134921  avg_L1_norm_grad         0.000040  w[0]    0.065 bias    5.442\n",
      "iter 18601/1000000  loss         0.134920  avg_L1_norm_grad         0.000040  w[0]    0.065 bias    5.442\n",
      "iter 18700/1000000  loss         0.134868  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.447\n",
      "iter 18701/1000000  loss         0.134867  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.447\n",
      "iter 18800/1000000  loss         0.134815  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.452\n",
      "iter 18801/1000000  loss         0.134815  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.452\n",
      "iter 18900/1000000  loss         0.134764  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.456\n",
      "iter 18901/1000000  loss         0.134763  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.456\n",
      "iter 19000/1000000  loss         0.134712  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.461\n",
      "iter 19001/1000000  loss         0.134712  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.461\n",
      "iter 19100/1000000  loss         0.134661  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.466\n",
      "iter 19101/1000000  loss         0.134661  avg_L1_norm_grad         0.000039  w[0]    0.065 bias    5.466\n",
      "iter 19200/1000000  loss         0.134611  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.470\n",
      "iter 19201/1000000  loss         0.134611  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.470\n",
      "iter 19300/1000000  loss         0.134561  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.475\n",
      "iter 19301/1000000  loss         0.134561  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.475\n",
      "iter 19400/1000000  loss         0.134512  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.480\n",
      "iter 19401/1000000  loss         0.134511  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.480\n",
      "iter 19500/1000000  loss         0.134463  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.484\n",
      "iter 19501/1000000  loss         0.134462  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.484\n",
      "iter 19600/1000000  loss         0.134414  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.488\n",
      "iter 19601/1000000  loss         0.134414  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.489\n",
      "iter 19700/1000000  loss         0.134366  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.493\n",
      "iter 19701/1000000  loss         0.134366  avg_L1_norm_grad         0.000038  w[0]    0.066 bias    5.493\n",
      "iter 19800/1000000  loss         0.134319  avg_L1_norm_grad         0.000037  w[0]    0.066 bias    5.497\n",
      "iter 19801/1000000  loss         0.134318  avg_L1_norm_grad         0.000037  w[0]    0.066 bias    5.497\n",
      "iter 19900/1000000  loss         0.134272  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.501\n",
      "iter 19901/1000000  loss         0.134271  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.502\n",
      "iter 20000/1000000  loss         0.134225  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.506\n",
      "iter 20001/1000000  loss         0.134225  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.506\n",
      "iter 20100/1000000  loss         0.134179  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.510\n",
      "iter 20101/1000000  loss         0.134178  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.510\n",
      "iter 20200/1000000  loss         0.134133  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.514\n",
      "iter 20201/1000000  loss         0.134132  avg_L1_norm_grad         0.000037  w[0]    0.067 bias    5.514\n",
      "iter 20300/1000000  loss         0.134088  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.518\n",
      "iter 20301/1000000  loss         0.134087  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.518\n",
      "iter 20400/1000000  loss         0.134043  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.522\n",
      "iter 20401/1000000  loss         0.134042  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.522\n",
      "iter 20500/1000000  loss         0.133998  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.526\n",
      "iter 20501/1000000  loss         0.133998  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.526\n",
      "iter 20600/1000000  loss         0.133954  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.530\n",
      "iter 20601/1000000  loss         0.133953  avg_L1_norm_grad         0.000036  w[0]    0.067 bias    5.530\n",
      "iter 20700/1000000  loss         0.133910  avg_L1_norm_grad         0.000036  w[0]    0.068 bias    5.534\n",
      "iter 20701/1000000  loss         0.133910  avg_L1_norm_grad         0.000036  w[0]    0.068 bias    5.534\n",
      "iter 20800/1000000  loss         0.133867  avg_L1_norm_grad         0.000036  w[0]    0.068 bias    5.538\n",
      "iter 20801/1000000  loss         0.133866  avg_L1_norm_grad         0.000036  w[0]    0.068 bias    5.538\n",
      "iter 20900/1000000  loss         0.133824  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.542\n",
      "iter 20901/1000000  loss         0.133823  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.542\n",
      "iter 21000/1000000  loss         0.133781  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.546\n",
      "iter 21001/1000000  loss         0.133781  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.546\n",
      "iter 21100/1000000  loss         0.133739  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.550\n",
      "iter 21101/1000000  loss         0.133739  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.550\n",
      "iter 21200/1000000  loss         0.133697  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.553\n",
      "iter 21201/1000000  loss         0.133697  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.553\n",
      "iter 21300/1000000  loss         0.133656  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.557\n",
      "iter 21301/1000000  loss         0.133656  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.557\n",
      "iter 21400/1000000  loss         0.133615  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.561\n",
      "iter 21401/1000000  loss         0.133614  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.561\n",
      "iter 21500/1000000  loss         0.133574  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.564\n",
      "iter 21501/1000000  loss         0.133574  avg_L1_norm_grad         0.000035  w[0]    0.068 bias    5.564\n",
      "iter 21600/1000000  loss         0.133534  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.568\n",
      "iter 21601/1000000  loss         0.133533  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.568\n",
      "iter 21700/1000000  loss         0.133494  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.571\n",
      "iter 21701/1000000  loss         0.133494  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.571\n",
      "iter 21800/1000000  loss         0.133454  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.575\n",
      "iter 21801/1000000  loss         0.133454  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.575\n",
      "iter 21900/1000000  loss         0.133415  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.578\n",
      "iter 21901/1000000  loss         0.133415  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.578\n",
      "iter 22000/1000000  loss         0.133376  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.582\n",
      "iter 22001/1000000  loss         0.133376  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.582\n",
      "iter 22100/1000000  loss         0.133338  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.585\n",
      "iter 22101/1000000  loss         0.133337  avg_L1_norm_grad         0.000034  w[0]    0.069 bias    5.585\n",
      "iter 22200/1000000  loss         0.133299  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.589\n",
      "iter 22201/1000000  loss         0.133299  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.589\n",
      "iter 22300/1000000  loss         0.133261  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.592\n",
      "iter 22301/1000000  loss         0.133261  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.592\n",
      "iter 22400/1000000  loss         0.133224  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.595\n",
      "iter 22401/1000000  loss         0.133223  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22500/1000000  loss         0.133187  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.598\n",
      "iter 22501/1000000  loss         0.133186  avg_L1_norm_grad         0.000033  w[0]    0.069 bias    5.598\n",
      "iter 22600/1000000  loss         0.133150  avg_L1_norm_grad         0.000033  w[0]    0.070 bias    5.602\n",
      "iter 22601/1000000  loss         0.133149  avg_L1_norm_grad         0.000033  w[0]    0.070 bias    5.602\n",
      "iter 22700/1000000  loss         0.133113  avg_L1_norm_grad         0.000033  w[0]    0.070 bias    5.605\n",
      "iter 22701/1000000  loss         0.133113  avg_L1_norm_grad         0.000033  w[0]    0.070 bias    5.605\n",
      "iter 22800/1000000  loss         0.133077  avg_L1_norm_grad         0.000033  w[0]    0.070 bias    5.608\n",
      "iter 22801/1000000  loss         0.133076  avg_L1_norm_grad         0.000033  w[0]    0.070 bias    5.608\n",
      "iter 22900/1000000  loss         0.133041  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.611\n",
      "iter 22901/1000000  loss         0.133040  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.611\n",
      "iter 23000/1000000  loss         0.133005  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.614\n",
      "iter 23001/1000000  loss         0.133005  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.614\n",
      "iter 23100/1000000  loss         0.132970  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.617\n",
      "iter 23101/1000000  loss         0.132969  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.617\n",
      "iter 23200/1000000  loss         0.132935  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.620\n",
      "iter 23201/1000000  loss         0.132934  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.620\n",
      "iter 23300/1000000  loss         0.132900  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.623\n",
      "iter 23301/1000000  loss         0.132900  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.623\n",
      "iter 23400/1000000  loss         0.132865  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.626\n",
      "iter 23401/1000000  loss         0.132865  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.626\n",
      "iter 23500/1000000  loss         0.132831  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.629\n",
      "iter 23501/1000000  loss         0.132831  avg_L1_norm_grad         0.000032  w[0]    0.070 bias    5.629\n",
      "iter 23600/1000000  loss         0.132797  avg_L1_norm_grad         0.000032  w[0]    0.071 bias    5.632\n",
      "iter 23601/1000000  loss         0.132797  avg_L1_norm_grad         0.000032  w[0]    0.071 bias    5.632\n",
      "iter 23700/1000000  loss         0.132764  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.635\n",
      "iter 23701/1000000  loss         0.132763  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.635\n",
      "iter 23800/1000000  loss         0.132730  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.638\n",
      "iter 23801/1000000  loss         0.132730  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.638\n",
      "iter 23900/1000000  loss         0.132697  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.641\n",
      "iter 23901/1000000  loss         0.132697  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.641\n",
      "iter 24000/1000000  loss         0.132665  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.643\n",
      "iter 24001/1000000  loss         0.132664  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.643\n",
      "iter 24100/1000000  loss         0.132632  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.646\n",
      "iter 24101/1000000  loss         0.132632  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.646\n",
      "iter 24200/1000000  loss         0.132600  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.649\n",
      "iter 24201/1000000  loss         0.132600  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.649\n",
      "iter 24300/1000000  loss         0.132568  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.651\n",
      "iter 24301/1000000  loss         0.132568  avg_L1_norm_grad         0.000031  w[0]    0.071 bias    5.652\n",
      "iter 24400/1000000  loss         0.132536  avg_L1_norm_grad         0.000030  w[0]    0.071 bias    5.654\n",
      "iter 24401/1000000  loss         0.132536  avg_L1_norm_grad         0.000030  w[0]    0.071 bias    5.654\n",
      "iter 24500/1000000  loss         0.132505  avg_L1_norm_grad         0.000030  w[0]    0.071 bias    5.657\n",
      "iter 24501/1000000  loss         0.132504  avg_L1_norm_grad         0.000030  w[0]    0.071 bias    5.657\n",
      "iter 24600/1000000  loss         0.132474  avg_L1_norm_grad         0.000030  w[0]    0.071 bias    5.659\n",
      "iter 24601/1000000  loss         0.132473  avg_L1_norm_grad         0.000030  w[0]    0.071 bias    5.659\n",
      "iter 24700/1000000  loss         0.132443  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.662\n",
      "iter 24701/1000000  loss         0.132442  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.662\n",
      "iter 24800/1000000  loss         0.132412  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.665\n",
      "iter 24801/1000000  loss         0.132412  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.665\n",
      "iter 24900/1000000  loss         0.132382  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.667\n",
      "iter 24901/1000000  loss         0.132381  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.667\n",
      "iter 25000/1000000  loss         0.132351  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.670\n",
      "iter 25001/1000000  loss         0.132351  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.670\n",
      "iter 25100/1000000  loss         0.132321  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.672\n",
      "iter 25101/1000000  loss         0.132321  avg_L1_norm_grad         0.000030  w[0]    0.072 bias    5.672\n",
      "iter 25200/1000000  loss         0.132292  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.675\n",
      "iter 25201/1000000  loss         0.132291  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.675\n",
      "iter 25300/1000000  loss         0.132262  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.677\n",
      "iter 25301/1000000  loss         0.132262  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.677\n",
      "iter 25400/1000000  loss         0.132233  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.679\n",
      "iter 25401/1000000  loss         0.132233  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.679\n",
      "iter 25500/1000000  loss         0.132204  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.682\n",
      "iter 25501/1000000  loss         0.132204  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.682\n",
      "iter 25600/1000000  loss         0.132175  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.684\n",
      "iter 25601/1000000  loss         0.132175  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.684\n",
      "iter 25700/1000000  loss         0.132147  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.686\n",
      "iter 25701/1000000  loss         0.132147  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.686\n",
      "iter 25800/1000000  loss         0.132119  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.689\n",
      "iter 25801/1000000  loss         0.132118  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.689\n",
      "iter 25900/1000000  loss         0.132091  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.691\n",
      "iter 25901/1000000  loss         0.132090  avg_L1_norm_grad         0.000029  w[0]    0.072 bias    5.691\n",
      "iter 26000/1000000  loss         0.132063  avg_L1_norm_grad         0.000029  w[0]    0.073 bias    5.693\n",
      "iter 26001/1000000  loss         0.132062  avg_L1_norm_grad         0.000029  w[0]    0.073 bias    5.693\n",
      "iter 26100/1000000  loss         0.132035  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.696\n",
      "iter 26101/1000000  loss         0.132035  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.696\n",
      "iter 26200/1000000  loss         0.132008  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.698\n",
      "iter 26201/1000000  loss         0.132007  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.698\n",
      "iter 26300/1000000  loss         0.131981  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.700\n",
      "iter 26301/1000000  loss         0.131980  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26400/1000000  loss         0.131954  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.702\n",
      "iter 26401/1000000  loss         0.131953  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.702\n",
      "iter 26500/1000000  loss         0.131927  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.704\n",
      "iter 26501/1000000  loss         0.131927  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.704\n",
      "iter 26600/1000000  loss         0.131900  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.706\n",
      "iter 26601/1000000  loss         0.131900  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.706\n",
      "iter 26700/1000000  loss         0.131874  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.708\n",
      "iter 26701/1000000  loss         0.131874  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.708\n",
      "iter 26800/1000000  loss         0.131848  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.711\n",
      "iter 26801/1000000  loss         0.131848  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.711\n",
      "iter 26900/1000000  loss         0.131822  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.713\n",
      "iter 26901/1000000  loss         0.131822  avg_L1_norm_grad         0.000028  w[0]    0.073 bias    5.713\n",
      "iter 27000/1000000  loss         0.131796  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.715\n",
      "iter 27001/1000000  loss         0.131796  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.715\n",
      "iter 27100/1000000  loss         0.131771  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.717\n",
      "iter 27101/1000000  loss         0.131771  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.717\n",
      "iter 27200/1000000  loss         0.131745  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.719\n",
      "iter 27201/1000000  loss         0.131745  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.719\n",
      "iter 27300/1000000  loss         0.131720  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.721\n",
      "iter 27301/1000000  loss         0.131720  avg_L1_norm_grad         0.000027  w[0]    0.073 bias    5.721\n",
      "iter 27400/1000000  loss         0.131695  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.723\n",
      "iter 27401/1000000  loss         0.131695  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.723\n",
      "iter 27500/1000000  loss         0.131671  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.725\n",
      "iter 27501/1000000  loss         0.131670  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.725\n",
      "iter 27600/1000000  loss         0.131646  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.726\n",
      "iter 27601/1000000  loss         0.131646  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.726\n",
      "iter 27700/1000000  loss         0.131622  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.728\n",
      "iter 27701/1000000  loss         0.131622  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.728\n",
      "iter 27800/1000000  loss         0.131598  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.730\n",
      "iter 27801/1000000  loss         0.131597  avg_L1_norm_grad         0.000027  w[0]    0.074 bias    5.730\n",
      "iter 27900/1000000  loss         0.131574  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.732\n",
      "iter 27901/1000000  loss         0.131573  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.732\n",
      "iter 28000/1000000  loss         0.131550  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.734\n",
      "iter 28001/1000000  loss         0.131550  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.734\n",
      "iter 28100/1000000  loss         0.131526  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.736\n",
      "iter 28101/1000000  loss         0.131526  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.736\n",
      "iter 28200/1000000  loss         0.131503  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.738\n",
      "iter 28201/1000000  loss         0.131503  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.738\n",
      "iter 28300/1000000  loss         0.131480  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.739\n",
      "iter 28301/1000000  loss         0.131479  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.739\n",
      "iter 28400/1000000  loss         0.131457  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.741\n",
      "iter 28401/1000000  loss         0.131456  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.741\n",
      "iter 28500/1000000  loss         0.131434  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.743\n",
      "iter 28501/1000000  loss         0.131433  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.743\n",
      "iter 28600/1000000  loss         0.131411  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.745\n",
      "iter 28601/1000000  loss         0.131411  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.745\n",
      "iter 28700/1000000  loss         0.131388  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.746\n",
      "iter 28701/1000000  loss         0.131388  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.746\n",
      "iter 28800/1000000  loss         0.131366  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.748\n",
      "iter 28801/1000000  loss         0.131366  avg_L1_norm_grad         0.000026  w[0]    0.074 bias    5.748\n",
      "iter 28900/1000000  loss         0.131344  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.750\n",
      "iter 28901/1000000  loss         0.131344  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.750\n",
      "iter 29000/1000000  loss         0.131322  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.751\n",
      "iter 29001/1000000  loss         0.131322  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.751\n",
      "iter 29100/1000000  loss         0.131300  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.753\n",
      "iter 29101/1000000  loss         0.131300  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.753\n",
      "iter 29200/1000000  loss         0.131278  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.755\n",
      "iter 29201/1000000  loss         0.131278  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.755\n",
      "iter 29300/1000000  loss         0.131257  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.756\n",
      "iter 29301/1000000  loss         0.131256  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.756\n",
      "iter 29400/1000000  loss         0.131235  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.758\n",
      "iter 29401/1000000  loss         0.131235  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.758\n",
      "iter 29500/1000000  loss         0.131214  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.759\n",
      "iter 29501/1000000  loss         0.131214  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.759\n",
      "iter 29600/1000000  loss         0.131193  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.761\n",
      "iter 29601/1000000  loss         0.131193  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.761\n",
      "iter 29700/1000000  loss         0.131172  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.763\n",
      "iter 29701/1000000  loss         0.131172  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.763\n",
      "iter 29800/1000000  loss         0.131151  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.764\n",
      "iter 29801/1000000  loss         0.131151  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.764\n",
      "iter 29900/1000000  loss         0.131131  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.766\n",
      "iter 29901/1000000  loss         0.131130  avg_L1_norm_grad         0.000025  w[0]    0.075 bias    5.766\n",
      "iter 30000/1000000  loss         0.131110  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.767\n",
      "iter 30001/1000000  loss         0.131110  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.767\n",
      "iter 30100/1000000  loss         0.131090  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.769\n",
      "iter 30101/1000000  loss         0.131090  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.769\n",
      "iter 30200/1000000  loss         0.131070  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.770\n",
      "iter 30201/1000000  loss         0.131070  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30300/1000000  loss         0.131050  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.772\n",
      "iter 30301/1000000  loss         0.131050  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.772\n",
      "iter 30400/1000000  loss         0.131030  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.773\n",
      "iter 30401/1000000  loss         0.131030  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.773\n",
      "iter 30500/1000000  loss         0.131010  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.774\n",
      "iter 30501/1000000  loss         0.131010  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.774\n",
      "iter 30600/1000000  loss         0.130991  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.776\n",
      "iter 30601/1000000  loss         0.130990  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    5.776\n",
      "iter 30700/1000000  loss         0.130971  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.777\n",
      "iter 30701/1000000  loss         0.130971  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.777\n",
      "iter 30800/1000000  loss         0.130952  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.779\n",
      "iter 30801/1000000  loss         0.130952  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.779\n",
      "iter 30900/1000000  loss         0.130933  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.780\n",
      "iter 30901/1000000  loss         0.130933  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.780\n",
      "iter 31000/1000000  loss         0.130914  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.781\n",
      "iter 31001/1000000  loss         0.130914  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.781\n",
      "iter 31100/1000000  loss         0.130895  avg_L1_norm_grad         0.000024  w[0]    0.076 bias    5.783\n",
      "iter 31101/1000000  loss         0.130895  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.783\n",
      "iter 31200/1000000  loss         0.130876  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.784\n",
      "iter 31201/1000000  loss         0.130876  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.784\n",
      "iter 31300/1000000  loss         0.130858  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.785\n",
      "iter 31301/1000000  loss         0.130858  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.785\n",
      "iter 31400/1000000  loss         0.130839  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.787\n",
      "iter 31401/1000000  loss         0.130839  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.787\n",
      "iter 31500/1000000  loss         0.130821  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.788\n",
      "iter 31501/1000000  loss         0.130821  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.788\n",
      "iter 31600/1000000  loss         0.130803  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.789\n",
      "iter 31601/1000000  loss         0.130803  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.789\n",
      "iter 31700/1000000  loss         0.130785  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.791\n",
      "iter 31701/1000000  loss         0.130785  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.791\n",
      "iter 31800/1000000  loss         0.130767  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.792\n",
      "iter 31801/1000000  loss         0.130767  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.792\n",
      "iter 31900/1000000  loss         0.130749  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.793\n",
      "iter 31901/1000000  loss         0.130749  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.793\n",
      "iter 32000/1000000  loss         0.130731  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.794\n",
      "iter 32001/1000000  loss         0.130731  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.794\n",
      "iter 32100/1000000  loss         0.130714  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.795\n",
      "iter 32101/1000000  loss         0.130714  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.795\n",
      "iter 32200/1000000  loss         0.130696  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.797\n",
      "iter 32201/1000000  loss         0.130696  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.797\n",
      "iter 32300/1000000  loss         0.130679  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.798\n",
      "iter 32301/1000000  loss         0.130679  avg_L1_norm_grad         0.000023  w[0]    0.076 bias    5.798\n",
      "iter 32400/1000000  loss         0.130662  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.799\n",
      "iter 32401/1000000  loss         0.130662  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.799\n",
      "iter 32500/1000000  loss         0.130645  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.800\n",
      "iter 32501/1000000  loss         0.130645  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.800\n",
      "iter 32600/1000000  loss         0.130628  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.801\n",
      "iter 32601/1000000  loss         0.130628  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.801\n",
      "iter 32700/1000000  loss         0.130611  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.803\n",
      "iter 32701/1000000  loss         0.130611  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    5.803\n",
      "iter 32800/1000000  loss         0.130595  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.804\n",
      "iter 32801/1000000  loss         0.130594  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.804\n",
      "iter 32900/1000000  loss         0.130578  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.805\n",
      "iter 32901/1000000  loss         0.130578  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.805\n",
      "iter 33000/1000000  loss         0.130562  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.806\n",
      "iter 33001/1000000  loss         0.130561  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.806\n",
      "iter 33100/1000000  loss         0.130545  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.807\n",
      "iter 33101/1000000  loss         0.130545  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.807\n",
      "iter 33200/1000000  loss         0.130529  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.808\n",
      "iter 33201/1000000  loss         0.130529  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.808\n",
      "iter 33300/1000000  loss         0.130513  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.809\n",
      "iter 33301/1000000  loss         0.130513  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.809\n",
      "iter 33400/1000000  loss         0.130497  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.810\n",
      "iter 33401/1000000  loss         0.130497  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.810\n",
      "iter 33500/1000000  loss         0.130481  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.811\n",
      "iter 33501/1000000  loss         0.130481  avg_L1_norm_grad         0.000022  w[0]    0.077 bias    5.811\n",
      "iter 33600/1000000  loss         0.130465  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.812\n",
      "iter 33601/1000000  loss         0.130465  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.812\n",
      "iter 33700/1000000  loss         0.130450  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.813\n",
      "iter 33701/1000000  loss         0.130449  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.813\n",
      "iter 33800/1000000  loss         0.130434  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.814\n",
      "iter 33801/1000000  loss         0.130434  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.814\n",
      "iter 33900/1000000  loss         0.130419  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.815\n",
      "iter 33901/1000000  loss         0.130419  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.815\n",
      "iter 34000/1000000  loss         0.130403  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.816\n",
      "iter 34001/1000000  loss         0.130403  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.816\n",
      "iter 34100/1000000  loss         0.130388  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.817\n",
      "iter 34101/1000000  loss         0.130388  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34200/1000000  loss         0.130373  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.818\n",
      "iter 34201/1000000  loss         0.130373  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.818\n",
      "iter 34300/1000000  loss         0.130358  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.819\n",
      "iter 34301/1000000  loss         0.130358  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.819\n",
      "iter 34400/1000000  loss         0.130343  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.820\n",
      "iter 34401/1000000  loss         0.130343  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.820\n",
      "iter 34500/1000000  loss         0.130328  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.821\n",
      "iter 34501/1000000  loss         0.130328  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.821\n",
      "iter 34600/1000000  loss         0.130314  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.822\n",
      "iter 34601/1000000  loss         0.130314  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.822\n",
      "iter 34700/1000000  loss         0.130299  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.823\n",
      "iter 34701/1000000  loss         0.130299  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.823\n",
      "iter 34800/1000000  loss         0.130285  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.824\n",
      "iter 34801/1000000  loss         0.130284  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.824\n",
      "iter 34900/1000000  loss         0.130270  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.825\n",
      "iter 34901/1000000  loss         0.130270  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    5.825\n",
      "iter 35000/1000000  loss         0.130256  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    5.826\n",
      "iter 35001/1000000  loss         0.130256  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    5.826\n",
      "iter 35100/1000000  loss         0.130242  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    5.827\n",
      "iter 35101/1000000  loss         0.130242  avg_L1_norm_grad         0.000020  w[0]    0.077 bias    5.827\n",
      "iter 35200/1000000  loss         0.130228  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.828\n",
      "iter 35201/1000000  loss         0.130228  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.828\n",
      "iter 35300/1000000  loss         0.130214  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.829\n",
      "iter 35301/1000000  loss         0.130214  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.829\n",
      "iter 35400/1000000  loss         0.130200  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.830\n",
      "iter 35401/1000000  loss         0.130200  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.830\n",
      "iter 35500/1000000  loss         0.130186  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.830\n",
      "iter 35501/1000000  loss         0.130186  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.830\n",
      "iter 35600/1000000  loss         0.130172  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.831\n",
      "iter 35601/1000000  loss         0.130172  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.831\n",
      "iter 35700/1000000  loss         0.130159  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.832\n",
      "iter 35701/1000000  loss         0.130159  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.832\n",
      "iter 35800/1000000  loss         0.130145  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.833\n",
      "iter 35801/1000000  loss         0.130145  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.833\n",
      "iter 35900/1000000  loss         0.130132  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.834\n",
      "iter 35901/1000000  loss         0.130132  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.834\n",
      "iter 36000/1000000  loss         0.130118  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.835\n",
      "iter 36001/1000000  loss         0.130118  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.835\n",
      "iter 36100/1000000  loss         0.130105  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.835\n",
      "iter 36101/1000000  loss         0.130105  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.835\n",
      "iter 36200/1000000  loss         0.130092  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.836\n",
      "iter 36201/1000000  loss         0.130092  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.836\n",
      "iter 36300/1000000  loss         0.130079  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.837\n",
      "iter 36301/1000000  loss         0.130079  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.837\n",
      "iter 36400/1000000  loss         0.130066  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.838\n",
      "iter 36401/1000000  loss         0.130066  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    5.838\n",
      "iter 36500/1000000  loss         0.130053  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.839\n",
      "iter 36501/1000000  loss         0.130053  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.839\n",
      "iter 36600/1000000  loss         0.130040  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.839\n",
      "iter 36601/1000000  loss         0.130040  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.839\n",
      "iter 36700/1000000  loss         0.130028  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.840\n",
      "iter 36701/1000000  loss         0.130027  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.840\n",
      "iter 36800/1000000  loss         0.130015  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.841\n",
      "iter 36801/1000000  loss         0.130015  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.841\n",
      "iter 36900/1000000  loss         0.130002  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.842\n",
      "iter 36901/1000000  loss         0.130002  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.842\n",
      "iter 37000/1000000  loss         0.129990  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.842\n",
      "iter 37001/1000000  loss         0.129990  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.842\n",
      "iter 37100/1000000  loss         0.129978  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.843\n",
      "iter 37101/1000000  loss         0.129978  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.843\n",
      "iter 37200/1000000  loss         0.129965  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.844\n",
      "iter 37201/1000000  loss         0.129965  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.844\n",
      "iter 37300/1000000  loss         0.129953  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.845\n",
      "iter 37301/1000000  loss         0.129953  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.845\n",
      "iter 37400/1000000  loss         0.129941  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.845\n",
      "iter 37401/1000000  loss         0.129941  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.845\n",
      "iter 37500/1000000  loss         0.129929  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.846\n",
      "iter 37501/1000000  loss         0.129929  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.846\n",
      "iter 37600/1000000  loss         0.129917  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.847\n",
      "iter 37601/1000000  loss         0.129917  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.847\n",
      "iter 37700/1000000  loss         0.129905  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.847\n",
      "iter 37701/1000000  loss         0.129905  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.847\n",
      "iter 37800/1000000  loss         0.129893  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.848\n",
      "iter 37801/1000000  loss         0.129893  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.848\n",
      "iter 37900/1000000  loss         0.129882  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.849\n",
      "iter 37901/1000000  loss         0.129882  avg_L1_norm_grad         0.000019  w[0]    0.078 bias    5.849\n",
      "iter 38000/1000000  loss         0.129870  avg_L1_norm_grad         0.000018  w[0]    0.078 bias    5.850\n",
      "iter 38001/1000000  loss         0.129870  avg_L1_norm_grad         0.000018  w[0]    0.078 bias    5.850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38100/1000000  loss         0.129858  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.850\n",
      "iter 38101/1000000  loss         0.129858  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.850\n",
      "iter 38200/1000000  loss         0.129847  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.851\n",
      "iter 38201/1000000  loss         0.129847  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.851\n",
      "iter 38300/1000000  loss         0.129836  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.852\n",
      "iter 38301/1000000  loss         0.129835  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.852\n",
      "iter 38400/1000000  loss         0.129824  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.852\n",
      "iter 38401/1000000  loss         0.129824  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.852\n",
      "iter 38500/1000000  loss         0.129813  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.853\n",
      "iter 38501/1000000  loss         0.129813  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.853\n",
      "iter 38600/1000000  loss         0.129802  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.853\n",
      "iter 38601/1000000  loss         0.129802  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.853\n",
      "iter 38700/1000000  loss         0.129791  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.854\n",
      "iter 38701/1000000  loss         0.129791  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.854\n",
      "iter 38800/1000000  loss         0.129780  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.855\n",
      "iter 38801/1000000  loss         0.129780  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.855\n",
      "iter 38900/1000000  loss         0.129769  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.855\n",
      "iter 38901/1000000  loss         0.129769  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.855\n",
      "iter 39000/1000000  loss         0.129758  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.856\n",
      "iter 39001/1000000  loss         0.129758  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.856\n",
      "iter 39100/1000000  loss         0.129747  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.857\n",
      "iter 39101/1000000  loss         0.129747  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.857\n",
      "iter 39200/1000000  loss         0.129736  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.857\n",
      "iter 39201/1000000  loss         0.129736  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.857\n",
      "iter 39300/1000000  loss         0.129726  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.858\n",
      "iter 39301/1000000  loss         0.129726  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.858\n",
      "iter 39400/1000000  loss         0.129715  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.858\n",
      "iter 39401/1000000  loss         0.129715  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.858\n",
      "iter 39500/1000000  loss         0.129704  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.859\n",
      "iter 39501/1000000  loss         0.129704  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.859\n",
      "iter 39600/1000000  loss         0.129694  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.859\n",
      "iter 39601/1000000  loss         0.129694  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    5.859\n",
      "iter 39700/1000000  loss         0.129684  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.860\n",
      "iter 39701/1000000  loss         0.129684  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.860\n",
      "iter 39800/1000000  loss         0.129673  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.861\n",
      "iter 39801/1000000  loss         0.129673  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.861\n",
      "iter 39900/1000000  loss         0.129663  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.861\n",
      "iter 39901/1000000  loss         0.129663  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.861\n",
      "iter 40000/1000000  loss         0.129653  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.862\n",
      "iter 40001/1000000  loss         0.129653  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.862\n",
      "iter 40100/1000000  loss         0.129643  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.862\n",
      "iter 40101/1000000  loss         0.129643  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.862\n",
      "iter 40200/1000000  loss         0.129633  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.863\n",
      "iter 40201/1000000  loss         0.129633  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.863\n",
      "iter 40300/1000000  loss         0.129623  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.863\n",
      "iter 40301/1000000  loss         0.129623  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.863\n",
      "iter 40400/1000000  loss         0.129613  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.864\n",
      "iter 40401/1000000  loss         0.129613  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.864\n",
      "iter 40500/1000000  loss         0.129603  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.864\n",
      "iter 40501/1000000  loss         0.129603  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.864\n",
      "iter 40600/1000000  loss         0.129593  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.865\n",
      "iter 40601/1000000  loss         0.129593  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.865\n",
      "iter 40700/1000000  loss         0.129583  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.865\n",
      "iter 40701/1000000  loss         0.129583  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.865\n",
      "iter 40800/1000000  loss         0.129574  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.866\n",
      "iter 40801/1000000  loss         0.129574  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.866\n",
      "iter 40900/1000000  loss         0.129564  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.866\n",
      "iter 40901/1000000  loss         0.129564  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.866\n",
      "iter 41000/1000000  loss         0.129555  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.867\n",
      "iter 41001/1000000  loss         0.129555  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.867\n",
      "iter 41100/1000000  loss         0.129545  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.867\n",
      "iter 41101/1000000  loss         0.129545  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.867\n",
      "iter 41200/1000000  loss         0.129536  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.868\n",
      "iter 41201/1000000  loss         0.129536  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.868\n",
      "iter 41300/1000000  loss         0.129526  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.868\n",
      "iter 41301/1000000  loss         0.129526  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.868\n",
      "iter 41400/1000000  loss         0.129517  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.869\n",
      "iter 41401/1000000  loss         0.129517  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    5.869\n",
      "iter 41500/1000000  loss         0.129508  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.869\n",
      "iter 41501/1000000  loss         0.129508  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.869\n",
      "iter 41600/1000000  loss         0.129499  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.870\n",
      "iter 41601/1000000  loss         0.129499  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.870\n",
      "iter 41700/1000000  loss         0.129490  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.870\n",
      "iter 41701/1000000  loss         0.129489  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.870\n",
      "iter 41800/1000000  loss         0.129480  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.871\n",
      "iter 41801/1000000  loss         0.129480  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    5.871\n",
      "iter 41900/1000000  loss         0.129472  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.871\n",
      "iter 41901/1000000  loss         0.129471  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42000/1000000  loss         0.129463  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.872\n",
      "iter 42001/1000000  loss         0.129462  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.872\n",
      "iter 42100/1000000  loss         0.129454  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.872\n",
      "iter 42101/1000000  loss         0.129454  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.872\n",
      "iter 42200/1000000  loss         0.129445  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.873\n",
      "iter 42201/1000000  loss         0.129445  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.873\n",
      "iter 42300/1000000  loss         0.129436  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.873\n",
      "iter 42301/1000000  loss         0.129436  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.873\n",
      "iter 42400/1000000  loss         0.129427  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.873\n",
      "iter 42401/1000000  loss         0.129427  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.873\n",
      "iter 42500/1000000  loss         0.129419  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.874\n",
      "iter 42501/1000000  loss         0.129419  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.874\n",
      "iter 42600/1000000  loss         0.129410  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.874\n",
      "iter 42601/1000000  loss         0.129410  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.874\n",
      "iter 42700/1000000  loss         0.129402  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.875\n",
      "iter 42701/1000000  loss         0.129401  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.875\n",
      "iter 42800/1000000  loss         0.129393  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.875\n",
      "iter 42801/1000000  loss         0.129393  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.875\n",
      "iter 42900/1000000  loss         0.129385  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.875\n",
      "iter 42901/1000000  loss         0.129385  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.875\n",
      "iter 43000/1000000  loss         0.129376  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.876\n",
      "iter 43001/1000000  loss         0.129376  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.876\n",
      "iter 43100/1000000  loss         0.129368  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.876\n",
      "iter 43101/1000000  loss         0.129368  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.876\n",
      "iter 43200/1000000  loss         0.129360  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.877\n",
      "iter 43201/1000000  loss         0.129360  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.877\n",
      "iter 43300/1000000  loss         0.129351  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.877\n",
      "iter 43301/1000000  loss         0.129351  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.877\n",
      "iter 43400/1000000  loss         0.129343  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.877\n",
      "iter 43401/1000000  loss         0.129343  avg_L1_norm_grad         0.000016  w[0]    0.080 bias    5.877\n",
      "iter 43500/1000000  loss         0.129335  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.878\n",
      "iter 43501/1000000  loss         0.129335  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.878\n",
      "iter 43600/1000000  loss         0.129327  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.878\n",
      "iter 43601/1000000  loss         0.129327  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.878\n",
      "iter 43700/1000000  loss         0.129319  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.879\n",
      "iter 43701/1000000  loss         0.129319  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.879\n",
      "iter 43800/1000000  loss         0.129311  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.879\n",
      "iter 43801/1000000  loss         0.129311  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.879\n",
      "iter 43900/1000000  loss         0.129303  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.879\n",
      "iter 43901/1000000  loss         0.129303  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.879\n",
      "iter 44000/1000000  loss         0.129295  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.880\n",
      "iter 44001/1000000  loss         0.129295  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.880\n",
      "iter 44100/1000000  loss         0.129288  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.880\n",
      "iter 44101/1000000  loss         0.129287  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.880\n",
      "iter 44200/1000000  loss         0.129280  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.880\n",
      "iter 44201/1000000  loss         0.129280  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.880\n",
      "iter 44300/1000000  loss         0.129272  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.881\n",
      "iter 44301/1000000  loss         0.129272  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.881\n",
      "iter 44400/1000000  loss         0.129264  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.881\n",
      "iter 44401/1000000  loss         0.129264  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.881\n",
      "iter 44500/1000000  loss         0.129257  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.881\n",
      "iter 44501/1000000  loss         0.129257  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.882\n",
      "iter 44600/1000000  loss         0.129249  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.882\n",
      "iter 44601/1000000  loss         0.129249  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.882\n",
      "iter 44700/1000000  loss         0.129242  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.882\n",
      "iter 44701/1000000  loss         0.129242  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.882\n",
      "iter 44800/1000000  loss         0.129234  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.883\n",
      "iter 44801/1000000  loss         0.129234  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.883\n",
      "iter 44900/1000000  loss         0.129227  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.883\n",
      "iter 44901/1000000  loss         0.129227  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.883\n",
      "iter 45000/1000000  loss         0.129219  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.883\n",
      "iter 45001/1000000  loss         0.129219  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.883\n",
      "iter 45100/1000000  loss         0.129212  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45101/1000000  loss         0.129212  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45200/1000000  loss         0.129205  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45201/1000000  loss         0.129205  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45300/1000000  loss         0.129198  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45301/1000000  loss         0.129198  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45400/1000000  loss         0.129190  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45401/1000000  loss         0.129190  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.884\n",
      "iter 45500/1000000  loss         0.129183  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.885\n",
      "iter 45501/1000000  loss         0.129183  avg_L1_norm_grad         0.000015  w[0]    0.080 bias    5.885\n",
      "iter 45600/1000000  loss         0.129176  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.885\n",
      "iter 45601/1000000  loss         0.129176  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.885\n",
      "iter 45700/1000000  loss         0.129169  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.885\n",
      "iter 45701/1000000  loss         0.129169  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.885\n",
      "iter 45800/1000000  loss         0.129162  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.886\n",
      "iter 45801/1000000  loss         0.129162  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 45900/1000000  loss         0.129155  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.886\n",
      "iter 45901/1000000  loss         0.129155  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.886\n",
      "iter 46000/1000000  loss         0.129148  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.886\n",
      "iter 46001/1000000  loss         0.129148  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.886\n",
      "iter 46100/1000000  loss         0.129141  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46101/1000000  loss         0.129141  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46200/1000000  loss         0.129134  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46201/1000000  loss         0.129134  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46300/1000000  loss         0.129128  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46301/1000000  loss         0.129128  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46400/1000000  loss         0.129121  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46401/1000000  loss         0.129121  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.887\n",
      "iter 46500/1000000  loss         0.129114  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.888\n",
      "iter 46501/1000000  loss         0.129114  avg_L1_norm_grad         0.000014  w[0]    0.080 bias    5.888\n",
      "Done. Converged after 46575 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr3 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "new_lr3.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Accuracy 0.956916666666664\n",
      "Ave Loaded\n",
      "New Accuracy 0.9702499999999973\n"
     ]
    }
   ],
   "source": [
    "y_hat_Origin=np.asarray(orig_lr3.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)\n",
    "\n",
    "y_hat_New=np.asarray(new_lr3.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030456  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.910573  avg_L1_norm_grad         0.028713  w[0]    0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.846116  avg_L1_norm_grad         0.019678  w[0]    0.001 bias    0.021\n",
      "iter    3/1000000  loss         0.801729  avg_L1_norm_grad         0.018492  w[0]    0.001 bias    0.025\n",
      "iter    4/1000000  loss         0.768177  avg_L1_norm_grad         0.013984  w[0]    0.001 bias    0.040\n",
      "iter    5/1000000  loss         0.742335  avg_L1_norm_grad         0.013379  w[0]    0.002 bias    0.047\n",
      "iter    6/1000000  loss         0.721229  avg_L1_norm_grad         0.011414  w[0]    0.002 bias    0.059\n",
      "iter    7/1000000  loss         0.703343  avg_L1_norm_grad         0.010922  w[0]    0.002 bias    0.067\n",
      "iter    8/1000000  loss         0.687689  avg_L1_norm_grad         0.010007  w[0]    0.002 bias    0.077\n",
      "iter    9/1000000  loss         0.673686  avg_L1_norm_grad         0.009548  w[0]    0.003 bias    0.086\n",
      "iter   10/1000000  loss         0.660959  avg_L1_norm_grad         0.009027  w[0]    0.003 bias    0.095\n",
      "iter   11/1000000  loss         0.649254  avg_L1_norm_grad         0.008638  w[0]    0.003 bias    0.104\n",
      "iter   12/1000000  loss         0.638393  avg_L1_norm_grad         0.008274  w[0]    0.003 bias    0.113\n",
      "iter   13/1000000  loss         0.628245  avg_L1_norm_grad         0.007961  w[0]    0.004 bias    0.122\n",
      "iter   14/1000000  loss         0.618712  avg_L1_norm_grad         0.007679  w[0]    0.004 bias    0.130\n",
      "iter   15/1000000  loss         0.609717  avg_L1_norm_grad         0.007427  w[0]    0.004 bias    0.138\n",
      "iter   16/1000000  loss         0.601199  avg_L1_norm_grad         0.007197  w[0]    0.004 bias    0.146\n",
      "iter   17/1000000  loss         0.593110  avg_L1_norm_grad         0.006987  w[0]    0.005 bias    0.154\n",
      "iter   18/1000000  loss         0.585410  avg_L1_norm_grad         0.006792  w[0]    0.005 bias    0.162\n",
      "iter   19/1000000  loss         0.578066  avg_L1_norm_grad         0.006612  w[0]    0.005 bias    0.170\n",
      "iter  100/1000000  loss         0.365063  avg_L1_norm_grad         0.002271  w[0]    0.012 bias    0.585\n",
      "iter  101/1000000  loss         0.364056  avg_L1_norm_grad         0.002254  w[0]    0.012 bias    0.589\n",
      "iter  200/1000000  loss         0.305023  avg_L1_norm_grad         0.001320  w[0]    0.012 bias    0.875\n",
      "iter  201/1000000  loss         0.304663  avg_L1_norm_grad         0.001315  w[0]    0.012 bias    0.878\n",
      "iter  300/1000000  loss         0.278848  avg_L1_norm_grad         0.000957  w[0]    0.011 bias    1.077\n",
      "iter  301/1000000  loss         0.278658  avg_L1_norm_grad         0.000955  w[0]    0.011 bias    1.078\n",
      "iter  400/1000000  loss         0.263853  avg_L1_norm_grad         0.000763  w[0]    0.009 bias    1.231\n",
      "iter  401/1000000  loss         0.263735  avg_L1_norm_grad         0.000762  w[0]    0.009 bias    1.233\n",
      "iter  500/1000000  loss         0.254065  avg_L1_norm_grad         0.000642  w[0]    0.006 bias    1.357\n",
      "iter  501/1000000  loss         0.253985  avg_L1_norm_grad         0.000642  w[0]    0.006 bias    1.358\n",
      "iter  600/1000000  loss         0.247159  avg_L1_norm_grad         0.000558  w[0]    0.004 bias    1.461\n",
      "iter  601/1000000  loss         0.247100  avg_L1_norm_grad         0.000557  w[0]    0.004 bias    1.462\n",
      "iter  700/1000000  loss         0.242024  avg_L1_norm_grad         0.000494  w[0]    0.002 bias    1.550\n",
      "iter  701/1000000  loss         0.241979  avg_L1_norm_grad         0.000493  w[0]    0.002 bias    1.551\n",
      "iter  800/1000000  loss         0.238062  avg_L1_norm_grad         0.000443  w[0]   -0.001 bias    1.627\n",
      "iter  801/1000000  loss         0.238027  avg_L1_norm_grad         0.000443  w[0]   -0.001 bias    1.628\n",
      "iter  900/1000000  loss         0.234919  avg_L1_norm_grad         0.000402  w[0]   -0.003 bias    1.695\n",
      "iter  901/1000000  loss         0.234891  avg_L1_norm_grad         0.000401  w[0]   -0.003 bias    1.696\n",
      "iter 1000/1000000  loss         0.232372  avg_L1_norm_grad         0.000367  w[0]   -0.004 bias    1.755\n",
      "iter 1001/1000000  loss         0.232349  avg_L1_norm_grad         0.000367  w[0]   -0.004 bias    1.756\n",
      "iter 1100/1000000  loss         0.230273  avg_L1_norm_grad         0.000338  w[0]   -0.006 bias    1.809\n",
      "iter 1101/1000000  loss         0.230254  avg_L1_norm_grad         0.000337  w[0]   -0.006 bias    1.809\n",
      "iter 1200/1000000  loss         0.228520  avg_L1_norm_grad         0.000312  w[0]   -0.007 bias    1.857\n",
      "iter 1201/1000000  loss         0.228504  avg_L1_norm_grad         0.000312  w[0]   -0.008 bias    1.857\n",
      "iter 1300/1000000  loss         0.227039  avg_L1_norm_grad         0.000290  w[0]   -0.009 bias    1.900\n",
      "iter 1301/1000000  loss         0.227026  avg_L1_norm_grad         0.000290  w[0]   -0.009 bias    1.900\n",
      "iter 1400/1000000  loss         0.225777  avg_L1_norm_grad         0.000270  w[0]   -0.010 bias    1.939\n",
      "iter 1401/1000000  loss         0.225765  avg_L1_norm_grad         0.000270  w[0]   -0.010 bias    1.940\n",
      "iter 1500/1000000  loss         0.224692  avg_L1_norm_grad         0.000252  w[0]   -0.011 bias    1.975\n",
      "iter 1501/1000000  loss         0.224682  avg_L1_norm_grad         0.000252  w[0]   -0.011 bias    1.976\n",
      "iter 1600/1000000  loss         0.223753  avg_L1_norm_grad         0.000236  w[0]   -0.012 bias    2.008\n",
      "iter 1601/1000000  loss         0.223745  avg_L1_norm_grad         0.000236  w[0]   -0.012 bias    2.008\n",
      "iter 1700/1000000  loss         0.222936  avg_L1_norm_grad         0.000222  w[0]   -0.013 bias    2.038\n",
      "iter 1701/1000000  loss         0.222929  avg_L1_norm_grad         0.000222  w[0]   -0.013 bias    2.038\n",
      "iter 1800/1000000  loss         0.222222  avg_L1_norm_grad         0.000209  w[0]   -0.013 bias    2.065\n",
      "iter 1801/1000000  loss         0.222215  avg_L1_norm_grad         0.000209  w[0]   -0.013 bias    2.066\n",
      "iter 1900/1000000  loss         0.221594  avg_L1_norm_grad         0.000197  w[0]   -0.014 bias    2.091\n",
      "iter 1901/1000000  loss         0.221588  avg_L1_norm_grad         0.000197  w[0]   -0.014 bias    2.091\n",
      "iter 2000/1000000  loss         0.221039  avg_L1_norm_grad         0.000186  w[0]   -0.014 bias    2.114\n",
      "iter 2001/1000000  loss         0.221034  avg_L1_norm_grad         0.000186  w[0]   -0.014 bias    2.114\n",
      "iter 2100/1000000  loss         0.220549  avg_L1_norm_grad         0.000176  w[0]   -0.015 bias    2.136\n",
      "iter 2101/1000000  loss         0.220544  avg_L1_norm_grad         0.000176  w[0]   -0.015 bias    2.136\n",
      "iter 2200/1000000  loss         0.220112  avg_L1_norm_grad         0.000167  w[0]   -0.015 bias    2.156\n",
      "iter 2201/1000000  loss         0.220108  avg_L1_norm_grad         0.000166  w[0]   -0.015 bias    2.156\n",
      "iter 2300/1000000  loss         0.219724  avg_L1_norm_grad         0.000158  w[0]   -0.015 bias    2.174\n",
      "iter 2301/1000000  loss         0.219720  avg_L1_norm_grad         0.000158  w[0]   -0.015 bias    2.175\n",
      "iter 2400/1000000  loss         0.219376  avg_L1_norm_grad         0.000150  w[0]   -0.015 bias    2.192\n",
      "iter 2401/1000000  loss         0.219373  avg_L1_norm_grad         0.000150  w[0]   -0.015 bias    2.192\n",
      "iter 2500/1000000  loss         0.219065  avg_L1_norm_grad         0.000142  w[0]   -0.015 bias    2.208\n",
      "iter 2501/1000000  loss         0.219062  avg_L1_norm_grad         0.000142  w[0]   -0.015 bias    2.208\n",
      "iter 2600/1000000  loss         0.218786  avg_L1_norm_grad         0.000135  w[0]   -0.016 bias    2.223\n",
      "iter 2601/1000000  loss         0.218783  avg_L1_norm_grad         0.000135  w[0]   -0.016 bias    2.223\n",
      "iter 2700/1000000  loss         0.218534  avg_L1_norm_grad         0.000129  w[0]   -0.016 bias    2.237\n",
      "iter 2701/1000000  loss         0.218532  avg_L1_norm_grad         0.000129  w[0]   -0.016 bias    2.237\n",
      "iter 2800/1000000  loss         0.218307  avg_L1_norm_grad         0.000123  w[0]   -0.016 bias    2.249\n",
      "iter 2801/1000000  loss         0.218305  avg_L1_norm_grad         0.000123  w[0]   -0.016 bias    2.250\n",
      "iter 2900/1000000  loss         0.218102  avg_L1_norm_grad         0.000117  w[0]   -0.015 bias    2.262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.218100  avg_L1_norm_grad         0.000117  w[0]   -0.015 bias    2.262\n",
      "iter 3000/1000000  loss         0.217917  avg_L1_norm_grad         0.000112  w[0]   -0.015 bias    2.273\n",
      "iter 3001/1000000  loss         0.217915  avg_L1_norm_grad         0.000112  w[0]   -0.015 bias    2.273\n",
      "iter 3100/1000000  loss         0.217749  avg_L1_norm_grad         0.000107  w[0]   -0.015 bias    2.284\n",
      "iter 3101/1000000  loss         0.217747  avg_L1_norm_grad         0.000107  w[0]   -0.015 bias    2.284\n",
      "iter 3200/1000000  loss         0.217596  avg_L1_norm_grad         0.000102  w[0]   -0.015 bias    2.293\n",
      "iter 3201/1000000  loss         0.217595  avg_L1_norm_grad         0.000102  w[0]   -0.015 bias    2.294\n",
      "iter 3300/1000000  loss         0.217458  avg_L1_norm_grad         0.000098  w[0]   -0.015 bias    2.303\n",
      "iter 3301/1000000  loss         0.217456  avg_L1_norm_grad         0.000098  w[0]   -0.015 bias    2.303\n",
      "iter 3400/1000000  loss         0.217331  avg_L1_norm_grad         0.000093  w[0]   -0.015 bias    2.311\n",
      "iter 3401/1000000  loss         0.217330  avg_L1_norm_grad         0.000093  w[0]   -0.015 bias    2.311\n",
      "iter 3500/1000000  loss         0.217216  avg_L1_norm_grad         0.000090  w[0]   -0.015 bias    2.320\n",
      "iter 3501/1000000  loss         0.217215  avg_L1_norm_grad         0.000089  w[0]   -0.015 bias    2.320\n",
      "iter 3600/1000000  loss         0.217111  avg_L1_norm_grad         0.000086  w[0]   -0.014 bias    2.327\n",
      "iter 3601/1000000  loss         0.217110  avg_L1_norm_grad         0.000086  w[0]   -0.014 bias    2.327\n",
      "iter 3700/1000000  loss         0.217015  avg_L1_norm_grad         0.000082  w[0]   -0.014 bias    2.334\n",
      "iter 3701/1000000  loss         0.217014  avg_L1_norm_grad         0.000082  w[0]   -0.014 bias    2.334\n",
      "iter 3800/1000000  loss         0.216927  avg_L1_norm_grad         0.000079  w[0]   -0.014 bias    2.341\n",
      "iter 3801/1000000  loss         0.216927  avg_L1_norm_grad         0.000079  w[0]   -0.014 bias    2.341\n",
      "iter 3900/1000000  loss         0.216847  avg_L1_norm_grad         0.000076  w[0]   -0.014 bias    2.348\n",
      "iter 3901/1000000  loss         0.216846  avg_L1_norm_grad         0.000075  w[0]   -0.014 bias    2.348\n",
      "iter 4000/1000000  loss         0.216773  avg_L1_norm_grad         0.000072  w[0]   -0.013 bias    2.354\n",
      "iter 4001/1000000  loss         0.216773  avg_L1_norm_grad         0.000072  w[0]   -0.013 bias    2.354\n",
      "iter 4100/1000000  loss         0.216706  avg_L1_norm_grad         0.000070  w[0]   -0.013 bias    2.359\n",
      "iter 4101/1000000  loss         0.216705  avg_L1_norm_grad         0.000069  w[0]   -0.013 bias    2.359\n",
      "iter 4200/1000000  loss         0.216644  avg_L1_norm_grad         0.000067  w[0]   -0.013 bias    2.364\n",
      "iter 4201/1000000  loss         0.216643  avg_L1_norm_grad         0.000067  w[0]   -0.013 bias    2.364\n",
      "iter 4300/1000000  loss         0.216587  avg_L1_norm_grad         0.000064  w[0]   -0.012 bias    2.369\n",
      "iter 4301/1000000  loss         0.216587  avg_L1_norm_grad         0.000064  w[0]   -0.012 bias    2.369\n",
      "iter 4400/1000000  loss         0.216535  avg_L1_norm_grad         0.000062  w[0]   -0.012 bias    2.374\n",
      "iter 4401/1000000  loss         0.216534  avg_L1_norm_grad         0.000062  w[0]   -0.012 bias    2.374\n",
      "iter 4500/1000000  loss         0.216487  avg_L1_norm_grad         0.000059  w[0]   -0.012 bias    2.379\n",
      "iter 4501/1000000  loss         0.216486  avg_L1_norm_grad         0.000059  w[0]   -0.012 bias    2.379\n",
      "iter 4600/1000000  loss         0.216442  avg_L1_norm_grad         0.000057  w[0]   -0.011 bias    2.383\n",
      "iter 4601/1000000  loss         0.216442  avg_L1_norm_grad         0.000057  w[0]   -0.011 bias    2.383\n",
      "iter 4700/1000000  loss         0.216401  avg_L1_norm_grad         0.000055  w[0]   -0.011 bias    2.387\n",
      "iter 4701/1000000  loss         0.216401  avg_L1_norm_grad         0.000055  w[0]   -0.011 bias    2.387\n",
      "iter 4800/1000000  loss         0.216364  avg_L1_norm_grad         0.000053  w[0]   -0.011 bias    2.390\n",
      "iter 4801/1000000  loss         0.216363  avg_L1_norm_grad         0.000053  w[0]   -0.011 bias    2.390\n",
      "iter 4900/1000000  loss         0.216329  avg_L1_norm_grad         0.000051  w[0]   -0.011 bias    2.394\n",
      "iter 4901/1000000  loss         0.216329  avg_L1_norm_grad         0.000051  w[0]   -0.011 bias    2.394\n",
      "iter 5000/1000000  loss         0.216297  avg_L1_norm_grad         0.000049  w[0]   -0.010 bias    2.397\n",
      "iter 5001/1000000  loss         0.216297  avg_L1_norm_grad         0.000049  w[0]   -0.010 bias    2.397\n",
      "iter 5100/1000000  loss         0.216267  avg_L1_norm_grad         0.000047  w[0]   -0.010 bias    2.400\n",
      "iter 5101/1000000  loss         0.216267  avg_L1_norm_grad         0.000047  w[0]   -0.010 bias    2.400\n",
      "iter 5200/1000000  loss         0.216240  avg_L1_norm_grad         0.000045  w[0]   -0.010 bias    2.403\n",
      "iter 5201/1000000  loss         0.216240  avg_L1_norm_grad         0.000045  w[0]   -0.010 bias    2.403\n",
      "iter 5300/1000000  loss         0.216215  avg_L1_norm_grad         0.000043  w[0]   -0.009 bias    2.406\n",
      "iter 5301/1000000  loss         0.216214  avg_L1_norm_grad         0.000043  w[0]   -0.009 bias    2.406\n",
      "iter 5400/1000000  loss         0.216191  avg_L1_norm_grad         0.000042  w[0]   -0.009 bias    2.409\n",
      "iter 5401/1000000  loss         0.216191  avg_L1_norm_grad         0.000042  w[0]   -0.009 bias    2.409\n",
      "iter 5500/1000000  loss         0.216169  avg_L1_norm_grad         0.000040  w[0]   -0.009 bias    2.411\n",
      "iter 5501/1000000  loss         0.216169  avg_L1_norm_grad         0.000040  w[0]   -0.009 bias    2.411\n",
      "iter 5600/1000000  loss         0.216149  avg_L1_norm_grad         0.000039  w[0]   -0.008 bias    2.414\n",
      "iter 5601/1000000  loss         0.216149  avg_L1_norm_grad         0.000039  w[0]   -0.008 bias    2.414\n",
      "iter 5700/1000000  loss         0.216131  avg_L1_norm_grad         0.000037  w[0]   -0.008 bias    2.416\n",
      "iter 5701/1000000  loss         0.216131  avg_L1_norm_grad         0.000037  w[0]   -0.008 bias    2.416\n",
      "iter 5800/1000000  loss         0.216114  avg_L1_norm_grad         0.000036  w[0]   -0.008 bias    2.418\n",
      "iter 5801/1000000  loss         0.216113  avg_L1_norm_grad         0.000036  w[0]   -0.008 bias    2.418\n",
      "iter 5900/1000000  loss         0.216098  avg_L1_norm_grad         0.000035  w[0]   -0.008 bias    2.420\n",
      "iter 5901/1000000  loss         0.216097  avg_L1_norm_grad         0.000035  w[0]   -0.008 bias    2.420\n",
      "iter 6000/1000000  loss         0.216083  avg_L1_norm_grad         0.000033  w[0]   -0.007 bias    2.422\n",
      "iter 6001/1000000  loss         0.216083  avg_L1_norm_grad         0.000033  w[0]   -0.007 bias    2.422\n",
      "iter 6100/1000000  loss         0.216069  avg_L1_norm_grad         0.000032  w[0]   -0.007 bias    2.424\n",
      "iter 6101/1000000  loss         0.216069  avg_L1_norm_grad         0.000032  w[0]   -0.007 bias    2.424\n",
      "iter 6200/1000000  loss         0.216056  avg_L1_norm_grad         0.000031  w[0]   -0.007 bias    2.426\n",
      "iter 6201/1000000  loss         0.216056  avg_L1_norm_grad         0.000031  w[0]   -0.007 bias    2.426\n",
      "iter 6300/1000000  loss         0.216044  avg_L1_norm_grad         0.000030  w[0]   -0.006 bias    2.427\n",
      "iter 6301/1000000  loss         0.216044  avg_L1_norm_grad         0.000030  w[0]   -0.006 bias    2.427\n",
      "iter 6400/1000000  loss         0.216033  avg_L1_norm_grad         0.000029  w[0]   -0.006 bias    2.429\n",
      "iter 6401/1000000  loss         0.216033  avg_L1_norm_grad         0.000029  w[0]   -0.006 bias    2.429\n",
      "iter 6500/1000000  loss         0.216023  avg_L1_norm_grad         0.000028  w[0]   -0.006 bias    2.430\n",
      "iter 6501/1000000  loss         0.216023  avg_L1_norm_grad         0.000028  w[0]   -0.006 bias    2.430\n",
      "iter 6600/1000000  loss         0.216014  avg_L1_norm_grad         0.000027  w[0]   -0.006 bias    2.432\n",
      "iter 6601/1000000  loss         0.216013  avg_L1_norm_grad         0.000027  w[0]   -0.006 bias    2.432\n",
      "iter 6700/1000000  loss         0.216005  avg_L1_norm_grad         0.000026  w[0]   -0.005 bias    2.433\n",
      "iter 6701/1000000  loss         0.216005  avg_L1_norm_grad         0.000026  w[0]   -0.005 bias    2.433\n",
      "iter 6800/1000000  loss         0.215997  avg_L1_norm_grad         0.000025  w[0]   -0.005 bias    2.434\n",
      "iter 6801/1000000  loss         0.215996  avg_L1_norm_grad         0.000025  w[0]   -0.005 bias    2.434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.215989  avg_L1_norm_grad         0.000024  w[0]   -0.005 bias    2.435\n",
      "iter 6901/1000000  loss         0.215989  avg_L1_norm_grad         0.000024  w[0]   -0.005 bias    2.435\n",
      "iter 7000/1000000  loss         0.215982  avg_L1_norm_grad         0.000023  w[0]   -0.005 bias    2.437\n",
      "iter 7001/1000000  loss         0.215982  avg_L1_norm_grad         0.000023  w[0]   -0.005 bias    2.437\n",
      "iter 7100/1000000  loss         0.215975  avg_L1_norm_grad         0.000022  w[0]   -0.004 bias    2.438\n",
      "iter 7101/1000000  loss         0.215975  avg_L1_norm_grad         0.000022  w[0]   -0.004 bias    2.438\n",
      "iter 7200/1000000  loss         0.215969  avg_L1_norm_grad         0.000022  w[0]   -0.004 bias    2.439\n",
      "iter 7201/1000000  loss         0.215969  avg_L1_norm_grad         0.000022  w[0]   -0.004 bias    2.439\n",
      "iter 7300/1000000  loss         0.215963  avg_L1_norm_grad         0.000021  w[0]   -0.004 bias    2.440\n",
      "iter 7301/1000000  loss         0.215963  avg_L1_norm_grad         0.000021  w[0]   -0.004 bias    2.440\n",
      "Done. Converged after 7374 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr100 = LRGD(alpha=100.0, step_size=0.1)\n",
    "orig_lr100.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "Ave Loaded\n",
      "Initializing w_G with 1570 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.026284  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.925063  avg_L1_norm_grad         0.038023  w[0]    0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.908890  avg_L1_norm_grad         0.038253  w[0]    0.001 bias    0.030\n",
      "iter    3/1000000  loss         0.968808  avg_L1_norm_grad         0.053273  w[0]    0.001 bias    0.014\n",
      "iter    4/1000000  loss         0.868858  avg_L1_norm_grad         0.043105  w[0]    0.002 bias    0.055\n",
      "iter    5/1000000  loss         0.902222  avg_L1_norm_grad         0.049064  w[0]    0.001 bias    0.035\n",
      "iter    6/1000000  loss         0.763399  avg_L1_norm_grad         0.033586  w[0]    0.002 bias    0.074\n",
      "iter    7/1000000  loss         0.759287  avg_L1_norm_grad         0.035895  w[0]    0.002 bias    0.060\n",
      "iter    8/1000000  loss         0.684963  avg_L1_norm_grad         0.024301  w[0]    0.003 bias    0.090\n",
      "iter    9/1000000  loss         0.669784  avg_L1_norm_grad         0.025214  w[0]    0.002 bias    0.082\n",
      "iter   10/1000000  loss         0.631327  avg_L1_norm_grad         0.017043  w[0]    0.003 bias    0.105\n",
      "iter   11/1000000  loss         0.615789  avg_L1_norm_grad         0.017349  w[0]    0.003 bias    0.102\n",
      "iter   12/1000000  loss         0.594057  avg_L1_norm_grad         0.011779  w[0]    0.003 bias    0.120\n",
      "iter   13/1000000  loss         0.580988  avg_L1_norm_grad         0.011753  w[0]    0.003 bias    0.120\n",
      "iter   14/1000000  loss         0.567157  avg_L1_norm_grad         0.008230  w[0]    0.004 bias    0.134\n",
      "iter   15/1000000  loss         0.556672  avg_L1_norm_grad         0.008059  w[0]    0.004 bias    0.136\n",
      "iter   16/1000000  loss         0.546734  avg_L1_norm_grad         0.006022  w[0]    0.004 bias    0.147\n",
      "iter   17/1000000  loss         0.538205  avg_L1_norm_grad         0.005840  w[0]    0.004 bias    0.152\n",
      "iter   18/1000000  loss         0.530300  avg_L1_norm_grad         0.004857  w[0]    0.004 bias    0.161\n",
      "iter   19/1000000  loss         0.523092  avg_L1_norm_grad         0.004726  w[0]    0.004 bias    0.166\n",
      "iter  100/1000000  loss         0.340867  avg_L1_norm_grad         0.001404  w[0]    0.007 bias    0.505\n",
      "iter  101/1000000  loss         0.340043  avg_L1_norm_grad         0.001394  w[0]    0.007 bias    0.508\n",
      "iter  200/1000000  loss         0.291100  avg_L1_norm_grad         0.000865  w[0]    0.004 bias    0.741\n",
      "iter  201/1000000  loss         0.290795  avg_L1_norm_grad         0.000862  w[0]    0.004 bias    0.743\n",
      "iter  300/1000000  loss         0.268557  avg_L1_norm_grad         0.000652  w[0]    0.002 bias    0.914\n",
      "iter  301/1000000  loss         0.268389  avg_L1_norm_grad         0.000651  w[0]    0.002 bias    0.916\n",
      "iter  400/1000000  loss         0.255096  avg_L1_norm_grad         0.000532  w[0]   -0.001 bias    1.056\n",
      "iter  401/1000000  loss         0.254988  avg_L1_norm_grad         0.000531  w[0]   -0.001 bias    1.057\n",
      "iter  500/1000000  loss         0.245941  avg_L1_norm_grad         0.000454  w[0]   -0.003 bias    1.178\n",
      "iter  501/1000000  loss         0.245864  avg_L1_norm_grad         0.000453  w[0]   -0.003 bias    1.179\n",
      "iter  600/1000000  loss         0.239223  avg_L1_norm_grad         0.000397  w[0]   -0.005 bias    1.287\n",
      "iter  601/1000000  loss         0.239165  avg_L1_norm_grad         0.000396  w[0]   -0.005 bias    1.288\n",
      "iter  700/1000000  loss         0.234045  avg_L1_norm_grad         0.000354  w[0]   -0.007 bias    1.384\n",
      "iter  701/1000000  loss         0.233999  avg_L1_norm_grad         0.000353  w[0]   -0.007 bias    1.385\n",
      "iter  800/1000000  loss         0.229915  avg_L1_norm_grad         0.000319  w[0]   -0.008 bias    1.473\n",
      "iter  801/1000000  loss         0.229878  avg_L1_norm_grad         0.000319  w[0]   -0.008 bias    1.474\n",
      "iter  900/1000000  loss         0.226538  avg_L1_norm_grad         0.000291  w[0]   -0.009 bias    1.556\n",
      "iter  901/1000000  loss         0.226507  avg_L1_norm_grad         0.000290  w[0]   -0.009 bias    1.556\n",
      "iter 1000/1000000  loss         0.223723  avg_L1_norm_grad         0.000267  w[0]   -0.009 bias    1.632\n",
      "iter 1001/1000000  loss         0.223698  avg_L1_norm_grad         0.000267  w[0]   -0.009 bias    1.632\n",
      "iter 1100/1000000  loss         0.221343  avg_L1_norm_grad         0.000247  w[0]   -0.010 bias    1.702\n",
      "iter 1101/1000000  loss         0.221321  avg_L1_norm_grad         0.000246  w[0]   -0.010 bias    1.703\n",
      "iter 1200/1000000  loss         0.219305  avg_L1_norm_grad         0.000229  w[0]   -0.010 bias    1.768\n",
      "iter 1201/1000000  loss         0.219286  avg_L1_norm_grad         0.000229  w[0]   -0.010 bias    1.769\n",
      "iter 1300/1000000  loss         0.217543  avg_L1_norm_grad         0.000214  w[0]   -0.010 bias    1.830\n",
      "iter 1301/1000000  loss         0.217527  avg_L1_norm_grad         0.000213  w[0]   -0.010 bias    1.831\n",
      "iter 1400/1000000  loss         0.216007  avg_L1_norm_grad         0.000200  w[0]   -0.010 bias    1.889\n",
      "iter 1401/1000000  loss         0.215993  avg_L1_norm_grad         0.000200  w[0]   -0.010 bias    1.889\n",
      "iter 1500/1000000  loss         0.214659  avg_L1_norm_grad         0.000188  w[0]   -0.010 bias    1.944\n",
      "iter 1501/1000000  loss         0.214646  avg_L1_norm_grad         0.000188  w[0]   -0.010 bias    1.944\n",
      "iter 1600/1000000  loss         0.213467  avg_L1_norm_grad         0.000177  w[0]   -0.010 bias    1.995\n",
      "iter 1601/1000000  loss         0.213456  avg_L1_norm_grad         0.000177  w[0]   -0.010 bias    1.996\n",
      "iter 1700/1000000  loss         0.212409  avg_L1_norm_grad         0.000167  w[0]   -0.010 bias    2.044\n",
      "iter 1701/1000000  loss         0.212399  avg_L1_norm_grad         0.000167  w[0]   -0.010 bias    2.045\n",
      "iter 1800/1000000  loss         0.211465  avg_L1_norm_grad         0.000158  w[0]   -0.010 bias    2.091\n",
      "iter 1801/1000000  loss         0.211456  avg_L1_norm_grad         0.000158  w[0]   -0.010 bias    2.091\n",
      "iter 1900/1000000  loss         0.210618  avg_L1_norm_grad         0.000150  w[0]   -0.009 bias    2.135\n",
      "iter 1901/1000000  loss         0.210610  avg_L1_norm_grad         0.000150  w[0]   -0.009 bias    2.135\n",
      "iter 2000/1000000  loss         0.209857  avg_L1_norm_grad         0.000143  w[0]   -0.009 bias    2.176\n",
      "iter 2001/1000000  loss         0.209850  avg_L1_norm_grad         0.000143  w[0]   -0.009 bias    2.177\n",
      "iter 2100/1000000  loss         0.209170  avg_L1_norm_grad         0.000136  w[0]   -0.008 bias    2.216\n",
      "iter 2101/1000000  loss         0.209164  avg_L1_norm_grad         0.000136  w[0]   -0.008 bias    2.216\n",
      "iter 2200/1000000  loss         0.208549  avg_L1_norm_grad         0.000130  w[0]   -0.008 bias    2.254\n",
      "iter 2201/1000000  loss         0.208543  avg_L1_norm_grad         0.000130  w[0]   -0.008 bias    2.254\n",
      "iter 2300/1000000  loss         0.207984  avg_L1_norm_grad         0.000124  w[0]   -0.007 bias    2.290\n",
      "iter 2301/1000000  loss         0.207979  avg_L1_norm_grad         0.000124  w[0]   -0.007 bias    2.290\n",
      "iter 2400/1000000  loss         0.207471  avg_L1_norm_grad         0.000118  w[0]   -0.007 bias    2.324\n",
      "iter 2401/1000000  loss         0.207466  avg_L1_norm_grad         0.000118  w[0]   -0.007 bias    2.324\n",
      "iter 2500/1000000  loss         0.207002  avg_L1_norm_grad         0.000113  w[0]   -0.006 bias    2.356\n",
      "iter 2501/1000000  loss         0.206998  avg_L1_norm_grad         0.000113  w[0]   -0.006 bias    2.357\n",
      "iter 2600/1000000  loss         0.206574  avg_L1_norm_grad         0.000108  w[0]   -0.006 bias    2.387\n",
      "iter 2601/1000000  loss         0.206570  avg_L1_norm_grad         0.000108  w[0]   -0.006 bias    2.388\n",
      "iter 2700/1000000  loss         0.206182  avg_L1_norm_grad         0.000104  w[0]   -0.005 bias    2.417\n",
      "iter 2701/1000000  loss         0.206178  avg_L1_norm_grad         0.000104  w[0]   -0.005 bias    2.417\n",
      "iter 2800/1000000  loss         0.205822  avg_L1_norm_grad         0.000100  w[0]   -0.005 bias    2.445\n",
      "iter 2801/1000000  loss         0.205819  avg_L1_norm_grad         0.000099  w[0]   -0.005 bias    2.446\n",
      "iter 2900/1000000  loss         0.205491  avg_L1_norm_grad         0.000096  w[0]   -0.004 bias    2.472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.205488  avg_L1_norm_grad         0.000095  w[0]   -0.004 bias    2.473\n",
      "iter 3000/1000000  loss         0.205187  avg_L1_norm_grad         0.000092  w[0]   -0.004 bias    2.498\n",
      "iter 3001/1000000  loss         0.205184  avg_L1_norm_grad         0.000092  w[0]   -0.004 bias    2.499\n",
      "iter 3100/1000000  loss         0.204906  avg_L1_norm_grad         0.000088  w[0]   -0.003 bias    2.523\n",
      "iter 3101/1000000  loss         0.204904  avg_L1_norm_grad         0.000088  w[0]   -0.003 bias    2.523\n",
      "iter 3200/1000000  loss         0.204647  avg_L1_norm_grad         0.000085  w[0]   -0.003 bias    2.547\n",
      "iter 3201/1000000  loss         0.204645  avg_L1_norm_grad         0.000085  w[0]   -0.003 bias    2.547\n",
      "iter 3300/1000000  loss         0.204408  avg_L1_norm_grad         0.000081  w[0]   -0.002 bias    2.569\n",
      "iter 3301/1000000  loss         0.204405  avg_L1_norm_grad         0.000081  w[0]   -0.002 bias    2.570\n",
      "iter 3400/1000000  loss         0.204186  avg_L1_norm_grad         0.000078  w[0]   -0.002 bias    2.591\n",
      "iter 3401/1000000  loss         0.204184  avg_L1_norm_grad         0.000078  w[0]   -0.002 bias    2.591\n",
      "iter 3500/1000000  loss         0.203981  avg_L1_norm_grad         0.000076  w[0]   -0.001 bias    2.612\n",
      "iter 3501/1000000  loss         0.203979  avg_L1_norm_grad         0.000075  w[0]   -0.001 bias    2.612\n",
      "iter 3600/1000000  loss         0.203790  avg_L1_norm_grad         0.000073  w[0]   -0.001 bias    2.632\n",
      "iter 3601/1000000  loss         0.203788  avg_L1_norm_grad         0.000073  w[0]   -0.001 bias    2.632\n",
      "iter 3700/1000000  loss         0.203613  avg_L1_norm_grad         0.000070  w[0]   -0.000 bias    2.651\n",
      "iter 3701/1000000  loss         0.203611  avg_L1_norm_grad         0.000070  w[0]   -0.000 bias    2.651\n",
      "iter 3800/1000000  loss         0.203449  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.669\n",
      "iter 3801/1000000  loss         0.203447  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.669\n",
      "iter 3900/1000000  loss         0.203296  avg_L1_norm_grad         0.000065  w[0]    0.001 bias    2.687\n",
      "iter 3901/1000000  loss         0.203294  avg_L1_norm_grad         0.000065  w[0]    0.001 bias    2.687\n",
      "iter 4000/1000000  loss         0.203153  avg_L1_norm_grad         0.000063  w[0]    0.001 bias    2.704\n",
      "iter 4001/1000000  loss         0.203152  avg_L1_norm_grad         0.000063  w[0]    0.001 bias    2.704\n",
      "iter 4100/1000000  loss         0.203020  avg_L1_norm_grad         0.000061  w[0]    0.001 bias    2.720\n",
      "iter 4101/1000000  loss         0.203019  avg_L1_norm_grad         0.000061  w[0]    0.001 bias    2.720\n",
      "iter 4200/1000000  loss         0.202896  avg_L1_norm_grad         0.000059  w[0]    0.002 bias    2.735\n",
      "iter 4201/1000000  loss         0.202895  avg_L1_norm_grad         0.000059  w[0]    0.002 bias    2.735\n",
      "iter 4300/1000000  loss         0.202781  avg_L1_norm_grad         0.000057  w[0]    0.002 bias    2.750\n",
      "iter 4301/1000000  loss         0.202780  avg_L1_norm_grad         0.000057  w[0]    0.002 bias    2.750\n",
      "iter 4400/1000000  loss         0.202673  avg_L1_norm_grad         0.000055  w[0]    0.003 bias    2.764\n",
      "iter 4401/1000000  loss         0.202672  avg_L1_norm_grad         0.000055  w[0]    0.003 bias    2.764\n",
      "iter 4500/1000000  loss         0.202572  avg_L1_norm_grad         0.000053  w[0]    0.003 bias    2.778\n",
      "iter 4501/1000000  loss         0.202571  avg_L1_norm_grad         0.000053  w[0]    0.003 bias    2.778\n",
      "iter 4600/1000000  loss         0.202477  avg_L1_norm_grad         0.000051  w[0]    0.003 bias    2.791\n",
      "iter 4601/1000000  loss         0.202476  avg_L1_norm_grad         0.000051  w[0]    0.003 bias    2.791\n",
      "iter 4700/1000000  loss         0.202389  avg_L1_norm_grad         0.000050  w[0]    0.004 bias    2.804\n",
      "iter 4701/1000000  loss         0.202388  avg_L1_norm_grad         0.000050  w[0]    0.004 bias    2.804\n",
      "iter 4800/1000000  loss         0.202306  avg_L1_norm_grad         0.000048  w[0]    0.004 bias    2.816\n",
      "iter 4801/1000000  loss         0.202305  avg_L1_norm_grad         0.000048  w[0]    0.004 bias    2.816\n",
      "iter 4900/1000000  loss         0.202228  avg_L1_norm_grad         0.000047  w[0]    0.005 bias    2.828\n",
      "iter 4901/1000000  loss         0.202227  avg_L1_norm_grad         0.000047  w[0]    0.005 bias    2.828\n",
      "iter 5000/1000000  loss         0.202155  avg_L1_norm_grad         0.000045  w[0]    0.005 bias    2.839\n",
      "iter 5001/1000000  loss         0.202155  avg_L1_norm_grad         0.000045  w[0]    0.005 bias    2.839\n",
      "iter 5100/1000000  loss         0.202087  avg_L1_norm_grad         0.000044  w[0]    0.005 bias    2.850\n",
      "iter 5101/1000000  loss         0.202086  avg_L1_norm_grad         0.000044  w[0]    0.005 bias    2.850\n",
      "iter 5200/1000000  loss         0.202023  avg_L1_norm_grad         0.000042  w[0]    0.005 bias    2.860\n",
      "iter 5201/1000000  loss         0.202022  avg_L1_norm_grad         0.000042  w[0]    0.005 bias    2.860\n",
      "iter 5300/1000000  loss         0.201962  avg_L1_norm_grad         0.000041  w[0]    0.006 bias    2.870\n",
      "iter 5301/1000000  loss         0.201962  avg_L1_norm_grad         0.000041  w[0]    0.006 bias    2.870\n",
      "iter 5400/1000000  loss         0.201906  avg_L1_norm_grad         0.000040  w[0]    0.006 bias    2.879\n",
      "iter 5401/1000000  loss         0.201905  avg_L1_norm_grad         0.000040  w[0]    0.006 bias    2.880\n",
      "iter 5500/1000000  loss         0.201852  avg_L1_norm_grad         0.000039  w[0]    0.006 bias    2.889\n",
      "iter 5501/1000000  loss         0.201852  avg_L1_norm_grad         0.000039  w[0]    0.006 bias    2.889\n",
      "iter 5600/1000000  loss         0.201802  avg_L1_norm_grad         0.000037  w[0]    0.007 bias    2.897\n",
      "iter 5601/1000000  loss         0.201801  avg_L1_norm_grad         0.000037  w[0]    0.007 bias    2.898\n",
      "iter 5700/1000000  loss         0.201755  avg_L1_norm_grad         0.000036  w[0]    0.007 bias    2.906\n",
      "iter 5701/1000000  loss         0.201754  avg_L1_norm_grad         0.000036  w[0]    0.007 bias    2.906\n",
      "iter 5800/1000000  loss         0.201710  avg_L1_norm_grad         0.000035  w[0]    0.007 bias    2.914\n",
      "iter 5801/1000000  loss         0.201710  avg_L1_norm_grad         0.000035  w[0]    0.007 bias    2.914\n",
      "iter 5900/1000000  loss         0.201668  avg_L1_norm_grad         0.000034  w[0]    0.007 bias    2.922\n",
      "iter 5901/1000000  loss         0.201668  avg_L1_norm_grad         0.000034  w[0]    0.007 bias    2.922\n",
      "iter 6000/1000000  loss         0.201629  avg_L1_norm_grad         0.000033  w[0]    0.008 bias    2.930\n",
      "iter 6001/1000000  loss         0.201628  avg_L1_norm_grad         0.000033  w[0]    0.008 bias    2.930\n",
      "iter 6100/1000000  loss         0.201591  avg_L1_norm_grad         0.000032  w[0]    0.008 bias    2.937\n",
      "iter 6101/1000000  loss         0.201591  avg_L1_norm_grad         0.000032  w[0]    0.008 bias    2.937\n",
      "iter 6200/1000000  loss         0.201556  avg_L1_norm_grad         0.000031  w[0]    0.008 bias    2.944\n",
      "iter 6201/1000000  loss         0.201556  avg_L1_norm_grad         0.000031  w[0]    0.008 bias    2.944\n",
      "iter 6300/1000000  loss         0.201523  avg_L1_norm_grad         0.000030  w[0]    0.008 bias    2.951\n",
      "iter 6301/1000000  loss         0.201523  avg_L1_norm_grad         0.000030  w[0]    0.008 bias    2.951\n",
      "iter 6400/1000000  loss         0.201492  avg_L1_norm_grad         0.000030  w[0]    0.009 bias    2.957\n",
      "iter 6401/1000000  loss         0.201491  avg_L1_norm_grad         0.000030  w[0]    0.009 bias    2.957\n",
      "iter 6500/1000000  loss         0.201462  avg_L1_norm_grad         0.000029  w[0]    0.009 bias    2.963\n",
      "iter 6501/1000000  loss         0.201462  avg_L1_norm_grad         0.000029  w[0]    0.009 bias    2.963\n",
      "iter 6600/1000000  loss         0.201434  avg_L1_norm_grad         0.000028  w[0]    0.009 bias    2.969\n",
      "iter 6601/1000000  loss         0.201434  avg_L1_norm_grad         0.000028  w[0]    0.009 bias    2.969\n",
      "iter 6700/1000000  loss         0.201408  avg_L1_norm_grad         0.000027  w[0]    0.009 bias    2.975\n",
      "iter 6701/1000000  loss         0.201407  avg_L1_norm_grad         0.000027  w[0]    0.009 bias    2.975\n",
      "iter 6800/1000000  loss         0.201382  avg_L1_norm_grad         0.000026  w[0]    0.009 bias    2.981\n",
      "iter 6801/1000000  loss         0.201382  avg_L1_norm_grad         0.000026  w[0]    0.009 bias    2.981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.201359  avg_L1_norm_grad         0.000026  w[0]    0.010 bias    2.986\n",
      "iter 6901/1000000  loss         0.201359  avg_L1_norm_grad         0.000026  w[0]    0.010 bias    2.986\n",
      "iter 7000/1000000  loss         0.201336  avg_L1_norm_grad         0.000025  w[0]    0.010 bias    2.991\n",
      "iter 7001/1000000  loss         0.201336  avg_L1_norm_grad         0.000025  w[0]    0.010 bias    2.991\n",
      "iter 7100/1000000  loss         0.201315  avg_L1_norm_grad         0.000024  w[0]    0.010 bias    2.996\n",
      "iter 7101/1000000  loss         0.201315  avg_L1_norm_grad         0.000024  w[0]    0.010 bias    2.996\n",
      "iter 7200/1000000  loss         0.201295  avg_L1_norm_grad         0.000024  w[0]    0.010 bias    3.001\n",
      "iter 7201/1000000  loss         0.201295  avg_L1_norm_grad         0.000024  w[0]    0.010 bias    3.001\n",
      "iter 7300/1000000  loss         0.201276  avg_L1_norm_grad         0.000023  w[0]    0.010 bias    3.005\n",
      "iter 7301/1000000  loss         0.201276  avg_L1_norm_grad         0.000023  w[0]    0.010 bias    3.005\n",
      "iter 7400/1000000  loss         0.201258  avg_L1_norm_grad         0.000022  w[0]    0.010 bias    3.010\n",
      "iter 7401/1000000  loss         0.201258  avg_L1_norm_grad         0.000022  w[0]    0.010 bias    3.010\n",
      "iter 7500/1000000  loss         0.201241  avg_L1_norm_grad         0.000022  w[0]    0.011 bias    3.014\n",
      "iter 7501/1000000  loss         0.201241  avg_L1_norm_grad         0.000022  w[0]    0.011 bias    3.014\n",
      "iter 7600/1000000  loss         0.201225  avg_L1_norm_grad         0.000021  w[0]    0.011 bias    3.018\n",
      "iter 7601/1000000  loss         0.201225  avg_L1_norm_grad         0.000021  w[0]    0.011 bias    3.018\n",
      "iter 7700/1000000  loss         0.201209  avg_L1_norm_grad         0.000021  w[0]    0.011 bias    3.022\n",
      "iter 7701/1000000  loss         0.201209  avg_L1_norm_grad         0.000021  w[0]    0.011 bias    3.022\n",
      "iter 7800/1000000  loss         0.201195  avg_L1_norm_grad         0.000020  w[0]    0.011 bias    3.026\n",
      "iter 7801/1000000  loss         0.201195  avg_L1_norm_grad         0.000020  w[0]    0.011 bias    3.026\n",
      "iter 7900/1000000  loss         0.201181  avg_L1_norm_grad         0.000020  w[0]    0.011 bias    3.029\n",
      "iter 7901/1000000  loss         0.201181  avg_L1_norm_grad         0.000020  w[0]    0.011 bias    3.029\n",
      "iter 8000/1000000  loss         0.201168  avg_L1_norm_grad         0.000019  w[0]    0.011 bias    3.033\n",
      "iter 8001/1000000  loss         0.201168  avg_L1_norm_grad         0.000019  w[0]    0.011 bias    3.033\n",
      "iter 8100/1000000  loss         0.201155  avg_L1_norm_grad         0.000019  w[0]    0.011 bias    3.036\n",
      "iter 8101/1000000  loss         0.201155  avg_L1_norm_grad         0.000019  w[0]    0.011 bias    3.036\n",
      "iter 8200/1000000  loss         0.201144  avg_L1_norm_grad         0.000018  w[0]    0.011 bias    3.039\n",
      "iter 8201/1000000  loss         0.201144  avg_L1_norm_grad         0.000018  w[0]    0.011 bias    3.039\n",
      "iter 8300/1000000  loss         0.201132  avg_L1_norm_grad         0.000018  w[0]    0.012 bias    3.043\n",
      "iter 8301/1000000  loss         0.201132  avg_L1_norm_grad         0.000018  w[0]    0.012 bias    3.043\n",
      "iter 8400/1000000  loss         0.201122  avg_L1_norm_grad         0.000017  w[0]    0.012 bias    3.046\n",
      "iter 8401/1000000  loss         0.201122  avg_L1_norm_grad         0.000017  w[0]    0.012 bias    3.046\n",
      "iter 8500/1000000  loss         0.201112  avg_L1_norm_grad         0.000017  w[0]    0.012 bias    3.048\n",
      "iter 8501/1000000  loss         0.201112  avg_L1_norm_grad         0.000017  w[0]    0.012 bias    3.049\n",
      "iter 8600/1000000  loss         0.201102  avg_L1_norm_grad         0.000016  w[0]    0.012 bias    3.051\n",
      "iter 8601/1000000  loss         0.201102  avg_L1_norm_grad         0.000016  w[0]    0.012 bias    3.051\n",
      "iter 8700/1000000  loss         0.201093  avg_L1_norm_grad         0.000016  w[0]    0.012 bias    3.054\n",
      "iter 8701/1000000  loss         0.201093  avg_L1_norm_grad         0.000016  w[0]    0.012 bias    3.054\n",
      "iter 8800/1000000  loss         0.201084  avg_L1_norm_grad         0.000016  w[0]    0.012 bias    3.057\n",
      "iter 8801/1000000  loss         0.201084  avg_L1_norm_grad         0.000016  w[0]    0.012 bias    3.057\n",
      "iter 8900/1000000  loss         0.201076  avg_L1_norm_grad         0.000015  w[0]    0.012 bias    3.059\n",
      "iter 8901/1000000  loss         0.201076  avg_L1_norm_grad         0.000015  w[0]    0.012 bias    3.059\n",
      "iter 9000/1000000  loss         0.201068  avg_L1_norm_grad         0.000015  w[0]    0.012 bias    3.061\n",
      "iter 9001/1000000  loss         0.201068  avg_L1_norm_grad         0.000015  w[0]    0.012 bias    3.061\n",
      "iter 9100/1000000  loss         0.201060  avg_L1_norm_grad         0.000015  w[0]    0.012 bias    3.064\n",
      "iter 9101/1000000  loss         0.201060  avg_L1_norm_grad         0.000015  w[0]    0.012 bias    3.064\n",
      "iter 9200/1000000  loss         0.201053  avg_L1_norm_grad         0.000014  w[0]    0.012 bias    3.066\n",
      "iter 9201/1000000  loss         0.201053  avg_L1_norm_grad         0.000014  w[0]    0.012 bias    3.066\n",
      "iter 9300/1000000  loss         0.201046  avg_L1_norm_grad         0.000014  w[0]    0.012 bias    3.068\n",
      "iter 9301/1000000  loss         0.201046  avg_L1_norm_grad         0.000014  w[0]    0.012 bias    3.068\n",
      "iter 9400/1000000  loss         0.201040  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.070\n",
      "iter 9401/1000000  loss         0.201040  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.070\n",
      "iter 9500/1000000  loss         0.201034  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.072\n",
      "iter 9501/1000000  loss         0.201034  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.072\n",
      "iter 9600/1000000  loss         0.201028  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.074\n",
      "iter 9601/1000000  loss         0.201028  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.074\n",
      "iter 9700/1000000  loss         0.201022  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.076\n",
      "iter 9701/1000000  loss         0.201022  avg_L1_norm_grad         0.000013  w[0]    0.013 bias    3.076\n",
      "iter 9800/1000000  loss         0.201017  avg_L1_norm_grad         0.000012  w[0]    0.013 bias    3.078\n",
      "iter 9801/1000000  loss         0.201017  avg_L1_norm_grad         0.000012  w[0]    0.013 bias    3.078\n",
      "iter 9900/1000000  loss         0.201012  avg_L1_norm_grad         0.000012  w[0]    0.013 bias    3.079\n",
      "iter 9901/1000000  loss         0.201012  avg_L1_norm_grad         0.000012  w[0]    0.013 bias    3.079\n",
      "iter 10000/1000000  loss         0.201007  avg_L1_norm_grad         0.000012  w[0]    0.013 bias    3.081\n",
      "iter 10001/1000000  loss         0.201007  avg_L1_norm_grad         0.000012  w[0]    0.013 bias    3.081\n",
      "iter 10100/1000000  loss         0.201003  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.082\n",
      "iter 10101/1000000  loss         0.201002  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.083\n",
      "iter 10200/1000000  loss         0.200998  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.084\n",
      "iter 10201/1000000  loss         0.200998  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.084\n",
      "iter 10300/1000000  loss         0.200994  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.085\n",
      "iter 10301/1000000  loss         0.200994  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.085\n",
      "iter 10400/1000000  loss         0.200990  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.087\n",
      "iter 10401/1000000  loss         0.200990  avg_L1_norm_grad         0.000011  w[0]    0.013 bias    3.087\n",
      "iter 10500/1000000  loss         0.200986  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.088\n",
      "iter 10501/1000000  loss         0.200986  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.088\n",
      "iter 10600/1000000  loss         0.200982  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.090\n",
      "iter 10601/1000000  loss         0.200982  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.090\n",
      "iter 10700/1000000  loss         0.200979  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.091\n",
      "iter 10701/1000000  loss         0.200979  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.200976  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.092\n",
      "iter 10801/1000000  loss         0.200976  avg_L1_norm_grad         0.000010  w[0]    0.013 bias    3.092\n",
      "iter 10900/1000000  loss         0.200973  avg_L1_norm_grad         0.000009  w[0]    0.013 bias    3.093\n",
      "iter 10901/1000000  loss         0.200972  avg_L1_norm_grad         0.000009  w[0]    0.013 bias    3.093\n",
      "iter 11000/1000000  loss         0.200970  avg_L1_norm_grad         0.000009  w[0]    0.013 bias    3.094\n",
      "iter 11001/1000000  loss         0.200969  avg_L1_norm_grad         0.000009  w[0]    0.013 bias    3.094\n",
      "iter 11100/1000000  loss         0.200967  avg_L1_norm_grad         0.000009  w[0]    0.013 bias    3.095\n",
      "iter 11101/1000000  loss         0.200967  avg_L1_norm_grad         0.000009  w[0]    0.013 bias    3.095\n",
      "iter 11200/1000000  loss         0.200964  avg_L1_norm_grad         0.000009  w[0]    0.014 bias    3.096\n",
      "iter 11201/1000000  loss         0.200964  avg_L1_norm_grad         0.000009  w[0]    0.014 bias    3.096\n",
      "Done. Converged after 11247 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr100 = LRGDF(alpha=100.0, step_size=0.1)\n",
    "new_lr100.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Accuracy 0.956916666666664\n",
      "Ave Loaded\n",
      "New Accuracy 0.9702499999999973\n"
     ]
    }
   ],
   "source": [
    "y_hat_Origin100=np.asarray(orig_lr100.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)\n",
    "\n",
    "y_hat_New100=np.asarray(new_lr100.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig3a, axes_arr = plt.subplots(nrows=1, ncols=2,figsize=(13,5))\n",
    "ax1=axes_arr[0]\n",
    "ax1.set_title('ROC'); ax1.set_xlabel(\"FPR\"); ax1.set_ylabel(\"TPR\");\n",
    "ax1.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "ax1.plot(fpr3te,tpr3te, label=\"Adding Square and Ave\")\n",
    "\n",
    "ax1.set_xlim([-0.0, 1.0]);\n",
    "ax1.set_ylim([-0.0, 1.0]);\n",
    "ax1.legend();\n",
    "\n",
    "ax2=axes_arr[1]\n",
    "ax2.set_title('Part of ROC'); ax2.set_xlabel(\"FPR\"); ax2.set_ylabel(\"TPR\");\n",
    "ax2.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "ax2.plot(fpr3te,tpr3te, label=\"Adding Square and All turn_on of X & Y with Noise\")\n",
    "\n",
    "ax2.set_xlim([0.0, 0.2]);\n",
    "ax2.set_ylim([0.8, 1.0]);\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr100.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXV2_100 Err: 0.44 0.993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr3.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test10.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXV2_2Err: 0.38 0.996133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
