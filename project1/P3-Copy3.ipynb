{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(1,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030265  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.911808  avg_L1_norm_grad         0.028655  w[0]   -0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.848053  avg_L1_norm_grad         0.019671  w[0]    0.000 bias    0.021\n",
      "iter    3/1000000  loss         0.804067  avg_L1_norm_grad         0.018522  w[0]    0.000 bias    0.025\n",
      "iter    4/1000000  loss         0.770729  avg_L1_norm_grad         0.013981  w[0]    0.001 bias    0.040\n",
      "iter    5/1000000  loss         0.745030  avg_L1_norm_grad         0.013394  w[0]    0.001 bias    0.047\n",
      "iter    6/1000000  loss         0.724020  avg_L1_norm_grad         0.011403  w[0]    0.001 bias    0.059\n",
      "iter    7/1000000  loss         0.706203  avg_L1_norm_grad         0.010936  w[0]    0.001 bias    0.067\n",
      "iter    8/1000000  loss         0.690598  avg_L1_norm_grad         0.010011  w[0]    0.001 bias    0.077\n",
      "iter    9/1000000  loss         0.676624  avg_L1_norm_grad         0.009560  w[0]    0.001 bias    0.086\n",
      "iter   10/1000000  loss         0.663910  avg_L1_norm_grad         0.009044  w[0]    0.002 bias    0.095\n",
      "iter   11/1000000  loss         0.652206  avg_L1_norm_grad         0.008657  w[0]    0.002 bias    0.104\n",
      "iter   12/1000000  loss         0.641335  avg_L1_norm_grad         0.008293  w[0]    0.002 bias    0.113\n",
      "iter   13/1000000  loss         0.631170  avg_L1_norm_grad         0.007983  w[0]    0.002 bias    0.122\n",
      "iter   14/1000000  loss         0.621612  avg_L1_norm_grad         0.007704  w[0]    0.002 bias    0.130\n",
      "iter   15/1000000  loss         0.612586  avg_L1_norm_grad         0.007456  w[0]    0.002 bias    0.138\n",
      "iter   16/1000000  loss         0.604033  avg_L1_norm_grad         0.007230  w[0]    0.002 bias    0.147\n",
      "iter   17/1000000  loss         0.595904  avg_L1_norm_grad         0.007022  w[0]    0.003 bias    0.155\n",
      "iter   18/1000000  loss         0.588161  avg_L1_norm_grad         0.006831  w[0]    0.003 bias    0.162\n",
      "iter   19/1000000  loss         0.580771  avg_L1_norm_grad         0.006653  w[0]    0.003 bias    0.170\n",
      "iter  100/1000000  loss         0.363399  avg_L1_norm_grad         0.002331  w[0]    0.006 bias    0.590\n",
      "iter  101/1000000  loss         0.362349  avg_L1_norm_grad         0.002313  w[0]    0.006 bias    0.594\n",
      "iter  200/1000000  loss         0.299756  avg_L1_norm_grad         0.001380  w[0]    0.005 bias    0.889\n",
      "iter  201/1000000  loss         0.299366  avg_L1_norm_grad         0.001375  w[0]    0.005 bias    0.891\n",
      "iter  300/1000000  loss         0.270859  avg_L1_norm_grad         0.001021  w[0]    0.003 bias    1.100\n",
      "iter  301/1000000  loss         0.270645  avg_L1_norm_grad         0.001018  w[0]    0.003 bias    1.102\n",
      "iter  400/1000000  loss         0.253601  avg_L1_norm_grad         0.000829  w[0]    0.001 bias    1.266\n",
      "iter  401/1000000  loss         0.253462  avg_L1_norm_grad         0.000827  w[0]    0.001 bias    1.267\n",
      "iter  500/1000000  loss         0.241859  avg_L1_norm_grad         0.000707  w[0]   -0.001 bias    1.403\n",
      "iter  501/1000000  loss         0.241760  avg_L1_norm_grad         0.000706  w[0]   -0.001 bias    1.404\n",
      "iter  600/1000000  loss         0.233225  avg_L1_norm_grad         0.000624  w[0]   -0.003 bias    1.520\n",
      "iter  601/1000000  loss         0.233150  avg_L1_norm_grad         0.000623  w[0]   -0.003 bias    1.521\n",
      "iter  700/1000000  loss         0.226540  avg_L1_norm_grad         0.000562  w[0]   -0.005 bias    1.622\n",
      "iter  701/1000000  loss         0.226481  avg_L1_norm_grad         0.000561  w[0]   -0.005 bias    1.623\n",
      "iter  800/1000000  loss         0.221171  avg_L1_norm_grad         0.000513  w[0]   -0.006 bias    1.713\n",
      "iter  801/1000000  loss         0.221122  avg_L1_norm_grad         0.000512  w[0]   -0.006 bias    1.714\n",
      "iter  900/1000000  loss         0.216737  avg_L1_norm_grad         0.000472  w[0]   -0.008 bias    1.795\n",
      "iter  901/1000000  loss         0.216697  avg_L1_norm_grad         0.000472  w[0]   -0.008 bias    1.795\n",
      "iter 1000/1000000  loss         0.212999  avg_L1_norm_grad         0.000439  w[0]   -0.009 bias    1.868\n",
      "iter 1001/1000000  loss         0.212964  avg_L1_norm_grad         0.000438  w[0]   -0.009 bias    1.869\n",
      "iter 1100/1000000  loss         0.209793  avg_L1_norm_grad         0.000410  w[0]   -0.010 bias    1.936\n",
      "iter 1101/1000000  loss         0.209763  avg_L1_norm_grad         0.000410  w[0]   -0.010 bias    1.937\n",
      "iter 1200/1000000  loss         0.207006  avg_L1_norm_grad         0.000386  w[0]   -0.011 bias    1.998\n",
      "iter 1201/1000000  loss         0.206980  avg_L1_norm_grad         0.000386  w[0]   -0.011 bias    1.999\n",
      "iter 1300/1000000  loss         0.204557  avg_L1_norm_grad         0.000365  w[0]   -0.012 bias    2.056\n",
      "iter 1301/1000000  loss         0.204533  avg_L1_norm_grad         0.000365  w[0]   -0.013 bias    2.056\n",
      "iter 1400/1000000  loss         0.202383  avg_L1_norm_grad         0.000346  w[0]   -0.013 bias    2.109\n",
      "iter 1401/1000000  loss         0.202362  avg_L1_norm_grad         0.000346  w[0]   -0.013 bias    2.110\n",
      "iter 1500/1000000  loss         0.200438  avg_L1_norm_grad         0.000330  w[0]   -0.014 bias    2.159\n",
      "iter 1501/1000000  loss         0.200420  avg_L1_norm_grad         0.000330  w[0]   -0.014 bias    2.160\n",
      "iter 1600/1000000  loss         0.198686  avg_L1_norm_grad         0.000315  w[0]   -0.015 bias    2.206\n",
      "iter 1601/1000000  loss         0.198670  avg_L1_norm_grad         0.000315  w[0]   -0.015 bias    2.207\n",
      "iter 1700/1000000  loss         0.197099  avg_L1_norm_grad         0.000302  w[0]   -0.016 bias    2.250\n",
      "iter 1701/1000000  loss         0.197084  avg_L1_norm_grad         0.000302  w[0]   -0.016 bias    2.251\n",
      "iter 1800/1000000  loss         0.195653  avg_L1_norm_grad         0.000290  w[0]   -0.016 bias    2.292\n",
      "iter 1801/1000000  loss         0.195640  avg_L1_norm_grad         0.000290  w[0]   -0.016 bias    2.292\n",
      "iter 1900/1000000  loss         0.194330  avg_L1_norm_grad         0.000279  w[0]   -0.017 bias    2.331\n",
      "iter 1901/1000000  loss         0.194318  avg_L1_norm_grad         0.000279  w[0]   -0.017 bias    2.331\n",
      "iter 2000/1000000  loss         0.193114  avg_L1_norm_grad         0.000268  w[0]   -0.017 bias    2.368\n",
      "iter 2001/1000000  loss         0.193103  avg_L1_norm_grad         0.000268  w[0]   -0.017 bias    2.368\n",
      "iter 2100/1000000  loss         0.191992  avg_L1_norm_grad         0.000259  w[0]   -0.017 bias    2.403\n",
      "iter 2101/1000000  loss         0.191982  avg_L1_norm_grad         0.000259  w[0]   -0.017 bias    2.404\n",
      "iter 2200/1000000  loss         0.190954  avg_L1_norm_grad         0.000250  w[0]   -0.018 bias    2.437\n",
      "iter 2201/1000000  loss         0.190944  avg_L1_norm_grad         0.000250  w[0]   -0.018 bias    2.437\n",
      "iter 2300/1000000  loss         0.189990  avg_L1_norm_grad         0.000242  w[0]   -0.018 bias    2.469\n",
      "iter 2301/1000000  loss         0.189981  avg_L1_norm_grad         0.000242  w[0]   -0.018 bias    2.469\n",
      "iter 2400/1000000  loss         0.189093  avg_L1_norm_grad         0.000234  w[0]   -0.018 bias    2.499\n",
      "iter 2401/1000000  loss         0.189084  avg_L1_norm_grad         0.000234  w[0]   -0.018 bias    2.499\n",
      "iter 2500/1000000  loss         0.188255  avg_L1_norm_grad         0.000227  w[0]   -0.018 bias    2.528\n",
      "iter 2501/1000000  loss         0.188247  avg_L1_norm_grad         0.000227  w[0]   -0.018 bias    2.528\n",
      "iter 2600/1000000  loss         0.187471  avg_L1_norm_grad         0.000221  w[0]   -0.019 bias    2.556\n",
      "iter 2601/1000000  loss         0.187463  avg_L1_norm_grad         0.000221  w[0]   -0.019 bias    2.556\n",
      "iter 2700/1000000  loss         0.186736  avg_L1_norm_grad         0.000214  w[0]   -0.019 bias    2.583\n",
      "iter 2701/1000000  loss         0.186729  avg_L1_norm_grad         0.000214  w[0]   -0.019 bias    2.583\n",
      "iter 2800/1000000  loss         0.186045  avg_L1_norm_grad         0.000208  w[0]   -0.019 bias    2.608\n",
      "iter 2801/1000000  loss         0.186039  avg_L1_norm_grad         0.000208  w[0]   -0.019 bias    2.608\n",
      "iter 2900/1000000  loss         0.185395  avg_L1_norm_grad         0.000203  w[0]   -0.019 bias    2.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.185389  avg_L1_norm_grad         0.000203  w[0]   -0.019 bias    2.633\n",
      "iter 3000/1000000  loss         0.184782  avg_L1_norm_grad         0.000198  w[0]   -0.019 bias    2.656\n",
      "iter 3001/1000000  loss         0.184776  avg_L1_norm_grad         0.000197  w[0]   -0.019 bias    2.657\n",
      "iter 3100/1000000  loss         0.184203  avg_L1_norm_grad         0.000192  w[0]   -0.019 bias    2.679\n",
      "iter 3101/1000000  loss         0.184198  avg_L1_norm_grad         0.000192  w[0]   -0.019 bias    2.679\n",
      "iter 3200/1000000  loss         0.183656  avg_L1_norm_grad         0.000188  w[0]   -0.019 bias    2.701\n",
      "iter 3201/1000000  loss         0.183650  avg_L1_norm_grad         0.000188  w[0]   -0.019 bias    2.701\n",
      "iter 3300/1000000  loss         0.183137  avg_L1_norm_grad         0.000183  w[0]   -0.018 bias    2.722\n",
      "iter 3301/1000000  loss         0.183132  avg_L1_norm_grad         0.000183  w[0]   -0.018 bias    2.722\n",
      "iter 3400/1000000  loss         0.182645  avg_L1_norm_grad         0.000179  w[0]   -0.018 bias    2.742\n",
      "iter 3401/1000000  loss         0.182640  avg_L1_norm_grad         0.000179  w[0]   -0.018 bias    2.742\n",
      "iter 3500/1000000  loss         0.182177  avg_L1_norm_grad         0.000175  w[0]   -0.018 bias    2.762\n",
      "iter 3501/1000000  loss         0.182173  avg_L1_norm_grad         0.000175  w[0]   -0.018 bias    2.762\n",
      "iter 3600/1000000  loss         0.181733  avg_L1_norm_grad         0.000171  w[0]   -0.018 bias    2.781\n",
      "iter 3601/1000000  loss         0.181729  avg_L1_norm_grad         0.000171  w[0]   -0.018 bias    2.781\n",
      "iter 3700/1000000  loss         0.181310  avg_L1_norm_grad         0.000167  w[0]   -0.018 bias    2.799\n",
      "iter 3701/1000000  loss         0.181306  avg_L1_norm_grad         0.000167  w[0]   -0.018 bias    2.799\n",
      "iter 3800/1000000  loss         0.180907  avg_L1_norm_grad         0.000163  w[0]   -0.018 bias    2.817\n",
      "iter 3801/1000000  loss         0.180903  avg_L1_norm_grad         0.000163  w[0]   -0.018 bias    2.817\n",
      "iter 3900/1000000  loss         0.180522  avg_L1_norm_grad         0.000160  w[0]   -0.017 bias    2.834\n",
      "iter 3901/1000000  loss         0.180519  avg_L1_norm_grad         0.000160  w[0]   -0.017 bias    2.834\n",
      "iter 4000/1000000  loss         0.180155  avg_L1_norm_grad         0.000157  w[0]   -0.017 bias    2.850\n",
      "iter 4001/1000000  loss         0.180152  avg_L1_norm_grad         0.000157  w[0]   -0.017 bias    2.851\n",
      "iter 4100/1000000  loss         0.179804  avg_L1_norm_grad         0.000153  w[0]   -0.017 bias    2.866\n",
      "iter 4101/1000000  loss         0.179801  avg_L1_norm_grad         0.000153  w[0]   -0.017 bias    2.867\n",
      "iter 4200/1000000  loss         0.179469  avg_L1_norm_grad         0.000150  w[0]   -0.017 bias    2.882\n",
      "iter 4201/1000000  loss         0.179466  avg_L1_norm_grad         0.000150  w[0]   -0.017 bias    2.882\n",
      "iter 4300/1000000  loss         0.179148  avg_L1_norm_grad         0.000147  w[0]   -0.016 bias    2.897\n",
      "iter 4301/1000000  loss         0.179145  avg_L1_norm_grad         0.000147  w[0]   -0.016 bias    2.897\n",
      "iter 4400/1000000  loss         0.178840  avg_L1_norm_grad         0.000144  w[0]   -0.016 bias    2.912\n",
      "iter 4401/1000000  loss         0.178837  avg_L1_norm_grad         0.000144  w[0]   -0.016 bias    2.912\n",
      "iter 4500/1000000  loss         0.178545  avg_L1_norm_grad         0.000142  w[0]   -0.016 bias    2.926\n",
      "iter 4501/1000000  loss         0.178542  avg_L1_norm_grad         0.000142  w[0]   -0.016 bias    2.926\n",
      "iter 4600/1000000  loss         0.178262  avg_L1_norm_grad         0.000139  w[0]   -0.015 bias    2.940\n",
      "iter 4601/1000000  loss         0.178260  avg_L1_norm_grad         0.000139  w[0]   -0.015 bias    2.940\n",
      "iter 4700/1000000  loss         0.177991  avg_L1_norm_grad         0.000136  w[0]   -0.015 bias    2.953\n",
      "iter 4701/1000000  loss         0.177988  avg_L1_norm_grad         0.000136  w[0]   -0.015 bias    2.953\n",
      "iter 4800/1000000  loss         0.177730  avg_L1_norm_grad         0.000134  w[0]   -0.015 bias    2.966\n",
      "iter 4801/1000000  loss         0.177727  avg_L1_norm_grad         0.000134  w[0]   -0.015 bias    2.966\n",
      "iter 4900/1000000  loss         0.177479  avg_L1_norm_grad         0.000131  w[0]   -0.014 bias    2.979\n",
      "iter 4901/1000000  loss         0.177477  avg_L1_norm_grad         0.000131  w[0]   -0.014 bias    2.979\n",
      "iter 5000/1000000  loss         0.177238  avg_L1_norm_grad         0.000129  w[0]   -0.014 bias    2.991\n",
      "iter 5001/1000000  loss         0.177236  avg_L1_norm_grad         0.000129  w[0]   -0.014 bias    2.991\n",
      "iter 5100/1000000  loss         0.177006  avg_L1_norm_grad         0.000127  w[0]   -0.014 bias    3.003\n",
      "iter 5101/1000000  loss         0.177004  avg_L1_norm_grad         0.000127  w[0]   -0.014 bias    3.004\n",
      "iter 5200/1000000  loss         0.176783  avg_L1_norm_grad         0.000124  w[0]   -0.013 bias    3.015\n",
      "iter 5201/1000000  loss         0.176781  avg_L1_norm_grad         0.000124  w[0]   -0.013 bias    3.015\n",
      "iter 5300/1000000  loss         0.176568  avg_L1_norm_grad         0.000122  w[0]   -0.013 bias    3.027\n",
      "iter 5301/1000000  loss         0.176566  avg_L1_norm_grad         0.000122  w[0]   -0.013 bias    3.027\n",
      "iter 5400/1000000  loss         0.176360  avg_L1_norm_grad         0.000120  w[0]   -0.013 bias    3.038\n",
      "iter 5401/1000000  loss         0.176358  avg_L1_norm_grad         0.000120  w[0]   -0.013 bias    3.038\n",
      "iter 5500/1000000  loss         0.176161  avg_L1_norm_grad         0.000118  w[0]   -0.012 bias    3.049\n",
      "iter 5501/1000000  loss         0.176159  avg_L1_norm_grad         0.000118  w[0]   -0.012 bias    3.049\n",
      "iter 5600/1000000  loss         0.175968  avg_L1_norm_grad         0.000116  w[0]   -0.012 bias    3.059\n",
      "iter 5601/1000000  loss         0.175966  avg_L1_norm_grad         0.000116  w[0]   -0.012 bias    3.059\n",
      "iter 5700/1000000  loss         0.175782  avg_L1_norm_grad         0.000114  w[0]   -0.012 bias    3.069\n",
      "iter 5701/1000000  loss         0.175780  avg_L1_norm_grad         0.000114  w[0]   -0.012 bias    3.070\n",
      "iter 5800/1000000  loss         0.175602  avg_L1_norm_grad         0.000112  w[0]   -0.011 bias    3.080\n",
      "iter 5801/1000000  loss         0.175600  avg_L1_norm_grad         0.000112  w[0]   -0.011 bias    3.080\n",
      "iter 5900/1000000  loss         0.175429  avg_L1_norm_grad         0.000111  w[0]   -0.011 bias    3.089\n",
      "iter 5901/1000000  loss         0.175427  avg_L1_norm_grad         0.000111  w[0]   -0.011 bias    3.090\n",
      "iter 6000/1000000  loss         0.175261  avg_L1_norm_grad         0.000109  w[0]   -0.010 bias    3.099\n",
      "iter 6001/1000000  loss         0.175259  avg_L1_norm_grad         0.000109  w[0]   -0.010 bias    3.099\n",
      "iter 6100/1000000  loss         0.175099  avg_L1_norm_grad         0.000107  w[0]   -0.010 bias    3.108\n",
      "iter 6101/1000000  loss         0.175098  avg_L1_norm_grad         0.000107  w[0]   -0.010 bias    3.109\n",
      "iter 6200/1000000  loss         0.174943  avg_L1_norm_grad         0.000105  w[0]   -0.010 bias    3.118\n",
      "iter 6201/1000000  loss         0.174941  avg_L1_norm_grad         0.000105  w[0]   -0.010 bias    3.118\n",
      "iter 6300/1000000  loss         0.174791  avg_L1_norm_grad         0.000104  w[0]   -0.009 bias    3.127\n",
      "iter 6301/1000000  loss         0.174790  avg_L1_norm_grad         0.000104  w[0]   -0.009 bias    3.127\n",
      "iter 6400/1000000  loss         0.174645  avg_L1_norm_grad         0.000102  w[0]   -0.009 bias    3.135\n",
      "iter 6401/1000000  loss         0.174643  avg_L1_norm_grad         0.000102  w[0]   -0.009 bias    3.135\n",
      "iter 6500/1000000  loss         0.174503  avg_L1_norm_grad         0.000101  w[0]   -0.009 bias    3.144\n",
      "iter 6501/1000000  loss         0.174501  avg_L1_norm_grad         0.000101  w[0]   -0.009 bias    3.144\n",
      "iter 6600/1000000  loss         0.174365  avg_L1_norm_grad         0.000099  w[0]   -0.008 bias    3.152\n",
      "iter 6601/1000000  loss         0.174364  avg_L1_norm_grad         0.000099  w[0]   -0.008 bias    3.152\n",
      "iter 6700/1000000  loss         0.174232  avg_L1_norm_grad         0.000098  w[0]   -0.008 bias    3.160\n",
      "iter 6701/1000000  loss         0.174231  avg_L1_norm_grad         0.000098  w[0]   -0.008 bias    3.161\n",
      "iter 6800/1000000  loss         0.174104  avg_L1_norm_grad         0.000096  w[0]   -0.007 bias    3.169\n",
      "iter 6801/1000000  loss         0.174102  avg_L1_norm_grad         0.000096  w[0]   -0.007 bias    3.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.173979  avg_L1_norm_grad         0.000095  w[0]   -0.007 bias    3.176\n",
      "iter 6901/1000000  loss         0.173977  avg_L1_norm_grad         0.000095  w[0]   -0.007 bias    3.176\n",
      "iter 7000/1000000  loss         0.173858  avg_L1_norm_grad         0.000093  w[0]   -0.007 bias    3.184\n",
      "iter 7001/1000000  loss         0.173857  avg_L1_norm_grad         0.000093  w[0]   -0.007 bias    3.184\n",
      "iter 7100/1000000  loss         0.173740  avg_L1_norm_grad         0.000092  w[0]   -0.006 bias    3.192\n",
      "iter 7101/1000000  loss         0.173739  avg_L1_norm_grad         0.000092  w[0]   -0.006 bias    3.192\n",
      "iter 7200/1000000  loss         0.173627  avg_L1_norm_grad         0.000091  w[0]   -0.006 bias    3.199\n",
      "iter 7201/1000000  loss         0.173625  avg_L1_norm_grad         0.000091  w[0]   -0.006 bias    3.199\n",
      "iter 7300/1000000  loss         0.173516  avg_L1_norm_grad         0.000089  w[0]   -0.005 bias    3.206\n",
      "iter 7301/1000000  loss         0.173515  avg_L1_norm_grad         0.000089  w[0]   -0.005 bias    3.206\n",
      "iter 7400/1000000  loss         0.173409  avg_L1_norm_grad         0.000088  w[0]   -0.005 bias    3.213\n",
      "iter 7401/1000000  loss         0.173408  avg_L1_norm_grad         0.000088  w[0]   -0.005 bias    3.213\n",
      "iter 7500/1000000  loss         0.173305  avg_L1_norm_grad         0.000087  w[0]   -0.005 bias    3.220\n",
      "iter 7501/1000000  loss         0.173304  avg_L1_norm_grad         0.000087  w[0]   -0.005 bias    3.220\n",
      "iter 7600/1000000  loss         0.173204  avg_L1_norm_grad         0.000085  w[0]   -0.004 bias    3.227\n",
      "iter 7601/1000000  loss         0.173203  avg_L1_norm_grad         0.000085  w[0]   -0.004 bias    3.227\n",
      "iter 7700/1000000  loss         0.173106  avg_L1_norm_grad         0.000084  w[0]   -0.004 bias    3.234\n",
      "iter 7701/1000000  loss         0.173105  avg_L1_norm_grad         0.000084  w[0]   -0.004 bias    3.234\n",
      "iter 7800/1000000  loss         0.173011  avg_L1_norm_grad         0.000083  w[0]   -0.004 bias    3.240\n",
      "iter 7801/1000000  loss         0.173010  avg_L1_norm_grad         0.000083  w[0]   -0.004 bias    3.240\n",
      "iter 7900/1000000  loss         0.172919  avg_L1_norm_grad         0.000082  w[0]   -0.003 bias    3.247\n",
      "iter 7901/1000000  loss         0.172918  avg_L1_norm_grad         0.000082  w[0]   -0.003 bias    3.247\n",
      "iter 8000/1000000  loss         0.172829  avg_L1_norm_grad         0.000081  w[0]   -0.003 bias    3.253\n",
      "iter 8001/1000000  loss         0.172828  avg_L1_norm_grad         0.000081  w[0]   -0.003 bias    3.253\n",
      "iter 8100/1000000  loss         0.172742  avg_L1_norm_grad         0.000080  w[0]   -0.003 bias    3.259\n",
      "iter 8101/1000000  loss         0.172741  avg_L1_norm_grad         0.000080  w[0]   -0.003 bias    3.259\n",
      "iter 8200/1000000  loss         0.172657  avg_L1_norm_grad         0.000079  w[0]   -0.002 bias    3.265\n",
      "iter 8201/1000000  loss         0.172656  avg_L1_norm_grad         0.000079  w[0]   -0.002 bias    3.265\n",
      "iter 8300/1000000  loss         0.172574  avg_L1_norm_grad         0.000077  w[0]   -0.002 bias    3.271\n",
      "iter 8301/1000000  loss         0.172574  avg_L1_norm_grad         0.000077  w[0]   -0.002 bias    3.271\n",
      "iter 8400/1000000  loss         0.172494  avg_L1_norm_grad         0.000076  w[0]   -0.002 bias    3.277\n",
      "iter 8401/1000000  loss         0.172493  avg_L1_norm_grad         0.000076  w[0]   -0.002 bias    3.277\n",
      "iter 8500/1000000  loss         0.172416  avg_L1_norm_grad         0.000075  w[0]   -0.001 bias    3.283\n",
      "iter 8501/1000000  loss         0.172416  avg_L1_norm_grad         0.000075  w[0]   -0.001 bias    3.283\n",
      "iter 8600/1000000  loss         0.172340  avg_L1_norm_grad         0.000074  w[0]   -0.001 bias    3.288\n",
      "iter 8601/1000000  loss         0.172340  avg_L1_norm_grad         0.000074  w[0]   -0.001 bias    3.289\n",
      "iter 8700/1000000  loss         0.172267  avg_L1_norm_grad         0.000073  w[0]   -0.001 bias    3.294\n",
      "iter 8701/1000000  loss         0.172266  avg_L1_norm_grad         0.000073  w[0]   -0.001 bias    3.294\n",
      "iter 8800/1000000  loss         0.172195  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    3.299\n",
      "iter 8801/1000000  loss         0.172194  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    3.299\n",
      "iter 8900/1000000  loss         0.172125  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    3.305\n",
      "iter 8901/1000000  loss         0.172124  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    3.305\n",
      "iter 9000/1000000  loss         0.172057  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    3.310\n",
      "iter 9001/1000000  loss         0.172056  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    3.310\n",
      "iter 9100/1000000  loss         0.171991  avg_L1_norm_grad         0.000070  w[0]    0.001 bias    3.315\n",
      "iter 9101/1000000  loss         0.171990  avg_L1_norm_grad         0.000070  w[0]    0.001 bias    3.315\n",
      "iter 9200/1000000  loss         0.171926  avg_L1_norm_grad         0.000069  w[0]    0.001 bias    3.320\n",
      "iter 9201/1000000  loss         0.171926  avg_L1_norm_grad         0.000069  w[0]    0.001 bias    3.320\n",
      "iter 9300/1000000  loss         0.171864  avg_L1_norm_grad         0.000068  w[0]    0.001 bias    3.325\n",
      "iter 9301/1000000  loss         0.171863  avg_L1_norm_grad         0.000068  w[0]    0.001 bias    3.325\n",
      "iter 9400/1000000  loss         0.171802  avg_L1_norm_grad         0.000067  w[0]    0.002 bias    3.330\n",
      "iter 9401/1000000  loss         0.171802  avg_L1_norm_grad         0.000067  w[0]    0.002 bias    3.330\n",
      "iter 9500/1000000  loss         0.171743  avg_L1_norm_grad         0.000066  w[0]    0.002 bias    3.335\n",
      "iter 9501/1000000  loss         0.171742  avg_L1_norm_grad         0.000066  w[0]    0.002 bias    3.335\n",
      "iter 9600/1000000  loss         0.171685  avg_L1_norm_grad         0.000065  w[0]    0.002 bias    3.340\n",
      "iter 9601/1000000  loss         0.171684  avg_L1_norm_grad         0.000065  w[0]    0.002 bias    3.340\n",
      "iter 9700/1000000  loss         0.171628  avg_L1_norm_grad         0.000064  w[0]    0.003 bias    3.344\n",
      "iter 9701/1000000  loss         0.171628  avg_L1_norm_grad         0.000064  w[0]    0.003 bias    3.344\n",
      "iter 9800/1000000  loss         0.171573  avg_L1_norm_grad         0.000064  w[0]    0.003 bias    3.349\n",
      "iter 9801/1000000  loss         0.171573  avg_L1_norm_grad         0.000064  w[0]    0.003 bias    3.349\n",
      "iter 9900/1000000  loss         0.171520  avg_L1_norm_grad         0.000063  w[0]    0.003 bias    3.353\n",
      "iter 9901/1000000  loss         0.171519  avg_L1_norm_grad         0.000063  w[0]    0.003 bias    3.354\n",
      "iter 10000/1000000  loss         0.171467  avg_L1_norm_grad         0.000062  w[0]    0.004 bias    3.358\n",
      "iter 10001/1000000  loss         0.171467  avg_L1_norm_grad         0.000062  w[0]    0.004 bias    3.358\n",
      "iter 10100/1000000  loss         0.171416  avg_L1_norm_grad         0.000061  w[0]    0.004 bias    3.362\n",
      "iter 10101/1000000  loss         0.171416  avg_L1_norm_grad         0.000061  w[0]    0.004 bias    3.362\n",
      "iter 10200/1000000  loss         0.171366  avg_L1_norm_grad         0.000061  w[0]    0.004 bias    3.367\n",
      "iter 10201/1000000  loss         0.171366  avg_L1_norm_grad         0.000061  w[0]    0.004 bias    3.367\n",
      "iter 10300/1000000  loss         0.171318  avg_L1_norm_grad         0.000060  w[0]    0.004 bias    3.371\n",
      "iter 10301/1000000  loss         0.171317  avg_L1_norm_grad         0.000060  w[0]    0.004 bias    3.371\n",
      "iter 10400/1000000  loss         0.171271  avg_L1_norm_grad         0.000059  w[0]    0.005 bias    3.375\n",
      "iter 10401/1000000  loss         0.171270  avg_L1_norm_grad         0.000059  w[0]    0.005 bias    3.375\n",
      "iter 10500/1000000  loss         0.171224  avg_L1_norm_grad         0.000058  w[0]    0.005 bias    3.379\n",
      "iter 10501/1000000  loss         0.171224  avg_L1_norm_grad         0.000058  w[0]    0.005 bias    3.379\n",
      "iter 10600/1000000  loss         0.171179  avg_L1_norm_grad         0.000058  w[0]    0.005 bias    3.383\n",
      "iter 10601/1000000  loss         0.171179  avg_L1_norm_grad         0.000058  w[0]    0.005 bias    3.383\n",
      "iter 10700/1000000  loss         0.171136  avg_L1_norm_grad         0.000057  w[0]    0.005 bias    3.387\n",
      "iter 10701/1000000  loss         0.171135  avg_L1_norm_grad         0.000057  w[0]    0.005 bias    3.387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.171093  avg_L1_norm_grad         0.000056  w[0]    0.006 bias    3.391\n",
      "iter 10801/1000000  loss         0.171092  avg_L1_norm_grad         0.000056  w[0]    0.006 bias    3.391\n",
      "iter 10900/1000000  loss         0.171051  avg_L1_norm_grad         0.000056  w[0]    0.006 bias    3.395\n",
      "iter 10901/1000000  loss         0.171051  avg_L1_norm_grad         0.000056  w[0]    0.006 bias    3.395\n",
      "iter 11000/1000000  loss         0.171010  avg_L1_norm_grad         0.000055  w[0]    0.006 bias    3.399\n",
      "iter 11001/1000000  loss         0.171010  avg_L1_norm_grad         0.000055  w[0]    0.006 bias    3.399\n",
      "iter 11100/1000000  loss         0.170970  avg_L1_norm_grad         0.000054  w[0]    0.006 bias    3.402\n",
      "iter 11101/1000000  loss         0.170970  avg_L1_norm_grad         0.000054  w[0]    0.006 bias    3.402\n",
      "iter 11200/1000000  loss         0.170932  avg_L1_norm_grad         0.000054  w[0]    0.007 bias    3.406\n",
      "iter 11201/1000000  loss         0.170931  avg_L1_norm_grad         0.000054  w[0]    0.007 bias    3.406\n",
      "iter 11300/1000000  loss         0.170894  avg_L1_norm_grad         0.000053  w[0]    0.007 bias    3.410\n",
      "iter 11301/1000000  loss         0.170893  avg_L1_norm_grad         0.000053  w[0]    0.007 bias    3.410\n",
      "iter 11400/1000000  loss         0.170857  avg_L1_norm_grad         0.000052  w[0]    0.007 bias    3.413\n",
      "iter 11401/1000000  loss         0.170856  avg_L1_norm_grad         0.000052  w[0]    0.007 bias    3.413\n",
      "iter 11500/1000000  loss         0.170820  avg_L1_norm_grad         0.000052  w[0]    0.007 bias    3.417\n",
      "iter 11501/1000000  loss         0.170820  avg_L1_norm_grad         0.000052  w[0]    0.007 bias    3.417\n",
      "iter 11600/1000000  loss         0.170785  avg_L1_norm_grad         0.000051  w[0]    0.008 bias    3.420\n",
      "iter 11601/1000000  loss         0.170785  avg_L1_norm_grad         0.000051  w[0]    0.008 bias    3.420\n",
      "iter 11700/1000000  loss         0.170751  avg_L1_norm_grad         0.000050  w[0]    0.008 bias    3.424\n",
      "iter 11701/1000000  loss         0.170750  avg_L1_norm_grad         0.000050  w[0]    0.008 bias    3.424\n",
      "iter 11800/1000000  loss         0.170717  avg_L1_norm_grad         0.000050  w[0]    0.008 bias    3.427\n",
      "iter 11801/1000000  loss         0.170717  avg_L1_norm_grad         0.000050  w[0]    0.008 bias    3.427\n",
      "iter 11900/1000000  loss         0.170684  avg_L1_norm_grad         0.000049  w[0]    0.008 bias    3.430\n",
      "iter 11901/1000000  loss         0.170684  avg_L1_norm_grad         0.000049  w[0]    0.008 bias    3.430\n",
      "iter 12000/1000000  loss         0.170652  avg_L1_norm_grad         0.000049  w[0]    0.009 bias    3.434\n",
      "iter 12001/1000000  loss         0.170652  avg_L1_norm_grad         0.000049  w[0]    0.009 bias    3.434\n",
      "iter 12100/1000000  loss         0.170621  avg_L1_norm_grad         0.000048  w[0]    0.009 bias    3.437\n",
      "iter 12101/1000000  loss         0.170620  avg_L1_norm_grad         0.000048  w[0]    0.009 bias    3.437\n",
      "iter 12200/1000000  loss         0.170590  avg_L1_norm_grad         0.000048  w[0]    0.009 bias    3.440\n",
      "iter 12201/1000000  loss         0.170590  avg_L1_norm_grad         0.000048  w[0]    0.009 bias    3.440\n",
      "iter 12300/1000000  loss         0.170560  avg_L1_norm_grad         0.000047  w[0]    0.009 bias    3.443\n",
      "iter 12301/1000000  loss         0.170560  avg_L1_norm_grad         0.000047  w[0]    0.009 bias    3.443\n",
      "iter 12400/1000000  loss         0.170531  avg_L1_norm_grad         0.000047  w[0]    0.009 bias    3.446\n",
      "iter 12401/1000000  loss         0.170530  avg_L1_norm_grad         0.000047  w[0]    0.009 bias    3.446\n",
      "iter 12500/1000000  loss         0.170502  avg_L1_norm_grad         0.000046  w[0]    0.010 bias    3.449\n",
      "iter 12501/1000000  loss         0.170502  avg_L1_norm_grad         0.000046  w[0]    0.010 bias    3.449\n",
      "iter 12600/1000000  loss         0.170474  avg_L1_norm_grad         0.000045  w[0]    0.010 bias    3.452\n",
      "iter 12601/1000000  loss         0.170474  avg_L1_norm_grad         0.000045  w[0]    0.010 bias    3.452\n",
      "iter 12700/1000000  loss         0.170447  avg_L1_norm_grad         0.000045  w[0]    0.010 bias    3.455\n",
      "iter 12701/1000000  loss         0.170447  avg_L1_norm_grad         0.000045  w[0]    0.010 bias    3.455\n",
      "iter 12800/1000000  loss         0.170420  avg_L1_norm_grad         0.000044  w[0]    0.010 bias    3.458\n",
      "iter 12801/1000000  loss         0.170420  avg_L1_norm_grad         0.000044  w[0]    0.010 bias    3.458\n",
      "iter 12900/1000000  loss         0.170394  avg_L1_norm_grad         0.000044  w[0]    0.010 bias    3.461\n",
      "iter 12901/1000000  loss         0.170394  avg_L1_norm_grad         0.000044  w[0]    0.010 bias    3.461\n",
      "iter 13000/1000000  loss         0.170369  avg_L1_norm_grad         0.000043  w[0]    0.011 bias    3.464\n",
      "iter 13001/1000000  loss         0.170368  avg_L1_norm_grad         0.000043  w[0]    0.011 bias    3.464\n",
      "iter 13100/1000000  loss         0.170344  avg_L1_norm_grad         0.000043  w[0]    0.011 bias    3.467\n",
      "iter 13101/1000000  loss         0.170343  avg_L1_norm_grad         0.000043  w[0]    0.011 bias    3.467\n",
      "iter 13200/1000000  loss         0.170319  avg_L1_norm_grad         0.000043  w[0]    0.011 bias    3.470\n",
      "iter 13201/1000000  loss         0.170319  avg_L1_norm_grad         0.000043  w[0]    0.011 bias    3.470\n",
      "iter 13300/1000000  loss         0.170295  avg_L1_norm_grad         0.000042  w[0]    0.011 bias    3.472\n",
      "iter 13301/1000000  loss         0.170295  avg_L1_norm_grad         0.000042  w[0]    0.011 bias    3.472\n",
      "iter 13400/1000000  loss         0.170272  avg_L1_norm_grad         0.000042  w[0]    0.011 bias    3.475\n",
      "iter 13401/1000000  loss         0.170272  avg_L1_norm_grad         0.000042  w[0]    0.011 bias    3.475\n",
      "iter 13500/1000000  loss         0.170249  avg_L1_norm_grad         0.000041  w[0]    0.012 bias    3.478\n",
      "iter 13501/1000000  loss         0.170249  avg_L1_norm_grad         0.000041  w[0]    0.012 bias    3.478\n",
      "iter 13600/1000000  loss         0.170227  avg_L1_norm_grad         0.000041  w[0]    0.012 bias    3.480\n",
      "iter 13601/1000000  loss         0.170227  avg_L1_norm_grad         0.000041  w[0]    0.012 bias    3.480\n",
      "iter 13700/1000000  loss         0.170205  avg_L1_norm_grad         0.000040  w[0]    0.012 bias    3.483\n",
      "iter 13701/1000000  loss         0.170205  avg_L1_norm_grad         0.000040  w[0]    0.012 bias    3.483\n",
      "iter 13800/1000000  loss         0.170184  avg_L1_norm_grad         0.000040  w[0]    0.012 bias    3.486\n",
      "iter 13801/1000000  loss         0.170184  avg_L1_norm_grad         0.000040  w[0]    0.012 bias    3.486\n",
      "iter 13900/1000000  loss         0.170163  avg_L1_norm_grad         0.000039  w[0]    0.012 bias    3.488\n",
      "iter 13901/1000000  loss         0.170163  avg_L1_norm_grad         0.000039  w[0]    0.012 bias    3.488\n",
      "iter 14000/1000000  loss         0.170143  avg_L1_norm_grad         0.000039  w[0]    0.012 bias    3.491\n",
      "iter 14001/1000000  loss         0.170142  avg_L1_norm_grad         0.000039  w[0]    0.012 bias    3.491\n",
      "iter 14100/1000000  loss         0.170123  avg_L1_norm_grad         0.000039  w[0]    0.013 bias    3.493\n",
      "iter 14101/1000000  loss         0.170122  avg_L1_norm_grad         0.000039  w[0]    0.013 bias    3.493\n",
      "iter 14200/1000000  loss         0.170103  avg_L1_norm_grad         0.000038  w[0]    0.013 bias    3.496\n",
      "iter 14201/1000000  loss         0.170103  avg_L1_norm_grad         0.000038  w[0]    0.013 bias    3.496\n",
      "iter 14300/1000000  loss         0.170084  avg_L1_norm_grad         0.000038  w[0]    0.013 bias    3.498\n",
      "iter 14301/1000000  loss         0.170084  avg_L1_norm_grad         0.000038  w[0]    0.013 bias    3.498\n",
      "iter 14400/1000000  loss         0.170065  avg_L1_norm_grad         0.000037  w[0]    0.013 bias    3.500\n",
      "iter 14401/1000000  loss         0.170065  avg_L1_norm_grad         0.000037  w[0]    0.013 bias    3.500\n",
      "iter 14500/1000000  loss         0.170047  avg_L1_norm_grad         0.000037  w[0]    0.013 bias    3.503\n",
      "iter 14501/1000000  loss         0.170047  avg_L1_norm_grad         0.000037  w[0]    0.013 bias    3.503\n",
      "iter 14600/1000000  loss         0.170029  avg_L1_norm_grad         0.000036  w[0]    0.013 bias    3.505\n",
      "iter 14601/1000000  loss         0.170029  avg_L1_norm_grad         0.000036  w[0]    0.013 bias    3.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.170011  avg_L1_norm_grad         0.000036  w[0]    0.013 bias    3.507\n",
      "iter 14701/1000000  loss         0.170011  avg_L1_norm_grad         0.000036  w[0]    0.013 bias    3.507\n",
      "iter 14800/1000000  loss         0.169994  avg_L1_norm_grad         0.000036  w[0]    0.014 bias    3.509\n",
      "iter 14801/1000000  loss         0.169994  avg_L1_norm_grad         0.000036  w[0]    0.014 bias    3.509\n",
      "iter 14900/1000000  loss         0.169977  avg_L1_norm_grad         0.000035  w[0]    0.014 bias    3.512\n",
      "iter 14901/1000000  loss         0.169977  avg_L1_norm_grad         0.000035  w[0]    0.014 bias    3.512\n",
      "iter 15000/1000000  loss         0.169961  avg_L1_norm_grad         0.000035  w[0]    0.014 bias    3.514\n",
      "iter 15001/1000000  loss         0.169961  avg_L1_norm_grad         0.000035  w[0]    0.014 bias    3.514\n",
      "iter 15100/1000000  loss         0.169945  avg_L1_norm_grad         0.000035  w[0]    0.014 bias    3.516\n",
      "iter 15101/1000000  loss         0.169945  avg_L1_norm_grad         0.000035  w[0]    0.014 bias    3.516\n",
      "iter 15200/1000000  loss         0.169929  avg_L1_norm_grad         0.000034  w[0]    0.014 bias    3.518\n",
      "iter 15201/1000000  loss         0.169929  avg_L1_norm_grad         0.000034  w[0]    0.014 bias    3.518\n",
      "iter 15300/1000000  loss         0.169914  avg_L1_norm_grad         0.000034  w[0]    0.014 bias    3.520\n",
      "iter 15301/1000000  loss         0.169914  avg_L1_norm_grad         0.000034  w[0]    0.014 bias    3.520\n",
      "iter 15400/1000000  loss         0.169899  avg_L1_norm_grad         0.000034  w[0]    0.014 bias    3.522\n",
      "iter 15401/1000000  loss         0.169898  avg_L1_norm_grad         0.000034  w[0]    0.014 bias    3.522\n",
      "iter 15500/1000000  loss         0.169884  avg_L1_norm_grad         0.000033  w[0]    0.015 bias    3.524\n",
      "iter 15501/1000000  loss         0.169884  avg_L1_norm_grad         0.000033  w[0]    0.015 bias    3.524\n",
      "iter 15600/1000000  loss         0.169869  avg_L1_norm_grad         0.000033  w[0]    0.015 bias    3.527\n",
      "iter 15601/1000000  loss         0.169869  avg_L1_norm_grad         0.000033  w[0]    0.015 bias    3.527\n",
      "iter 15700/1000000  loss         0.169855  avg_L1_norm_grad         0.000032  w[0]    0.015 bias    3.529\n",
      "iter 15701/1000000  loss         0.169855  avg_L1_norm_grad         0.000032  w[0]    0.015 bias    3.529\n",
      "iter 15800/1000000  loss         0.169841  avg_L1_norm_grad         0.000032  w[0]    0.015 bias    3.531\n",
      "iter 15801/1000000  loss         0.169841  avg_L1_norm_grad         0.000032  w[0]    0.015 bias    3.531\n",
      "iter 15900/1000000  loss         0.169828  avg_L1_norm_grad         0.000032  w[0]    0.015 bias    3.532\n",
      "iter 15901/1000000  loss         0.169828  avg_L1_norm_grad         0.000032  w[0]    0.015 bias    3.532\n",
      "iter 16000/1000000  loss         0.169814  avg_L1_norm_grad         0.000031  w[0]    0.015 bias    3.534\n",
      "iter 16001/1000000  loss         0.169814  avg_L1_norm_grad         0.000031  w[0]    0.015 bias    3.534\n",
      "iter 16100/1000000  loss         0.169801  avg_L1_norm_grad         0.000031  w[0]    0.015 bias    3.536\n",
      "iter 16101/1000000  loss         0.169801  avg_L1_norm_grad         0.000031  w[0]    0.015 bias    3.536\n",
      "iter 16200/1000000  loss         0.169789  avg_L1_norm_grad         0.000031  w[0]    0.015 bias    3.538\n",
      "iter 16201/1000000  loss         0.169788  avg_L1_norm_grad         0.000031  w[0]    0.015 bias    3.538\n",
      "iter 16300/1000000  loss         0.169776  avg_L1_norm_grad         0.000031  w[0]    0.016 bias    3.540\n",
      "iter 16301/1000000  loss         0.169776  avg_L1_norm_grad         0.000031  w[0]    0.016 bias    3.540\n",
      "iter 16400/1000000  loss         0.169764  avg_L1_norm_grad         0.000030  w[0]    0.016 bias    3.542\n",
      "iter 16401/1000000  loss         0.169764  avg_L1_norm_grad         0.000030  w[0]    0.016 bias    3.542\n",
      "iter 16500/1000000  loss         0.169752  avg_L1_norm_grad         0.000030  w[0]    0.016 bias    3.544\n",
      "iter 16501/1000000  loss         0.169752  avg_L1_norm_grad         0.000030  w[0]    0.016 bias    3.544\n",
      "iter 16600/1000000  loss         0.169740  avg_L1_norm_grad         0.000030  w[0]    0.016 bias    3.546\n",
      "iter 16601/1000000  loss         0.169740  avg_L1_norm_grad         0.000030  w[0]    0.016 bias    3.546\n",
      "iter 16700/1000000  loss         0.169729  avg_L1_norm_grad         0.000029  w[0]    0.016 bias    3.547\n",
      "iter 16701/1000000  loss         0.169728  avg_L1_norm_grad         0.000029  w[0]    0.016 bias    3.547\n",
      "iter 16800/1000000  loss         0.169717  avg_L1_norm_grad         0.000029  w[0]    0.016 bias    3.549\n",
      "iter 16801/1000000  loss         0.169717  avg_L1_norm_grad         0.000029  w[0]    0.016 bias    3.549\n",
      "iter 16900/1000000  loss         0.169706  avg_L1_norm_grad         0.000029  w[0]    0.016 bias    3.551\n",
      "iter 16901/1000000  loss         0.169706  avg_L1_norm_grad         0.000029  w[0]    0.016 bias    3.551\n",
      "iter 17000/1000000  loss         0.169695  avg_L1_norm_grad         0.000028  w[0]    0.016 bias    3.553\n",
      "iter 17001/1000000  loss         0.169695  avg_L1_norm_grad         0.000028  w[0]    0.016 bias    3.553\n",
      "iter 17100/1000000  loss         0.169685  avg_L1_norm_grad         0.000028  w[0]    0.016 bias    3.554\n",
      "iter 17101/1000000  loss         0.169685  avg_L1_norm_grad         0.000028  w[0]    0.016 bias    3.554\n",
      "iter 17200/1000000  loss         0.169674  avg_L1_norm_grad         0.000028  w[0]    0.016 bias    3.556\n",
      "iter 17201/1000000  loss         0.169674  avg_L1_norm_grad         0.000028  w[0]    0.016 bias    3.556\n",
      "iter 17300/1000000  loss         0.169664  avg_L1_norm_grad         0.000028  w[0]    0.017 bias    3.558\n",
      "iter 17301/1000000  loss         0.169664  avg_L1_norm_grad         0.000028  w[0]    0.017 bias    3.558\n",
      "iter 17400/1000000  loss         0.169654  avg_L1_norm_grad         0.000027  w[0]    0.017 bias    3.559\n",
      "iter 17401/1000000  loss         0.169654  avg_L1_norm_grad         0.000027  w[0]    0.017 bias    3.559\n",
      "iter 17500/1000000  loss         0.169644  avg_L1_norm_grad         0.000027  w[0]    0.017 bias    3.561\n",
      "iter 17501/1000000  loss         0.169644  avg_L1_norm_grad         0.000027  w[0]    0.017 bias    3.561\n",
      "iter 17600/1000000  loss         0.169635  avg_L1_norm_grad         0.000027  w[0]    0.017 bias    3.563\n",
      "iter 17601/1000000  loss         0.169635  avg_L1_norm_grad         0.000027  w[0]    0.017 bias    3.563\n",
      "iter 17700/1000000  loss         0.169625  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.564\n",
      "iter 17701/1000000  loss         0.169625  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.564\n",
      "iter 17800/1000000  loss         0.169616  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.566\n",
      "iter 17801/1000000  loss         0.169616  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.566\n",
      "iter 17900/1000000  loss         0.169607  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.567\n",
      "iter 17901/1000000  loss         0.169607  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.567\n",
      "iter 18000/1000000  loss         0.169598  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.569\n",
      "iter 18001/1000000  loss         0.169598  avg_L1_norm_grad         0.000026  w[0]    0.017 bias    3.569\n",
      "iter 18100/1000000  loss         0.169590  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.570\n",
      "iter 18101/1000000  loss         0.169590  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.570\n",
      "iter 18200/1000000  loss         0.169581  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.572\n",
      "iter 18201/1000000  loss         0.169581  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.572\n",
      "iter 18300/1000000  loss         0.169573  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.573\n",
      "iter 18301/1000000  loss         0.169573  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.573\n",
      "iter 18400/1000000  loss         0.169565  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.575\n",
      "iter 18401/1000000  loss         0.169565  avg_L1_norm_grad         0.000025  w[0]    0.017 bias    3.575\n",
      "iter 18500/1000000  loss         0.169557  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.576\n",
      "iter 18501/1000000  loss         0.169557  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18600/1000000  loss         0.169549  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.578\n",
      "iter 18601/1000000  loss         0.169549  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.578\n",
      "iter 18700/1000000  loss         0.169541  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.579\n",
      "iter 18701/1000000  loss         0.169541  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.579\n",
      "iter 18800/1000000  loss         0.169534  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.580\n",
      "iter 18801/1000000  loss         0.169534  avg_L1_norm_grad         0.000024  w[0]    0.018 bias    3.580\n",
      "iter 18900/1000000  loss         0.169526  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.582\n",
      "iter 18901/1000000  loss         0.169526  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.582\n",
      "iter 19000/1000000  loss         0.169519  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.583\n",
      "iter 19001/1000000  loss         0.169519  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.583\n",
      "iter 19100/1000000  loss         0.169512  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.585\n",
      "iter 19101/1000000  loss         0.169512  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.585\n",
      "iter 19200/1000000  loss         0.169505  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.586\n",
      "iter 19201/1000000  loss         0.169505  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.586\n",
      "iter 19300/1000000  loss         0.169498  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.587\n",
      "iter 19301/1000000  loss         0.169498  avg_L1_norm_grad         0.000023  w[0]    0.018 bias    3.587\n",
      "iter 19400/1000000  loss         0.169491  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.589\n",
      "iter 19401/1000000  loss         0.169491  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.589\n",
      "iter 19500/1000000  loss         0.169485  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.590\n",
      "iter 19501/1000000  loss         0.169485  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.590\n",
      "iter 19600/1000000  loss         0.169478  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.591\n",
      "iter 19601/1000000  loss         0.169478  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.591\n",
      "iter 19700/1000000  loss         0.169472  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.592\n",
      "iter 19701/1000000  loss         0.169472  avg_L1_norm_grad         0.000022  w[0]    0.018 bias    3.592\n",
      "iter 19800/1000000  loss         0.169466  avg_L1_norm_grad         0.000021  w[0]    0.018 bias    3.594\n",
      "iter 19801/1000000  loss         0.169466  avg_L1_norm_grad         0.000021  w[0]    0.018 bias    3.594\n",
      "iter 19900/1000000  loss         0.169460  avg_L1_norm_grad         0.000021  w[0]    0.018 bias    3.595\n",
      "iter 19901/1000000  loss         0.169460  avg_L1_norm_grad         0.000021  w[0]    0.018 bias    3.595\n",
      "iter 20000/1000000  loss         0.169454  avg_L1_norm_grad         0.000021  w[0]    0.019 bias    3.596\n",
      "iter 20001/1000000  loss         0.169454  avg_L1_norm_grad         0.000021  w[0]    0.019 bias    3.596\n",
      "iter 20100/1000000  loss         0.169448  avg_L1_norm_grad         0.000021  w[0]    0.019 bias    3.597\n",
      "iter 20101/1000000  loss         0.169448  avg_L1_norm_grad         0.000021  w[0]    0.019 bias    3.597\n",
      "iter 20200/1000000  loss         0.169442  avg_L1_norm_grad         0.000021  w[0]    0.019 bias    3.598\n",
      "iter 20201/1000000  loss         0.169442  avg_L1_norm_grad         0.000021  w[0]    0.019 bias    3.599\n",
      "iter 20300/1000000  loss         0.169437  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.600\n",
      "iter 20301/1000000  loss         0.169436  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.600\n",
      "iter 20400/1000000  loss         0.169431  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.601\n",
      "iter 20401/1000000  loss         0.169431  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.601\n",
      "iter 20500/1000000  loss         0.169426  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.602\n",
      "iter 20501/1000000  loss         0.169426  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.602\n",
      "iter 20600/1000000  loss         0.169420  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.603\n",
      "iter 20601/1000000  loss         0.169420  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.603\n",
      "iter 20700/1000000  loss         0.169415  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.604\n",
      "iter 20701/1000000  loss         0.169415  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.604\n",
      "iter 20800/1000000  loss         0.169410  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.605\n",
      "iter 20801/1000000  loss         0.169410  avg_L1_norm_grad         0.000020  w[0]    0.019 bias    3.605\n",
      "iter 20900/1000000  loss         0.169405  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.607\n",
      "iter 20901/1000000  loss         0.169405  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.607\n",
      "iter 21000/1000000  loss         0.169400  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.608\n",
      "iter 21001/1000000  loss         0.169400  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.608\n",
      "iter 21100/1000000  loss         0.169395  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.609\n",
      "iter 21101/1000000  loss         0.169395  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.609\n",
      "iter 21200/1000000  loss         0.169390  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.610\n",
      "iter 21201/1000000  loss         0.169390  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.610\n",
      "iter 21300/1000000  loss         0.169386  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.611\n",
      "iter 21301/1000000  loss         0.169386  avg_L1_norm_grad         0.000019  w[0]    0.019 bias    3.611\n",
      "iter 21400/1000000  loss         0.169381  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.612\n",
      "iter 21401/1000000  loss         0.169381  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.612\n",
      "iter 21500/1000000  loss         0.169377  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.613\n",
      "iter 21501/1000000  loss         0.169377  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.613\n",
      "iter 21600/1000000  loss         0.169372  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.614\n",
      "iter 21601/1000000  loss         0.169372  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.614\n",
      "iter 21700/1000000  loss         0.169368  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.615\n",
      "iter 21701/1000000  loss         0.169368  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.615\n",
      "iter 21800/1000000  loss         0.169364  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.616\n",
      "iter 21801/1000000  loss         0.169364  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.616\n",
      "iter 21900/1000000  loss         0.169360  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.617\n",
      "iter 21901/1000000  loss         0.169360  avg_L1_norm_grad         0.000018  w[0]    0.019 bias    3.617\n",
      "iter 22000/1000000  loss         0.169356  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.618\n",
      "iter 22001/1000000  loss         0.169356  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.618\n",
      "iter 22100/1000000  loss         0.169352  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.619\n",
      "iter 22101/1000000  loss         0.169352  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.619\n",
      "iter 22200/1000000  loss         0.169348  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.620\n",
      "iter 22201/1000000  loss         0.169348  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.620\n",
      "iter 22300/1000000  loss         0.169344  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.621\n",
      "iter 22301/1000000  loss         0.169344  avg_L1_norm_grad         0.000017  w[0]    0.020 bias    3.621\n",
      "Done. Converged after 22367 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 1572 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030389  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.950920  avg_L1_norm_grad         0.054581  w[0]   -0.000 bias   -0.000\n",
      "iter    2/1000000  loss         1.219484  avg_L1_norm_grad         0.084968  w[0]    0.001 bias    0.035\n",
      "iter    3/1000000  loss         1.843166  avg_L1_norm_grad         0.098623  w[0]   -0.001 bias   -0.004\n",
      "iter    4/1000000  loss         0.960468  avg_L1_norm_grad         0.068063  w[0]    0.001 bias    0.057\n",
      "iter    5/1000000  loss         1.257835  avg_L1_norm_grad         0.076600  w[0]    0.000 bias    0.027\n",
      "iter    6/1000000  loss         0.867165  avg_L1_norm_grad         0.060886  w[0]    0.001 bias    0.077\n",
      "iter    7/1000000  loss         1.028030  avg_L1_norm_grad         0.063432  w[0]    0.001 bias    0.051\n",
      "iter    8/1000000  loss         0.755940  avg_L1_norm_grad         0.050512  w[0]    0.002 bias    0.093\n",
      "iter    9/1000000  loss         0.839559  avg_L1_norm_grad         0.051693  w[0]    0.001 bias    0.072\n",
      "iter   10/1000000  loss         0.682473  avg_L1_norm_grad         0.044120  w[0]    0.002 bias    0.108\n",
      "iter   11/1000000  loss         0.735273  avg_L1_norm_grad         0.044822  w[0]    0.001 bias    0.090\n",
      "iter   12/1000000  loss         0.627560  avg_L1_norm_grad         0.039546  w[0]    0.002 bias    0.121\n",
      "iter   13/1000000  loss         0.664182  avg_L1_norm_grad         0.039986  w[0]    0.001 bias    0.106\n",
      "iter   14/1000000  loss         0.583591  avg_L1_norm_grad         0.035793  w[0]    0.002 bias    0.134\n",
      "iter   15/1000000  loss         0.609257  avg_L1_norm_grad         0.035987  w[0]    0.002 bias    0.121\n",
      "iter   16/1000000  loss         0.546562  avg_L1_norm_grad         0.032318  w[0]    0.002 bias    0.146\n",
      "iter   17/1000000  loss         0.563474  avg_L1_norm_grad         0.032254  w[0]    0.002 bias    0.134\n",
      "iter   18/1000000  loss         0.514412  avg_L1_norm_grad         0.028868  w[0]    0.002 bias    0.158\n",
      "iter   19/1000000  loss         0.524007  avg_L1_norm_grad         0.028542  w[0]    0.002 bias    0.147\n",
      "iter  100/1000000  loss         0.280451  avg_L1_norm_grad         0.001237  w[0]    0.001 bias    0.394\n",
      "iter  101/1000000  loss         0.279743  avg_L1_norm_grad         0.001229  w[0]    0.001 bias    0.395\n",
      "iter  200/1000000  loss         0.238013  avg_L1_norm_grad         0.000789  w[0]   -0.002 bias    0.534\n",
      "iter  201/1000000  loss         0.237755  avg_L1_norm_grad         0.000787  w[0]   -0.002 bias    0.535\n",
      "iter  300/1000000  loss         0.218954  avg_L1_norm_grad         0.000611  w[0]   -0.005 bias    0.622\n",
      "iter  301/1000000  loss         0.218813  avg_L1_norm_grad         0.000610  w[0]   -0.005 bias    0.623\n",
      "iter  400/1000000  loss         0.207515  avg_L1_norm_grad         0.000509  w[0]   -0.008 bias    0.685\n",
      "iter  401/1000000  loss         0.207422  avg_L1_norm_grad         0.000508  w[0]   -0.008 bias    0.686\n",
      "iter  500/1000000  loss         0.199651  avg_L1_norm_grad         0.000440  w[0]   -0.010 bias    0.734\n",
      "iter  501/1000000  loss         0.199584  avg_L1_norm_grad         0.000439  w[0]   -0.010 bias    0.735\n",
      "iter  600/1000000  loss         0.193805  avg_L1_norm_grad         0.000390  w[0]   -0.012 bias    0.774\n",
      "iter  601/1000000  loss         0.193754  avg_L1_norm_grad         0.000390  w[0]   -0.012 bias    0.774\n",
      "iter  700/1000000  loss         0.189236  avg_L1_norm_grad         0.000352  w[0]   -0.013 bias    0.807\n",
      "iter  701/1000000  loss         0.189195  avg_L1_norm_grad         0.000352  w[0]   -0.013 bias    0.807\n",
      "iter  800/1000000  loss         0.185537  avg_L1_norm_grad         0.000322  w[0]   -0.014 bias    0.835\n",
      "iter  801/1000000  loss         0.185503  avg_L1_norm_grad         0.000321  w[0]   -0.014 bias    0.835\n",
      "iter  900/1000000  loss         0.182464  avg_L1_norm_grad         0.000297  w[0]   -0.015 bias    0.859\n",
      "iter  901/1000000  loss         0.182436  avg_L1_norm_grad         0.000296  w[0]   -0.015 bias    0.859\n",
      "iter 1000/1000000  loss         0.179861  avg_L1_norm_grad         0.000275  w[0]   -0.016 bias    0.880\n",
      "iter 1001/1000000  loss         0.179837  avg_L1_norm_grad         0.000275  w[0]   -0.016 bias    0.880\n",
      "iter 1100/1000000  loss         0.177620  avg_L1_norm_grad         0.000257  w[0]   -0.016 bias    0.898\n",
      "iter 1101/1000000  loss         0.177599  avg_L1_norm_grad         0.000257  w[0]   -0.016 bias    0.898\n",
      "iter 1200/1000000  loss         0.175666  avg_L1_norm_grad         0.000242  w[0]   -0.017 bias    0.915\n",
      "iter 1201/1000000  loss         0.175648  avg_L1_norm_grad         0.000242  w[0]   -0.017 bias    0.915\n",
      "iter 1300/1000000  loss         0.173944  avg_L1_norm_grad         0.000228  w[0]   -0.017 bias    0.929\n",
      "iter 1301/1000000  loss         0.173928  avg_L1_norm_grad         0.000228  w[0]   -0.017 bias    0.929\n",
      "iter 1400/1000000  loss         0.172413  avg_L1_norm_grad         0.000216  w[0]   -0.017 bias    0.943\n",
      "iter 1401/1000000  loss         0.172398  avg_L1_norm_grad         0.000216  w[0]   -0.017 bias    0.943\n",
      "iter 1500/1000000  loss         0.171040  avg_L1_norm_grad         0.000205  w[0]   -0.017 bias    0.955\n",
      "iter 1501/1000000  loss         0.171027  avg_L1_norm_grad         0.000205  w[0]   -0.017 bias    0.955\n",
      "iter 1600/1000000  loss         0.169803  avg_L1_norm_grad         0.000196  w[0]   -0.017 bias    0.966\n",
      "iter 1601/1000000  loss         0.169791  avg_L1_norm_grad         0.000196  w[0]   -0.017 bias    0.966\n",
      "iter 1700/1000000  loss         0.168679  avg_L1_norm_grad         0.000187  w[0]   -0.017 bias    0.976\n",
      "iter 1701/1000000  loss         0.168669  avg_L1_norm_grad         0.000187  w[0]   -0.017 bias    0.976\n",
      "iter 1800/1000000  loss         0.167655  avg_L1_norm_grad         0.000179  w[0]   -0.017 bias    0.985\n",
      "iter 1801/1000000  loss         0.167645  avg_L1_norm_grad         0.000179  w[0]   -0.017 bias    0.985\n",
      "iter 1900/1000000  loss         0.166716  avg_L1_norm_grad         0.000171  w[0]   -0.017 bias    0.993\n",
      "iter 1901/1000000  loss         0.166708  avg_L1_norm_grad         0.000171  w[0]   -0.017 bias    0.993\n",
      "iter 2000/1000000  loss         0.165853  avg_L1_norm_grad         0.000165  w[0]   -0.017 bias    1.001\n",
      "iter 2001/1000000  loss         0.165845  avg_L1_norm_grad         0.000164  w[0]   -0.017 bias    1.001\n",
      "iter 2100/1000000  loss         0.165056  avg_L1_norm_grad         0.000158  w[0]   -0.016 bias    1.008\n",
      "iter 2101/1000000  loss         0.165048  avg_L1_norm_grad         0.000158  w[0]   -0.016 bias    1.008\n",
      "iter 2200/1000000  loss         0.164318  avg_L1_norm_grad         0.000152  w[0]   -0.016 bias    1.015\n",
      "iter 2201/1000000  loss         0.164310  avg_L1_norm_grad         0.000152  w[0]   -0.016 bias    1.015\n",
      "iter 2300/1000000  loss         0.163632  avg_L1_norm_grad         0.000147  w[0]   -0.016 bias    1.021\n",
      "iter 2301/1000000  loss         0.163625  avg_L1_norm_grad         0.000147  w[0]   -0.016 bias    1.021\n",
      "iter 2400/1000000  loss         0.162992  avg_L1_norm_grad         0.000142  w[0]   -0.015 bias    1.027\n",
      "iter 2401/1000000  loss         0.162986  avg_L1_norm_grad         0.000142  w[0]   -0.015 bias    1.027\n",
      "iter 2500/1000000  loss         0.162396  avg_L1_norm_grad         0.000137  w[0]   -0.015 bias    1.032\n",
      "iter 2501/1000000  loss         0.162390  avg_L1_norm_grad         0.000137  w[0]   -0.015 bias    1.032\n",
      "iter 2600/1000000  loss         0.161837  avg_L1_norm_grad         0.000133  w[0]   -0.015 bias    1.037\n",
      "iter 2601/1000000  loss         0.161831  avg_L1_norm_grad         0.000133  w[0]   -0.015 bias    1.037\n",
      "iter 2700/1000000  loss         0.161313  avg_L1_norm_grad         0.000129  w[0]   -0.014 bias    1.042\n",
      "iter 2701/1000000  loss         0.161308  avg_L1_norm_grad         0.000129  w[0]   -0.014 bias    1.042\n",
      "iter 2800/1000000  loss         0.160820  avg_L1_norm_grad         0.000125  w[0]   -0.014 bias    1.046\n",
      "iter 2801/1000000  loss         0.160815  avg_L1_norm_grad         0.000125  w[0]   -0.014 bias    1.046\n",
      "iter 2900/1000000  loss         0.160357  avg_L1_norm_grad         0.000121  w[0]   -0.013 bias    1.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.160352  avg_L1_norm_grad         0.000121  w[0]   -0.013 bias    1.050\n",
      "iter 3000/1000000  loss         0.159919  avg_L1_norm_grad         0.000118  w[0]   -0.013 bias    1.054\n",
      "iter 3001/1000000  loss         0.159915  avg_L1_norm_grad         0.000118  w[0]   -0.013 bias    1.054\n",
      "iter 3100/1000000  loss         0.159506  avg_L1_norm_grad         0.000114  w[0]   -0.012 bias    1.057\n",
      "iter 3101/1000000  loss         0.159502  avg_L1_norm_grad         0.000114  w[0]   -0.012 bias    1.057\n",
      "iter 3200/1000000  loss         0.159115  avg_L1_norm_grad         0.000111  w[0]   -0.012 bias    1.061\n",
      "iter 3201/1000000  loss         0.159111  avg_L1_norm_grad         0.000111  w[0]   -0.012 bias    1.061\n",
      "iter 3300/1000000  loss         0.158745  avg_L1_norm_grad         0.000108  w[0]   -0.012 bias    1.064\n",
      "iter 3301/1000000  loss         0.158741  avg_L1_norm_grad         0.000108  w[0]   -0.012 bias    1.064\n",
      "iter 3400/1000000  loss         0.158394  avg_L1_norm_grad         0.000105  w[0]   -0.011 bias    1.067\n",
      "iter 3401/1000000  loss         0.158390  avg_L1_norm_grad         0.000105  w[0]   -0.011 bias    1.067\n",
      "iter 3500/1000000  loss         0.158060  avg_L1_norm_grad         0.000103  w[0]   -0.011 bias    1.070\n",
      "iter 3501/1000000  loss         0.158057  avg_L1_norm_grad         0.000103  w[0]   -0.011 bias    1.070\n",
      "iter 3600/1000000  loss         0.157743  avg_L1_norm_grad         0.000100  w[0]   -0.010 bias    1.072\n",
      "iter 3601/1000000  loss         0.157740  avg_L1_norm_grad         0.000100  w[0]   -0.010 bias    1.072\n",
      "iter 3700/1000000  loss         0.157441  avg_L1_norm_grad         0.000098  w[0]   -0.010 bias    1.075\n",
      "iter 3701/1000000  loss         0.157438  avg_L1_norm_grad         0.000098  w[0]   -0.010 bias    1.075\n",
      "iter 3800/1000000  loss         0.157154  avg_L1_norm_grad         0.000095  w[0]   -0.009 bias    1.077\n",
      "iter 3801/1000000  loss         0.157151  avg_L1_norm_grad         0.000095  w[0]   -0.009 bias    1.077\n",
      "iter 3900/1000000  loss         0.156880  avg_L1_norm_grad         0.000093  w[0]   -0.009 bias    1.079\n",
      "iter 3901/1000000  loss         0.156877  avg_L1_norm_grad         0.000093  w[0]   -0.009 bias    1.079\n",
      "iter 4000/1000000  loss         0.156618  avg_L1_norm_grad         0.000091  w[0]   -0.008 bias    1.081\n",
      "iter 4001/1000000  loss         0.156615  avg_L1_norm_grad         0.000091  w[0]   -0.008 bias    1.081\n",
      "iter 4100/1000000  loss         0.156368  avg_L1_norm_grad         0.000089  w[0]   -0.008 bias    1.083\n",
      "iter 4101/1000000  loss         0.156365  avg_L1_norm_grad         0.000089  w[0]   -0.008 bias    1.083\n",
      "iter 4200/1000000  loss         0.156128  avg_L1_norm_grad         0.000086  w[0]   -0.008 bias    1.085\n",
      "iter 4201/1000000  loss         0.156126  avg_L1_norm_grad         0.000086  w[0]   -0.008 bias    1.085\n",
      "iter 4300/1000000  loss         0.155899  avg_L1_norm_grad         0.000085  w[0]   -0.007 bias    1.087\n",
      "iter 4301/1000000  loss         0.155897  avg_L1_norm_grad         0.000084  w[0]   -0.007 bias    1.087\n",
      "iter 4400/1000000  loss         0.155680  avg_L1_norm_grad         0.000083  w[0]   -0.007 bias    1.089\n",
      "iter 4401/1000000  loss         0.155678  avg_L1_norm_grad         0.000083  w[0]   -0.007 bias    1.089\n",
      "iter 4500/1000000  loss         0.155470  avg_L1_norm_grad         0.000081  w[0]   -0.006 bias    1.090\n",
      "iter 4501/1000000  loss         0.155468  avg_L1_norm_grad         0.000081  w[0]   -0.006 bias    1.090\n",
      "iter 4600/1000000  loss         0.155268  avg_L1_norm_grad         0.000079  w[0]   -0.006 bias    1.092\n",
      "iter 4601/1000000  loss         0.155266  avg_L1_norm_grad         0.000079  w[0]   -0.006 bias    1.092\n",
      "iter 4700/1000000  loss         0.155075  avg_L1_norm_grad         0.000077  w[0]   -0.006 bias    1.093\n",
      "iter 4701/1000000  loss         0.155073  avg_L1_norm_grad         0.000077  w[0]   -0.006 bias    1.094\n",
      "iter 4800/1000000  loss         0.154889  avg_L1_norm_grad         0.000076  w[0]   -0.005 bias    1.095\n",
      "iter 4801/1000000  loss         0.154887  avg_L1_norm_grad         0.000076  w[0]   -0.005 bias    1.095\n",
      "iter 4900/1000000  loss         0.154710  avg_L1_norm_grad         0.000074  w[0]   -0.005 bias    1.096\n",
      "iter 4901/1000000  loss         0.154708  avg_L1_norm_grad         0.000074  w[0]   -0.005 bias    1.096\n",
      "iter 5000/1000000  loss         0.154538  avg_L1_norm_grad         0.000072  w[0]   -0.005 bias    1.098\n",
      "iter 5001/1000000  loss         0.154536  avg_L1_norm_grad         0.000072  w[0]   -0.005 bias    1.098\n",
      "iter 5100/1000000  loss         0.154373  avg_L1_norm_grad         0.000071  w[0]   -0.004 bias    1.099\n",
      "iter 5101/1000000  loss         0.154371  avg_L1_norm_grad         0.000071  w[0]   -0.004 bias    1.099\n",
      "iter 5200/1000000  loss         0.154213  avg_L1_norm_grad         0.000069  w[0]   -0.004 bias    1.100\n",
      "iter 5201/1000000  loss         0.154212  avg_L1_norm_grad         0.000069  w[0]   -0.004 bias    1.100\n",
      "iter 5300/1000000  loss         0.154060  avg_L1_norm_grad         0.000068  w[0]   -0.003 bias    1.101\n",
      "iter 5301/1000000  loss         0.154058  avg_L1_norm_grad         0.000068  w[0]   -0.003 bias    1.101\n",
      "iter 5400/1000000  loss         0.153912  avg_L1_norm_grad         0.000067  w[0]   -0.003 bias    1.102\n",
      "iter 5401/1000000  loss         0.153910  avg_L1_norm_grad         0.000067  w[0]   -0.003 bias    1.102\n",
      "iter 5500/1000000  loss         0.153769  avg_L1_norm_grad         0.000065  w[0]   -0.003 bias    1.103\n",
      "iter 5501/1000000  loss         0.153768  avg_L1_norm_grad         0.000065  w[0]   -0.003 bias    1.103\n",
      "iter 5600/1000000  loss         0.153631  avg_L1_norm_grad         0.000064  w[0]   -0.002 bias    1.104\n",
      "iter 5601/1000000  loss         0.153630  avg_L1_norm_grad         0.000064  w[0]   -0.002 bias    1.104\n",
      "iter 5700/1000000  loss         0.153498  avg_L1_norm_grad         0.000063  w[0]   -0.002 bias    1.105\n",
      "iter 5701/1000000  loss         0.153497  avg_L1_norm_grad         0.000063  w[0]   -0.002 bias    1.105\n",
      "iter 5800/1000000  loss         0.153370  avg_L1_norm_grad         0.000061  w[0]   -0.002 bias    1.106\n",
      "iter 5801/1000000  loss         0.153369  avg_L1_norm_grad         0.000061  w[0]   -0.002 bias    1.106\n",
      "iter 5900/1000000  loss         0.153246  avg_L1_norm_grad         0.000060  w[0]   -0.002 bias    1.107\n",
      "iter 5901/1000000  loss         0.153245  avg_L1_norm_grad         0.000060  w[0]   -0.002 bias    1.107\n",
      "iter 6000/1000000  loss         0.153126  avg_L1_norm_grad         0.000059  w[0]   -0.001 bias    1.108\n",
      "iter 6001/1000000  loss         0.153124  avg_L1_norm_grad         0.000059  w[0]   -0.001 bias    1.108\n",
      "iter 6100/1000000  loss         0.153009  avg_L1_norm_grad         0.000058  w[0]   -0.001 bias    1.109\n",
      "iter 6101/1000000  loss         0.153008  avg_L1_norm_grad         0.000058  w[0]   -0.001 bias    1.109\n",
      "iter 6200/1000000  loss         0.152897  avg_L1_norm_grad         0.000057  w[0]   -0.001 bias    1.110\n",
      "iter 6201/1000000  loss         0.152896  avg_L1_norm_grad         0.000057  w[0]   -0.001 bias    1.110\n",
      "iter 6300/1000000  loss         0.152788  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    1.110\n",
      "iter 6301/1000000  loss         0.152787  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    1.110\n",
      "iter 6400/1000000  loss         0.152683  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    1.111\n",
      "iter 6401/1000000  loss         0.152682  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    1.111\n",
      "iter 6500/1000000  loss         0.152581  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    1.112\n",
      "iter 6501/1000000  loss         0.152580  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    1.112\n",
      "iter 6600/1000000  loss         0.152481  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    1.113\n",
      "iter 6601/1000000  loss         0.152480  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    1.113\n",
      "iter 6700/1000000  loss         0.152385  avg_L1_norm_grad         0.000052  w[0]    0.001 bias    1.113\n",
      "iter 6701/1000000  loss         0.152384  avg_L1_norm_grad         0.000052  w[0]    0.001 bias    1.113\n",
      "iter 6800/1000000  loss         0.152292  avg_L1_norm_grad         0.000051  w[0]    0.001 bias    1.114\n",
      "iter 6801/1000000  loss         0.152291  avg_L1_norm_grad         0.000051  w[0]    0.001 bias    1.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.152202  avg_L1_norm_grad         0.000050  w[0]    0.001 bias    1.115\n",
      "iter 6901/1000000  loss         0.152201  avg_L1_norm_grad         0.000050  w[0]    0.001 bias    1.115\n",
      "iter 7000/1000000  loss         0.152114  avg_L1_norm_grad         0.000049  w[0]    0.001 bias    1.116\n",
      "iter 7001/1000000  loss         0.152113  avg_L1_norm_grad         0.000049  w[0]    0.001 bias    1.116\n",
      "iter 7100/1000000  loss         0.152028  avg_L1_norm_grad         0.000048  w[0]    0.001 bias    1.116\n",
      "iter 7101/1000000  loss         0.152028  avg_L1_norm_grad         0.000048  w[0]    0.001 bias    1.116\n",
      "iter 7200/1000000  loss         0.151945  avg_L1_norm_grad         0.000047  w[0]    0.002 bias    1.117\n",
      "iter 7201/1000000  loss         0.151945  avg_L1_norm_grad         0.000047  w[0]    0.002 bias    1.117\n",
      "iter 7300/1000000  loss         0.151865  avg_L1_norm_grad         0.000046  w[0]    0.002 bias    1.117\n",
      "iter 7301/1000000  loss         0.151864  avg_L1_norm_grad         0.000046  w[0]    0.002 bias    1.117\n",
      "iter 7400/1000000  loss         0.151786  avg_L1_norm_grad         0.000046  w[0]    0.002 bias    1.118\n",
      "iter 7401/1000000  loss         0.151786  avg_L1_norm_grad         0.000046  w[0]    0.002 bias    1.118\n",
      "iter 7500/1000000  loss         0.151710  avg_L1_norm_grad         0.000045  w[0]    0.002 bias    1.119\n",
      "iter 7501/1000000  loss         0.151709  avg_L1_norm_grad         0.000045  w[0]    0.002 bias    1.119\n",
      "iter 7600/1000000  loss         0.151636  avg_L1_norm_grad         0.000044  w[0]    0.002 bias    1.119\n",
      "iter 7601/1000000  loss         0.151635  avg_L1_norm_grad         0.000044  w[0]    0.002 bias    1.119\n",
      "iter 7700/1000000  loss         0.151564  avg_L1_norm_grad         0.000043  w[0]    0.003 bias    1.120\n",
      "iter 7701/1000000  loss         0.151563  avg_L1_norm_grad         0.000043  w[0]    0.003 bias    1.120\n",
      "iter 7800/1000000  loss         0.151493  avg_L1_norm_grad         0.000043  w[0]    0.003 bias    1.121\n",
      "iter 7801/1000000  loss         0.151493  avg_L1_norm_grad         0.000043  w[0]    0.003 bias    1.121\n",
      "iter 7900/1000000  loss         0.151425  avg_L1_norm_grad         0.000042  w[0]    0.003 bias    1.121\n",
      "iter 7901/1000000  loss         0.151424  avg_L1_norm_grad         0.000042  w[0]    0.003 bias    1.121\n",
      "iter 8000/1000000  loss         0.151358  avg_L1_norm_grad         0.000041  w[0]    0.003 bias    1.122\n",
      "iter 8001/1000000  loss         0.151358  avg_L1_norm_grad         0.000041  w[0]    0.003 bias    1.122\n",
      "iter 8100/1000000  loss         0.151293  avg_L1_norm_grad         0.000040  w[0]    0.003 bias    1.122\n",
      "iter 8101/1000000  loss         0.151292  avg_L1_norm_grad         0.000040  w[0]    0.003 bias    1.122\n",
      "iter 8200/1000000  loss         0.151230  avg_L1_norm_grad         0.000040  w[0]    0.003 bias    1.123\n",
      "iter 8201/1000000  loss         0.151229  avg_L1_norm_grad         0.000040  w[0]    0.003 bias    1.123\n",
      "iter 8300/1000000  loss         0.151168  avg_L1_norm_grad         0.000039  w[0]    0.004 bias    1.123\n",
      "iter 8301/1000000  loss         0.151167  avg_L1_norm_grad         0.000039  w[0]    0.004 bias    1.123\n",
      "iter 8400/1000000  loss         0.151108  avg_L1_norm_grad         0.000038  w[0]    0.004 bias    1.124\n",
      "iter 8401/1000000  loss         0.151107  avg_L1_norm_grad         0.000038  w[0]    0.004 bias    1.124\n",
      "iter 8500/1000000  loss         0.151049  avg_L1_norm_grad         0.000038  w[0]    0.004 bias    1.124\n",
      "iter 8501/1000000  loss         0.151048  avg_L1_norm_grad         0.000038  w[0]    0.004 bias    1.124\n",
      "iter 8600/1000000  loss         0.150991  avg_L1_norm_grad         0.000037  w[0]    0.004 bias    1.125\n",
      "iter 8601/1000000  loss         0.150991  avg_L1_norm_grad         0.000037  w[0]    0.004 bias    1.125\n",
      "iter 8700/1000000  loss         0.150935  avg_L1_norm_grad         0.000037  w[0]    0.004 bias    1.126\n",
      "iter 8701/1000000  loss         0.150935  avg_L1_norm_grad         0.000037  w[0]    0.004 bias    1.126\n",
      "iter 8800/1000000  loss         0.150880  avg_L1_norm_grad         0.000036  w[0]    0.004 bias    1.126\n",
      "iter 8801/1000000  loss         0.150880  avg_L1_norm_grad         0.000036  w[0]    0.004 bias    1.126\n",
      "iter 8900/1000000  loss         0.150827  avg_L1_norm_grad         0.000035  w[0]    0.004 bias    1.127\n",
      "iter 8901/1000000  loss         0.150826  avg_L1_norm_grad         0.000035  w[0]    0.004 bias    1.127\n",
      "iter 9000/1000000  loss         0.150774  avg_L1_norm_grad         0.000035  w[0]    0.004 bias    1.127\n",
      "iter 9001/1000000  loss         0.150774  avg_L1_norm_grad         0.000035  w[0]    0.004 bias    1.127\n",
      "iter 9100/1000000  loss         0.150723  avg_L1_norm_grad         0.000034  w[0]    0.005 bias    1.128\n",
      "iter 9101/1000000  loss         0.150723  avg_L1_norm_grad         0.000034  w[0]    0.005 bias    1.128\n",
      "iter 9200/1000000  loss         0.150673  avg_L1_norm_grad         0.000034  w[0]    0.005 bias    1.128\n",
      "iter 9201/1000000  loss         0.150673  avg_L1_norm_grad         0.000034  w[0]    0.005 bias    1.128\n",
      "iter 9300/1000000  loss         0.150624  avg_L1_norm_grad         0.000033  w[0]    0.005 bias    1.129\n",
      "iter 9301/1000000  loss         0.150624  avg_L1_norm_grad         0.000033  w[0]    0.005 bias    1.129\n",
      "iter 9400/1000000  loss         0.150576  avg_L1_norm_grad         0.000033  w[0]    0.005 bias    1.129\n",
      "iter 9401/1000000  loss         0.150576  avg_L1_norm_grad         0.000033  w[0]    0.005 bias    1.129\n",
      "iter 9500/1000000  loss         0.150530  avg_L1_norm_grad         0.000032  w[0]    0.005 bias    1.130\n",
      "iter 9501/1000000  loss         0.150529  avg_L1_norm_grad         0.000032  w[0]    0.005 bias    1.130\n",
      "iter 9600/1000000  loss         0.150484  avg_L1_norm_grad         0.000032  w[0]    0.005 bias    1.131\n",
      "iter 9601/1000000  loss         0.150483  avg_L1_norm_grad         0.000032  w[0]    0.005 bias    1.131\n",
      "iter 9700/1000000  loss         0.150439  avg_L1_norm_grad         0.000031  w[0]    0.005 bias    1.131\n",
      "iter 9701/1000000  loss         0.150439  avg_L1_norm_grad         0.000031  w[0]    0.005 bias    1.131\n",
      "iter 9800/1000000  loss         0.150395  avg_L1_norm_grad         0.000031  w[0]    0.005 bias    1.132\n",
      "iter 9801/1000000  loss         0.150395  avg_L1_norm_grad         0.000031  w[0]    0.005 bias    1.132\n",
      "iter 9900/1000000  loss         0.150352  avg_L1_norm_grad         0.000030  w[0]    0.005 bias    1.132\n",
      "iter 9901/1000000  loss         0.150352  avg_L1_norm_grad         0.000030  w[0]    0.005 bias    1.132\n",
      "iter 10000/1000000  loss         0.150310  avg_L1_norm_grad         0.000030  w[0]    0.005 bias    1.133\n",
      "iter 10001/1000000  loss         0.150310  avg_L1_norm_grad         0.000030  w[0]    0.005 bias    1.133\n",
      "iter 10100/1000000  loss         0.150269  avg_L1_norm_grad         0.000029  w[0]    0.005 bias    1.133\n",
      "iter 10101/1000000  loss         0.150268  avg_L1_norm_grad         0.000029  w[0]    0.005 bias    1.133\n",
      "iter 10200/1000000  loss         0.150228  avg_L1_norm_grad         0.000029  w[0]    0.006 bias    1.134\n",
      "iter 10201/1000000  loss         0.150228  avg_L1_norm_grad         0.000029  w[0]    0.006 bias    1.134\n",
      "iter 10300/1000000  loss         0.150188  avg_L1_norm_grad         0.000028  w[0]    0.006 bias    1.134\n",
      "iter 10301/1000000  loss         0.150188  avg_L1_norm_grad         0.000028  w[0]    0.006 bias    1.134\n",
      "iter 10400/1000000  loss         0.150150  avg_L1_norm_grad         0.000028  w[0]    0.006 bias    1.135\n",
      "iter 10401/1000000  loss         0.150149  avg_L1_norm_grad         0.000028  w[0]    0.006 bias    1.135\n",
      "iter 10500/1000000  loss         0.150111  avg_L1_norm_grad         0.000028  w[0]    0.006 bias    1.136\n",
      "iter 10501/1000000  loss         0.150111  avg_L1_norm_grad         0.000028  w[0]    0.006 bias    1.136\n",
      "iter 10600/1000000  loss         0.150074  avg_L1_norm_grad         0.000027  w[0]    0.006 bias    1.136\n",
      "iter 10601/1000000  loss         0.150074  avg_L1_norm_grad         0.000027  w[0]    0.006 bias    1.136\n",
      "iter 10700/1000000  loss         0.150037  avg_L1_norm_grad         0.000027  w[0]    0.006 bias    1.137\n",
      "iter 10701/1000000  loss         0.150037  avg_L1_norm_grad         0.000027  w[0]    0.006 bias    1.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.150001  avg_L1_norm_grad         0.000026  w[0]    0.006 bias    1.137\n",
      "iter 10801/1000000  loss         0.150001  avg_L1_norm_grad         0.000026  w[0]    0.006 bias    1.137\n",
      "iter 10900/1000000  loss         0.149966  avg_L1_norm_grad         0.000026  w[0]    0.006 bias    1.138\n",
      "iter 10901/1000000  loss         0.149965  avg_L1_norm_grad         0.000026  w[0]    0.006 bias    1.138\n",
      "iter 11000/1000000  loss         0.149931  avg_L1_norm_grad         0.000026  w[0]    0.006 bias    1.139\n",
      "iter 11001/1000000  loss         0.149931  avg_L1_norm_grad         0.000026  w[0]    0.006 bias    1.139\n",
      "iter 11100/1000000  loss         0.149897  avg_L1_norm_grad         0.000025  w[0]    0.006 bias    1.139\n",
      "iter 11101/1000000  loss         0.149896  avg_L1_norm_grad         0.000025  w[0]    0.006 bias    1.139\n",
      "iter 11200/1000000  loss         0.149863  avg_L1_norm_grad         0.000025  w[0]    0.006 bias    1.140\n",
      "iter 11201/1000000  loss         0.149863  avg_L1_norm_grad         0.000025  w[0]    0.006 bias    1.140\n",
      "iter 11300/1000000  loss         0.149830  avg_L1_norm_grad         0.000024  w[0]    0.006 bias    1.140\n",
      "iter 11301/1000000  loss         0.149830  avg_L1_norm_grad         0.000024  w[0]    0.006 bias    1.140\n",
      "iter 11400/1000000  loss         0.149798  avg_L1_norm_grad         0.000024  w[0]    0.006 bias    1.141\n",
      "iter 11401/1000000  loss         0.149798  avg_L1_norm_grad         0.000024  w[0]    0.006 bias    1.141\n",
      "iter 11500/1000000  loss         0.149766  avg_L1_norm_grad         0.000024  w[0]    0.006 bias    1.142\n",
      "iter 11501/1000000  loss         0.149766  avg_L1_norm_grad         0.000024  w[0]    0.006 bias    1.142\n",
      "iter 11600/1000000  loss         0.149735  avg_L1_norm_grad         0.000023  w[0]    0.006 bias    1.142\n",
      "iter 11601/1000000  loss         0.149735  avg_L1_norm_grad         0.000023  w[0]    0.006 bias    1.142\n",
      "iter 11700/1000000  loss         0.149704  avg_L1_norm_grad         0.000023  w[0]    0.006 bias    1.143\n",
      "iter 11701/1000000  loss         0.149704  avg_L1_norm_grad         0.000023  w[0]    0.006 bias    1.143\n",
      "iter 11800/1000000  loss         0.149674  avg_L1_norm_grad         0.000023  w[0]    0.006 bias    1.143\n",
      "iter 11801/1000000  loss         0.149674  avg_L1_norm_grad         0.000023  w[0]    0.006 bias    1.143\n",
      "iter 11900/1000000  loss         0.149644  avg_L1_norm_grad         0.000022  w[0]    0.006 bias    1.144\n",
      "iter 11901/1000000  loss         0.149644  avg_L1_norm_grad         0.000022  w[0]    0.006 bias    1.144\n",
      "iter 12000/1000000  loss         0.149615  avg_L1_norm_grad         0.000022  w[0]    0.006 bias    1.145\n",
      "iter 12001/1000000  loss         0.149614  avg_L1_norm_grad         0.000022  w[0]    0.006 bias    1.145\n",
      "iter 12100/1000000  loss         0.149586  avg_L1_norm_grad         0.000022  w[0]    0.006 bias    1.145\n",
      "iter 12101/1000000  loss         0.149586  avg_L1_norm_grad         0.000022  w[0]    0.006 bias    1.145\n",
      "iter 12200/1000000  loss         0.149558  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.146\n",
      "iter 12201/1000000  loss         0.149557  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.146\n",
      "iter 12300/1000000  loss         0.149530  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.147\n",
      "iter 12301/1000000  loss         0.149529  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.147\n",
      "iter 12400/1000000  loss         0.149502  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.147\n",
      "iter 12401/1000000  loss         0.149502  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.147\n",
      "iter 12500/1000000  loss         0.149475  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.148\n",
      "iter 12501/1000000  loss         0.149475  avg_L1_norm_grad         0.000021  w[0]    0.007 bias    1.148\n",
      "iter 12600/1000000  loss         0.149448  avg_L1_norm_grad         0.000020  w[0]    0.007 bias    1.149\n",
      "iter 12601/1000000  loss         0.149448  avg_L1_norm_grad         0.000020  w[0]    0.007 bias    1.149\n",
      "iter 12700/1000000  loss         0.149422  avg_L1_norm_grad         0.000020  w[0]    0.007 bias    1.149\n",
      "iter 12701/1000000  loss         0.149422  avg_L1_norm_grad         0.000020  w[0]    0.007 bias    1.149\n",
      "iter 12800/1000000  loss         0.149396  avg_L1_norm_grad         0.000020  w[0]    0.007 bias    1.150\n",
      "iter 12801/1000000  loss         0.149396  avg_L1_norm_grad         0.000020  w[0]    0.007 bias    1.150\n",
      "iter 12900/1000000  loss         0.149371  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.151\n",
      "iter 12901/1000000  loss         0.149371  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.151\n",
      "iter 13000/1000000  loss         0.149346  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.151\n",
      "iter 13001/1000000  loss         0.149345  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.151\n",
      "iter 13100/1000000  loss         0.149321  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.152\n",
      "iter 13101/1000000  loss         0.149321  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.152\n",
      "iter 13200/1000000  loss         0.149297  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.153\n",
      "iter 13201/1000000  loss         0.149296  avg_L1_norm_grad         0.000019  w[0]    0.007 bias    1.153\n",
      "iter 13300/1000000  loss         0.149272  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.154\n",
      "iter 13301/1000000  loss         0.149272  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.154\n",
      "iter 13400/1000000  loss         0.149249  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.154\n",
      "iter 13401/1000000  loss         0.149248  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.154\n",
      "iter 13500/1000000  loss         0.149225  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.155\n",
      "iter 13501/1000000  loss         0.149225  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.155\n",
      "iter 13600/1000000  loss         0.149202  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.156\n",
      "iter 13601/1000000  loss         0.149202  avg_L1_norm_grad         0.000018  w[0]    0.007 bias    1.156\n",
      "iter 13700/1000000  loss         0.149179  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.156\n",
      "iter 13701/1000000  loss         0.149179  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.156\n",
      "iter 13800/1000000  loss         0.149157  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.157\n",
      "iter 13801/1000000  loss         0.149157  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.157\n",
      "iter 13900/1000000  loss         0.149135  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.158\n",
      "iter 13901/1000000  loss         0.149135  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.158\n",
      "iter 14000/1000000  loss         0.149113  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.159\n",
      "iter 14001/1000000  loss         0.149113  avg_L1_norm_grad         0.000017  w[0]    0.007 bias    1.159\n",
      "iter 14100/1000000  loss         0.149091  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.159\n",
      "iter 14101/1000000  loss         0.149091  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.159\n",
      "iter 14200/1000000  loss         0.149070  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.160\n",
      "iter 14201/1000000  loss         0.149070  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.160\n",
      "iter 14300/1000000  loss         0.149049  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.161\n",
      "iter 14301/1000000  loss         0.149049  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.161\n",
      "iter 14400/1000000  loss         0.149028  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.162\n",
      "iter 14401/1000000  loss         0.149028  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.162\n",
      "iter 14500/1000000  loss         0.149007  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.163\n",
      "iter 14501/1000000  loss         0.149007  avg_L1_norm_grad         0.000016  w[0]    0.007 bias    1.163\n",
      "iter 14600/1000000  loss         0.148987  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.163\n",
      "iter 14601/1000000  loss         0.148987  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.148967  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.164\n",
      "iter 14701/1000000  loss         0.148967  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.164\n",
      "iter 14800/1000000  loss         0.148947  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.165\n",
      "iter 14801/1000000  loss         0.148947  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.165\n",
      "iter 14900/1000000  loss         0.148928  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.166\n",
      "iter 14901/1000000  loss         0.148927  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.166\n",
      "iter 15000/1000000  loss         0.148908  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.166\n",
      "iter 15001/1000000  loss         0.148908  avg_L1_norm_grad         0.000015  w[0]    0.007 bias    1.166\n",
      "iter 15100/1000000  loss         0.148889  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.167\n",
      "iter 15101/1000000  loss         0.148889  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.167\n",
      "iter 15200/1000000  loss         0.148870  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.168\n",
      "iter 15201/1000000  loss         0.148870  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.168\n",
      "iter 15300/1000000  loss         0.148851  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.169\n",
      "iter 15301/1000000  loss         0.148851  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.169\n",
      "iter 15400/1000000  loss         0.148833  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.170\n",
      "iter 15401/1000000  loss         0.148833  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.170\n",
      "iter 15500/1000000  loss         0.148815  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.171\n",
      "iter 15501/1000000  loss         0.148814  avg_L1_norm_grad         0.000014  w[0]    0.007 bias    1.171\n",
      "iter 15600/1000000  loss         0.148797  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.171\n",
      "iter 15601/1000000  loss         0.148796  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.171\n",
      "iter 15700/1000000  loss         0.148779  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.172\n",
      "iter 15701/1000000  loss         0.148778  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.172\n",
      "iter 15800/1000000  loss         0.148761  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.173\n",
      "iter 15801/1000000  loss         0.148761  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.173\n",
      "iter 15900/1000000  loss         0.148743  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.174\n",
      "iter 15901/1000000  loss         0.148743  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.174\n",
      "iter 16000/1000000  loss         0.148726  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.175\n",
      "iter 16001/1000000  loss         0.148726  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.175\n",
      "iter 16100/1000000  loss         0.148709  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.176\n",
      "iter 16101/1000000  loss         0.148709  avg_L1_norm_grad         0.000013  w[0]    0.007 bias    1.176\n",
      "iter 16200/1000000  loss         0.148692  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.177\n",
      "iter 16201/1000000  loss         0.148692  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.177\n",
      "iter 16300/1000000  loss         0.148675  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.177\n",
      "iter 16301/1000000  loss         0.148675  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.177\n",
      "iter 16400/1000000  loss         0.148659  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.178\n",
      "iter 16401/1000000  loss         0.148658  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.178\n",
      "iter 16500/1000000  loss         0.148642  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.179\n",
      "iter 16501/1000000  loss         0.148642  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.179\n",
      "iter 16600/1000000  loss         0.148626  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.180\n",
      "iter 16601/1000000  loss         0.148626  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.180\n",
      "iter 16700/1000000  loss         0.148610  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.181\n",
      "iter 16701/1000000  loss         0.148610  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.181\n",
      "iter 16800/1000000  loss         0.148594  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.182\n",
      "iter 16801/1000000  loss         0.148594  avg_L1_norm_grad         0.000012  w[0]    0.007 bias    1.182\n",
      "iter 16900/1000000  loss         0.148578  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.183\n",
      "iter 16901/1000000  loss         0.148578  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.183\n",
      "iter 17000/1000000  loss         0.148562  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.184\n",
      "iter 17001/1000000  loss         0.148562  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.184\n",
      "iter 17100/1000000  loss         0.148547  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.185\n",
      "iter 17101/1000000  loss         0.148547  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.185\n",
      "iter 17200/1000000  loss         0.148531  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.186\n",
      "iter 17201/1000000  loss         0.148531  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.186\n",
      "iter 17300/1000000  loss         0.148516  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.187\n",
      "iter 17301/1000000  loss         0.148516  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.187\n",
      "iter 17400/1000000  loss         0.148501  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.187\n",
      "iter 17401/1000000  loss         0.148501  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.187\n",
      "iter 17500/1000000  loss         0.148486  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.188\n",
      "iter 17501/1000000  loss         0.148486  avg_L1_norm_grad         0.000011  w[0]    0.007 bias    1.188\n",
      "iter 17600/1000000  loss         0.148471  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.189\n",
      "iter 17601/1000000  loss         0.148471  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.189\n",
      "iter 17700/1000000  loss         0.148457  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.190\n",
      "iter 17701/1000000  loss         0.148456  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.190\n",
      "iter 17800/1000000  loss         0.148442  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.191\n",
      "iter 17801/1000000  loss         0.148442  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.191\n",
      "iter 17900/1000000  loss         0.148428  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.192\n",
      "iter 17901/1000000  loss         0.148427  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.192\n",
      "iter 18000/1000000  loss         0.148413  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.193\n",
      "iter 18001/1000000  loss         0.148413  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.193\n",
      "iter 18100/1000000  loss         0.148399  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.194\n",
      "iter 18101/1000000  loss         0.148399  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.194\n",
      "iter 18200/1000000  loss         0.148385  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.195\n",
      "iter 18201/1000000  loss         0.148385  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.195\n",
      "iter 18300/1000000  loss         0.148371  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.196\n",
      "iter 18301/1000000  loss         0.148371  avg_L1_norm_grad         0.000010  w[0]    0.007 bias    1.196\n",
      "iter 18400/1000000  loss         0.148357  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.197\n",
      "iter 18401/1000000  loss         0.148357  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.197\n",
      "iter 18500/1000000  loss         0.148344  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.198\n",
      "iter 18501/1000000  loss         0.148344  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18600/1000000  loss         0.148330  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.199\n",
      "iter 18601/1000000  loss         0.148330  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.199\n",
      "iter 18700/1000000  loss         0.148317  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.200\n",
      "iter 18701/1000000  loss         0.148316  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.200\n",
      "iter 18800/1000000  loss         0.148303  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.201\n",
      "iter 18801/1000000  loss         0.148303  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.201\n",
      "iter 18900/1000000  loss         0.148290  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.202\n",
      "iter 18901/1000000  loss         0.148290  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.202\n",
      "iter 19000/1000000  loss         0.148277  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.203\n",
      "iter 19001/1000000  loss         0.148277  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.203\n",
      "iter 19100/1000000  loss         0.148264  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.204\n",
      "iter 19101/1000000  loss         0.148264  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.204\n",
      "iter 19200/1000000  loss         0.148251  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.205\n",
      "iter 19201/1000000  loss         0.148251  avg_L1_norm_grad         0.000009  w[0]    0.007 bias    1.205\n",
      "iter 19300/1000000  loss         0.148238  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.206\n",
      "iter 19301/1000000  loss         0.148238  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.206\n",
      "iter 19400/1000000  loss         0.148225  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.207\n",
      "iter 19401/1000000  loss         0.148225  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.207\n",
      "iter 19500/1000000  loss         0.148213  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.208\n",
      "iter 19501/1000000  loss         0.148213  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.208\n",
      "iter 19600/1000000  loss         0.148200  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.209\n",
      "iter 19601/1000000  loss         0.148200  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.209\n",
      "iter 19700/1000000  loss         0.148188  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.210\n",
      "iter 19701/1000000  loss         0.148188  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.210\n",
      "iter 19800/1000000  loss         0.148176  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.211\n",
      "iter 19801/1000000  loss         0.148175  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.211\n",
      "iter 19900/1000000  loss         0.148163  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.212\n",
      "iter 19901/1000000  loss         0.148163  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.212\n",
      "iter 20000/1000000  loss         0.148151  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.213\n",
      "iter 20001/1000000  loss         0.148151  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.213\n",
      "iter 20100/1000000  loss         0.148139  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.214\n",
      "iter 20101/1000000  loss         0.148139  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.214\n",
      "iter 20200/1000000  loss         0.148127  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.215\n",
      "iter 20201/1000000  loss         0.148127  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.215\n",
      "iter 20300/1000000  loss         0.148115  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.216\n",
      "iter 20301/1000000  loss         0.148115  avg_L1_norm_grad         0.000008  w[0]    0.007 bias    1.216\n",
      "iter 20400/1000000  loss         0.148103  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.218\n",
      "iter 20401/1000000  loss         0.148103  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.218\n",
      "iter 20500/1000000  loss         0.148092  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.219\n",
      "iter 20501/1000000  loss         0.148092  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.219\n",
      "iter 20600/1000000  loss         0.148080  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.220\n",
      "iter 20601/1000000  loss         0.148080  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.220\n",
      "iter 20700/1000000  loss         0.148069  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.221\n",
      "iter 20701/1000000  loss         0.148068  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.221\n",
      "iter 20800/1000000  loss         0.148057  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.222\n",
      "iter 20801/1000000  loss         0.148057  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.222\n",
      "iter 20900/1000000  loss         0.148046  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.223\n",
      "iter 20901/1000000  loss         0.148046  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.223\n",
      "iter 21000/1000000  loss         0.148034  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.224\n",
      "iter 21001/1000000  loss         0.148034  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.224\n",
      "iter 21100/1000000  loss         0.148023  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.225\n",
      "iter 21101/1000000  loss         0.148023  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.225\n",
      "iter 21200/1000000  loss         0.148012  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.226\n",
      "iter 21201/1000000  loss         0.148012  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.226\n",
      "iter 21300/1000000  loss         0.148001  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.227\n",
      "iter 21301/1000000  loss         0.148001  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.227\n",
      "iter 21400/1000000  loss         0.147990  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.228\n",
      "iter 21401/1000000  loss         0.147990  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.228\n",
      "iter 21500/1000000  loss         0.147979  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.229\n",
      "iter 21501/1000000  loss         0.147979  avg_L1_norm_grad         0.000007  w[0]    0.007 bias    1.229\n",
      "iter 21600/1000000  loss         0.147968  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.231\n",
      "iter 21601/1000000  loss         0.147968  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.231\n",
      "iter 21700/1000000  loss         0.147958  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.232\n",
      "iter 21701/1000000  loss         0.147958  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.232\n",
      "iter 21800/1000000  loss         0.147947  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.233\n",
      "iter 21801/1000000  loss         0.147947  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.233\n",
      "iter 21900/1000000  loss         0.147936  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.234\n",
      "iter 21901/1000000  loss         0.147936  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.234\n",
      "iter 22000/1000000  loss         0.147926  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.235\n",
      "iter 22001/1000000  loss         0.147926  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.235\n",
      "iter 22100/1000000  loss         0.147915  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.236\n",
      "iter 22101/1000000  loss         0.147915  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.236\n",
      "iter 22200/1000000  loss         0.147905  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.237\n",
      "iter 22201/1000000  loss         0.147905  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.237\n",
      "iter 22300/1000000  loss         0.147895  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.238\n",
      "iter 22301/1000000  loss         0.147894  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.238\n",
      "iter 22400/1000000  loss         0.147884  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.239\n",
      "iter 22401/1000000  loss         0.147884  avg_L1_norm_grad         0.000006  w[0]    0.007 bias    1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22500/1000000  loss         0.147874  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.241\n",
      "iter 22501/1000000  loss         0.147874  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.241\n",
      "iter 22600/1000000  loss         0.147864  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.242\n",
      "iter 22601/1000000  loss         0.147864  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.242\n",
      "iter 22700/1000000  loss         0.147854  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.243\n",
      "iter 22701/1000000  loss         0.147854  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.243\n",
      "iter 22800/1000000  loss         0.147844  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.244\n",
      "iter 22801/1000000  loss         0.147844  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.244\n",
      "iter 22900/1000000  loss         0.147834  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.245\n",
      "iter 22901/1000000  loss         0.147834  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.245\n",
      "iter 23000/1000000  loss         0.147824  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.246\n",
      "iter 23001/1000000  loss         0.147824  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.246\n",
      "iter 23100/1000000  loss         0.147814  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.247\n",
      "iter 23101/1000000  loss         0.147814  avg_L1_norm_grad         0.000006  w[0]    0.006 bias    1.247\n",
      "iter 23200/1000000  loss         0.147804  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.249\n",
      "iter 23201/1000000  loss         0.147804  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.249\n",
      "iter 23300/1000000  loss         0.147795  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.250\n",
      "iter 23301/1000000  loss         0.147795  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.250\n",
      "iter 23400/1000000  loss         0.147785  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.251\n",
      "iter 23401/1000000  loss         0.147785  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.251\n",
      "iter 23500/1000000  loss         0.147776  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.252\n",
      "iter 23501/1000000  loss         0.147776  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.252\n",
      "iter 23600/1000000  loss         0.147766  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.253\n",
      "iter 23601/1000000  loss         0.147766  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.253\n",
      "iter 23700/1000000  loss         0.147757  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.254\n",
      "iter 23701/1000000  loss         0.147757  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.254\n",
      "iter 23800/1000000  loss         0.147747  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.256\n",
      "iter 23801/1000000  loss         0.147747  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.256\n",
      "iter 23900/1000000  loss         0.147738  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.257\n",
      "iter 23901/1000000  loss         0.147738  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.257\n",
      "iter 24000/1000000  loss         0.147729  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.258\n",
      "iter 24001/1000000  loss         0.147729  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.258\n",
      "iter 24100/1000000  loss         0.147719  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.259\n",
      "iter 24101/1000000  loss         0.147719  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.259\n",
      "iter 24200/1000000  loss         0.147710  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.260\n",
      "iter 24201/1000000  loss         0.147710  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.260\n",
      "iter 24300/1000000  loss         0.147701  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.261\n",
      "iter 24301/1000000  loss         0.147701  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.261\n",
      "iter 24400/1000000  loss         0.147692  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.263\n",
      "iter 24401/1000000  loss         0.147692  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.263\n",
      "iter 24500/1000000  loss         0.147683  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.264\n",
      "iter 24501/1000000  loss         0.147683  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.264\n",
      "iter 24600/1000000  loss         0.147674  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.265\n",
      "iter 24601/1000000  loss         0.147674  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.265\n",
      "iter 24700/1000000  loss         0.147665  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.266\n",
      "iter 24701/1000000  loss         0.147665  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.266\n",
      "iter 24800/1000000  loss         0.147656  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.267\n",
      "iter 24801/1000000  loss         0.147656  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.267\n",
      "iter 24900/1000000  loss         0.147647  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.269\n",
      "iter 24901/1000000  loss         0.147647  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.269\n",
      "iter 25000/1000000  loss         0.147639  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.270\n",
      "iter 25001/1000000  loss         0.147639  avg_L1_norm_grad         0.000005  w[0]    0.006 bias    1.270\n",
      "iter 25100/1000000  loss         0.147630  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.271\n",
      "iter 25101/1000000  loss         0.147630  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.271\n",
      "iter 25200/1000000  loss         0.147621  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.272\n",
      "iter 25201/1000000  loss         0.147621  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.272\n",
      "iter 25300/1000000  loss         0.147613  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.273\n",
      "iter 25301/1000000  loss         0.147613  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.273\n",
      "iter 25400/1000000  loss         0.147604  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.275\n",
      "iter 25401/1000000  loss         0.147604  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.275\n",
      "iter 25500/1000000  loss         0.147596  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.276\n",
      "iter 25501/1000000  loss         0.147596  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.276\n",
      "iter 25600/1000000  loss         0.147587  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.277\n",
      "iter 25601/1000000  loss         0.147587  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.277\n",
      "iter 25700/1000000  loss         0.147579  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.278\n",
      "iter 25701/1000000  loss         0.147579  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.278\n",
      "iter 25800/1000000  loss         0.147570  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.279\n",
      "iter 25801/1000000  loss         0.147570  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.279\n",
      "iter 25900/1000000  loss         0.147562  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.281\n",
      "iter 25901/1000000  loss         0.147562  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.281\n",
      "iter 26000/1000000  loss         0.147554  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.282\n",
      "iter 26001/1000000  loss         0.147554  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.282\n",
      "iter 26100/1000000  loss         0.147546  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.283\n",
      "iter 26101/1000000  loss         0.147546  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.283\n",
      "iter 26200/1000000  loss         0.147538  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.284\n",
      "iter 26201/1000000  loss         0.147537  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.284\n",
      "iter 26300/1000000  loss         0.147529  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.285\n",
      "iter 26301/1000000  loss         0.147529  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26400/1000000  loss         0.147521  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.287\n",
      "iter 26401/1000000  loss         0.147521  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.287\n",
      "iter 26500/1000000  loss         0.147513  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.288\n",
      "iter 26501/1000000  loss         0.147513  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.288\n",
      "iter 26600/1000000  loss         0.147505  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.289\n",
      "iter 26601/1000000  loss         0.147505  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.289\n",
      "iter 26700/1000000  loss         0.147497  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.290\n",
      "iter 26701/1000000  loss         0.147497  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.290\n",
      "iter 26800/1000000  loss         0.147489  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.292\n",
      "iter 26801/1000000  loss         0.147489  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.292\n",
      "iter 26900/1000000  loss         0.147482  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.293\n",
      "iter 26901/1000000  loss         0.147481  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.293\n",
      "iter 27000/1000000  loss         0.147474  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.294\n",
      "iter 27001/1000000  loss         0.147474  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.294\n",
      "iter 27100/1000000  loss         0.147466  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.295\n",
      "iter 27101/1000000  loss         0.147466  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.295\n",
      "iter 27200/1000000  loss         0.147458  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.296\n",
      "iter 27201/1000000  loss         0.147458  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.296\n",
      "iter 27300/1000000  loss         0.147451  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.298\n",
      "iter 27301/1000000  loss         0.147450  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.298\n",
      "iter 27400/1000000  loss         0.147443  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.299\n",
      "iter 27401/1000000  loss         0.147443  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.299\n",
      "iter 27500/1000000  loss         0.147435  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.300\n",
      "iter 27501/1000000  loss         0.147435  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.300\n",
      "iter 27600/1000000  loss         0.147428  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.301\n",
      "iter 27601/1000000  loss         0.147428  avg_L1_norm_grad         0.000004  w[0]    0.006 bias    1.301\n",
      "iter 27700/1000000  loss         0.147420  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.303\n",
      "iter 27701/1000000  loss         0.147420  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.303\n",
      "iter 27800/1000000  loss         0.147413  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.304\n",
      "iter 27801/1000000  loss         0.147413  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.304\n",
      "iter 27900/1000000  loss         0.147405  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.305\n",
      "iter 27901/1000000  loss         0.147405  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.305\n",
      "iter 28000/1000000  loss         0.147398  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.306\n",
      "iter 28001/1000000  loss         0.147398  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.306\n",
      "iter 28100/1000000  loss         0.147390  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.308\n",
      "iter 28101/1000000  loss         0.147390  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.308\n",
      "iter 28200/1000000  loss         0.147383  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.309\n",
      "iter 28201/1000000  loss         0.147383  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.309\n",
      "iter 28300/1000000  loss         0.147376  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.310\n",
      "iter 28301/1000000  loss         0.147376  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.310\n",
      "iter 28400/1000000  loss         0.147369  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.311\n",
      "iter 28401/1000000  loss         0.147369  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.311\n",
      "iter 28500/1000000  loss         0.147361  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.313\n",
      "iter 28501/1000000  loss         0.147361  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.313\n",
      "iter 28600/1000000  loss         0.147354  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.314\n",
      "iter 28601/1000000  loss         0.147354  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.314\n",
      "iter 28700/1000000  loss         0.147347  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.315\n",
      "iter 28701/1000000  loss         0.147347  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.315\n",
      "iter 28800/1000000  loss         0.147340  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.316\n",
      "iter 28801/1000000  loss         0.147340  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.316\n",
      "iter 28900/1000000  loss         0.147333  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.318\n",
      "iter 28901/1000000  loss         0.147333  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.318\n",
      "iter 29000/1000000  loss         0.147326  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.319\n",
      "iter 29001/1000000  loss         0.147326  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.319\n",
      "iter 29100/1000000  loss         0.147319  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.320\n",
      "iter 29101/1000000  loss         0.147319  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.320\n",
      "iter 29200/1000000  loss         0.147312  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.321\n",
      "iter 29201/1000000  loss         0.147312  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.321\n",
      "iter 29300/1000000  loss         0.147305  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.323\n",
      "iter 29301/1000000  loss         0.147305  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.323\n",
      "iter 29400/1000000  loss         0.147298  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.324\n",
      "iter 29401/1000000  loss         0.147298  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.324\n",
      "iter 29500/1000000  loss         0.147291  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.325\n",
      "iter 29501/1000000  loss         0.147291  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.325\n",
      "iter 29600/1000000  loss         0.147284  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.326\n",
      "iter 29601/1000000  loss         0.147284  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.326\n",
      "iter 29700/1000000  loss         0.147278  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.328\n",
      "iter 29701/1000000  loss         0.147278  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.328\n",
      "iter 29800/1000000  loss         0.147271  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.329\n",
      "iter 29801/1000000  loss         0.147271  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.329\n",
      "iter 29900/1000000  loss         0.147264  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.330\n",
      "iter 29901/1000000  loss         0.147264  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.330\n",
      "iter 30000/1000000  loss         0.147258  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.331\n",
      "iter 30001/1000000  loss         0.147257  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.331\n",
      "iter 30100/1000000  loss         0.147251  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.333\n",
      "iter 30101/1000000  loss         0.147251  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.333\n",
      "iter 30200/1000000  loss         0.147244  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.334\n",
      "iter 30201/1000000  loss         0.147244  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30300/1000000  loss         0.147238  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.335\n",
      "iter 30301/1000000  loss         0.147238  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.335\n",
      "iter 30400/1000000  loss         0.147231  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.336\n",
      "iter 30401/1000000  loss         0.147231  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.336\n",
      "iter 30500/1000000  loss         0.147225  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.338\n",
      "iter 30501/1000000  loss         0.147225  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.338\n",
      "iter 30600/1000000  loss         0.147218  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.339\n",
      "iter 30601/1000000  loss         0.147218  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.339\n",
      "iter 30700/1000000  loss         0.147212  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.340\n",
      "iter 30701/1000000  loss         0.147212  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.340\n",
      "iter 30800/1000000  loss         0.147205  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.341\n",
      "iter 30801/1000000  loss         0.147205  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.341\n",
      "iter 30900/1000000  loss         0.147199  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.343\n",
      "iter 30901/1000000  loss         0.147199  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.343\n",
      "iter 31000/1000000  loss         0.147193  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.344\n",
      "iter 31001/1000000  loss         0.147193  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.344\n",
      "iter 31100/1000000  loss         0.147186  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.345\n",
      "iter 31101/1000000  loss         0.147186  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.345\n",
      "iter 31200/1000000  loss         0.147180  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.346\n",
      "iter 31201/1000000  loss         0.147180  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.346\n",
      "iter 31300/1000000  loss         0.147174  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.348\n",
      "iter 31301/1000000  loss         0.147174  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.348\n",
      "iter 31400/1000000  loss         0.147168  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.349\n",
      "iter 31401/1000000  loss         0.147167  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.349\n",
      "iter 31500/1000000  loss         0.147161  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.350\n",
      "iter 31501/1000000  loss         0.147161  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.350\n",
      "iter 31600/1000000  loss         0.147155  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.351\n",
      "iter 31601/1000000  loss         0.147155  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.351\n",
      "iter 31700/1000000  loss         0.147149  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.353\n",
      "iter 31701/1000000  loss         0.147149  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.353\n",
      "iter 31800/1000000  loss         0.147143  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.354\n",
      "iter 31801/1000000  loss         0.147143  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.354\n",
      "iter 31900/1000000  loss         0.147137  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.355\n",
      "iter 31901/1000000  loss         0.147137  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.355\n",
      "iter 32000/1000000  loss         0.147131  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.356\n",
      "iter 32001/1000000  loss         0.147131  avg_L1_norm_grad         0.000003  w[0]    0.006 bias    1.356\n",
      "iter 32100/1000000  loss         0.147125  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.358\n",
      "iter 32101/1000000  loss         0.147125  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.358\n",
      "iter 32200/1000000  loss         0.147119  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.359\n",
      "iter 32201/1000000  loss         0.147119  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.359\n",
      "iter 32300/1000000  loss         0.147113  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.360\n",
      "iter 32301/1000000  loss         0.147113  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.360\n",
      "iter 32400/1000000  loss         0.147107  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.361\n",
      "iter 32401/1000000  loss         0.147107  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.361\n",
      "iter 32500/1000000  loss         0.147101  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.363\n",
      "iter 32501/1000000  loss         0.147101  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.363\n",
      "iter 32600/1000000  loss         0.147095  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.364\n",
      "iter 32601/1000000  loss         0.147095  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.364\n",
      "iter 32700/1000000  loss         0.147089  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.365\n",
      "iter 32701/1000000  loss         0.147089  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.365\n",
      "iter 32800/1000000  loss         0.147084  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.367\n",
      "iter 32801/1000000  loss         0.147084  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.367\n",
      "iter 32900/1000000  loss         0.147078  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.368\n",
      "iter 32901/1000000  loss         0.147078  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.368\n",
      "iter 33000/1000000  loss         0.147072  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.369\n",
      "iter 33001/1000000  loss         0.147072  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.369\n",
      "iter 33100/1000000  loss         0.147066  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.370\n",
      "iter 33101/1000000  loss         0.147066  avg_L1_norm_grad         0.000002  w[0]    0.006 bias    1.370\n",
      "iter 33200/1000000  loss         0.147061  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.372\n",
      "iter 33201/1000000  loss         0.147061  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.372\n",
      "iter 33300/1000000  loss         0.147055  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.373\n",
      "iter 33301/1000000  loss         0.147055  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.373\n",
      "iter 33400/1000000  loss         0.147049  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.374\n",
      "iter 33401/1000000  loss         0.147049  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.374\n",
      "iter 33500/1000000  loss         0.147044  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.375\n",
      "iter 33501/1000000  loss         0.147044  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.375\n",
      "iter 33600/1000000  loss         0.147038  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.377\n",
      "iter 33601/1000000  loss         0.147038  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.377\n",
      "iter 33700/1000000  loss         0.147033  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.378\n",
      "iter 33701/1000000  loss         0.147033  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.378\n",
      "iter 33800/1000000  loss         0.147027  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.379\n",
      "iter 33801/1000000  loss         0.147027  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.379\n",
      "iter 33900/1000000  loss         0.147022  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.380\n",
      "iter 33901/1000000  loss         0.147022  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.380\n",
      "iter 34000/1000000  loss         0.147016  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.382\n",
      "iter 34001/1000000  loss         0.147016  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.382\n",
      "iter 34100/1000000  loss         0.147011  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34101/1000000  loss         0.147011  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.383\n",
      "iter 34200/1000000  loss         0.147005  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.384\n",
      "iter 34201/1000000  loss         0.147005  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.384\n",
      "iter 34300/1000000  loss         0.147000  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.385\n",
      "iter 34301/1000000  loss         0.147000  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.385\n",
      "iter 34400/1000000  loss         0.146995  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.387\n",
      "iter 34401/1000000  loss         0.146995  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.387\n",
      "iter 34500/1000000  loss         0.146989  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.388\n",
      "iter 34501/1000000  loss         0.146989  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.388\n",
      "iter 34600/1000000  loss         0.146984  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.389\n",
      "iter 34601/1000000  loss         0.146984  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.389\n",
      "iter 34700/1000000  loss         0.146979  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.390\n",
      "iter 34701/1000000  loss         0.146979  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.390\n",
      "iter 34800/1000000  loss         0.146973  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.392\n",
      "iter 34801/1000000  loss         0.146973  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.392\n",
      "iter 34900/1000000  loss         0.146968  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.393\n",
      "iter 34901/1000000  loss         0.146968  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.393\n",
      "iter 35000/1000000  loss         0.146963  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.394\n",
      "iter 35001/1000000  loss         0.146963  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.394\n",
      "iter 35100/1000000  loss         0.146958  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.395\n",
      "iter 35101/1000000  loss         0.146958  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.395\n",
      "iter 35200/1000000  loss         0.146953  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.397\n",
      "iter 35201/1000000  loss         0.146953  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.397\n",
      "iter 35300/1000000  loss         0.146947  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.398\n",
      "iter 35301/1000000  loss         0.146947  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.398\n",
      "iter 35400/1000000  loss         0.146942  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.399\n",
      "iter 35401/1000000  loss         0.146942  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.399\n",
      "iter 35500/1000000  loss         0.146937  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.400\n",
      "iter 35501/1000000  loss         0.146937  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.400\n",
      "iter 35600/1000000  loss         0.146932  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.402\n",
      "iter 35601/1000000  loss         0.146932  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.402\n",
      "iter 35700/1000000  loss         0.146927  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.403\n",
      "iter 35701/1000000  loss         0.146927  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.403\n",
      "iter 35800/1000000  loss         0.146922  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.404\n",
      "iter 35801/1000000  loss         0.146922  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.404\n",
      "iter 35900/1000000  loss         0.146917  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.405\n",
      "iter 35901/1000000  loss         0.146917  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.405\n",
      "iter 36000/1000000  loss         0.146912  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.407\n",
      "iter 36001/1000000  loss         0.146912  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.407\n",
      "iter 36100/1000000  loss         0.146907  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.408\n",
      "iter 36101/1000000  loss         0.146907  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.408\n",
      "iter 36200/1000000  loss         0.146902  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.409\n",
      "iter 36201/1000000  loss         0.146902  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.409\n",
      "iter 36300/1000000  loss         0.146897  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.410\n",
      "iter 36301/1000000  loss         0.146897  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.410\n",
      "iter 36400/1000000  loss         0.146893  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.412\n",
      "iter 36401/1000000  loss         0.146892  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.412\n",
      "iter 36500/1000000  loss         0.146888  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.413\n",
      "iter 36501/1000000  loss         0.146888  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.413\n",
      "iter 36600/1000000  loss         0.146883  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.414\n",
      "iter 36601/1000000  loss         0.146883  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.414\n",
      "iter 36700/1000000  loss         0.146878  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.415\n",
      "iter 36701/1000000  loss         0.146878  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.415\n",
      "iter 36800/1000000  loss         0.146873  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.417\n",
      "iter 36801/1000000  loss         0.146873  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.417\n",
      "iter 36900/1000000  loss         0.146868  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.418\n",
      "iter 36901/1000000  loss         0.146868  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.418\n",
      "iter 37000/1000000  loss         0.146864  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.419\n",
      "iter 37001/1000000  loss         0.146864  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.419\n",
      "iter 37100/1000000  loss         0.146859  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.420\n",
      "iter 37101/1000000  loss         0.146859  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.420\n",
      "iter 37200/1000000  loss         0.146854  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.422\n",
      "iter 37201/1000000  loss         0.146854  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.422\n",
      "iter 37300/1000000  loss         0.146850  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.423\n",
      "iter 37301/1000000  loss         0.146850  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.423\n",
      "iter 37400/1000000  loss         0.146845  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.424\n",
      "iter 37401/1000000  loss         0.146845  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.424\n",
      "iter 37500/1000000  loss         0.146840  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.425\n",
      "iter 37501/1000000  loss         0.146840  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.425\n",
      "iter 37600/1000000  loss         0.146836  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.427\n",
      "iter 37601/1000000  loss         0.146836  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.427\n",
      "iter 37700/1000000  loss         0.146831  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.428\n",
      "iter 37701/1000000  loss         0.146831  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.428\n",
      "iter 37800/1000000  loss         0.146827  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.429\n",
      "iter 37801/1000000  loss         0.146827  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.429\n",
      "iter 37900/1000000  loss         0.146822  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.430\n",
      "iter 37901/1000000  loss         0.146822  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.430\n",
      "iter 38000/1000000  loss         0.146818  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38001/1000000  loss         0.146818  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.431\n",
      "iter 38100/1000000  loss         0.146813  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.433\n",
      "iter 38101/1000000  loss         0.146813  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.433\n",
      "iter 38200/1000000  loss         0.146809  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.434\n",
      "iter 38201/1000000  loss         0.146809  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.434\n",
      "iter 38300/1000000  loss         0.146804  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.435\n",
      "iter 38301/1000000  loss         0.146804  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.435\n",
      "iter 38400/1000000  loss         0.146800  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.436\n",
      "iter 38401/1000000  loss         0.146800  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.436\n",
      "iter 38500/1000000  loss         0.146795  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.438\n",
      "iter 38501/1000000  loss         0.146795  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.438\n",
      "iter 38600/1000000  loss         0.146791  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.439\n",
      "iter 38601/1000000  loss         0.146791  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.439\n",
      "iter 38700/1000000  loss         0.146787  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.440\n",
      "iter 38701/1000000  loss         0.146787  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.440\n",
      "iter 38800/1000000  loss         0.146782  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.441\n",
      "iter 38801/1000000  loss         0.146782  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.441\n",
      "iter 38900/1000000  loss         0.146778  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.443\n",
      "iter 38901/1000000  loss         0.146778  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.443\n",
      "iter 39000/1000000  loss         0.146774  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.444\n",
      "iter 39001/1000000  loss         0.146774  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.444\n",
      "iter 39100/1000000  loss         0.146769  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.445\n",
      "iter 39101/1000000  loss         0.146769  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.445\n",
      "iter 39200/1000000  loss         0.146765  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.446\n",
      "iter 39201/1000000  loss         0.146765  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.446\n",
      "iter 39300/1000000  loss         0.146761  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.447\n",
      "iter 39301/1000000  loss         0.146761  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.447\n",
      "iter 39400/1000000  loss         0.146757  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.449\n",
      "iter 39401/1000000  loss         0.146757  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.449\n",
      "iter 39500/1000000  loss         0.146752  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.450\n",
      "iter 39501/1000000  loss         0.146752  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.450\n",
      "iter 39600/1000000  loss         0.146748  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.451\n",
      "iter 39601/1000000  loss         0.146748  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.451\n",
      "iter 39700/1000000  loss         0.146744  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.452\n",
      "iter 39701/1000000  loss         0.146744  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.452\n",
      "iter 39800/1000000  loss         0.146740  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.453\n",
      "iter 39801/1000000  loss         0.146740  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.453\n",
      "iter 39900/1000000  loss         0.146736  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.455\n",
      "iter 39901/1000000  loss         0.146736  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.455\n",
      "iter 40000/1000000  loss         0.146732  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.456\n",
      "iter 40001/1000000  loss         0.146732  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.456\n",
      "iter 40100/1000000  loss         0.146728  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.457\n",
      "iter 40101/1000000  loss         0.146728  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.457\n",
      "iter 40200/1000000  loss         0.146723  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.458\n",
      "iter 40201/1000000  loss         0.146723  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.458\n",
      "iter 40300/1000000  loss         0.146719  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.460\n",
      "iter 40301/1000000  loss         0.146719  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.460\n",
      "iter 40400/1000000  loss         0.146715  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.461\n",
      "iter 40401/1000000  loss         0.146715  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.461\n",
      "iter 40500/1000000  loss         0.146711  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.462\n",
      "iter 40501/1000000  loss         0.146711  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.462\n",
      "iter 40600/1000000  loss         0.146707  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.463\n",
      "iter 40601/1000000  loss         0.146707  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.463\n",
      "iter 40700/1000000  loss         0.146703  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.464\n",
      "iter 40701/1000000  loss         0.146703  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.464\n",
      "iter 40800/1000000  loss         0.146699  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.466\n",
      "iter 40801/1000000  loss         0.146699  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.466\n",
      "iter 40900/1000000  loss         0.146696  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.467\n",
      "iter 40901/1000000  loss         0.146695  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.467\n",
      "iter 41000/1000000  loss         0.146692  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.468\n",
      "iter 41001/1000000  loss         0.146692  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.468\n",
      "iter 41100/1000000  loss         0.146688  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.469\n",
      "iter 41101/1000000  loss         0.146688  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.469\n",
      "iter 41200/1000000  loss         0.146684  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.470\n",
      "iter 41201/1000000  loss         0.146684  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.470\n",
      "iter 41300/1000000  loss         0.146680  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.472\n",
      "iter 41301/1000000  loss         0.146680  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.472\n",
      "iter 41400/1000000  loss         0.146676  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.473\n",
      "iter 41401/1000000  loss         0.146676  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.473\n",
      "iter 41500/1000000  loss         0.146672  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.474\n",
      "iter 41501/1000000  loss         0.146672  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.474\n",
      "iter 41600/1000000  loss         0.146668  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.475\n",
      "iter 41601/1000000  loss         0.146668  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.475\n",
      "iter 41700/1000000  loss         0.146665  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.476\n",
      "iter 41701/1000000  loss         0.146665  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.476\n",
      "iter 41800/1000000  loss         0.146661  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.478\n",
      "iter 41801/1000000  loss         0.146661  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.478\n",
      "iter 41900/1000000  loss         0.146657  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 41901/1000000  loss         0.146657  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.479\n",
      "iter 42000/1000000  loss         0.146653  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.480\n",
      "iter 42001/1000000  loss         0.146653  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.480\n",
      "iter 42100/1000000  loss         0.146650  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.481\n",
      "iter 42101/1000000  loss         0.146650  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.481\n",
      "iter 42200/1000000  loss         0.146646  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.482\n",
      "iter 42201/1000000  loss         0.146646  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.482\n",
      "iter 42300/1000000  loss         0.146642  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.483\n",
      "iter 42301/1000000  loss         0.146642  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.483\n",
      "iter 42400/1000000  loss         0.146639  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.485\n",
      "iter 42401/1000000  loss         0.146639  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.485\n",
      "iter 42500/1000000  loss         0.146635  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.486\n",
      "iter 42501/1000000  loss         0.146635  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.486\n",
      "iter 42600/1000000  loss         0.146631  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.487\n",
      "iter 42601/1000000  loss         0.146631  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.487\n",
      "iter 42700/1000000  loss         0.146628  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.488\n",
      "iter 42701/1000000  loss         0.146628  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.488\n",
      "iter 42800/1000000  loss         0.146624  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.489\n",
      "iter 42801/1000000  loss         0.146624  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.489\n",
      "iter 42900/1000000  loss         0.146620  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.491\n",
      "iter 42901/1000000  loss         0.146620  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.491\n",
      "iter 43000/1000000  loss         0.146617  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.492\n",
      "iter 43001/1000000  loss         0.146617  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.492\n",
      "iter 43100/1000000  loss         0.146613  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.493\n",
      "iter 43101/1000000  loss         0.146613  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.493\n",
      "iter 43200/1000000  loss         0.146610  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.494\n",
      "iter 43201/1000000  loss         0.146610  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.494\n",
      "iter 43300/1000000  loss         0.146606  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.495\n",
      "iter 43301/1000000  loss         0.146606  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.495\n",
      "iter 43400/1000000  loss         0.146603  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.496\n",
      "iter 43401/1000000  loss         0.146603  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.496\n",
      "iter 43500/1000000  loss         0.146599  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.498\n",
      "iter 43501/1000000  loss         0.146599  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.498\n",
      "iter 43600/1000000  loss         0.146596  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.499\n",
      "iter 43601/1000000  loss         0.146596  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.499\n",
      "iter 43700/1000000  loss         0.146592  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.500\n",
      "iter 43701/1000000  loss         0.146592  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.500\n",
      "iter 43800/1000000  loss         0.146589  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.501\n",
      "iter 43801/1000000  loss         0.146589  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.501\n",
      "iter 43900/1000000  loss         0.146586  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.502\n",
      "iter 43901/1000000  loss         0.146585  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.502\n",
      "iter 44000/1000000  loss         0.146582  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.503\n",
      "iter 44001/1000000  loss         0.146582  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.503\n",
      "iter 44100/1000000  loss         0.146579  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.504\n",
      "iter 44101/1000000  loss         0.146579  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.505\n",
      "iter 44200/1000000  loss         0.146575  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.506\n",
      "iter 44201/1000000  loss         0.146575  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.506\n",
      "iter 44300/1000000  loss         0.146572  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.507\n",
      "iter 44301/1000000  loss         0.146572  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.507\n",
      "iter 44400/1000000  loss         0.146569  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.508\n",
      "iter 44401/1000000  loss         0.146569  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.508\n",
      "iter 44500/1000000  loss         0.146565  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.509\n",
      "iter 44501/1000000  loss         0.146565  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.509\n",
      "iter 44600/1000000  loss         0.146562  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.510\n",
      "iter 44601/1000000  loss         0.146562  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.510\n",
      "iter 44700/1000000  loss         0.146559  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.511\n",
      "iter 44701/1000000  loss         0.146559  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.511\n",
      "iter 44800/1000000  loss         0.146555  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.513\n",
      "iter 44801/1000000  loss         0.146555  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.513\n",
      "iter 44900/1000000  loss         0.146552  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.514\n",
      "iter 44901/1000000  loss         0.146552  avg_L1_norm_grad         0.000002  w[0]    0.005 bias    1.514\n",
      "iter 45000/1000000  loss         0.146549  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.515\n",
      "iter 45001/1000000  loss         0.146549  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.515\n",
      "iter 45100/1000000  loss         0.146546  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.516\n",
      "iter 45101/1000000  loss         0.146546  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.516\n",
      "iter 45200/1000000  loss         0.146542  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.517\n",
      "iter 45201/1000000  loss         0.146542  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.517\n",
      "iter 45300/1000000  loss         0.146539  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.518\n",
      "iter 45301/1000000  loss         0.146539  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.518\n",
      "iter 45400/1000000  loss         0.146536  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.519\n",
      "iter 45401/1000000  loss         0.146536  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.519\n",
      "iter 45500/1000000  loss         0.146533  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.521\n",
      "iter 45501/1000000  loss         0.146533  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.521\n",
      "iter 45600/1000000  loss         0.146530  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.522\n",
      "iter 45601/1000000  loss         0.146530  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.522\n",
      "iter 45700/1000000  loss         0.146527  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.523\n",
      "iter 45701/1000000  loss         0.146526  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.523\n",
      "iter 45800/1000000  loss         0.146523  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 45801/1000000  loss         0.146523  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.524\n",
      "iter 45900/1000000  loss         0.146520  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.525\n",
      "iter 45901/1000000  loss         0.146520  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.525\n",
      "iter 46000/1000000  loss         0.146517  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.526\n",
      "iter 46001/1000000  loss         0.146517  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.526\n",
      "iter 46100/1000000  loss         0.146514  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.527\n",
      "iter 46101/1000000  loss         0.146514  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.527\n",
      "iter 46200/1000000  loss         0.146511  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.528\n",
      "iter 46201/1000000  loss         0.146511  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.528\n",
      "iter 46300/1000000  loss         0.146508  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.530\n",
      "iter 46301/1000000  loss         0.146508  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.530\n",
      "iter 46400/1000000  loss         0.146505  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.531\n",
      "iter 46401/1000000  loss         0.146505  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.531\n",
      "iter 46500/1000000  loss         0.146502  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.532\n",
      "iter 46501/1000000  loss         0.146502  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.532\n",
      "iter 46600/1000000  loss         0.146499  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.533\n",
      "iter 46601/1000000  loss         0.146499  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.533\n",
      "iter 46700/1000000  loss         0.146496  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.534\n",
      "iter 46701/1000000  loss         0.146496  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.534\n",
      "iter 46800/1000000  loss         0.146493  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.535\n",
      "iter 46801/1000000  loss         0.146493  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.535\n",
      "iter 46900/1000000  loss         0.146490  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.536\n",
      "iter 46901/1000000  loss         0.146490  avg_L1_norm_grad         0.000001  w[0]    0.005 bias    1.536\n",
      "iter 47000/1000000  loss         0.146487  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.537\n",
      "iter 47001/1000000  loss         0.146487  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.537\n",
      "iter 47100/1000000  loss         0.146484  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.539\n",
      "iter 47101/1000000  loss         0.146484  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.539\n",
      "iter 47200/1000000  loss         0.146481  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.540\n",
      "iter 47201/1000000  loss         0.146481  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.540\n",
      "iter 47300/1000000  loss         0.146478  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.541\n",
      "iter 47301/1000000  loss         0.146478  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.541\n",
      "iter 47400/1000000  loss         0.146475  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.542\n",
      "iter 47401/1000000  loss         0.146475  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.542\n",
      "iter 47500/1000000  loss         0.146472  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.543\n",
      "iter 47501/1000000  loss         0.146472  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.543\n",
      "iter 47600/1000000  loss         0.146469  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.544\n",
      "iter 47601/1000000  loss         0.146469  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.544\n",
      "iter 47700/1000000  loss         0.146467  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.545\n",
      "iter 47701/1000000  loss         0.146466  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.545\n",
      "iter 47800/1000000  loss         0.146464  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.546\n",
      "iter 47801/1000000  loss         0.146464  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.546\n",
      "iter 47900/1000000  loss         0.146461  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.547\n",
      "iter 47901/1000000  loss         0.146461  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.547\n",
      "iter 48000/1000000  loss         0.146458  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.548\n",
      "iter 48001/1000000  loss         0.146458  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.549\n",
      "iter 48100/1000000  loss         0.146455  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.550\n",
      "iter 48101/1000000  loss         0.146455  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.550\n",
      "iter 48200/1000000  loss         0.146452  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.551\n",
      "iter 48201/1000000  loss         0.146452  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.551\n",
      "iter 48300/1000000  loss         0.146450  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.552\n",
      "iter 48301/1000000  loss         0.146450  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.552\n",
      "iter 48400/1000000  loss         0.146447  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.553\n",
      "iter 48401/1000000  loss         0.146447  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.553\n",
      "iter 48500/1000000  loss         0.146444  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.554\n",
      "iter 48501/1000000  loss         0.146444  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.554\n",
      "iter 48600/1000000  loss         0.146441  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.555\n",
      "iter 48601/1000000  loss         0.146441  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.555\n",
      "iter 48700/1000000  loss         0.146439  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.556\n",
      "iter 48701/1000000  loss         0.146438  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.556\n",
      "iter 48800/1000000  loss         0.146436  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.557\n",
      "iter 48801/1000000  loss         0.146436  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.557\n",
      "iter 48900/1000000  loss         0.146433  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.558\n",
      "iter 48901/1000000  loss         0.146433  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.558\n",
      "iter 49000/1000000  loss         0.146430  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.559\n",
      "iter 49001/1000000  loss         0.146430  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.559\n",
      "iter 49100/1000000  loss         0.146428  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.560\n",
      "iter 49101/1000000  loss         0.146428  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.560\n",
      "iter 49200/1000000  loss         0.146425  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.562\n",
      "iter 49201/1000000  loss         0.146425  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.562\n",
      "iter 49300/1000000  loss         0.146422  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.563\n",
      "iter 49301/1000000  loss         0.146422  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.563\n",
      "iter 49400/1000000  loss         0.146420  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.564\n",
      "iter 49401/1000000  loss         0.146420  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.564\n",
      "iter 49500/1000000  loss         0.146417  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.565\n",
      "iter 49501/1000000  loss         0.146417  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.565\n",
      "iter 49600/1000000  loss         0.146414  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.566\n",
      "iter 49601/1000000  loss         0.146414  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.566\n",
      "iter 49700/1000000  loss         0.146412  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 49701/1000000  loss         0.146412  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.567\n",
      "iter 49800/1000000  loss         0.146409  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.568\n",
      "iter 49801/1000000  loss         0.146409  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.568\n",
      "iter 49900/1000000  loss         0.146407  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.569\n",
      "iter 49901/1000000  loss         0.146407  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.569\n",
      "iter 50000/1000000  loss         0.146404  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.570\n",
      "iter 50001/1000000  loss         0.146404  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.570\n",
      "iter 50100/1000000  loss         0.146401  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.571\n",
      "iter 50101/1000000  loss         0.146401  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.571\n",
      "iter 50200/1000000  loss         0.146399  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.572\n",
      "iter 50201/1000000  loss         0.146399  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.572\n",
      "iter 50300/1000000  loss         0.146396  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.573\n",
      "iter 50301/1000000  loss         0.146396  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.573\n",
      "iter 50400/1000000  loss         0.146394  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.574\n",
      "iter 50401/1000000  loss         0.146394  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.574\n",
      "iter 50500/1000000  loss         0.146391  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.575\n",
      "iter 50501/1000000  loss         0.146391  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.575\n",
      "iter 50600/1000000  loss         0.146389  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.576\n",
      "iter 50601/1000000  loss         0.146389  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.576\n",
      "iter 50700/1000000  loss         0.146386  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.578\n",
      "iter 50701/1000000  loss         0.146386  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.578\n",
      "iter 50800/1000000  loss         0.146384  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.579\n",
      "iter 50801/1000000  loss         0.146384  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.579\n",
      "iter 50900/1000000  loss         0.146381  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.580\n",
      "iter 50901/1000000  loss         0.146381  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.580\n",
      "iter 51000/1000000  loss         0.146379  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.581\n",
      "iter 51001/1000000  loss         0.146379  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.581\n",
      "iter 51100/1000000  loss         0.146376  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.582\n",
      "iter 51101/1000000  loss         0.146376  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.582\n",
      "iter 51200/1000000  loss         0.146374  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.583\n",
      "iter 51201/1000000  loss         0.146374  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.583\n",
      "iter 51300/1000000  loss         0.146372  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.584\n",
      "iter 51301/1000000  loss         0.146372  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.584\n",
      "iter 51400/1000000  loss         0.146369  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.585\n",
      "iter 51401/1000000  loss         0.146369  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.585\n",
      "iter 51500/1000000  loss         0.146367  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.586\n",
      "iter 51501/1000000  loss         0.146367  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.586\n",
      "iter 51600/1000000  loss         0.146364  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.587\n",
      "iter 51601/1000000  loss         0.146364  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.587\n",
      "iter 51700/1000000  loss         0.146362  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.588\n",
      "iter 51701/1000000  loss         0.146362  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.588\n",
      "iter 51800/1000000  loss         0.146360  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.589\n",
      "iter 51801/1000000  loss         0.146360  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.589\n",
      "iter 51900/1000000  loss         0.146357  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.590\n",
      "iter 51901/1000000  loss         0.146357  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.590\n",
      "iter 52000/1000000  loss         0.146355  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.591\n",
      "iter 52001/1000000  loss         0.146355  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.591\n",
      "iter 52100/1000000  loss         0.146352  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.592\n",
      "iter 52101/1000000  loss         0.146352  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.592\n",
      "iter 52200/1000000  loss         0.146350  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.593\n",
      "iter 52201/1000000  loss         0.146350  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.593\n",
      "iter 52300/1000000  loss         0.146348  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.594\n",
      "iter 52301/1000000  loss         0.146348  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.594\n",
      "iter 52400/1000000  loss         0.146346  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.595\n",
      "iter 52401/1000000  loss         0.146345  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.595\n",
      "iter 52500/1000000  loss         0.146343  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.596\n",
      "iter 52501/1000000  loss         0.146343  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.596\n",
      "iter 52600/1000000  loss         0.146341  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.597\n",
      "iter 52601/1000000  loss         0.146341  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.597\n",
      "iter 52700/1000000  loss         0.146339  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.598\n",
      "iter 52701/1000000  loss         0.146339  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.598\n",
      "iter 52800/1000000  loss         0.146336  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.599\n",
      "iter 52801/1000000  loss         0.146336  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.599\n",
      "iter 52900/1000000  loss         0.146334  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.600\n",
      "iter 52901/1000000  loss         0.146334  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.600\n",
      "iter 53000/1000000  loss         0.146332  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.601\n",
      "iter 53001/1000000  loss         0.146332  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.601\n",
      "iter 53100/1000000  loss         0.146330  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.602\n",
      "iter 53101/1000000  loss         0.146330  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.602\n",
      "iter 53200/1000000  loss         0.146327  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.603\n",
      "iter 53201/1000000  loss         0.146327  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.603\n",
      "iter 53300/1000000  loss         0.146325  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.604\n",
      "iter 53301/1000000  loss         0.146325  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.604\n",
      "iter 53400/1000000  loss         0.146323  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.605\n",
      "iter 53401/1000000  loss         0.146323  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.605\n",
      "iter 53500/1000000  loss         0.146321  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.606\n",
      "iter 53501/1000000  loss         0.146321  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.606\n",
      "iter 53600/1000000  loss         0.146319  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 53601/1000000  loss         0.146319  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.607\n",
      "iter 53700/1000000  loss         0.146316  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.608\n",
      "iter 53701/1000000  loss         0.146316  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.608\n",
      "iter 53800/1000000  loss         0.146314  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.609\n",
      "iter 53801/1000000  loss         0.146314  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.609\n",
      "iter 53900/1000000  loss         0.146312  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.610\n",
      "iter 53901/1000000  loss         0.146312  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.610\n",
      "iter 54000/1000000  loss         0.146310  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.611\n",
      "iter 54001/1000000  loss         0.146310  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.611\n",
      "iter 54100/1000000  loss         0.146308  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.612\n",
      "iter 54101/1000000  loss         0.146308  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.612\n",
      "iter 54200/1000000  loss         0.146306  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.613\n",
      "iter 54201/1000000  loss         0.146306  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.613\n",
      "iter 54300/1000000  loss         0.146304  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.614\n",
      "iter 54301/1000000  loss         0.146304  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.614\n",
      "iter 54400/1000000  loss         0.146301  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.615\n",
      "iter 54401/1000000  loss         0.146301  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.615\n",
      "iter 54500/1000000  loss         0.146299  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.616\n",
      "iter 54501/1000000  loss         0.146299  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.616\n",
      "iter 54600/1000000  loss         0.146297  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.617\n",
      "iter 54601/1000000  loss         0.146297  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.617\n",
      "iter 54700/1000000  loss         0.146295  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.618\n",
      "iter 54701/1000000  loss         0.146295  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.618\n",
      "iter 54800/1000000  loss         0.146293  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.619\n",
      "iter 54801/1000000  loss         0.146293  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.619\n",
      "iter 54900/1000000  loss         0.146291  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.620\n",
      "iter 54901/1000000  loss         0.146291  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.620\n",
      "iter 55000/1000000  loss         0.146289  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.621\n",
      "iter 55001/1000000  loss         0.146289  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.621\n",
      "iter 55100/1000000  loss         0.146287  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.622\n",
      "iter 55101/1000000  loss         0.146287  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.622\n",
      "iter 55200/1000000  loss         0.146285  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.623\n",
      "iter 55201/1000000  loss         0.146285  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.623\n",
      "iter 55300/1000000  loss         0.146283  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.624\n",
      "iter 55301/1000000  loss         0.146283  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.624\n",
      "iter 55400/1000000  loss         0.146281  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.625\n",
      "iter 55401/1000000  loss         0.146281  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.625\n",
      "iter 55500/1000000  loss         0.146279  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.626\n",
      "iter 55501/1000000  loss         0.146279  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.626\n",
      "iter 55600/1000000  loss         0.146277  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.627\n",
      "iter 55601/1000000  loss         0.146277  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.627\n",
      "iter 55700/1000000  loss         0.146275  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.628\n",
      "iter 55701/1000000  loss         0.146275  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.628\n",
      "iter 55800/1000000  loss         0.146273  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.629\n",
      "iter 55801/1000000  loss         0.146273  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.629\n",
      "iter 55900/1000000  loss         0.146271  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.630\n",
      "iter 55901/1000000  loss         0.146271  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.630\n",
      "iter 56000/1000000  loss         0.146269  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.631\n",
      "iter 56001/1000000  loss         0.146269  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.631\n",
      "iter 56100/1000000  loss         0.146267  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.632\n",
      "iter 56101/1000000  loss         0.146267  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.632\n",
      "iter 56200/1000000  loss         0.146265  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.633\n",
      "iter 56201/1000000  loss         0.146265  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.633\n",
      "iter 56300/1000000  loss         0.146263  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.634\n",
      "iter 56301/1000000  loss         0.146263  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.634\n",
      "iter 56400/1000000  loss         0.146261  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.635\n",
      "iter 56401/1000000  loss         0.146261  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.635\n",
      "iter 56500/1000000  loss         0.146259  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.636\n",
      "iter 56501/1000000  loss         0.146259  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.636\n",
      "iter 56600/1000000  loss         0.146257  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.637\n",
      "iter 56601/1000000  loss         0.146257  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.637\n",
      "iter 56700/1000000  loss         0.146256  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.638\n",
      "iter 56701/1000000  loss         0.146256  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.638\n",
      "iter 56800/1000000  loss         0.146254  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.639\n",
      "iter 56801/1000000  loss         0.146254  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.639\n",
      "iter 56900/1000000  loss         0.146252  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.640\n",
      "iter 56901/1000000  loss         0.146252  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.640\n",
      "iter 57000/1000000  loss         0.146250  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.640\n",
      "iter 57001/1000000  loss         0.146250  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.641\n",
      "iter 57100/1000000  loss         0.146248  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.641\n",
      "iter 57101/1000000  loss         0.146248  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.641\n",
      "iter 57200/1000000  loss         0.146246  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.642\n",
      "iter 57201/1000000  loss         0.146246  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.642\n",
      "iter 57300/1000000  loss         0.146244  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.643\n",
      "iter 57301/1000000  loss         0.146244  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.643\n",
      "iter 57400/1000000  loss         0.146243  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.644\n",
      "iter 57401/1000000  loss         0.146243  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.644\n",
      "iter 57500/1000000  loss         0.146241  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 57501/1000000  loss         0.146241  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.645\n",
      "iter 57600/1000000  loss         0.146239  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.646\n",
      "iter 57601/1000000  loss         0.146239  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.646\n",
      "iter 57700/1000000  loss         0.146237  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.647\n",
      "iter 57701/1000000  loss         0.146237  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.647\n",
      "iter 57800/1000000  loss         0.146235  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.648\n",
      "iter 57801/1000000  loss         0.146235  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.648\n",
      "iter 57900/1000000  loss         0.146234  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.649\n",
      "iter 57901/1000000  loss         0.146233  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.649\n",
      "iter 58000/1000000  loss         0.146232  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.650\n",
      "iter 58001/1000000  loss         0.146232  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.650\n",
      "iter 58100/1000000  loss         0.146230  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.651\n",
      "iter 58101/1000000  loss         0.146230  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.651\n",
      "iter 58200/1000000  loss         0.146228  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.652\n",
      "iter 58201/1000000  loss         0.146228  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.652\n",
      "iter 58300/1000000  loss         0.146226  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.653\n",
      "iter 58301/1000000  loss         0.146226  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.653\n",
      "iter 58400/1000000  loss         0.146225  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.654\n",
      "iter 58401/1000000  loss         0.146225  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.654\n",
      "iter 58500/1000000  loss         0.146223  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.654\n",
      "iter 58501/1000000  loss         0.146223  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.654\n",
      "iter 58600/1000000  loss         0.146221  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.655\n",
      "iter 58601/1000000  loss         0.146221  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.655\n",
      "iter 58700/1000000  loss         0.146219  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.656\n",
      "iter 58701/1000000  loss         0.146219  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.656\n",
      "iter 58800/1000000  loss         0.146218  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.657\n",
      "iter 58801/1000000  loss         0.146218  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.657\n",
      "iter 58900/1000000  loss         0.146216  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.658\n",
      "iter 58901/1000000  loss         0.146216  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.658\n",
      "iter 59000/1000000  loss         0.146214  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.659\n",
      "iter 59001/1000000  loss         0.146214  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.659\n",
      "iter 59100/1000000  loss         0.146213  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.660\n",
      "iter 59101/1000000  loss         0.146213  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.660\n",
      "iter 59200/1000000  loss         0.146211  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.661\n",
      "iter 59201/1000000  loss         0.146211  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.661\n",
      "iter 59300/1000000  loss         0.146209  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.662\n",
      "iter 59301/1000000  loss         0.146209  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.662\n",
      "iter 59400/1000000  loss         0.146208  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.663\n",
      "iter 59401/1000000  loss         0.146208  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.663\n",
      "iter 59500/1000000  loss         0.146206  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.664\n",
      "iter 59501/1000000  loss         0.146206  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.664\n",
      "iter 59600/1000000  loss         0.146204  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.664\n",
      "iter 59601/1000000  loss         0.146204  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.665\n",
      "iter 59700/1000000  loss         0.146203  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.665\n",
      "iter 59701/1000000  loss         0.146203  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.665\n",
      "iter 59800/1000000  loss         0.146201  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.666\n",
      "iter 59801/1000000  loss         0.146201  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.666\n",
      "iter 59900/1000000  loss         0.146199  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.667\n",
      "iter 59901/1000000  loss         0.146199  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.667\n",
      "iter 60000/1000000  loss         0.146198  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.668\n",
      "iter 60001/1000000  loss         0.146198  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.668\n",
      "iter 60100/1000000  loss         0.146196  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.669\n",
      "iter 60101/1000000  loss         0.146196  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.669\n",
      "iter 60200/1000000  loss         0.146194  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.670\n",
      "iter 60201/1000000  loss         0.146194  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.670\n",
      "iter 60300/1000000  loss         0.146193  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.671\n",
      "iter 60301/1000000  loss         0.146193  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.671\n",
      "iter 60400/1000000  loss         0.146191  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.672\n",
      "iter 60401/1000000  loss         0.146191  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.672\n",
      "iter 60500/1000000  loss         0.146190  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.673\n",
      "iter 60501/1000000  loss         0.146190  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.673\n",
      "iter 60600/1000000  loss         0.146188  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.673\n",
      "iter 60601/1000000  loss         0.146188  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.673\n",
      "iter 60700/1000000  loss         0.146187  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.674\n",
      "iter 60701/1000000  loss         0.146186  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.674\n",
      "iter 60800/1000000  loss         0.146185  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.675\n",
      "iter 60801/1000000  loss         0.146185  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.675\n",
      "iter 60900/1000000  loss         0.146183  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.676\n",
      "iter 60901/1000000  loss         0.146183  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.676\n",
      "iter 61000/1000000  loss         0.146182  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.677\n",
      "iter 61001/1000000  loss         0.146182  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.677\n",
      "iter 61100/1000000  loss         0.146180  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.678\n",
      "iter 61101/1000000  loss         0.146180  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.678\n",
      "iter 61200/1000000  loss         0.146179  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.679\n",
      "iter 61201/1000000  loss         0.146179  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.679\n",
      "iter 61300/1000000  loss         0.146177  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.680\n",
      "iter 61301/1000000  loss         0.146177  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.680\n",
      "iter 61400/1000000  loss         0.146176  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 61401/1000000  loss         0.146176  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.680\n",
      "iter 61500/1000000  loss         0.146174  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.681\n",
      "iter 61501/1000000  loss         0.146174  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.681\n",
      "iter 61600/1000000  loss         0.146173  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.682\n",
      "iter 61601/1000000  loss         0.146173  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.682\n",
      "iter 61700/1000000  loss         0.146171  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.683\n",
      "iter 61701/1000000  loss         0.146171  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.683\n",
      "iter 61800/1000000  loss         0.146170  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.684\n",
      "iter 61801/1000000  loss         0.146170  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.684\n",
      "iter 61900/1000000  loss         0.146168  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.685\n",
      "iter 61901/1000000  loss         0.146168  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.685\n",
      "iter 62000/1000000  loss         0.146167  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.686\n",
      "iter 62001/1000000  loss         0.146167  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.686\n",
      "iter 62100/1000000  loss         0.146165  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.686\n",
      "iter 62101/1000000  loss         0.146165  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.687\n",
      "iter 62200/1000000  loss         0.146164  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.687\n",
      "iter 62201/1000000  loss         0.146164  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.687\n",
      "iter 62300/1000000  loss         0.146162  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.688\n",
      "iter 62301/1000000  loss         0.146162  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.688\n",
      "iter 62400/1000000  loss         0.146161  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.689\n",
      "iter 62401/1000000  loss         0.146161  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.689\n",
      "iter 62500/1000000  loss         0.146159  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.690\n",
      "iter 62501/1000000  loss         0.146159  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.690\n",
      "iter 62600/1000000  loss         0.146158  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.691\n",
      "iter 62601/1000000  loss         0.146158  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.691\n",
      "iter 62700/1000000  loss         0.146156  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.692\n",
      "iter 62701/1000000  loss         0.146156  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.692\n",
      "iter 62800/1000000  loss         0.146155  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.692\n",
      "iter 62801/1000000  loss         0.146155  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.692\n",
      "iter 62900/1000000  loss         0.146154  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.693\n",
      "iter 62901/1000000  loss         0.146154  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.693\n",
      "iter 63000/1000000  loss         0.146152  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.694\n",
      "iter 63001/1000000  loss         0.146152  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.694\n",
      "iter 63100/1000000  loss         0.146151  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.695\n",
      "iter 63101/1000000  loss         0.146151  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.695\n",
      "iter 63200/1000000  loss         0.146149  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.696\n",
      "iter 63201/1000000  loss         0.146149  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.696\n",
      "iter 63300/1000000  loss         0.146148  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.697\n",
      "iter 63301/1000000  loss         0.146148  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.697\n",
      "iter 63400/1000000  loss         0.146147  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.698\n",
      "iter 63401/1000000  loss         0.146147  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.698\n",
      "iter 63500/1000000  loss         0.146145  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.698\n",
      "iter 63501/1000000  loss         0.146145  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.698\n",
      "iter 63600/1000000  loss         0.146144  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.699\n",
      "iter 63601/1000000  loss         0.146144  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.699\n",
      "iter 63700/1000000  loss         0.146142  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.700\n",
      "iter 63701/1000000  loss         0.146142  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.700\n",
      "iter 63800/1000000  loss         0.146141  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.701\n",
      "iter 63801/1000000  loss         0.146141  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.701\n",
      "iter 63900/1000000  loss         0.146140  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.702\n",
      "iter 63901/1000000  loss         0.146140  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.702\n",
      "iter 64000/1000000  loss         0.146138  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.703\n",
      "iter 64001/1000000  loss         0.146138  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.703\n",
      "iter 64100/1000000  loss         0.146137  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.703\n",
      "iter 64101/1000000  loss         0.146137  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.703\n",
      "iter 64200/1000000  loss         0.146136  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.704\n",
      "iter 64201/1000000  loss         0.146136  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.704\n",
      "iter 64300/1000000  loss         0.146134  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.705\n",
      "iter 64301/1000000  loss         0.146134  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.705\n",
      "iter 64400/1000000  loss         0.146133  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.706\n",
      "iter 64401/1000000  loss         0.146133  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.706\n",
      "iter 64500/1000000  loss         0.146132  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.707\n",
      "iter 64501/1000000  loss         0.146132  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.707\n",
      "iter 64600/1000000  loss         0.146130  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.707\n",
      "iter 64601/1000000  loss         0.146130  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.707\n",
      "iter 64700/1000000  loss         0.146129  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.708\n",
      "iter 64701/1000000  loss         0.146129  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.708\n",
      "iter 64800/1000000  loss         0.146128  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.709\n",
      "iter 64801/1000000  loss         0.146128  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.709\n",
      "iter 64900/1000000  loss         0.146126  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.710\n",
      "iter 64901/1000000  loss         0.146126  avg_L1_norm_grad         0.000001  w[0]    0.004 bias    1.710\n",
      "iter 65000/1000000  loss         0.146125  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.711\n",
      "iter 65001/1000000  loss         0.146125  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.711\n",
      "iter 65100/1000000  loss         0.146124  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.712\n",
      "iter 65101/1000000  loss         0.146124  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.712\n",
      "iter 65200/1000000  loss         0.146123  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.712\n",
      "iter 65201/1000000  loss         0.146123  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.712\n",
      "iter 65300/1000000  loss         0.146121  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 65301/1000000  loss         0.146121  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.713\n",
      "iter 65400/1000000  loss         0.146120  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.714\n",
      "iter 65401/1000000  loss         0.146120  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.714\n",
      "iter 65500/1000000  loss         0.146119  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.715\n",
      "iter 65501/1000000  loss         0.146119  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.715\n",
      "iter 65600/1000000  loss         0.146117  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.716\n",
      "iter 65601/1000000  loss         0.146117  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.716\n",
      "iter 65700/1000000  loss         0.146116  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.716\n",
      "iter 65701/1000000  loss         0.146116  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.716\n",
      "iter 65800/1000000  loss         0.146115  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.717\n",
      "iter 65801/1000000  loss         0.146115  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.717\n",
      "iter 65900/1000000  loss         0.146114  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.718\n",
      "iter 65901/1000000  loss         0.146114  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.718\n",
      "iter 66000/1000000  loss         0.146112  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.719\n",
      "iter 66001/1000000  loss         0.146112  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.719\n",
      "iter 66100/1000000  loss         0.146111  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.720\n",
      "iter 66101/1000000  loss         0.146111  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.720\n",
      "iter 66200/1000000  loss         0.146110  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.720\n",
      "iter 66201/1000000  loss         0.146110  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.720\n",
      "iter 66300/1000000  loss         0.146109  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.721\n",
      "iter 66301/1000000  loss         0.146109  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.721\n",
      "iter 66400/1000000  loss         0.146108  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.722\n",
      "iter 66401/1000000  loss         0.146108  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.722\n",
      "iter 66500/1000000  loss         0.146106  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.723\n",
      "iter 66501/1000000  loss         0.146106  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.723\n",
      "iter 66600/1000000  loss         0.146105  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.723\n",
      "iter 66601/1000000  loss         0.146105  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.723\n",
      "iter 66700/1000000  loss         0.146104  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.724\n",
      "iter 66701/1000000  loss         0.146104  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.724\n",
      "iter 66800/1000000  loss         0.146103  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.725\n",
      "iter 66801/1000000  loss         0.146103  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.725\n",
      "iter 66900/1000000  loss         0.146102  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.726\n",
      "iter 66901/1000000  loss         0.146102  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.726\n",
      "iter 67000/1000000  loss         0.146100  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.727\n",
      "iter 67001/1000000  loss         0.146100  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.727\n",
      "iter 67100/1000000  loss         0.146099  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.727\n",
      "iter 67101/1000000  loss         0.146099  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.727\n",
      "iter 67200/1000000  loss         0.146098  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.728\n",
      "iter 67201/1000000  loss         0.146098  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.728\n",
      "iter 67300/1000000  loss         0.146097  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.729\n",
      "iter 67301/1000000  loss         0.146097  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.729\n",
      "iter 67400/1000000  loss         0.146096  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.730\n",
      "iter 67401/1000000  loss         0.146096  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.730\n",
      "iter 67500/1000000  loss         0.146095  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.730\n",
      "iter 67501/1000000  loss         0.146095  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.730\n",
      "iter 67600/1000000  loss         0.146093  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.731\n",
      "iter 67601/1000000  loss         0.146093  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.731\n",
      "iter 67700/1000000  loss         0.146092  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.732\n",
      "iter 67701/1000000  loss         0.146092  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.732\n",
      "iter 67800/1000000  loss         0.146091  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.733\n",
      "iter 67801/1000000  loss         0.146091  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.733\n",
      "iter 67900/1000000  loss         0.146090  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.734\n",
      "iter 67901/1000000  loss         0.146090  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.734\n",
      "iter 68000/1000000  loss         0.146089  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.734\n",
      "iter 68001/1000000  loss         0.146089  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.734\n",
      "iter 68100/1000000  loss         0.146088  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.735\n",
      "iter 68101/1000000  loss         0.146088  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.735\n",
      "iter 68200/1000000  loss         0.146087  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.736\n",
      "iter 68201/1000000  loss         0.146087  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.736\n",
      "iter 68300/1000000  loss         0.146086  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.737\n",
      "iter 68301/1000000  loss         0.146086  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.737\n",
      "iter 68400/1000000  loss         0.146084  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.737\n",
      "iter 68401/1000000  loss         0.146084  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.737\n",
      "iter 68500/1000000  loss         0.146083  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.738\n",
      "iter 68501/1000000  loss         0.146083  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.738\n",
      "iter 68600/1000000  loss         0.146082  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.739\n",
      "iter 68601/1000000  loss         0.146082  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.739\n",
      "iter 68700/1000000  loss         0.146081  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.740\n",
      "iter 68701/1000000  loss         0.146081  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.740\n",
      "iter 68800/1000000  loss         0.146080  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.740\n",
      "iter 68801/1000000  loss         0.146080  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.740\n",
      "iter 68900/1000000  loss         0.146079  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.741\n",
      "iter 68901/1000000  loss         0.146079  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.741\n",
      "iter 69000/1000000  loss         0.146078  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.742\n",
      "iter 69001/1000000  loss         0.146078  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.742\n",
      "iter 69100/1000000  loss         0.146077  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.743\n",
      "iter 69101/1000000  loss         0.146077  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.743\n",
      "iter 69200/1000000  loss         0.146076  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69201/1000000  loss         0.146076  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.743\n",
      "iter 69300/1000000  loss         0.146075  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.744\n",
      "iter 69301/1000000  loss         0.146075  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.744\n",
      "iter 69400/1000000  loss         0.146074  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.745\n",
      "iter 69401/1000000  loss         0.146074  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.745\n",
      "iter 69500/1000000  loss         0.146073  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.746\n",
      "iter 69501/1000000  loss         0.146073  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.746\n",
      "iter 69600/1000000  loss         0.146072  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.746\n",
      "iter 69601/1000000  loss         0.146072  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.746\n",
      "iter 69700/1000000  loss         0.146071  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.747\n",
      "iter 69701/1000000  loss         0.146071  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.747\n",
      "iter 69800/1000000  loss         0.146069  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.748\n",
      "iter 69801/1000000  loss         0.146069  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.748\n",
      "iter 69900/1000000  loss         0.146068  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.749\n",
      "iter 69901/1000000  loss         0.146068  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.749\n",
      "iter 70000/1000000  loss         0.146067  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.749\n",
      "iter 70001/1000000  loss         0.146067  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.749\n",
      "iter 70100/1000000  loss         0.146066  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.750\n",
      "iter 70101/1000000  loss         0.146066  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.750\n",
      "iter 70200/1000000  loss         0.146065  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.751\n",
      "iter 70201/1000000  loss         0.146065  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.751\n",
      "iter 70300/1000000  loss         0.146064  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.751\n",
      "iter 70301/1000000  loss         0.146064  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.751\n",
      "iter 70400/1000000  loss         0.146063  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.752\n",
      "iter 70401/1000000  loss         0.146063  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.752\n",
      "iter 70500/1000000  loss         0.146062  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.753\n",
      "iter 70501/1000000  loss         0.146062  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.753\n",
      "iter 70600/1000000  loss         0.146061  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.754\n",
      "iter 70601/1000000  loss         0.146061  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.754\n",
      "iter 70700/1000000  loss         0.146060  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.754\n",
      "iter 70701/1000000  loss         0.146060  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.754\n",
      "iter 70800/1000000  loss         0.146059  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.755\n",
      "iter 70801/1000000  loss         0.146059  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.755\n",
      "iter 70900/1000000  loss         0.146058  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.756\n",
      "iter 70901/1000000  loss         0.146058  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.756\n",
      "iter 71000/1000000  loss         0.146057  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.757\n",
      "iter 71001/1000000  loss         0.146057  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.757\n",
      "iter 71100/1000000  loss         0.146056  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.757\n",
      "iter 71101/1000000  loss         0.146056  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.757\n",
      "iter 71200/1000000  loss         0.146055  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.758\n",
      "iter 71201/1000000  loss         0.146055  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.758\n",
      "iter 71300/1000000  loss         0.146054  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.759\n",
      "iter 71301/1000000  loss         0.146054  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.759\n",
      "iter 71400/1000000  loss         0.146054  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.759\n",
      "iter 71401/1000000  loss         0.146053  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.759\n",
      "iter 71500/1000000  loss         0.146053  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.760\n",
      "iter 71501/1000000  loss         0.146053  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.760\n",
      "iter 71600/1000000  loss         0.146052  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.761\n",
      "iter 71601/1000000  loss         0.146052  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.761\n",
      "iter 71700/1000000  loss         0.146051  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.762\n",
      "iter 71701/1000000  loss         0.146051  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.762\n",
      "iter 71800/1000000  loss         0.146050  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.762\n",
      "iter 71801/1000000  loss         0.146050  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.762\n",
      "iter 71900/1000000  loss         0.146049  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.763\n",
      "iter 71901/1000000  loss         0.146049  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.763\n",
      "iter 72000/1000000  loss         0.146048  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.764\n",
      "iter 72001/1000000  loss         0.146048  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.764\n",
      "iter 72100/1000000  loss         0.146047  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.764\n",
      "iter 72101/1000000  loss         0.146047  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.764\n",
      "iter 72200/1000000  loss         0.146046  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.765\n",
      "iter 72201/1000000  loss         0.146046  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.765\n",
      "iter 72300/1000000  loss         0.146045  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.766\n",
      "iter 72301/1000000  loss         0.146045  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.766\n",
      "iter 72400/1000000  loss         0.146044  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.766\n",
      "iter 72401/1000000  loss         0.146044  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.766\n",
      "iter 72500/1000000  loss         0.146043  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.767\n",
      "iter 72501/1000000  loss         0.146043  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.767\n",
      "iter 72600/1000000  loss         0.146042  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.768\n",
      "iter 72601/1000000  loss         0.146042  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.768\n",
      "iter 72700/1000000  loss         0.146041  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.769\n",
      "iter 72701/1000000  loss         0.146041  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.769\n",
      "iter 72800/1000000  loss         0.146040  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.769\n",
      "iter 72801/1000000  loss         0.146040  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.769\n",
      "iter 72900/1000000  loss         0.146040  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.770\n",
      "iter 72901/1000000  loss         0.146040  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.770\n",
      "iter 73000/1000000  loss         0.146039  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.771\n",
      "iter 73001/1000000  loss         0.146039  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.771\n",
      "iter 73100/1000000  loss         0.146038  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 73101/1000000  loss         0.146038  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.771\n",
      "iter 73200/1000000  loss         0.146037  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.772\n",
      "iter 73201/1000000  loss         0.146037  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.772\n",
      "iter 73300/1000000  loss         0.146036  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.773\n",
      "iter 73301/1000000  loss         0.146036  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.773\n",
      "iter 73400/1000000  loss         0.146035  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.773\n",
      "iter 73401/1000000  loss         0.146035  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.773\n",
      "iter 73500/1000000  loss         0.146034  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.774\n",
      "iter 73501/1000000  loss         0.146034  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.774\n",
      "iter 73600/1000000  loss         0.146033  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.775\n",
      "iter 73601/1000000  loss         0.146033  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.775\n",
      "iter 73700/1000000  loss         0.146032  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.775\n",
      "iter 73701/1000000  loss         0.146032  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.775\n",
      "iter 73800/1000000  loss         0.146032  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.776\n",
      "iter 73801/1000000  loss         0.146032  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.776\n",
      "iter 73900/1000000  loss         0.146031  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.777\n",
      "iter 73901/1000000  loss         0.146031  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.777\n",
      "iter 74000/1000000  loss         0.146030  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.777\n",
      "iter 74001/1000000  loss         0.146030  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.777\n",
      "iter 74100/1000000  loss         0.146029  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.778\n",
      "iter 74101/1000000  loss         0.146029  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.778\n",
      "iter 74200/1000000  loss         0.146028  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.779\n",
      "iter 74201/1000000  loss         0.146028  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.779\n",
      "iter 74300/1000000  loss         0.146027  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.779\n",
      "iter 74301/1000000  loss         0.146027  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.779\n",
      "iter 74400/1000000  loss         0.146027  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.780\n",
      "iter 74401/1000000  loss         0.146027  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.780\n",
      "iter 74500/1000000  loss         0.146026  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.781\n",
      "iter 74501/1000000  loss         0.146026  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.781\n",
      "iter 74600/1000000  loss         0.146025  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.781\n",
      "iter 74601/1000000  loss         0.146025  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.781\n",
      "iter 74700/1000000  loss         0.146024  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.782\n",
      "iter 74701/1000000  loss         0.146024  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.782\n",
      "iter 74800/1000000  loss         0.146023  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.783\n",
      "iter 74801/1000000  loss         0.146023  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.783\n",
      "iter 74900/1000000  loss         0.146022  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.783\n",
      "iter 74901/1000000  loss         0.146022  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.783\n",
      "iter 75000/1000000  loss         0.146022  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.784\n",
      "iter 75001/1000000  loss         0.146022  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.784\n",
      "iter 75100/1000000  loss         0.146021  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.785\n",
      "iter 75101/1000000  loss         0.146021  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.785\n",
      "iter 75200/1000000  loss         0.146020  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.785\n",
      "iter 75201/1000000  loss         0.146020  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.785\n",
      "iter 75300/1000000  loss         0.146019  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.786\n",
      "iter 75301/1000000  loss         0.146019  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.786\n",
      "iter 75400/1000000  loss         0.146018  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.787\n",
      "iter 75401/1000000  loss         0.146018  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.787\n",
      "iter 75500/1000000  loss         0.146018  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.787\n",
      "iter 75501/1000000  loss         0.146018  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.787\n",
      "iter 75600/1000000  loss         0.146017  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.788\n",
      "iter 75601/1000000  loss         0.146017  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.788\n",
      "iter 75700/1000000  loss         0.146016  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.789\n",
      "iter 75701/1000000  loss         0.146016  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.789\n",
      "iter 75800/1000000  loss         0.146015  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.789\n",
      "iter 75801/1000000  loss         0.146015  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.789\n",
      "iter 75900/1000000  loss         0.146014  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.790\n",
      "iter 75901/1000000  loss         0.146014  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.790\n",
      "iter 76000/1000000  loss         0.146014  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.791\n",
      "iter 76001/1000000  loss         0.146014  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.791\n",
      "iter 76100/1000000  loss         0.146013  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.791\n",
      "iter 76101/1000000  loss         0.146013  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.791\n",
      "iter 76200/1000000  loss         0.146012  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.792\n",
      "iter 76201/1000000  loss         0.146012  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.792\n",
      "iter 76300/1000000  loss         0.146011  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.793\n",
      "iter 76301/1000000  loss         0.146011  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.793\n",
      "iter 76400/1000000  loss         0.146011  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.793\n",
      "iter 76401/1000000  loss         0.146010  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.793\n",
      "iter 76500/1000000  loss         0.146010  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.794\n",
      "iter 76501/1000000  loss         0.146010  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.794\n",
      "iter 76600/1000000  loss         0.146009  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.794\n",
      "iter 76601/1000000  loss         0.146009  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.794\n",
      "iter 76700/1000000  loss         0.146008  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.795\n",
      "iter 76701/1000000  loss         0.146008  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.795\n",
      "iter 76800/1000000  loss         0.146007  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.796\n",
      "iter 76801/1000000  loss         0.146007  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.796\n",
      "iter 76900/1000000  loss         0.146007  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.796\n",
      "iter 76901/1000000  loss         0.146007  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.796\n",
      "iter 77000/1000000  loss         0.146006  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 77001/1000000  loss         0.146006  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.797\n",
      "iter 77100/1000000  loss         0.146005  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.798\n",
      "iter 77101/1000000  loss         0.146005  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.798\n",
      "iter 77200/1000000  loss         0.146004  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.798\n",
      "iter 77201/1000000  loss         0.146004  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.798\n",
      "iter 77300/1000000  loss         0.146004  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.799\n",
      "iter 77301/1000000  loss         0.146004  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.799\n",
      "iter 77400/1000000  loss         0.146003  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.800\n",
      "iter 77401/1000000  loss         0.146003  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.800\n",
      "iter 77500/1000000  loss         0.146002  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.800\n",
      "iter 77501/1000000  loss         0.146002  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.800\n",
      "iter 77600/1000000  loss         0.146002  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.801\n",
      "iter 77601/1000000  loss         0.146002  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.801\n",
      "iter 77700/1000000  loss         0.146001  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.801\n",
      "iter 77701/1000000  loss         0.146001  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.801\n",
      "iter 77800/1000000  loss         0.146000  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.802\n",
      "iter 77801/1000000  loss         0.146000  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.802\n",
      "iter 77900/1000000  loss         0.145999  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.803\n",
      "iter 77901/1000000  loss         0.145999  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.803\n",
      "iter 78000/1000000  loss         0.145999  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.803\n",
      "iter 78001/1000000  loss         0.145999  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.803\n",
      "iter 78100/1000000  loss         0.145998  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.804\n",
      "iter 78101/1000000  loss         0.145998  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.804\n",
      "iter 78200/1000000  loss         0.145997  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.805\n",
      "iter 78201/1000000  loss         0.145997  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.805\n",
      "iter 78300/1000000  loss         0.145997  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.805\n",
      "iter 78301/1000000  loss         0.145997  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.805\n",
      "iter 78400/1000000  loss         0.145996  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.806\n",
      "iter 78401/1000000  loss         0.145996  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.806\n",
      "iter 78500/1000000  loss         0.145995  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.806\n",
      "iter 78501/1000000  loss         0.145995  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.806\n",
      "iter 78600/1000000  loss         0.145994  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.807\n",
      "iter 78601/1000000  loss         0.145994  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.807\n",
      "iter 78700/1000000  loss         0.145994  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.808\n",
      "iter 78701/1000000  loss         0.145994  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.808\n",
      "iter 78800/1000000  loss         0.145993  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.808\n",
      "iter 78801/1000000  loss         0.145993  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.808\n",
      "iter 78900/1000000  loss         0.145992  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.809\n",
      "iter 78901/1000000  loss         0.145992  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.809\n",
      "iter 79000/1000000  loss         0.145992  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.809\n",
      "iter 79001/1000000  loss         0.145992  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.809\n",
      "iter 79100/1000000  loss         0.145991  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.810\n",
      "iter 79101/1000000  loss         0.145991  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.810\n",
      "iter 79200/1000000  loss         0.145990  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.811\n",
      "iter 79201/1000000  loss         0.145990  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.811\n",
      "iter 79300/1000000  loss         0.145990  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.811\n",
      "iter 79301/1000000  loss         0.145990  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.811\n",
      "iter 79400/1000000  loss         0.145989  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.812\n",
      "iter 79401/1000000  loss         0.145989  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.812\n",
      "iter 79500/1000000  loss         0.145988  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.812\n",
      "iter 79501/1000000  loss         0.145988  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.812\n",
      "iter 79600/1000000  loss         0.145988  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.813\n",
      "iter 79601/1000000  loss         0.145988  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.813\n",
      "iter 79700/1000000  loss         0.145987  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.814\n",
      "iter 79701/1000000  loss         0.145987  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.814\n",
      "iter 79800/1000000  loss         0.145986  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.814\n",
      "iter 79801/1000000  loss         0.145986  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.814\n",
      "iter 79900/1000000  loss         0.145986  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.815\n",
      "iter 79901/1000000  loss         0.145986  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.815\n",
      "iter 80000/1000000  loss         0.145985  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.815\n",
      "iter 80001/1000000  loss         0.145985  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.815\n",
      "iter 80100/1000000  loss         0.145984  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.816\n",
      "iter 80101/1000000  loss         0.145984  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.816\n",
      "iter 80200/1000000  loss         0.145984  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.817\n",
      "iter 80201/1000000  loss         0.145984  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.817\n",
      "iter 80300/1000000  loss         0.145983  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.817\n",
      "iter 80301/1000000  loss         0.145983  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.817\n",
      "iter 80400/1000000  loss         0.145983  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.818\n",
      "iter 80401/1000000  loss         0.145983  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.818\n",
      "iter 80500/1000000  loss         0.145982  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.818\n",
      "iter 80501/1000000  loss         0.145982  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.818\n",
      "iter 80600/1000000  loss         0.145981  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.819\n",
      "iter 80601/1000000  loss         0.145981  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.819\n",
      "iter 80700/1000000  loss         0.145981  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.820\n",
      "iter 80701/1000000  loss         0.145981  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.820\n",
      "iter 80800/1000000  loss         0.145980  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.820\n",
      "iter 80801/1000000  loss         0.145980  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.820\n",
      "iter 80900/1000000  loss         0.145979  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 80901/1000000  loss         0.145979  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.821\n",
      "iter 81000/1000000  loss         0.145979  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.821\n",
      "iter 81001/1000000  loss         0.145979  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.821\n",
      "iter 81100/1000000  loss         0.145978  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.822\n",
      "iter 81101/1000000  loss         0.145978  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.822\n",
      "iter 81200/1000000  loss         0.145978  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.822\n",
      "iter 81201/1000000  loss         0.145978  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.822\n",
      "iter 81300/1000000  loss         0.145977  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.823\n",
      "iter 81301/1000000  loss         0.145977  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.823\n",
      "iter 81400/1000000  loss         0.145976  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.824\n",
      "iter 81401/1000000  loss         0.145976  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.824\n",
      "iter 81500/1000000  loss         0.145976  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.824\n",
      "iter 81501/1000000  loss         0.145976  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.824\n",
      "iter 81600/1000000  loss         0.145975  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.825\n",
      "iter 81601/1000000  loss         0.145975  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.825\n",
      "iter 81700/1000000  loss         0.145975  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.825\n",
      "iter 81701/1000000  loss         0.145974  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.825\n",
      "iter 81800/1000000  loss         0.145974  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.826\n",
      "iter 81801/1000000  loss         0.145974  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.826\n",
      "iter 81900/1000000  loss         0.145973  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.826\n",
      "iter 81901/1000000  loss         0.145973  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.826\n",
      "iter 82000/1000000  loss         0.145973  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.827\n",
      "iter 82001/1000000  loss         0.145973  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.827\n",
      "iter 82100/1000000  loss         0.145972  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.828\n",
      "iter 82101/1000000  loss         0.145972  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.828\n",
      "iter 82200/1000000  loss         0.145972  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.828\n",
      "iter 82201/1000000  loss         0.145972  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.828\n",
      "iter 82300/1000000  loss         0.145971  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.829\n",
      "iter 82301/1000000  loss         0.145971  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.829\n",
      "iter 82400/1000000  loss         0.145970  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.829\n",
      "iter 82401/1000000  loss         0.145970  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.829\n",
      "iter 82500/1000000  loss         0.145970  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.830\n",
      "iter 82501/1000000  loss         0.145970  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.830\n",
      "iter 82600/1000000  loss         0.145969  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.830\n",
      "iter 82601/1000000  loss         0.145969  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.830\n",
      "iter 82700/1000000  loss         0.145969  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.831\n",
      "iter 82701/1000000  loss         0.145969  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.831\n",
      "iter 82800/1000000  loss         0.145968  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.831\n",
      "iter 82801/1000000  loss         0.145968  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.831\n",
      "iter 82900/1000000  loss         0.145967  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.832\n",
      "iter 82901/1000000  loss         0.145967  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.832\n",
      "iter 83000/1000000  loss         0.145967  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.833\n",
      "iter 83001/1000000  loss         0.145967  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.833\n",
      "iter 83100/1000000  loss         0.145966  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.833\n",
      "iter 83101/1000000  loss         0.145966  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.833\n",
      "iter 83200/1000000  loss         0.145966  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.834\n",
      "iter 83201/1000000  loss         0.145966  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.834\n",
      "iter 83300/1000000  loss         0.145965  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.834\n",
      "iter 83301/1000000  loss         0.145965  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.834\n",
      "iter 83400/1000000  loss         0.145965  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.835\n",
      "iter 83401/1000000  loss         0.145965  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.835\n",
      "iter 83500/1000000  loss         0.145964  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.835\n",
      "iter 83501/1000000  loss         0.145964  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.835\n",
      "iter 83600/1000000  loss         0.145964  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.836\n",
      "iter 83601/1000000  loss         0.145964  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.836\n",
      "iter 83700/1000000  loss         0.145963  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.836\n",
      "iter 83701/1000000  loss         0.145963  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.836\n",
      "iter 83800/1000000  loss         0.145962  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.837\n",
      "iter 83801/1000000  loss         0.145962  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.837\n",
      "iter 83900/1000000  loss         0.145962  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.838\n",
      "iter 83901/1000000  loss         0.145962  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.838\n",
      "iter 84000/1000000  loss         0.145961  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.838\n",
      "iter 84001/1000000  loss         0.145961  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.838\n",
      "iter 84100/1000000  loss         0.145961  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.839\n",
      "iter 84101/1000000  loss         0.145961  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.839\n",
      "iter 84200/1000000  loss         0.145960  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.839\n",
      "iter 84201/1000000  loss         0.145960  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.839\n",
      "iter 84300/1000000  loss         0.145960  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.840\n",
      "iter 84301/1000000  loss         0.145960  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.840\n",
      "iter 84400/1000000  loss         0.145959  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.840\n",
      "iter 84401/1000000  loss         0.145959  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.840\n",
      "iter 84500/1000000  loss         0.145959  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.841\n",
      "iter 84501/1000000  loss         0.145959  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.841\n",
      "iter 84600/1000000  loss         0.145958  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.841\n",
      "iter 84601/1000000  loss         0.145958  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.841\n",
      "iter 84700/1000000  loss         0.145958  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.842\n",
      "iter 84701/1000000  loss         0.145958  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.842\n",
      "iter 84800/1000000  loss         0.145957  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 84801/1000000  loss         0.145957  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.842\n",
      "iter 84900/1000000  loss         0.145957  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.843\n",
      "iter 84901/1000000  loss         0.145957  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.843\n",
      "iter 85000/1000000  loss         0.145956  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.843\n",
      "iter 85001/1000000  loss         0.145956  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.843\n",
      "iter 85100/1000000  loss         0.145956  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.844\n",
      "iter 85101/1000000  loss         0.145956  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.844\n",
      "iter 85200/1000000  loss         0.145955  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.845\n",
      "iter 85201/1000000  loss         0.145955  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.845\n",
      "iter 85300/1000000  loss         0.145955  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.845\n",
      "iter 85301/1000000  loss         0.145955  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.845\n",
      "iter 85400/1000000  loss         0.145954  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.846\n",
      "iter 85401/1000000  loss         0.145954  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.846\n",
      "iter 85500/1000000  loss         0.145954  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.846\n",
      "iter 85501/1000000  loss         0.145954  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.846\n",
      "iter 85600/1000000  loss         0.145953  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.847\n",
      "iter 85601/1000000  loss         0.145953  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.847\n",
      "iter 85700/1000000  loss         0.145953  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.847\n",
      "iter 85701/1000000  loss         0.145953  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.847\n",
      "iter 85800/1000000  loss         0.145952  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.848\n",
      "iter 85801/1000000  loss         0.145952  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.848\n",
      "iter 85900/1000000  loss         0.145952  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.848\n",
      "iter 85901/1000000  loss         0.145952  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.848\n",
      "iter 86000/1000000  loss         0.145951  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.849\n",
      "iter 86001/1000000  loss         0.145951  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.849\n",
      "iter 86100/1000000  loss         0.145951  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.849\n",
      "iter 86101/1000000  loss         0.145951  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.849\n",
      "iter 86200/1000000  loss         0.145950  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.850\n",
      "iter 86201/1000000  loss         0.145950  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.850\n",
      "iter 86300/1000000  loss         0.145950  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.850\n",
      "iter 86301/1000000  loss         0.145950  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.850\n",
      "iter 86400/1000000  loss         0.145949  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.851\n",
      "iter 86401/1000000  loss         0.145949  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.851\n",
      "iter 86500/1000000  loss         0.145949  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.851\n",
      "iter 86501/1000000  loss         0.145949  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.851\n",
      "iter 86600/1000000  loss         0.145948  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.852\n",
      "iter 86601/1000000  loss         0.145948  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.852\n",
      "iter 86700/1000000  loss         0.145948  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.852\n",
      "iter 86701/1000000  loss         0.145948  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.852\n",
      "iter 86800/1000000  loss         0.145947  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.853\n",
      "iter 86801/1000000  loss         0.145947  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.853\n",
      "iter 86900/1000000  loss         0.145947  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.853\n",
      "iter 86901/1000000  loss         0.145947  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.853\n",
      "iter 87000/1000000  loss         0.145946  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.854\n",
      "iter 87001/1000000  loss         0.145946  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.854\n",
      "iter 87100/1000000  loss         0.145946  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.854\n",
      "iter 87101/1000000  loss         0.145946  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.854\n",
      "iter 87200/1000000  loss         0.145945  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.855\n",
      "iter 87201/1000000  loss         0.145945  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.855\n",
      "iter 87300/1000000  loss         0.145945  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.855\n",
      "iter 87301/1000000  loss         0.145945  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.855\n",
      "iter 87400/1000000  loss         0.145944  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.856\n",
      "iter 87401/1000000  loss         0.145944  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.856\n",
      "iter 87500/1000000  loss         0.145944  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.856\n",
      "iter 87501/1000000  loss         0.145944  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.856\n",
      "iter 87600/1000000  loss         0.145943  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.857\n",
      "iter 87601/1000000  loss         0.145943  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.857\n",
      "iter 87700/1000000  loss         0.145943  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.857\n",
      "iter 87701/1000000  loss         0.145943  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.857\n",
      "iter 87800/1000000  loss         0.145943  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.858\n",
      "iter 87801/1000000  loss         0.145943  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.858\n",
      "iter 87900/1000000  loss         0.145942  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.858\n",
      "iter 87901/1000000  loss         0.145942  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.858\n",
      "iter 88000/1000000  loss         0.145942  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.859\n",
      "iter 88001/1000000  loss         0.145942  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.859\n",
      "iter 88100/1000000  loss         0.145941  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.859\n",
      "iter 88101/1000000  loss         0.145941  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.859\n",
      "iter 88200/1000000  loss         0.145941  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.860\n",
      "iter 88201/1000000  loss         0.145941  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.860\n",
      "iter 88300/1000000  loss         0.145940  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.860\n",
      "iter 88301/1000000  loss         0.145940  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.860\n",
      "iter 88400/1000000  loss         0.145940  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.861\n",
      "iter 88401/1000000  loss         0.145940  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.861\n",
      "iter 88500/1000000  loss         0.145939  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.861\n",
      "iter 88501/1000000  loss         0.145939  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.861\n",
      "iter 88600/1000000  loss         0.145939  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.862\n",
      "iter 88601/1000000  loss         0.145939  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.862\n",
      "iter 88700/1000000  loss         0.145939  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 88701/1000000  loss         0.145939  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.862\n",
      "iter 88800/1000000  loss         0.145938  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.863\n",
      "iter 88801/1000000  loss         0.145938  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.863\n",
      "iter 88900/1000000  loss         0.145938  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.863\n",
      "iter 88901/1000000  loss         0.145938  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.863\n",
      "iter 89000/1000000  loss         0.145937  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.864\n",
      "iter 89001/1000000  loss         0.145937  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.864\n",
      "iter 89100/1000000  loss         0.145937  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.864\n",
      "iter 89101/1000000  loss         0.145937  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.864\n",
      "iter 89200/1000000  loss         0.145936  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.865\n",
      "iter 89201/1000000  loss         0.145936  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.865\n",
      "iter 89300/1000000  loss         0.145936  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.865\n",
      "iter 89301/1000000  loss         0.145936  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.865\n",
      "iter 89400/1000000  loss         0.145936  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.866\n",
      "iter 89401/1000000  loss         0.145936  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.866\n",
      "iter 89500/1000000  loss         0.145935  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.866\n",
      "iter 89501/1000000  loss         0.145935  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.866\n",
      "iter 89600/1000000  loss         0.145935  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.867\n",
      "iter 89601/1000000  loss         0.145935  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.867\n",
      "iter 89700/1000000  loss         0.145934  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.867\n",
      "iter 89701/1000000  loss         0.145934  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.867\n",
      "iter 89800/1000000  loss         0.145934  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.868\n",
      "iter 89801/1000000  loss         0.145934  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.868\n",
      "iter 89900/1000000  loss         0.145933  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.868\n",
      "iter 89901/1000000  loss         0.145933  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.868\n",
      "iter 90000/1000000  loss         0.145933  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.869\n",
      "iter 90001/1000000  loss         0.145933  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.869\n",
      "iter 90100/1000000  loss         0.145933  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.869\n",
      "iter 90101/1000000  loss         0.145933  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.869\n",
      "iter 90200/1000000  loss         0.145932  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.870\n",
      "iter 90201/1000000  loss         0.145932  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.870\n",
      "iter 90300/1000000  loss         0.145932  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.870\n",
      "iter 90301/1000000  loss         0.145932  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.870\n",
      "iter 90400/1000000  loss         0.145931  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.871\n",
      "iter 90401/1000000  loss         0.145931  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.871\n",
      "iter 90500/1000000  loss         0.145931  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.871\n",
      "iter 90501/1000000  loss         0.145931  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.871\n",
      "iter 90600/1000000  loss         0.145931  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.871\n",
      "iter 90601/1000000  loss         0.145931  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.871\n",
      "iter 90700/1000000  loss         0.145930  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.872\n",
      "iter 90701/1000000  loss         0.145930  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.872\n",
      "iter 90800/1000000  loss         0.145930  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.872\n",
      "iter 90801/1000000  loss         0.145930  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.872\n",
      "iter 90900/1000000  loss         0.145929  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.873\n",
      "iter 90901/1000000  loss         0.145929  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.873\n",
      "iter 91000/1000000  loss         0.145929  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.873\n",
      "iter 91001/1000000  loss         0.145929  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.873\n",
      "iter 91100/1000000  loss         0.145929  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.874\n",
      "iter 91101/1000000  loss         0.145929  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.874\n",
      "iter 91200/1000000  loss         0.145928  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.874\n",
      "iter 91201/1000000  loss         0.145928  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.874\n",
      "iter 91300/1000000  loss         0.145928  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.875\n",
      "iter 91301/1000000  loss         0.145928  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.875\n",
      "iter 91400/1000000  loss         0.145928  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.875\n",
      "iter 91401/1000000  loss         0.145928  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.875\n",
      "iter 91500/1000000  loss         0.145927  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.876\n",
      "iter 91501/1000000  loss         0.145927  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.876\n",
      "iter 91600/1000000  loss         0.145927  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.876\n",
      "iter 91601/1000000  loss         0.145927  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.876\n",
      "iter 91700/1000000  loss         0.145926  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.877\n",
      "iter 91701/1000000  loss         0.145926  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.877\n",
      "iter 91800/1000000  loss         0.145926  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.877\n",
      "iter 91801/1000000  loss         0.145926  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.877\n",
      "iter 91900/1000000  loss         0.145926  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.877\n",
      "iter 91901/1000000  loss         0.145926  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.877\n",
      "iter 92000/1000000  loss         0.145925  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.878\n",
      "iter 92001/1000000  loss         0.145925  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.878\n",
      "iter 92100/1000000  loss         0.145925  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.878\n",
      "iter 92101/1000000  loss         0.145925  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.878\n",
      "iter 92200/1000000  loss         0.145925  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.879\n",
      "iter 92201/1000000  loss         0.145924  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.879\n",
      "iter 92300/1000000  loss         0.145924  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.879\n",
      "iter 92301/1000000  loss         0.145924  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.879\n",
      "iter 92400/1000000  loss         0.145924  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.880\n",
      "iter 92401/1000000  loss         0.145924  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.880\n",
      "iter 92500/1000000  loss         0.145923  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.880\n",
      "iter 92501/1000000  loss         0.145923  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.880\n",
      "iter 92600/1000000  loss         0.145923  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 92601/1000000  loss         0.145923  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.881\n",
      "iter 92700/1000000  loss         0.145923  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.881\n",
      "iter 92701/1000000  loss         0.145923  avg_L1_norm_grad         0.000001  w[0]    0.003 bias    1.881\n",
      "iter 92800/1000000  loss         0.145922  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.882\n",
      "iter 92801/1000000  loss         0.145922  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.882\n",
      "iter 92900/1000000  loss         0.145922  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.882\n",
      "iter 92901/1000000  loss         0.145922  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.882\n",
      "iter 93000/1000000  loss         0.145922  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.882\n",
      "iter 93001/1000000  loss         0.145922  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.882\n",
      "iter 93100/1000000  loss         0.145921  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.883\n",
      "iter 93101/1000000  loss         0.145921  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.883\n",
      "iter 93200/1000000  loss         0.145921  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.883\n",
      "iter 93201/1000000  loss         0.145921  avg_L1_norm_grad         0.000000  w[0]    0.003 bias    1.883\n",
      "iter 93300/1000000  loss         0.145921  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.884\n",
      "iter 93301/1000000  loss         0.145921  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.884\n",
      "iter 93400/1000000  loss         0.145920  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.884\n",
      "iter 93401/1000000  loss         0.145920  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.884\n",
      "iter 93500/1000000  loss         0.145920  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.885\n",
      "iter 93501/1000000  loss         0.145920  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.885\n",
      "iter 93600/1000000  loss         0.145919  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.885\n",
      "iter 93601/1000000  loss         0.145919  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.885\n",
      "iter 93700/1000000  loss         0.145919  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.886\n",
      "iter 93701/1000000  loss         0.145919  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.886\n",
      "iter 93800/1000000  loss         0.145919  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.886\n",
      "iter 93801/1000000  loss         0.145919  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.886\n",
      "iter 93900/1000000  loss         0.145918  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.886\n",
      "iter 93901/1000000  loss         0.145918  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.886\n",
      "iter 94000/1000000  loss         0.145918  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.887\n",
      "iter 94001/1000000  loss         0.145918  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.887\n",
      "iter 94100/1000000  loss         0.145918  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.887\n",
      "iter 94101/1000000  loss         0.145918  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.887\n",
      "iter 94200/1000000  loss         0.145917  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.888\n",
      "iter 94201/1000000  loss         0.145917  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.888\n",
      "iter 94300/1000000  loss         0.145917  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.888\n",
      "iter 94301/1000000  loss         0.145917  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.888\n",
      "iter 94400/1000000  loss         0.145917  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.889\n",
      "iter 94401/1000000  loss         0.145917  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.889\n",
      "iter 94500/1000000  loss         0.145916  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.889\n",
      "iter 94501/1000000  loss         0.145916  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.889\n",
      "iter 94600/1000000  loss         0.145916  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.889\n",
      "iter 94601/1000000  loss         0.145916  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.889\n",
      "iter 94700/1000000  loss         0.145916  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.890\n",
      "iter 94701/1000000  loss         0.145916  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.890\n",
      "iter 94800/1000000  loss         0.145915  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.890\n",
      "iter 94801/1000000  loss         0.145915  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.890\n",
      "iter 94900/1000000  loss         0.145915  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.891\n",
      "iter 94901/1000000  loss         0.145915  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.891\n",
      "iter 95000/1000000  loss         0.145915  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.891\n",
      "iter 95001/1000000  loss         0.145915  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.891\n",
      "iter 95100/1000000  loss         0.145914  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.892\n",
      "iter 95101/1000000  loss         0.145914  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.892\n",
      "iter 95200/1000000  loss         0.145914  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.892\n",
      "iter 95201/1000000  loss         0.145914  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.892\n",
      "iter 95300/1000000  loss         0.145914  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.892\n",
      "iter 95301/1000000  loss         0.145914  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.892\n",
      "iter 95400/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.893\n",
      "iter 95401/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.893\n",
      "iter 95500/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.893\n",
      "iter 95501/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.893\n",
      "iter 95600/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.894\n",
      "iter 95601/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.894\n",
      "iter 95700/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.894\n",
      "iter 95701/1000000  loss         0.145913  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.894\n",
      "iter 95800/1000000  loss         0.145912  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.895\n",
      "iter 95801/1000000  loss         0.145912  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.895\n",
      "iter 95900/1000000  loss         0.145912  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.895\n",
      "iter 95901/1000000  loss         0.145912  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.895\n",
      "iter 96000/1000000  loss         0.145912  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.895\n",
      "iter 96001/1000000  loss         0.145912  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.895\n",
      "iter 96100/1000000  loss         0.145911  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.896\n",
      "iter 96101/1000000  loss         0.145911  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.896\n",
      "iter 96200/1000000  loss         0.145911  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.896\n",
      "iter 96201/1000000  loss         0.145911  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.896\n",
      "iter 96300/1000000  loss         0.145911  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.897\n",
      "iter 96301/1000000  loss         0.145911  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.897\n",
      "iter 96400/1000000  loss         0.145910  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.897\n",
      "iter 96401/1000000  loss         0.145910  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.897\n",
      "iter 96500/1000000  loss         0.145910  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 96501/1000000  loss         0.145910  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.897\n",
      "iter 96600/1000000  loss         0.145910  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.898\n",
      "iter 96601/1000000  loss         0.145910  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.898\n",
      "iter 96700/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.898\n",
      "iter 96701/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.898\n",
      "iter 96800/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.899\n",
      "iter 96801/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.899\n",
      "iter 96900/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.899\n",
      "iter 96901/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.899\n",
      "iter 97000/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.899\n",
      "iter 97001/1000000  loss         0.145909  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.899\n",
      "iter 97100/1000000  loss         0.145908  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.900\n",
      "iter 97101/1000000  loss         0.145908  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.900\n",
      "iter 97200/1000000  loss         0.145908  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.900\n",
      "iter 97201/1000000  loss         0.145908  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.900\n",
      "iter 97300/1000000  loss         0.145908  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.901\n",
      "iter 97301/1000000  loss         0.145908  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.901\n",
      "iter 97400/1000000  loss         0.145907  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.901\n",
      "iter 97401/1000000  loss         0.145907  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.901\n",
      "iter 97500/1000000  loss         0.145907  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.902\n",
      "iter 97501/1000000  loss         0.145907  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.902\n",
      "iter 97600/1000000  loss         0.145907  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.902\n",
      "iter 97601/1000000  loss         0.145907  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.902\n",
      "iter 97700/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.902\n",
      "iter 97701/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.902\n",
      "iter 97800/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.903\n",
      "iter 97801/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.903\n",
      "iter 97900/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.903\n",
      "iter 97901/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.903\n",
      "iter 98000/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.904\n",
      "iter 98001/1000000  loss         0.145906  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.904\n",
      "iter 98100/1000000  loss         0.145905  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.904\n",
      "iter 98101/1000000  loss         0.145905  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.904\n",
      "iter 98200/1000000  loss         0.145905  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.904\n",
      "iter 98201/1000000  loss         0.145905  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.904\n",
      "iter 98300/1000000  loss         0.145905  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.905\n",
      "iter 98301/1000000  loss         0.145905  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.905\n",
      "iter 98400/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.905\n",
      "iter 98401/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.905\n",
      "iter 98500/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.905\n",
      "iter 98501/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.905\n",
      "iter 98600/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.906\n",
      "iter 98601/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.906\n",
      "iter 98700/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.906\n",
      "iter 98701/1000000  loss         0.145904  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.906\n",
      "iter 98800/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.907\n",
      "iter 98801/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.907\n",
      "iter 98900/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.907\n",
      "iter 98901/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.907\n",
      "iter 99000/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.907\n",
      "iter 99001/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.907\n",
      "iter 99100/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.908\n",
      "iter 99101/1000000  loss         0.145903  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.908\n",
      "iter 99200/1000000  loss         0.145902  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.908\n",
      "iter 99201/1000000  loss         0.145902  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.908\n",
      "iter 99300/1000000  loss         0.145902  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.909\n",
      "iter 99301/1000000  loss         0.145902  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.909\n",
      "iter 99400/1000000  loss         0.145902  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.909\n",
      "iter 99401/1000000  loss         0.145902  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.909\n",
      "iter 99500/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.909\n",
      "iter 99501/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.909\n",
      "iter 99600/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.910\n",
      "iter 99601/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.910\n",
      "iter 99700/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.910\n",
      "iter 99701/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.910\n",
      "iter 99800/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.911\n",
      "iter 99801/1000000  loss         0.145901  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.911\n",
      "iter 99900/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.911\n",
      "iter 99901/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.911\n",
      "iter 100000/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.911\n",
      "iter 100001/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.911\n",
      "iter 100100/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.912\n",
      "iter 100101/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.912\n",
      "iter 100200/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.912\n",
      "iter 100201/1000000  loss         0.145900  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.912\n",
      "iter 100300/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.912\n",
      "iter 100301/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.912\n",
      "iter 100400/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100401/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.913\n",
      "iter 100500/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.913\n",
      "iter 100501/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.913\n",
      "iter 100600/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.914\n",
      "iter 100601/1000000  loss         0.145899  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.914\n",
      "iter 100700/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.914\n",
      "iter 100701/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.914\n",
      "iter 100800/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.914\n",
      "iter 100801/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.914\n",
      "iter 100900/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.915\n",
      "iter 100901/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.915\n",
      "iter 101000/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.915\n",
      "iter 101001/1000000  loss         0.145898  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.915\n",
      "iter 101100/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.915\n",
      "iter 101101/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.915\n",
      "iter 101200/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.916\n",
      "iter 101201/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.916\n",
      "iter 101300/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.916\n",
      "iter 101301/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.916\n",
      "iter 101400/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.917\n",
      "iter 101401/1000000  loss         0.145897  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.917\n",
      "iter 101500/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.917\n",
      "iter 101501/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.917\n",
      "iter 101600/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.917\n",
      "iter 101601/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.917\n",
      "iter 101700/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.918\n",
      "iter 101701/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.918\n",
      "iter 101800/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.918\n",
      "iter 101801/1000000  loss         0.145896  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.918\n",
      "iter 101900/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.918\n",
      "iter 101901/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.918\n",
      "iter 102000/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.919\n",
      "iter 102001/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.919\n",
      "iter 102100/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.919\n",
      "iter 102101/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.919\n",
      "iter 102200/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.919\n",
      "iter 102201/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.919\n",
      "iter 102300/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.920\n",
      "iter 102301/1000000  loss         0.145895  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.920\n",
      "iter 102400/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.920\n",
      "iter 102401/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.920\n",
      "iter 102500/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.921\n",
      "iter 102501/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.921\n",
      "iter 102600/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.921\n",
      "iter 102601/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.921\n",
      "iter 102700/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.921\n",
      "iter 102701/1000000  loss         0.145894  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.921\n",
      "iter 102800/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.922\n",
      "iter 102801/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.922\n",
      "iter 102900/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.922\n",
      "iter 102901/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.922\n",
      "iter 103000/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.922\n",
      "iter 103001/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.922\n",
      "iter 103100/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.923\n",
      "iter 103101/1000000  loss         0.145893  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.923\n",
      "iter 103200/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.923\n",
      "iter 103201/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.923\n",
      "iter 103300/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.923\n",
      "iter 103301/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.923\n",
      "iter 103400/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.924\n",
      "iter 103401/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.924\n",
      "iter 103500/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.924\n",
      "iter 103501/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.924\n",
      "iter 103600/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.924\n",
      "iter 103601/1000000  loss         0.145892  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.924\n",
      "iter 103700/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.925\n",
      "iter 103701/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.925\n",
      "iter 103800/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.925\n",
      "iter 103801/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.925\n",
      "iter 103900/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.926\n",
      "iter 103901/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.926\n",
      "iter 104000/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.926\n",
      "iter 104001/1000000  loss         0.145891  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.926\n",
      "iter 104100/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.926\n",
      "iter 104101/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.926\n",
      "iter 104200/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.927\n",
      "iter 104201/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 104300/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.927\n",
      "iter 104301/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.927\n",
      "iter 104400/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.927\n",
      "iter 104401/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.927\n",
      "iter 104500/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.928\n",
      "iter 104501/1000000  loss         0.145890  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.928\n",
      "iter 104600/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.928\n",
      "iter 104601/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.928\n",
      "iter 104700/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.928\n",
      "iter 104701/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.928\n",
      "iter 104800/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.929\n",
      "iter 104801/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.929\n",
      "iter 104900/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.929\n",
      "iter 104901/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.929\n",
      "iter 105000/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.929\n",
      "iter 105001/1000000  loss         0.145889  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.929\n",
      "iter 105100/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.930\n",
      "iter 105101/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.930\n",
      "iter 105200/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.930\n",
      "iter 105201/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.930\n",
      "iter 105300/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.930\n",
      "iter 105301/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.930\n",
      "iter 105400/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.931\n",
      "iter 105401/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.931\n",
      "iter 105500/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.931\n",
      "iter 105501/1000000  loss         0.145888  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.931\n",
      "iter 105600/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.931\n",
      "iter 105601/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.931\n",
      "iter 105700/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.932\n",
      "iter 105701/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.932\n",
      "iter 105800/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.932\n",
      "iter 105801/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.932\n",
      "iter 105900/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.932\n",
      "iter 105901/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.932\n",
      "iter 106000/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.933\n",
      "iter 106001/1000000  loss         0.145887  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.933\n",
      "iter 106100/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.933\n",
      "iter 106101/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.933\n",
      "iter 106200/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.933\n",
      "iter 106201/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.933\n",
      "iter 106300/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.934\n",
      "iter 106301/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.934\n",
      "iter 106400/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.934\n",
      "iter 106401/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.934\n",
      "iter 106500/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.934\n",
      "iter 106501/1000000  loss         0.145886  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.934\n",
      "iter 106600/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.935\n",
      "iter 106601/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.935\n",
      "iter 106700/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.935\n",
      "iter 106701/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.935\n",
      "iter 106800/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.935\n",
      "iter 106801/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.935\n",
      "iter 106900/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.936\n",
      "iter 106901/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.936\n",
      "iter 107000/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.936\n",
      "iter 107001/1000000  loss         0.145885  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.936\n",
      "iter 107100/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.936\n",
      "iter 107101/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.936\n",
      "iter 107200/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.937\n",
      "iter 107201/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.937\n",
      "iter 107300/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.937\n",
      "iter 107301/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.937\n",
      "iter 107400/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.937\n",
      "iter 107401/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.937\n",
      "iter 107500/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.938\n",
      "iter 107501/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.938\n",
      "iter 107600/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.938\n",
      "iter 107601/1000000  loss         0.145884  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.938\n",
      "iter 107700/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.938\n",
      "iter 107701/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.938\n",
      "iter 107800/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 107801/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 107900/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 107901/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 108000/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 108001/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 108100/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 108101/1000000  loss         0.145883  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.939\n",
      "iter 108200/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.940\n",
      "iter 108201/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.940\n",
      "iter 108300/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.940\n",
      "iter 108301/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.940\n",
      "iter 108400/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.940\n",
      "iter 108401/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.940\n",
      "iter 108500/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.941\n",
      "iter 108501/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.941\n",
      "iter 108600/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.941\n",
      "iter 108601/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.941\n",
      "iter 108700/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.941\n",
      "iter 108701/1000000  loss         0.145882  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.941\n",
      "iter 108800/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.942\n",
      "iter 108801/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.942\n",
      "iter 108900/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.942\n",
      "iter 108901/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.942\n",
      "iter 109000/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.942\n",
      "iter 109001/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.942\n",
      "iter 109100/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.943\n",
      "iter 109101/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.943\n",
      "iter 109200/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.943\n",
      "iter 109201/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.943\n",
      "iter 109300/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.943\n",
      "iter 109301/1000000  loss         0.145881  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.943\n",
      "iter 109400/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109401/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109500/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109501/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109600/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109601/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109700/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109701/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.944\n",
      "iter 109800/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.945\n",
      "iter 109801/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.945\n",
      "iter 109900/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.945\n",
      "iter 109901/1000000  loss         0.145880  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.945\n",
      "iter 110000/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.945\n",
      "iter 110001/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.945\n",
      "iter 110100/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.946\n",
      "iter 110101/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.946\n",
      "iter 110200/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.946\n",
      "iter 110201/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.946\n",
      "iter 110300/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.946\n",
      "iter 110301/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.946\n",
      "iter 110400/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110401/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110500/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110501/1000000  loss         0.145879  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110600/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110601/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110700/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110701/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.947\n",
      "iter 110800/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.948\n",
      "iter 110801/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.948\n",
      "iter 110900/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.948\n",
      "iter 110901/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.948\n",
      "iter 111000/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.948\n",
      "iter 111001/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.948\n",
      "iter 111100/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.949\n",
      "iter 111101/1000000  loss         0.145878  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.949\n",
      "iter 111200/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.949\n",
      "iter 111201/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.949\n",
      "iter 111300/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.949\n",
      "iter 111301/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.949\n",
      "iter 111400/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111401/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111500/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111501/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111600/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111601/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111700/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111701/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.950\n",
      "iter 111800/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.951\n",
      "iter 111801/1000000  loss         0.145877  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.951\n",
      "iter 111900/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.951\n",
      "iter 111901/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 112000/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.951\n",
      "iter 112001/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.951\n",
      "iter 112100/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112101/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112200/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112201/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112300/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112301/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112400/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112401/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.952\n",
      "iter 112500/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.953\n",
      "iter 112501/1000000  loss         0.145876  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.953\n",
      "iter 112600/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.953\n",
      "iter 112601/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.953\n",
      "iter 112700/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.953\n",
      "iter 112701/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.953\n",
      "iter 112800/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 112801/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 112900/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 112901/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 113000/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 113001/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 113100/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 113101/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.954\n",
      "iter 113200/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.955\n",
      "iter 113201/1000000  loss         0.145875  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.955\n",
      "iter 113300/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.955\n",
      "iter 113301/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.955\n",
      "iter 113400/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.955\n",
      "iter 113401/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.955\n",
      "iter 113500/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113501/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113600/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113601/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113700/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113701/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113800/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113801/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.956\n",
      "iter 113900/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.957\n",
      "iter 113901/1000000  loss         0.145874  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.957\n",
      "iter 114000/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.957\n",
      "iter 114001/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.957\n",
      "iter 114100/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.957\n",
      "iter 114101/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.957\n",
      "iter 114200/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114201/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114300/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114301/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114400/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114401/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114500/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114501/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.958\n",
      "iter 114600/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114601/1000000  loss         0.145873  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114700/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114701/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114800/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114801/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114900/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 114901/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.959\n",
      "iter 115000/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.960\n",
      "iter 115001/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.960\n",
      "iter 115100/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.960\n",
      "iter 115101/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.960\n",
      "iter 115200/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.960\n",
      "iter 115201/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.960\n",
      "iter 115300/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115301/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115400/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115401/1000000  loss         0.145872  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115500/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115501/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115600/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115601/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.961\n",
      "iter 115700/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 115701/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 115800/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 115801/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 115900/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 115901/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 116000/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 116001/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.962\n",
      "iter 116100/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116101/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116200/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116201/1000000  loss         0.145871  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116300/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116301/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116400/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116401/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.963\n",
      "iter 116500/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.964\n",
      "iter 116501/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.964\n",
      "iter 116600/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.964\n",
      "iter 116601/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.964\n",
      "iter 116700/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.964\n",
      "iter 116701/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.964\n",
      "iter 116800/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 116801/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 116900/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 116901/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 117000/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 117001/1000000  loss         0.145870  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 117100/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 117101/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.965\n",
      "iter 117200/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117201/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117300/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117301/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117400/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117401/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117500/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117501/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.966\n",
      "iter 117600/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117601/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117700/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117701/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117800/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117801/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117900/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 117901/1000000  loss         0.145869  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.967\n",
      "iter 118000/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118001/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118100/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118101/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118200/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118201/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118300/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118301/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.968\n",
      "iter 118400/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118401/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118500/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118501/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118600/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118601/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118700/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118701/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.969\n",
      "iter 118800/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.970\n",
      "iter 118801/1000000  loss         0.145868  avg_L1_norm_grad         0.000000  w[0]    0.002 bias    1.970\n",
      "Done. Converged after 118831 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr = LRGDF(alpha=10.0, step_size=0.1)\n",
    "new_lr.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 1572 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028672  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.940200  avg_L1_norm_grad         0.056914  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         1.270086  avg_L1_norm_grad         0.096707  w[0]    0.002 bias    0.033\n",
      "iter    3/1000000  loss         2.079411  avg_L1_norm_grad         0.115724  w[0]   -0.001 bias   -0.009\n",
      "iter    4/1000000  loss         1.090420  avg_L1_norm_grad         0.088005  w[0]    0.003 bias    0.055\n",
      "iter    5/1000000  loss         1.564500  avg_L1_norm_grad         0.098511  w[0]    0.000 bias    0.016\n",
      "iter    6/1000000  loss         0.929073  avg_L1_norm_grad         0.074289  w[0]    0.003 bias    0.072\n",
      "iter    7/1000000  loss         1.168500  avg_L1_norm_grad         0.078440  w[0]    0.001 bias    0.040\n",
      "iter    8/1000000  loss         0.794259  avg_L1_norm_grad         0.060979  w[0]    0.004 bias    0.086\n",
      "iter    9/1000000  loss         0.917650  avg_L1_norm_grad         0.063183  w[0]    0.002 bias    0.061\n",
      "iter   10/1000000  loss         0.705856  avg_L1_norm_grad         0.052309  w[0]    0.004 bias    0.098\n",
      "iter   11/1000000  loss         0.780182  avg_L1_norm_grad         0.053837  w[0]    0.003 bias    0.077\n",
      "iter   12/1000000  loss         0.642130  avg_L1_norm_grad         0.046268  w[0]    0.005 bias    0.110\n",
      "iter   13/1000000  loss         0.691911  avg_L1_norm_grad         0.047453  w[0]    0.004 bias    0.091\n",
      "iter   14/1000000  loss         0.592702  avg_L1_norm_grad         0.041578  w[0]    0.005 bias    0.120\n",
      "iter   15/1000000  loss         0.627423  avg_L1_norm_grad         0.042479  w[0]    0.004 bias    0.104\n",
      "iter   16/1000000  loss         0.552200  avg_L1_norm_grad         0.037509  w[0]    0.006 bias    0.130\n",
      "iter   17/1000000  loss         0.575971  avg_L1_norm_grad         0.038123  w[0]    0.005 bias    0.115\n",
      "iter   18/1000000  loss         0.517751  avg_L1_norm_grad         0.033672  w[0]    0.006 bias    0.138\n",
      "iter   19/1000000  loss         0.532817  avg_L1_norm_grad         0.033998  w[0]    0.005 bias    0.125\n",
      "iter  100/1000000  loss         0.280776  avg_L1_norm_grad         0.001287  w[0]    0.011 bias    0.290\n",
      "iter  101/1000000  loss         0.280137  avg_L1_norm_grad         0.001278  w[0]    0.011 bias    0.291\n",
      "iter  200/1000000  loss         0.242371  avg_L1_norm_grad         0.000814  w[0]    0.013 bias    0.360\n",
      "iter  201/1000000  loss         0.242136  avg_L1_norm_grad         0.000812  w[0]    0.013 bias    0.361\n",
      "iter  300/1000000  loss         0.225001  avg_L1_norm_grad         0.000627  w[0]    0.015 bias    0.395\n",
      "iter  301/1000000  loss         0.224872  avg_L1_norm_grad         0.000626  w[0]    0.015 bias    0.395\n",
      "iter  400/1000000  loss         0.214585  avg_L1_norm_grad         0.000518  w[0]    0.017 bias    0.415\n",
      "iter  401/1000000  loss         0.214501  avg_L1_norm_grad         0.000517  w[0]    0.017 bias    0.415\n",
      "iter  500/1000000  loss         0.207489  avg_L1_norm_grad         0.000444  w[0]    0.019 bias    0.427\n",
      "iter  501/1000000  loss         0.207429  avg_L1_norm_grad         0.000443  w[0]    0.019 bias    0.428\n",
      "iter  600/1000000  loss         0.202292  avg_L1_norm_grad         0.000389  w[0]    0.021 bias    0.435\n",
      "iter  601/1000000  loss         0.202247  avg_L1_norm_grad         0.000388  w[0]    0.021 bias    0.436\n",
      "iter  700/1000000  loss         0.198308  avg_L1_norm_grad         0.000346  w[0]    0.023 bias    0.441\n",
      "iter  701/1000000  loss         0.198273  avg_L1_norm_grad         0.000345  w[0]    0.023 bias    0.441\n",
      "iter  800/1000000  loss         0.195153  avg_L1_norm_grad         0.000311  w[0]    0.025 bias    0.444\n",
      "iter  801/1000000  loss         0.195125  avg_L1_norm_grad         0.000311  w[0]    0.025 bias    0.444\n",
      "iter  900/1000000  loss         0.192597  avg_L1_norm_grad         0.000283  w[0]    0.028 bias    0.446\n",
      "iter  901/1000000  loss         0.192574  avg_L1_norm_grad         0.000283  w[0]    0.028 bias    0.446\n",
      "iter 1000/1000000  loss         0.190487  avg_L1_norm_grad         0.000258  w[0]    0.030 bias    0.447\n",
      "iter 1001/1000000  loss         0.190468  avg_L1_norm_grad         0.000258  w[0]    0.030 bias    0.447\n",
      "iter 1100/1000000  loss         0.188721  avg_L1_norm_grad         0.000237  w[0]    0.032 bias    0.447\n",
      "iter 1101/1000000  loss         0.188705  avg_L1_norm_grad         0.000237  w[0]    0.032 bias    0.447\n",
      "iter 1200/1000000  loss         0.187226  avg_L1_norm_grad         0.000219  w[0]    0.033 bias    0.446\n",
      "iter 1201/1000000  loss         0.187212  avg_L1_norm_grad         0.000219  w[0]    0.033 bias    0.446\n",
      "iter 1300/1000000  loss         0.185948  avg_L1_norm_grad         0.000203  w[0]    0.035 bias    0.445\n",
      "iter 1301/1000000  loss         0.185936  avg_L1_norm_grad         0.000203  w[0]    0.035 bias    0.445\n",
      "iter 1400/1000000  loss         0.184847  avg_L1_norm_grad         0.000189  w[0]    0.037 bias    0.444\n",
      "iter 1401/1000000  loss         0.184837  avg_L1_norm_grad         0.000189  w[0]    0.037 bias    0.444\n",
      "iter 1500/1000000  loss         0.183891  avg_L1_norm_grad         0.000177  w[0]    0.038 bias    0.442\n",
      "iter 1501/1000000  loss         0.183882  avg_L1_norm_grad         0.000177  w[0]    0.038 bias    0.442\n",
      "iter 1600/1000000  loss         0.183057  avg_L1_norm_grad         0.000165  w[0]    0.040 bias    0.441\n",
      "iter 1601/1000000  loss         0.183049  avg_L1_norm_grad         0.000165  w[0]    0.040 bias    0.441\n",
      "iter 1700/1000000  loss         0.182325  avg_L1_norm_grad         0.000155  w[0]    0.041 bias    0.439\n",
      "iter 1701/1000000  loss         0.182318  avg_L1_norm_grad         0.000155  w[0]    0.041 bias    0.439\n",
      "iter 1800/1000000  loss         0.181679  avg_L1_norm_grad         0.000146  w[0]    0.043 bias    0.436\n",
      "iter 1801/1000000  loss         0.181673  avg_L1_norm_grad         0.000146  w[0]    0.043 bias    0.436\n",
      "iter 1900/1000000  loss         0.181108  avg_L1_norm_grad         0.000137  w[0]    0.044 bias    0.434\n",
      "iter 1901/1000000  loss         0.181102  avg_L1_norm_grad         0.000137  w[0]    0.044 bias    0.434\n",
      "iter 2000/1000000  loss         0.180600  avg_L1_norm_grad         0.000129  w[0]    0.045 bias    0.431\n",
      "iter 2001/1000000  loss         0.180595  avg_L1_norm_grad         0.000129  w[0]    0.045 bias    0.431\n",
      "iter 2100/1000000  loss         0.180147  avg_L1_norm_grad         0.000122  w[0]    0.046 bias    0.429\n",
      "iter 2101/1000000  loss         0.180143  avg_L1_norm_grad         0.000122  w[0]    0.046 bias    0.429\n",
      "iter 2200/1000000  loss         0.179742  avg_L1_norm_grad         0.000116  w[0]    0.048 bias    0.426\n",
      "iter 2201/1000000  loss         0.179738  avg_L1_norm_grad         0.000115  w[0]    0.048 bias    0.426\n",
      "iter 2300/1000000  loss         0.179379  avg_L1_norm_grad         0.000109  w[0]    0.049 bias    0.423\n",
      "iter 2301/1000000  loss         0.179376  avg_L1_norm_grad         0.000109  w[0]    0.049 bias    0.423\n",
      "iter 2400/1000000  loss         0.179052  avg_L1_norm_grad         0.000104  w[0]    0.050 bias    0.420\n",
      "iter 2401/1000000  loss         0.179049  avg_L1_norm_grad         0.000104  w[0]    0.050 bias    0.420\n",
      "iter 2500/1000000  loss         0.178758  avg_L1_norm_grad         0.000098  w[0]    0.050 bias    0.417\n",
      "iter 2501/1000000  loss         0.178755  avg_L1_norm_grad         0.000098  w[0]    0.051 bias    0.417\n",
      "iter 2600/1000000  loss         0.178492  avg_L1_norm_grad         0.000093  w[0]    0.051 bias    0.415\n",
      "iter 2601/1000000  loss         0.178490  avg_L1_norm_grad         0.000093  w[0]    0.051 bias    0.414\n",
      "iter 2700/1000000  loss         0.178252  avg_L1_norm_grad         0.000089  w[0]    0.052 bias    0.412\n",
      "iter 2701/1000000  loss         0.178249  avg_L1_norm_grad         0.000089  w[0]    0.052 bias    0.412\n",
      "iter 2800/1000000  loss         0.178033  avg_L1_norm_grad         0.000084  w[0]    0.053 bias    0.409\n",
      "iter 2801/1000000  loss         0.178031  avg_L1_norm_grad         0.000084  w[0]    0.053 bias    0.409\n",
      "iter 2900/1000000  loss         0.177835  avg_L1_norm_grad         0.000080  w[0]    0.054 bias    0.406\n",
      "iter 2901/1000000  loss         0.177833  avg_L1_norm_grad         0.000080  w[0]    0.054 bias    0.405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.177655  avg_L1_norm_grad         0.000076  w[0]    0.054 bias    0.402\n",
      "iter 3001/1000000  loss         0.177653  avg_L1_norm_grad         0.000076  w[0]    0.054 bias    0.402\n",
      "iter 3100/1000000  loss         0.177490  avg_L1_norm_grad         0.000073  w[0]    0.055 bias    0.399\n",
      "iter 3101/1000000  loss         0.177489  avg_L1_norm_grad         0.000073  w[0]    0.055 bias    0.399\n",
      "iter 3200/1000000  loss         0.177340  avg_L1_norm_grad         0.000069  w[0]    0.056 bias    0.396\n",
      "iter 3201/1000000  loss         0.177339  avg_L1_norm_grad         0.000069  w[0]    0.056 bias    0.396\n",
      "iter 3300/1000000  loss         0.177203  avg_L1_norm_grad         0.000066  w[0]    0.056 bias    0.393\n",
      "iter 3301/1000000  loss         0.177201  avg_L1_norm_grad         0.000066  w[0]    0.056 bias    0.393\n",
      "iter 3400/1000000  loss         0.177077  avg_L1_norm_grad         0.000063  w[0]    0.057 bias    0.390\n",
      "iter 3401/1000000  loss         0.177076  avg_L1_norm_grad         0.000063  w[0]    0.057 bias    0.390\n",
      "iter 3500/1000000  loss         0.176962  avg_L1_norm_grad         0.000060  w[0]    0.057 bias    0.387\n",
      "iter 3501/1000000  loss         0.176961  avg_L1_norm_grad         0.000060  w[0]    0.057 bias    0.387\n",
      "iter 3600/1000000  loss         0.176856  avg_L1_norm_grad         0.000058  w[0]    0.058 bias    0.384\n",
      "iter 3601/1000000  loss         0.176855  avg_L1_norm_grad         0.000058  w[0]    0.058 bias    0.384\n",
      "iter 3700/1000000  loss         0.176759  avg_L1_norm_grad         0.000055  w[0]    0.058 bias    0.381\n",
      "iter 3701/1000000  loss         0.176758  avg_L1_norm_grad         0.000055  w[0]    0.058 bias    0.381\n",
      "iter 3800/1000000  loss         0.176670  avg_L1_norm_grad         0.000053  w[0]    0.059 bias    0.379\n",
      "iter 3801/1000000  loss         0.176669  avg_L1_norm_grad         0.000053  w[0]    0.059 bias    0.379\n",
      "iter 3900/1000000  loss         0.176588  avg_L1_norm_grad         0.000050  w[0]    0.059 bias    0.376\n",
      "iter 3901/1000000  loss         0.176587  avg_L1_norm_grad         0.000050  w[0]    0.059 bias    0.376\n",
      "iter 4000/1000000  loss         0.176512  avg_L1_norm_grad         0.000048  w[0]    0.060 bias    0.373\n",
      "iter 4001/1000000  loss         0.176511  avg_L1_norm_grad         0.000048  w[0]    0.060 bias    0.373\n",
      "iter 4100/1000000  loss         0.176442  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    0.370\n",
      "iter 4101/1000000  loss         0.176442  avg_L1_norm_grad         0.000046  w[0]    0.060 bias    0.370\n",
      "iter 4200/1000000  loss         0.176378  avg_L1_norm_grad         0.000044  w[0]    0.060 bias    0.367\n",
      "iter 4201/1000000  loss         0.176377  avg_L1_norm_grad         0.000044  w[0]    0.060 bias    0.367\n",
      "iter 4300/1000000  loss         0.176318  avg_L1_norm_grad         0.000042  w[0]    0.061 bias    0.364\n",
      "iter 4301/1000000  loss         0.176318  avg_L1_norm_grad         0.000042  w[0]    0.061 bias    0.364\n",
      "iter 4400/1000000  loss         0.176263  avg_L1_norm_grad         0.000041  w[0]    0.061 bias    0.362\n",
      "iter 4401/1000000  loss         0.176263  avg_L1_norm_grad         0.000041  w[0]    0.061 bias    0.362\n",
      "iter 4500/1000000  loss         0.176212  avg_L1_norm_grad         0.000039  w[0]    0.061 bias    0.359\n",
      "iter 4501/1000000  loss         0.176212  avg_L1_norm_grad         0.000039  w[0]    0.061 bias    0.359\n",
      "iter 4600/1000000  loss         0.176165  avg_L1_norm_grad         0.000037  w[0]    0.062 bias    0.356\n",
      "iter 4601/1000000  loss         0.176164  avg_L1_norm_grad         0.000037  w[0]    0.062 bias    0.356\n",
      "iter 4700/1000000  loss         0.176121  avg_L1_norm_grad         0.000036  w[0]    0.062 bias    0.354\n",
      "iter 4701/1000000  loss         0.176120  avg_L1_norm_grad         0.000036  w[0]    0.062 bias    0.353\n",
      "iter 4800/1000000  loss         0.176080  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    0.351\n",
      "iter 4801/1000000  loss         0.176080  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    0.351\n",
      "iter 4900/1000000  loss         0.176042  avg_L1_norm_grad         0.000033  w[0]    0.062 bias    0.348\n",
      "iter 4901/1000000  loss         0.176042  avg_L1_norm_grad         0.000033  w[0]    0.062 bias    0.348\n",
      "iter 5000/1000000  loss         0.176007  avg_L1_norm_grad         0.000032  w[0]    0.063 bias    0.346\n",
      "iter 5001/1000000  loss         0.176007  avg_L1_norm_grad         0.000032  w[0]    0.063 bias    0.346\n",
      "iter 5100/1000000  loss         0.175975  avg_L1_norm_grad         0.000030  w[0]    0.063 bias    0.343\n",
      "iter 5101/1000000  loss         0.175974  avg_L1_norm_grad         0.000030  w[0]    0.063 bias    0.343\n",
      "iter 5200/1000000  loss         0.175944  avg_L1_norm_grad         0.000029  w[0]    0.063 bias    0.341\n",
      "iter 5201/1000000  loss         0.175944  avg_L1_norm_grad         0.000029  w[0]    0.063 bias    0.341\n",
      "iter 5300/1000000  loss         0.175916  avg_L1_norm_grad         0.000028  w[0]    0.063 bias    0.338\n",
      "iter 5301/1000000  loss         0.175916  avg_L1_norm_grad         0.000028  w[0]    0.063 bias    0.338\n",
      "iter 5400/1000000  loss         0.175889  avg_L1_norm_grad         0.000027  w[0]    0.064 bias    0.336\n",
      "iter 5401/1000000  loss         0.175889  avg_L1_norm_grad         0.000027  w[0]    0.064 bias    0.336\n",
      "iter 5500/1000000  loss         0.175865  avg_L1_norm_grad         0.000026  w[0]    0.064 bias    0.334\n",
      "iter 5501/1000000  loss         0.175865  avg_L1_norm_grad         0.000026  w[0]    0.064 bias    0.334\n",
      "iter 5600/1000000  loss         0.175842  avg_L1_norm_grad         0.000025  w[0]    0.064 bias    0.331\n",
      "iter 5601/1000000  loss         0.175842  avg_L1_norm_grad         0.000025  w[0]    0.064 bias    0.331\n",
      "iter 5700/1000000  loss         0.175821  avg_L1_norm_grad         0.000024  w[0]    0.064 bias    0.329\n",
      "iter 5701/1000000  loss         0.175820  avg_L1_norm_grad         0.000024  w[0]    0.064 bias    0.329\n",
      "iter 5800/1000000  loss         0.175800  avg_L1_norm_grad         0.000023  w[0]    0.064 bias    0.327\n",
      "iter 5801/1000000  loss         0.175800  avg_L1_norm_grad         0.000023  w[0]    0.064 bias    0.327\n",
      "iter 5900/1000000  loss         0.175782  avg_L1_norm_grad         0.000022  w[0]    0.064 bias    0.325\n",
      "iter 5901/1000000  loss         0.175782  avg_L1_norm_grad         0.000022  w[0]    0.064 bias    0.325\n",
      "iter 6000/1000000  loss         0.175764  avg_L1_norm_grad         0.000021  w[0]    0.065 bias    0.323\n",
      "iter 6001/1000000  loss         0.175764  avg_L1_norm_grad         0.000021  w[0]    0.065 bias    0.323\n",
      "iter 6100/1000000  loss         0.175748  avg_L1_norm_grad         0.000021  w[0]    0.065 bias    0.321\n",
      "iter 6101/1000000  loss         0.175748  avg_L1_norm_grad         0.000020  w[0]    0.065 bias    0.321\n",
      "iter 6200/1000000  loss         0.175733  avg_L1_norm_grad         0.000020  w[0]    0.065 bias    0.319\n",
      "iter 6201/1000000  loss         0.175732  avg_L1_norm_grad         0.000020  w[0]    0.065 bias    0.319\n",
      "iter 6300/1000000  loss         0.175718  avg_L1_norm_grad         0.000019  w[0]    0.065 bias    0.317\n",
      "iter 6301/1000000  loss         0.175718  avg_L1_norm_grad         0.000019  w[0]    0.065 bias    0.317\n",
      "iter 6400/1000000  loss         0.175705  avg_L1_norm_grad         0.000018  w[0]    0.065 bias    0.315\n",
      "iter 6401/1000000  loss         0.175705  avg_L1_norm_grad         0.000018  w[0]    0.065 bias    0.315\n",
      "iter 6500/1000000  loss         0.175692  avg_L1_norm_grad         0.000018  w[0]    0.065 bias    0.313\n",
      "iter 6501/1000000  loss         0.175692  avg_L1_norm_grad         0.000018  w[0]    0.065 bias    0.313\n",
      "iter 6600/1000000  loss         0.175680  avg_L1_norm_grad         0.000017  w[0]    0.065 bias    0.311\n",
      "iter 6601/1000000  loss         0.175680  avg_L1_norm_grad         0.000017  w[0]    0.065 bias    0.311\n",
      "iter 6700/1000000  loss         0.175669  avg_L1_norm_grad         0.000016  w[0]    0.065 bias    0.309\n",
      "iter 6701/1000000  loss         0.175669  avg_L1_norm_grad         0.000016  w[0]    0.065 bias    0.309\n",
      "iter 6800/1000000  loss         0.175659  avg_L1_norm_grad         0.000016  w[0]    0.066 bias    0.307\n",
      "iter 6801/1000000  loss         0.175659  avg_L1_norm_grad         0.000016  w[0]    0.066 bias    0.307\n",
      "iter 6900/1000000  loss         0.175649  avg_L1_norm_grad         0.000015  w[0]    0.066 bias    0.305\n",
      "iter 6901/1000000  loss         0.175649  avg_L1_norm_grad         0.000015  w[0]    0.066 bias    0.305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.175640  avg_L1_norm_grad         0.000015  w[0]    0.066 bias    0.304\n",
      "iter 7001/1000000  loss         0.175640  avg_L1_norm_grad         0.000015  w[0]    0.066 bias    0.304\n",
      "iter 7100/1000000  loss         0.175631  avg_L1_norm_grad         0.000014  w[0]    0.066 bias    0.302\n",
      "iter 7101/1000000  loss         0.175631  avg_L1_norm_grad         0.000014  w[0]    0.066 bias    0.302\n",
      "iter 7200/1000000  loss         0.175623  avg_L1_norm_grad         0.000014  w[0]    0.066 bias    0.300\n",
      "iter 7201/1000000  loss         0.175623  avg_L1_norm_grad         0.000014  w[0]    0.066 bias    0.300\n",
      "iter 7300/1000000  loss         0.175615  avg_L1_norm_grad         0.000013  w[0]    0.066 bias    0.298\n",
      "iter 7301/1000000  loss         0.175615  avg_L1_norm_grad         0.000013  w[0]    0.066 bias    0.298\n",
      "iter 7400/1000000  loss         0.175608  avg_L1_norm_grad         0.000013  w[0]    0.066 bias    0.297\n",
      "iter 7401/1000000  loss         0.175608  avg_L1_norm_grad         0.000013  w[0]    0.066 bias    0.297\n",
      "iter 7500/1000000  loss         0.175601  avg_L1_norm_grad         0.000012  w[0]    0.066 bias    0.295\n",
      "iter 7501/1000000  loss         0.175601  avg_L1_norm_grad         0.000012  w[0]    0.066 bias    0.295\n",
      "iter 7600/1000000  loss         0.175595  avg_L1_norm_grad         0.000012  w[0]    0.066 bias    0.294\n",
      "iter 7601/1000000  loss         0.175595  avg_L1_norm_grad         0.000012  w[0]    0.066 bias    0.294\n",
      "iter 7700/1000000  loss         0.175589  avg_L1_norm_grad         0.000011  w[0]    0.066 bias    0.292\n",
      "iter 7701/1000000  loss         0.175589  avg_L1_norm_grad         0.000011  w[0]    0.066 bias    0.292\n",
      "iter 7800/1000000  loss         0.175583  avg_L1_norm_grad         0.000011  w[0]    0.066 bias    0.291\n",
      "iter 7801/1000000  loss         0.175583  avg_L1_norm_grad         0.000011  w[0]    0.066 bias    0.291\n",
      "iter 7900/1000000  loss         0.175578  avg_L1_norm_grad         0.000010  w[0]    0.066 bias    0.289\n",
      "iter 7901/1000000  loss         0.175578  avg_L1_norm_grad         0.000010  w[0]    0.066 bias    0.289\n",
      "iter 8000/1000000  loss         0.175573  avg_L1_norm_grad         0.000010  w[0]    0.067 bias    0.288\n",
      "iter 8001/1000000  loss         0.175573  avg_L1_norm_grad         0.000010  w[0]    0.067 bias    0.288\n",
      "iter 8100/1000000  loss         0.175568  avg_L1_norm_grad         0.000010  w[0]    0.067 bias    0.287\n",
      "iter 8101/1000000  loss         0.175568  avg_L1_norm_grad         0.000010  w[0]    0.067 bias    0.287\n",
      "iter 8200/1000000  loss         0.175563  avg_L1_norm_grad         0.000009  w[0]    0.067 bias    0.285\n",
      "iter 8201/1000000  loss         0.175563  avg_L1_norm_grad         0.000009  w[0]    0.067 bias    0.285\n",
      "iter 8300/1000000  loss         0.175559  avg_L1_norm_grad         0.000009  w[0]    0.067 bias    0.284\n",
      "iter 8301/1000000  loss         0.175559  avg_L1_norm_grad         0.000009  w[0]    0.067 bias    0.284\n",
      "iter 8400/1000000  loss         0.175555  avg_L1_norm_grad         0.000009  w[0]    0.067 bias    0.283\n",
      "iter 8401/1000000  loss         0.175555  avg_L1_norm_grad         0.000009  w[0]    0.067 bias    0.283\n",
      "iter 8500/1000000  loss         0.175551  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.281\n",
      "iter 8501/1000000  loss         0.175551  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.281\n",
      "iter 8600/1000000  loss         0.175547  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.280\n",
      "iter 8601/1000000  loss         0.175547  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.280\n",
      "iter 8700/1000000  loss         0.175544  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.279\n",
      "iter 8701/1000000  loss         0.175544  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.279\n",
      "iter 8800/1000000  loss         0.175541  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.278\n",
      "iter 8801/1000000  loss         0.175541  avg_L1_norm_grad         0.000008  w[0]    0.067 bias    0.278\n",
      "iter 8900/1000000  loss         0.175538  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.276\n",
      "iter 8901/1000000  loss         0.175538  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.276\n",
      "iter 9000/1000000  loss         0.175535  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.275\n",
      "iter 9001/1000000  loss         0.175535  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.275\n",
      "iter 9100/1000000  loss         0.175532  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.274\n",
      "iter 9101/1000000  loss         0.175532  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.274\n",
      "iter 9200/1000000  loss         0.175529  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.273\n",
      "iter 9201/1000000  loss         0.175529  avg_L1_norm_grad         0.000007  w[0]    0.067 bias    0.273\n",
      "iter 9300/1000000  loss         0.175527  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.272\n",
      "iter 9301/1000000  loss         0.175527  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.272\n",
      "iter 9400/1000000  loss         0.175525  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.271\n",
      "iter 9401/1000000  loss         0.175525  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.271\n",
      "iter 9500/1000000  loss         0.175522  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.270\n",
      "iter 9501/1000000  loss         0.175522  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.270\n",
      "iter 9600/1000000  loss         0.175520  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.269\n",
      "iter 9601/1000000  loss         0.175520  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.269\n",
      "iter 9700/1000000  loss         0.175518  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.268\n",
      "iter 9701/1000000  loss         0.175518  avg_L1_norm_grad         0.000006  w[0]    0.067 bias    0.268\n",
      "iter 9800/1000000  loss         0.175516  avg_L1_norm_grad         0.000005  w[0]    0.067 bias    0.267\n",
      "iter 9801/1000000  loss         0.175516  avg_L1_norm_grad         0.000005  w[0]    0.067 bias    0.267\n",
      "iter 9900/1000000  loss         0.175515  avg_L1_norm_grad         0.000005  w[0]    0.067 bias    0.266\n",
      "iter 9901/1000000  loss         0.175514  avg_L1_norm_grad         0.000005  w[0]    0.067 bias    0.266\n",
      "iter 10000/1000000  loss         0.175513  avg_L1_norm_grad         0.000005  w[0]    0.067 bias    0.265\n",
      "iter 10001/1000000  loss         0.175513  avg_L1_norm_grad         0.000005  w[0]    0.067 bias    0.265\n",
      "iter 10100/1000000  loss         0.175511  avg_L1_norm_grad         0.000005  w[0]    0.068 bias    0.264\n",
      "iter 10101/1000000  loss         0.175511  avg_L1_norm_grad         0.000005  w[0]    0.068 bias    0.264\n",
      "iter 10200/1000000  loss         0.175510  avg_L1_norm_grad         0.000005  w[0]    0.068 bias    0.264\n",
      "iter 10201/1000000  loss         0.175510  avg_L1_norm_grad         0.000005  w[0]    0.068 bias    0.264\n",
      "iter 10300/1000000  loss         0.175508  avg_L1_norm_grad         0.000005  w[0]    0.068 bias    0.263\n",
      "iter 10301/1000000  loss         0.175508  avg_L1_norm_grad         0.000005  w[0]    0.068 bias    0.263\n",
      "iter 10400/1000000  loss         0.175507  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.262\n",
      "iter 10401/1000000  loss         0.175507  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.262\n",
      "iter 10500/1000000  loss         0.175505  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.261\n",
      "iter 10501/1000000  loss         0.175505  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.261\n",
      "iter 10600/1000000  loss         0.175504  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.260\n",
      "iter 10601/1000000  loss         0.175504  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.260\n",
      "iter 10700/1000000  loss         0.175503  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.260\n",
      "iter 10701/1000000  loss         0.175503  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.260\n",
      "iter 10800/1000000  loss         0.175502  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.259\n",
      "iter 10801/1000000  loss         0.175502  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.175501  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.258\n",
      "iter 10901/1000000  loss         0.175501  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.258\n",
      "iter 11000/1000000  loss         0.175500  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.257\n",
      "iter 11001/1000000  loss         0.175500  avg_L1_norm_grad         0.000004  w[0]    0.068 bias    0.257\n",
      "iter 11100/1000000  loss         0.175499  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.257\n",
      "iter 11101/1000000  loss         0.175499  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.257\n",
      "iter 11200/1000000  loss         0.175498  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.256\n",
      "iter 11201/1000000  loss         0.175498  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.256\n",
      "iter 11300/1000000  loss         0.175497  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.255\n",
      "iter 11301/1000000  loss         0.175497  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.255\n",
      "iter 11400/1000000  loss         0.175496  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.255\n",
      "iter 11401/1000000  loss         0.175496  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.255\n",
      "iter 11500/1000000  loss         0.175495  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.254\n",
      "iter 11501/1000000  loss         0.175495  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.254\n",
      "iter 11600/1000000  loss         0.175494  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.254\n",
      "iter 11601/1000000  loss         0.175494  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.254\n",
      "iter 11700/1000000  loss         0.175493  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.253\n",
      "iter 11701/1000000  loss         0.175493  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.253\n",
      "iter 11800/1000000  loss         0.175493  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.252\n",
      "iter 11801/1000000  loss         0.175493  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.252\n",
      "iter 11900/1000000  loss         0.175492  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.252\n",
      "iter 11901/1000000  loss         0.175492  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.252\n",
      "iter 12000/1000000  loss         0.175491  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.251\n",
      "iter 12001/1000000  loss         0.175491  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.251\n",
      "iter 12100/1000000  loss         0.175491  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.251\n",
      "iter 12101/1000000  loss         0.175491  avg_L1_norm_grad         0.000003  w[0]    0.068 bias    0.251\n",
      "iter 12200/1000000  loss         0.175490  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.250\n",
      "iter 12201/1000000  loss         0.175490  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.250\n",
      "iter 12300/1000000  loss         0.175490  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.250\n",
      "iter 12301/1000000  loss         0.175490  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.250\n",
      "iter 12400/1000000  loss         0.175489  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.249\n",
      "iter 12401/1000000  loss         0.175489  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.249\n",
      "iter 12500/1000000  loss         0.175489  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.249\n",
      "iter 12501/1000000  loss         0.175489  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.249\n",
      "iter 12600/1000000  loss         0.175488  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.248\n",
      "iter 12601/1000000  loss         0.175488  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.248\n",
      "iter 12700/1000000  loss         0.175488  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.248\n",
      "iter 12701/1000000  loss         0.175488  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.248\n",
      "iter 12800/1000000  loss         0.175487  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.247\n",
      "iter 12801/1000000  loss         0.175487  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.247\n",
      "iter 12900/1000000  loss         0.175487  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.247\n",
      "iter 12901/1000000  loss         0.175487  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.247\n",
      "iter 13000/1000000  loss         0.175486  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.246\n",
      "iter 13001/1000000  loss         0.175486  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.246\n",
      "iter 13100/1000000  loss         0.175486  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.246\n",
      "iter 13101/1000000  loss         0.175486  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.246\n",
      "iter 13200/1000000  loss         0.175485  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.246\n",
      "iter 13201/1000000  loss         0.175485  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.246\n",
      "iter 13300/1000000  loss         0.175485  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.245\n",
      "iter 13301/1000000  loss         0.175485  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.245\n",
      "iter 13400/1000000  loss         0.175485  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.245\n",
      "iter 13401/1000000  loss         0.175485  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.245\n",
      "iter 13500/1000000  loss         0.175484  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.244\n",
      "iter 13501/1000000  loss         0.175484  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.244\n",
      "iter 13600/1000000  loss         0.175484  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.244\n",
      "iter 13601/1000000  loss         0.175484  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.244\n",
      "iter 13700/1000000  loss         0.175484  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.244\n",
      "iter 13701/1000000  loss         0.175484  avg_L1_norm_grad         0.000002  w[0]    0.068 bias    0.244\n",
      "iter 13800/1000000  loss         0.175484  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.243\n",
      "iter 13801/1000000  loss         0.175484  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.243\n",
      "iter 13900/1000000  loss         0.175483  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.243\n",
      "iter 13901/1000000  loss         0.175483  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.243\n",
      "iter 14000/1000000  loss         0.175483  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.243\n",
      "iter 14001/1000000  loss         0.175483  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.243\n",
      "iter 14100/1000000  loss         0.175483  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.242\n",
      "iter 14101/1000000  loss         0.175483  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.242\n",
      "iter 14200/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.242\n",
      "iter 14201/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.242\n",
      "iter 14300/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.242\n",
      "iter 14301/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.242\n",
      "iter 14400/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14401/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14500/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14501/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14600/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14601/1000000  loss         0.175482  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14700/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n",
      "iter 14701/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14800/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 14801/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 14900/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 14901/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 15000/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 15001/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 15100/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 15101/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.240\n",
      "iter 15200/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15201/1000000  loss         0.175481  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15300/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15301/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15400/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15401/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15500/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15501/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.239\n",
      "iter 15600/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.238\n",
      "iter 15601/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.238\n",
      "iter 15700/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.238\n",
      "iter 15701/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.238\n",
      "iter 15800/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.238\n",
      "iter 15801/1000000  loss         0.175480  avg_L1_norm_grad         0.000001  w[0]    0.068 bias    0.238\n",
      "Done. Converged after 15862 iterations.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028548  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.918371  avg_L1_norm_grad         0.029962  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.857017  avg_L1_norm_grad         0.020524  w[0]    0.001 bias    0.019\n",
      "iter    3/1000000  loss         0.812126  avg_L1_norm_grad         0.021487  w[0]    0.001 bias    0.022\n",
      "iter    4/1000000  loss         0.776618  avg_L1_norm_grad         0.015146  w[0]    0.002 bias    0.037\n",
      "iter    5/1000000  loss         0.748830  avg_L1_norm_grad         0.015631  w[0]    0.002 bias    0.041\n",
      "iter    6/1000000  loss         0.725960  avg_L1_norm_grad         0.012287  w[0]    0.002 bias    0.053\n",
      "iter    7/1000000  loss         0.706768  avg_L1_norm_grad         0.012360  w[0]    0.003 bias    0.060\n",
      "iter    8/1000000  loss         0.690119  avg_L1_norm_grad         0.010819  w[0]    0.003 bias    0.069\n",
      "iter    9/1000000  loss         0.675354  avg_L1_norm_grad         0.010583  w[0]    0.003 bias    0.077\n",
      "iter   10/1000000  loss         0.662019  avg_L1_norm_grad         0.009836  w[0]    0.004 bias    0.085\n",
      "iter   11/1000000  loss         0.649819  avg_L1_norm_grad         0.009504  w[0]    0.004 bias    0.093\n",
      "iter   12/1000000  loss         0.638548  avg_L1_norm_grad         0.009066  w[0]    0.004 bias    0.100\n",
      "iter   13/1000000  loss         0.628057  avg_L1_norm_grad         0.008751  w[0]    0.005 bias    0.108\n",
      "iter   14/1000000  loss         0.618235  avg_L1_norm_grad         0.008436  w[0]    0.005 bias    0.115\n",
      "iter   15/1000000  loss         0.608994  avg_L1_norm_grad         0.008164  w[0]    0.005 bias    0.122\n",
      "iter   16/1000000  loss         0.600267  avg_L1_norm_grad         0.007912  w[0]    0.006 bias    0.129\n",
      "iter   17/1000000  loss         0.591998  avg_L1_norm_grad         0.007682  w[0]    0.006 bias    0.136\n",
      "iter   18/1000000  loss         0.584141  avg_L1_norm_grad         0.007468  w[0]    0.006 bias    0.143\n",
      "iter   19/1000000  loss         0.576659  avg_L1_norm_grad         0.007270  w[0]    0.006 bias    0.150\n",
      "iter  100/1000000  loss         0.363253  avg_L1_norm_grad         0.002541  w[0]    0.019 bias    0.482\n",
      "iter  101/1000000  loss         0.362270  avg_L1_norm_grad         0.002521  w[0]    0.019 bias    0.485\n",
      "iter  200/1000000  loss         0.305147  avg_L1_norm_grad         0.001468  w[0]    0.026 bias    0.691\n",
      "iter  201/1000000  loss         0.304801  avg_L1_norm_grad         0.001462  w[0]    0.026 bias    0.692\n",
      "iter  300/1000000  loss         0.279911  avg_L1_norm_grad         0.001063  w[0]    0.030 bias    0.827\n",
      "iter  301/1000000  loss         0.279727  avg_L1_norm_grad         0.001060  w[0]    0.030 bias    0.828\n",
      "iter  400/1000000  loss         0.265300  avg_L1_norm_grad         0.000848  w[0]    0.033 bias    0.927\n",
      "iter  401/1000000  loss         0.265185  avg_L1_norm_grad         0.000846  w[0]    0.033 bias    0.928\n",
      "iter  500/1000000  loss         0.255634  avg_L1_norm_grad         0.000712  w[0]    0.035 bias    1.006\n",
      "iter  501/1000000  loss         0.255554  avg_L1_norm_grad         0.000711  w[0]    0.035 bias    1.006\n",
      "iter  600/1000000  loss         0.248720  avg_L1_norm_grad         0.000618  w[0]    0.037 bias    1.070\n",
      "iter  601/1000000  loss         0.248661  avg_L1_norm_grad         0.000617  w[0]    0.037 bias    1.071\n",
      "iter  700/1000000  loss         0.243515  avg_L1_norm_grad         0.000547  w[0]    0.040 bias    1.125\n",
      "iter  701/1000000  loss         0.243469  avg_L1_norm_grad         0.000547  w[0]    0.040 bias    1.126\n",
      "iter  800/1000000  loss         0.239451  avg_L1_norm_grad         0.000492  w[0]    0.042 bias    1.172\n",
      "iter  801/1000000  loss         0.239414  avg_L1_norm_grad         0.000491  w[0]    0.042 bias    1.173\n",
      "iter  900/1000000  loss         0.236191  avg_L1_norm_grad         0.000446  w[0]    0.044 bias    1.214\n",
      "iter  901/1000000  loss         0.236162  avg_L1_norm_grad         0.000445  w[0]    0.044 bias    1.214\n",
      "iter 1000/1000000  loss         0.233523  avg_L1_norm_grad         0.000408  w[0]    0.046 bias    1.250\n",
      "iter 1001/1000000  loss         0.233498  avg_L1_norm_grad         0.000407  w[0]    0.046 bias    1.251\n",
      "iter 1100/1000000  loss         0.231302  avg_L1_norm_grad         0.000375  w[0]    0.048 bias    1.283\n",
      "iter 1101/1000000  loss         0.231282  avg_L1_norm_grad         0.000375  w[0]    0.048 bias    1.284\n",
      "iter 1200/1000000  loss         0.229430  avg_L1_norm_grad         0.000347  w[0]    0.050 bias    1.313\n",
      "iter 1201/1000000  loss         0.229412  avg_L1_norm_grad         0.000347  w[0]    0.050 bias    1.313\n",
      "iter 1300/1000000  loss         0.227834  avg_L1_norm_grad         0.000323  w[0]    0.052 bias    1.340\n",
      "iter 1301/1000000  loss         0.227819  avg_L1_norm_grad         0.000322  w[0]    0.052 bias    1.340\n",
      "iter 1400/1000000  loss         0.226462  avg_L1_norm_grad         0.000301  w[0]    0.054 bias    1.365\n",
      "iter 1401/1000000  loss         0.226449  avg_L1_norm_grad         0.000301  w[0]    0.054 bias    1.365\n",
      "iter 1500/1000000  loss         0.225273  avg_L1_norm_grad         0.000281  w[0]    0.056 bias    1.388\n",
      "iter 1501/1000000  loss         0.225261  avg_L1_norm_grad         0.000281  w[0]    0.056 bias    1.389\n",
      "iter 1600/1000000  loss         0.224236  avg_L1_norm_grad         0.000264  w[0]    0.058 bias    1.410\n",
      "iter 1601/1000000  loss         0.224226  avg_L1_norm_grad         0.000264  w[0]    0.058 bias    1.410\n",
      "iter 1700/1000000  loss         0.223326  avg_L1_norm_grad         0.000248  w[0]    0.060 bias    1.430\n",
      "iter 1701/1000000  loss         0.223318  avg_L1_norm_grad         0.000248  w[0]    0.060 bias    1.430\n",
      "iter 1800/1000000  loss         0.222524  avg_L1_norm_grad         0.000234  w[0]    0.062 bias    1.448\n",
      "iter 1801/1000000  loss         0.222517  avg_L1_norm_grad         0.000233  w[0]    0.062 bias    1.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1900/1000000  loss         0.221815  avg_L1_norm_grad         0.000220  w[0]    0.063 bias    1.466\n",
      "iter 1901/1000000  loss         0.221808  avg_L1_norm_grad         0.000220  w[0]    0.063 bias    1.466\n",
      "iter 2000/1000000  loss         0.221185  avg_L1_norm_grad         0.000208  w[0]    0.065 bias    1.482\n",
      "iter 2001/1000000  loss         0.221179  avg_L1_norm_grad         0.000208  w[0]    0.065 bias    1.482\n",
      "iter 2100/1000000  loss         0.220623  avg_L1_norm_grad         0.000197  w[0]    0.067 bias    1.498\n",
      "iter 2101/1000000  loss         0.220617  avg_L1_norm_grad         0.000197  w[0]    0.067 bias    1.498\n",
      "iter 2200/1000000  loss         0.220120  avg_L1_norm_grad         0.000187  w[0]    0.068 bias    1.512\n",
      "iter 2201/1000000  loss         0.220115  avg_L1_norm_grad         0.000186  w[0]    0.068 bias    1.512\n",
      "iter 2300/1000000  loss         0.219670  avg_L1_norm_grad         0.000177  w[0]    0.070 bias    1.526\n",
      "iter 2301/1000000  loss         0.219665  avg_L1_norm_grad         0.000177  w[0]    0.070 bias    1.526\n",
      "iter 2400/1000000  loss         0.219264  avg_L1_norm_grad         0.000168  w[0]    0.071 bias    1.539\n",
      "iter 2401/1000000  loss         0.219261  avg_L1_norm_grad         0.000168  w[0]    0.071 bias    1.539\n",
      "iter 2500/1000000  loss         0.218900  avg_L1_norm_grad         0.000160  w[0]    0.073 bias    1.551\n",
      "iter 2501/1000000  loss         0.218896  avg_L1_norm_grad         0.000160  w[0]    0.073 bias    1.551\n",
      "iter 2600/1000000  loss         0.218570  avg_L1_norm_grad         0.000152  w[0]    0.074 bias    1.563\n",
      "iter 2601/1000000  loss         0.218567  avg_L1_norm_grad         0.000152  w[0]    0.074 bias    1.563\n",
      "iter 2700/1000000  loss         0.218272  avg_L1_norm_grad         0.000145  w[0]    0.075 bias    1.574\n",
      "iter 2701/1000000  loss         0.218269  avg_L1_norm_grad         0.000145  w[0]    0.075 bias    1.574\n",
      "iter 2800/1000000  loss         0.218002  avg_L1_norm_grad         0.000138  w[0]    0.076 bias    1.585\n",
      "iter 2801/1000000  loss         0.217999  avg_L1_norm_grad         0.000138  w[0]    0.076 bias    1.585\n",
      "iter 2900/1000000  loss         0.217757  avg_L1_norm_grad         0.000132  w[0]    0.078 bias    1.595\n",
      "iter 2901/1000000  loss         0.217754  avg_L1_norm_grad         0.000132  w[0]    0.078 bias    1.595\n",
      "iter 3000/1000000  loss         0.217534  avg_L1_norm_grad         0.000126  w[0]    0.079 bias    1.605\n",
      "iter 3001/1000000  loss         0.217532  avg_L1_norm_grad         0.000126  w[0]    0.079 bias    1.605\n",
      "iter 3100/1000000  loss         0.217331  avg_L1_norm_grad         0.000121  w[0]    0.080 bias    1.614\n",
      "iter 3101/1000000  loss         0.217329  avg_L1_norm_grad         0.000121  w[0]    0.080 bias    1.614\n",
      "iter 3200/1000000  loss         0.217146  avg_L1_norm_grad         0.000115  w[0]    0.081 bias    1.623\n",
      "iter 3201/1000000  loss         0.217144  avg_L1_norm_grad         0.000115  w[0]    0.081 bias    1.623\n",
      "iter 3300/1000000  loss         0.216977  avg_L1_norm_grad         0.000110  w[0]    0.082 bias    1.631\n",
      "iter 3301/1000000  loss         0.216975  avg_L1_norm_grad         0.000110  w[0]    0.082 bias    1.631\n",
      "iter 3400/1000000  loss         0.216823  avg_L1_norm_grad         0.000106  w[0]    0.083 bias    1.639\n",
      "iter 3401/1000000  loss         0.216821  avg_L1_norm_grad         0.000105  w[0]    0.083 bias    1.639\n",
      "iter 3500/1000000  loss         0.216682  avg_L1_norm_grad         0.000101  w[0]    0.084 bias    1.647\n",
      "iter 3501/1000000  loss         0.216680  avg_L1_norm_grad         0.000101  w[0]    0.084 bias    1.647\n",
      "iter 3600/1000000  loss         0.216552  avg_L1_norm_grad         0.000097  w[0]    0.085 bias    1.654\n",
      "iter 3601/1000000  loss         0.216551  avg_L1_norm_grad         0.000097  w[0]    0.085 bias    1.654\n",
      "iter 3700/1000000  loss         0.216434  avg_L1_norm_grad         0.000093  w[0]    0.086 bias    1.661\n",
      "iter 3701/1000000  loss         0.216433  avg_L1_norm_grad         0.000093  w[0]    0.086 bias    1.661\n",
      "iter 3800/1000000  loss         0.216325  avg_L1_norm_grad         0.000089  w[0]    0.087 bias    1.668\n",
      "iter 3801/1000000  loss         0.216324  avg_L1_norm_grad         0.000089  w[0]    0.087 bias    1.668\n",
      "iter 3900/1000000  loss         0.216225  avg_L1_norm_grad         0.000085  w[0]    0.087 bias    1.674\n",
      "iter 3901/1000000  loss         0.216224  avg_L1_norm_grad         0.000085  w[0]    0.087 bias    1.674\n",
      "iter 4000/1000000  loss         0.216134  avg_L1_norm_grad         0.000082  w[0]    0.088 bias    1.681\n",
      "iter 4001/1000000  loss         0.216133  avg_L1_norm_grad         0.000082  w[0]    0.088 bias    1.681\n",
      "iter 4100/1000000  loss         0.216049  avg_L1_norm_grad         0.000079  w[0]    0.089 bias    1.687\n",
      "iter 4101/1000000  loss         0.216049  avg_L1_norm_grad         0.000078  w[0]    0.089 bias    1.687\n",
      "iter 4200/1000000  loss         0.215972  avg_L1_norm_grad         0.000075  w[0]    0.090 bias    1.692\n",
      "iter 4201/1000000  loss         0.215971  avg_L1_norm_grad         0.000075  w[0]    0.090 bias    1.692\n",
      "iter 4300/1000000  loss         0.215900  avg_L1_norm_grad         0.000072  w[0]    0.090 bias    1.698\n",
      "iter 4301/1000000  loss         0.215900  avg_L1_norm_grad         0.000072  w[0]    0.090 bias    1.698\n",
      "iter 4400/1000000  loss         0.215834  avg_L1_norm_grad         0.000070  w[0]    0.091 bias    1.703\n",
      "iter 4401/1000000  loss         0.215834  avg_L1_norm_grad         0.000069  w[0]    0.091 bias    1.703\n",
      "iter 4500/1000000  loss         0.215774  avg_L1_norm_grad         0.000067  w[0]    0.092 bias    1.708\n",
      "iter 4501/1000000  loss         0.215773  avg_L1_norm_grad         0.000067  w[0]    0.092 bias    1.708\n",
      "iter 4600/1000000  loss         0.215718  avg_L1_norm_grad         0.000064  w[0]    0.092 bias    1.713\n",
      "iter 4601/1000000  loss         0.215717  avg_L1_norm_grad         0.000064  w[0]    0.092 bias    1.713\n",
      "iter 4700/1000000  loss         0.215666  avg_L1_norm_grad         0.000062  w[0]    0.093 bias    1.718\n",
      "iter 4701/1000000  loss         0.215665  avg_L1_norm_grad         0.000062  w[0]    0.093 bias    1.718\n",
      "iter 4800/1000000  loss         0.215618  avg_L1_norm_grad         0.000059  w[0]    0.093 bias    1.722\n",
      "iter 4801/1000000  loss         0.215618  avg_L1_norm_grad         0.000059  w[0]    0.093 bias    1.722\n",
      "iter 4900/1000000  loss         0.215574  avg_L1_norm_grad         0.000057  w[0]    0.094 bias    1.726\n",
      "iter 4901/1000000  loss         0.215573  avg_L1_norm_grad         0.000057  w[0]    0.094 bias    1.726\n",
      "iter 5000/1000000  loss         0.215533  avg_L1_norm_grad         0.000055  w[0]    0.094 bias    1.730\n",
      "iter 5001/1000000  loss         0.215532  avg_L1_norm_grad         0.000055  w[0]    0.094 bias    1.730\n",
      "iter 5100/1000000  loss         0.215495  avg_L1_norm_grad         0.000053  w[0]    0.095 bias    1.734\n",
      "iter 5101/1000000  loss         0.215495  avg_L1_norm_grad         0.000053  w[0]    0.095 bias    1.734\n",
      "iter 5200/1000000  loss         0.215460  avg_L1_norm_grad         0.000051  w[0]    0.095 bias    1.738\n",
      "iter 5201/1000000  loss         0.215460  avg_L1_norm_grad         0.000051  w[0]    0.095 bias    1.738\n",
      "iter 5300/1000000  loss         0.215427  avg_L1_norm_grad         0.000049  w[0]    0.096 bias    1.742\n",
      "iter 5301/1000000  loss         0.215427  avg_L1_norm_grad         0.000049  w[0]    0.096 bias    1.742\n",
      "iter 5400/1000000  loss         0.215397  avg_L1_norm_grad         0.000047  w[0]    0.096 bias    1.745\n",
      "iter 5401/1000000  loss         0.215397  avg_L1_norm_grad         0.000047  w[0]    0.096 bias    1.745\n",
      "iter 5500/1000000  loss         0.215370  avg_L1_norm_grad         0.000045  w[0]    0.097 bias    1.749\n",
      "iter 5501/1000000  loss         0.215369  avg_L1_norm_grad         0.000045  w[0]    0.097 bias    1.749\n",
      "iter 5600/1000000  loss         0.215344  avg_L1_norm_grad         0.000044  w[0]    0.097 bias    1.752\n",
      "iter 5601/1000000  loss         0.215343  avg_L1_norm_grad         0.000044  w[0]    0.097 bias    1.752\n",
      "iter 5700/1000000  loss         0.215320  avg_L1_norm_grad         0.000042  w[0]    0.098 bias    1.755\n",
      "iter 5701/1000000  loss         0.215319  avg_L1_norm_grad         0.000042  w[0]    0.098 bias    1.755\n",
      "iter 5800/1000000  loss         0.215297  avg_L1_norm_grad         0.000041  w[0]    0.098 bias    1.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5801/1000000  loss         0.215297  avg_L1_norm_grad         0.000041  w[0]    0.098 bias    1.758\n",
      "iter 5900/1000000  loss         0.215277  avg_L1_norm_grad         0.000039  w[0]    0.098 bias    1.761\n",
      "iter 5901/1000000  loss         0.215277  avg_L1_norm_grad         0.000039  w[0]    0.098 bias    1.761\n",
      "iter 6000/1000000  loss         0.215257  avg_L1_norm_grad         0.000038  w[0]    0.099 bias    1.764\n",
      "iter 6001/1000000  loss         0.215257  avg_L1_norm_grad         0.000038  w[0]    0.099 bias    1.764\n",
      "iter 6100/1000000  loss         0.215240  avg_L1_norm_grad         0.000036  w[0]    0.099 bias    1.767\n",
      "iter 6101/1000000  loss         0.215239  avg_L1_norm_grad         0.000036  w[0]    0.099 bias    1.767\n",
      "iter 6200/1000000  loss         0.215223  avg_L1_norm_grad         0.000035  w[0]    0.099 bias    1.769\n",
      "iter 6201/1000000  loss         0.215223  avg_L1_norm_grad         0.000035  w[0]    0.099 bias    1.769\n",
      "iter 6300/1000000  loss         0.215208  avg_L1_norm_grad         0.000034  w[0]    0.100 bias    1.772\n",
      "iter 6301/1000000  loss         0.215207  avg_L1_norm_grad         0.000034  w[0]    0.100 bias    1.772\n",
      "iter 6400/1000000  loss         0.215193  avg_L1_norm_grad         0.000033  w[0]    0.100 bias    1.774\n",
      "iter 6401/1000000  loss         0.215193  avg_L1_norm_grad         0.000033  w[0]    0.100 bias    1.774\n",
      "iter 6500/1000000  loss         0.215180  avg_L1_norm_grad         0.000031  w[0]    0.100 bias    1.776\n",
      "iter 6501/1000000  loss         0.215180  avg_L1_norm_grad         0.000031  w[0]    0.100 bias    1.776\n",
      "iter 6600/1000000  loss         0.215167  avg_L1_norm_grad         0.000030  w[0]    0.100 bias    1.779\n",
      "iter 6601/1000000  loss         0.215167  avg_L1_norm_grad         0.000030  w[0]    0.100 bias    1.779\n",
      "iter 6700/1000000  loss         0.215156  avg_L1_norm_grad         0.000029  w[0]    0.101 bias    1.781\n",
      "iter 6701/1000000  loss         0.215156  avg_L1_norm_grad         0.000029  w[0]    0.101 bias    1.781\n",
      "iter 6800/1000000  loss         0.215145  avg_L1_norm_grad         0.000028  w[0]    0.101 bias    1.783\n",
      "iter 6801/1000000  loss         0.215145  avg_L1_norm_grad         0.000028  w[0]    0.101 bias    1.783\n",
      "iter 6900/1000000  loss         0.215135  avg_L1_norm_grad         0.000027  w[0]    0.101 bias    1.785\n",
      "iter 6901/1000000  loss         0.215135  avg_L1_norm_grad         0.000027  w[0]    0.101 bias    1.785\n",
      "iter 7000/1000000  loss         0.215126  avg_L1_norm_grad         0.000026  w[0]    0.101 bias    1.787\n",
      "iter 7001/1000000  loss         0.215126  avg_L1_norm_grad         0.000026  w[0]    0.101 bias    1.787\n",
      "iter 7100/1000000  loss         0.215117  avg_L1_norm_grad         0.000025  w[0]    0.102 bias    1.789\n",
      "iter 7101/1000000  loss         0.215117  avg_L1_norm_grad         0.000025  w[0]    0.102 bias    1.789\n",
      "iter 7200/1000000  loss         0.215109  avg_L1_norm_grad         0.000025  w[0]    0.102 bias    1.791\n",
      "iter 7201/1000000  loss         0.215109  avg_L1_norm_grad         0.000025  w[0]    0.102 bias    1.791\n",
      "iter 7300/1000000  loss         0.215101  avg_L1_norm_grad         0.000024  w[0]    0.102 bias    1.792\n",
      "iter 7301/1000000  loss         0.215101  avg_L1_norm_grad         0.000024  w[0]    0.102 bias    1.792\n",
      "iter 7400/1000000  loss         0.215094  avg_L1_norm_grad         0.000023  w[0]    0.102 bias    1.794\n",
      "iter 7401/1000000  loss         0.215094  avg_L1_norm_grad         0.000023  w[0]    0.102 bias    1.794\n",
      "iter 7500/1000000  loss         0.215088  avg_L1_norm_grad         0.000022  w[0]    0.102 bias    1.796\n",
      "iter 7501/1000000  loss         0.215088  avg_L1_norm_grad         0.000022  w[0]    0.102 bias    1.796\n",
      "iter 7600/1000000  loss         0.215081  avg_L1_norm_grad         0.000021  w[0]    0.103 bias    1.797\n",
      "iter 7601/1000000  loss         0.215081  avg_L1_norm_grad         0.000021  w[0]    0.103 bias    1.797\n",
      "iter 7700/1000000  loss         0.215076  avg_L1_norm_grad         0.000021  w[0]    0.103 bias    1.799\n",
      "iter 7701/1000000  loss         0.215076  avg_L1_norm_grad         0.000021  w[0]    0.103 bias    1.799\n",
      "iter 7800/1000000  loss         0.215070  avg_L1_norm_grad         0.000020  w[0]    0.103 bias    1.800\n",
      "iter 7801/1000000  loss         0.215070  avg_L1_norm_grad         0.000020  w[0]    0.103 bias    1.800\n",
      "iter 7900/1000000  loss         0.215065  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    1.801\n",
      "iter 7901/1000000  loss         0.215065  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    1.801\n",
      "iter 8000/1000000  loss         0.215061  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    1.803\n",
      "iter 8001/1000000  loss         0.215061  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    1.803\n",
      "iter 8100/1000000  loss         0.215056  avg_L1_norm_grad         0.000018  w[0]    0.103 bias    1.804\n",
      "iter 8101/1000000  loss         0.215056  avg_L1_norm_grad         0.000018  w[0]    0.103 bias    1.804\n",
      "iter 8200/1000000  loss         0.215052  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    1.805\n",
      "iter 8201/1000000  loss         0.215052  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    1.805\n",
      "iter 8300/1000000  loss         0.215048  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    1.806\n",
      "iter 8301/1000000  loss         0.215048  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    1.806\n",
      "iter 8400/1000000  loss         0.215045  avg_L1_norm_grad         0.000016  w[0]    0.104 bias    1.808\n",
      "iter 8401/1000000  loss         0.215045  avg_L1_norm_grad         0.000016  w[0]    0.104 bias    1.808\n",
      "iter 8500/1000000  loss         0.215041  avg_L1_norm_grad         0.000016  w[0]    0.104 bias    1.809\n",
      "iter 8501/1000000  loss         0.215041  avg_L1_norm_grad         0.000016  w[0]    0.104 bias    1.809\n",
      "iter 8600/1000000  loss         0.215038  avg_L1_norm_grad         0.000015  w[0]    0.104 bias    1.810\n",
      "iter 8601/1000000  loss         0.215038  avg_L1_norm_grad         0.000015  w[0]    0.104 bias    1.810\n",
      "iter 8700/1000000  loss         0.215035  avg_L1_norm_grad         0.000015  w[0]    0.104 bias    1.811\n",
      "iter 8701/1000000  loss         0.215035  avg_L1_norm_grad         0.000015  w[0]    0.104 bias    1.811\n",
      "Done. Converged after 8788 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9366666666666407\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "No Noise New 0.9369444444444184\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Accuracy 0.9581944444444417\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "New Accuracy 0.9622222222222195\n"
     ]
    }
   ],
   "source": [
    "y_hat_Origin=np.asarray(orig_lr.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)\n",
    "\n",
    "y_hat_New=np.asarray(new_lr.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "plt.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "plt.plot(fpr3te,tpr3te, label=\"Adding All Linear\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "plt.plot(fpr1Tte,tpr1Tte, label=\"No added Feature No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "plt.plot(fprTte,tprTte, label=\"Adding All Linear\")\n",
    "\n",
    "plt.xlim([-0.0, 1.0]);\n",
    "plt.ylim([-0.0, 1.0]);\n",
    "plt.legend();\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXV1\n",
    "Error Rate: 0.435 0.993667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
