{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(0,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "#orig_lr2 = LRGD(alpha=10.0, step_size=0.1)\n",
    "#orig_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_Origin=np.asarray(orig_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "#tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "#acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "#print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "#new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "#new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 1572 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028984  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.942317  avg_L1_norm_grad         0.056494  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         1.259524  avg_L1_norm_grad         0.094324  w[0]    0.002 bias    0.034\n",
      "iter    3/1000000  loss         2.026858  avg_L1_norm_grad         0.112214  w[0]   -0.000 bias   -0.008\n",
      "iter    4/1000000  loss         1.060370  avg_L1_norm_grad         0.083845  w[0]    0.003 bias    0.056\n",
      "iter    5/1000000  loss         1.496390  avg_L1_norm_grad         0.093988  w[0]    0.001 bias    0.019\n",
      "iter    6/1000000  loss         0.912403  avg_L1_norm_grad         0.071334  w[0]    0.004 bias    0.073\n",
      "iter    7/1000000  loss         1.135081  avg_L1_norm_grad         0.075217  w[0]    0.003 bias    0.043\n",
      "iter    8/1000000  loss         0.782245  avg_L1_norm_grad         0.058627  w[0]    0.005 bias    0.088\n",
      "iter    9/1000000  loss         0.898298  avg_L1_norm_grad         0.060818  w[0]    0.004 bias    0.063\n",
      "iter   10/1000000  loss         0.698038  avg_L1_norm_grad         0.050612  w[0]    0.006 bias    0.101\n",
      "iter   11/1000000  loss         0.769535  avg_L1_norm_grad         0.052170  w[0]    0.005 bias    0.080\n",
      "iter   12/1000000  loss         0.636529  avg_L1_norm_grad         0.044956  w[0]    0.006 bias    0.113\n",
      "iter   13/1000000  loss         0.685056  avg_L1_norm_grad         0.046161  w[0]    0.006 bias    0.095\n",
      "iter   14/1000000  loss         0.588192  avg_L1_norm_grad         0.040456  w[0]    0.007 bias    0.124\n",
      "iter   15/1000000  loss         0.622080  avg_L1_norm_grad         0.041358  w[0]    0.006 bias    0.108\n",
      "iter   16/1000000  loss         0.548167  avg_L1_norm_grad         0.036455  w[0]    0.008 bias    0.134\n",
      "iter   17/1000000  loss         0.571104  avg_L1_norm_grad         0.037052  w[0]    0.007 bias    0.119\n",
      "iter   18/1000000  loss         0.513897  avg_L1_norm_grad         0.032615  w[0]    0.008 bias    0.143\n",
      "iter   19/1000000  loss         0.528034  avg_L1_norm_grad         0.032910  w[0]    0.008 bias    0.130\n",
      "iter  100/1000000  loss         0.279194  avg_L1_norm_grad         0.001292  w[0]    0.021 bias    0.308\n",
      "iter  101/1000000  loss         0.278551  avg_L1_norm_grad         0.001283  w[0]    0.021 bias    0.309\n",
      "iter  200/1000000  loss         0.240791  avg_L1_norm_grad         0.000813  w[0]    0.029 bias    0.388\n",
      "iter  201/1000000  loss         0.240558  avg_L1_norm_grad         0.000811  w[0]    0.029 bias    0.388\n",
      "iter  300/1000000  loss         0.223519  avg_L1_norm_grad         0.000624  w[0]    0.035 bias    0.428\n",
      "iter  301/1000000  loss         0.223391  avg_L1_norm_grad         0.000623  w[0]    0.035 bias    0.429\n",
      "iter  400/1000000  loss         0.213115  avg_L1_norm_grad         0.000517  w[0]    0.040 bias    0.453\n",
      "iter  401/1000000  loss         0.213031  avg_L1_norm_grad         0.000516  w[0]    0.040 bias    0.453\n",
      "iter  500/1000000  loss         0.205955  avg_L1_norm_grad         0.000446  w[0]    0.044 bias    0.468\n",
      "iter  501/1000000  loss         0.205895  avg_L1_norm_grad         0.000446  w[0]    0.044 bias    0.469\n",
      "iter  600/1000000  loss         0.200649  avg_L1_norm_grad         0.000394  w[0]    0.047 bias    0.479\n",
      "iter  601/1000000  loss         0.200603  avg_L1_norm_grad         0.000394  w[0]    0.047 bias    0.479\n",
      "iter  700/1000000  loss         0.196532  avg_L1_norm_grad         0.000354  w[0]    0.049 bias    0.487\n",
      "iter  701/1000000  loss         0.196495  avg_L1_norm_grad         0.000353  w[0]    0.049 bias    0.487\n",
      "iter  800/1000000  loss         0.193236  avg_L1_norm_grad         0.000321  w[0]    0.052 bias    0.492\n",
      "iter  801/1000000  loss         0.193207  avg_L1_norm_grad         0.000321  w[0]    0.052 bias    0.492\n",
      "iter  900/1000000  loss         0.190539  avg_L1_norm_grad         0.000293  w[0]    0.053 bias    0.496\n",
      "iter  901/1000000  loss         0.190515  avg_L1_norm_grad         0.000293  w[0]    0.054 bias    0.496\n",
      "iter 1000/1000000  loss         0.188295  avg_L1_norm_grad         0.000269  w[0]    0.055 bias    0.499\n",
      "iter 1001/1000000  loss         0.188275  avg_L1_norm_grad         0.000269  w[0]    0.055 bias    0.499\n",
      "iter 1100/1000000  loss         0.186403  avg_L1_norm_grad         0.000249  w[0]    0.057 bias    0.502\n",
      "iter 1101/1000000  loss         0.186385  avg_L1_norm_grad         0.000249  w[0]    0.057 bias    0.502\n",
      "iter 1200/1000000  loss         0.184791  avg_L1_norm_grad         0.000231  w[0]    0.058 bias    0.504\n",
      "iter 1201/1000000  loss         0.184776  avg_L1_norm_grad         0.000231  w[0]    0.058 bias    0.504\n",
      "iter 1300/1000000  loss         0.183405  avg_L1_norm_grad         0.000215  w[0]    0.060 bias    0.505\n",
      "iter 1301/1000000  loss         0.183392  avg_L1_norm_grad         0.000214  w[0]    0.060 bias    0.505\n",
      "iter 1400/1000000  loss         0.182204  avg_L1_norm_grad         0.000200  w[0]    0.061 bias    0.506\n",
      "iter 1401/1000000  loss         0.182193  avg_L1_norm_grad         0.000200  w[0]    0.061 bias    0.506\n",
      "iter 1500/1000000  loss         0.181158  avg_L1_norm_grad         0.000188  w[0]    0.062 bias    0.507\n",
      "iter 1501/1000000  loss         0.181148  avg_L1_norm_grad         0.000187  w[0]    0.062 bias    0.507\n",
      "iter 1600/1000000  loss         0.180241  avg_L1_norm_grad         0.000176  w[0]    0.063 bias    0.508\n",
      "iter 1601/1000000  loss         0.180233  avg_L1_norm_grad         0.000176  w[0]    0.063 bias    0.508\n",
      "iter 1700/1000000  loss         0.179434  avg_L1_norm_grad         0.000165  w[0]    0.064 bias    0.508\n",
      "iter 1701/1000000  loss         0.179426  avg_L1_norm_grad         0.000165  w[0]    0.064 bias    0.508\n",
      "iter 1800/1000000  loss         0.178719  avg_L1_norm_grad         0.000156  w[0]    0.064 bias    0.509\n",
      "iter 1801/1000000  loss         0.178713  avg_L1_norm_grad         0.000155  w[0]    0.064 bias    0.509\n",
      "iter 1900/1000000  loss         0.178085  avg_L1_norm_grad         0.000147  w[0]    0.065 bias    0.509\n",
      "iter 1901/1000000  loss         0.178079  avg_L1_norm_grad         0.000147  w[0]    0.065 bias    0.509\n",
      "iter 2000/1000000  loss         0.177520  avg_L1_norm_grad         0.000138  w[0]    0.066 bias    0.509\n",
      "iter 2001/1000000  loss         0.177515  avg_L1_norm_grad         0.000138  w[0]    0.066 bias    0.509\n",
      "iter 2100/1000000  loss         0.177015  avg_L1_norm_grad         0.000131  w[0]    0.067 bias    0.509\n",
      "iter 2101/1000000  loss         0.177010  avg_L1_norm_grad         0.000131  w[0]    0.067 bias    0.509\n",
      "iter 2200/1000000  loss         0.176563  avg_L1_norm_grad         0.000124  w[0]    0.067 bias    0.509\n",
      "iter 2201/1000000  loss         0.176558  avg_L1_norm_grad         0.000124  w[0]    0.067 bias    0.509\n",
      "iter 2300/1000000  loss         0.176156  avg_L1_norm_grad         0.000118  w[0]    0.068 bias    0.509\n",
      "iter 2301/1000000  loss         0.176152  avg_L1_norm_grad         0.000117  w[0]    0.068 bias    0.509\n",
      "iter 2400/1000000  loss         0.175789  avg_L1_norm_grad         0.000111  w[0]    0.069 bias    0.509\n",
      "iter 2401/1000000  loss         0.175786  avg_L1_norm_grad         0.000111  w[0]    0.069 bias    0.508\n",
      "iter 2500/1000000  loss         0.175459  avg_L1_norm_grad         0.000106  w[0]    0.069 bias    0.508\n",
      "iter 2501/1000000  loss         0.175455  avg_L1_norm_grad         0.000106  w[0]    0.069 bias    0.508\n",
      "iter 2600/1000000  loss         0.175159  avg_L1_norm_grad         0.000101  w[0]    0.070 bias    0.508\n",
      "iter 2601/1000000  loss         0.175157  avg_L1_norm_grad         0.000101  w[0]    0.070 bias    0.508\n",
      "iter 2700/1000000  loss         0.174888  avg_L1_norm_grad         0.000096  w[0]    0.070 bias    0.507\n",
      "iter 2701/1000000  loss         0.174886  avg_L1_norm_grad         0.000096  w[0]    0.070 bias    0.507\n",
      "iter 2800/1000000  loss         0.174642  avg_L1_norm_grad         0.000091  w[0]    0.071 bias    0.507\n",
      "iter 2801/1000000  loss         0.174640  avg_L1_norm_grad         0.000091  w[0]    0.071 bias    0.507\n",
      "iter 2900/1000000  loss         0.174418  avg_L1_norm_grad         0.000087  w[0]    0.071 bias    0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.174416  avg_L1_norm_grad         0.000087  w[0]    0.071 bias    0.506\n",
      "iter 3000/1000000  loss         0.174214  avg_L1_norm_grad         0.000083  w[0]    0.071 bias    0.506\n",
      "iter 3001/1000000  loss         0.174212  avg_L1_norm_grad         0.000083  w[0]    0.071 bias    0.505\n",
      "iter 3100/1000000  loss         0.174028  avg_L1_norm_grad         0.000079  w[0]    0.072 bias    0.505\n",
      "iter 3101/1000000  loss         0.174027  avg_L1_norm_grad         0.000079  w[0]    0.072 bias    0.505\n",
      "iter 3200/1000000  loss         0.173859  avg_L1_norm_grad         0.000075  w[0]    0.072 bias    0.504\n",
      "iter 3201/1000000  loss         0.173857  avg_L1_norm_grad         0.000075  w[0]    0.072 bias    0.504\n",
      "iter 3300/1000000  loss         0.173703  avg_L1_norm_grad         0.000072  w[0]    0.073 bias    0.503\n",
      "iter 3301/1000000  loss         0.173702  avg_L1_norm_grad         0.000072  w[0]    0.073 bias    0.503\n",
      "iter 3400/1000000  loss         0.173561  avg_L1_norm_grad         0.000069  w[0]    0.073 bias    0.503\n",
      "iter 3401/1000000  loss         0.173560  avg_L1_norm_grad         0.000069  w[0]    0.073 bias    0.503\n",
      "iter 3500/1000000  loss         0.173431  avg_L1_norm_grad         0.000066  w[0]    0.073 bias    0.502\n",
      "iter 3501/1000000  loss         0.173430  avg_L1_norm_grad         0.000066  w[0]    0.073 bias    0.502\n",
      "iter 3600/1000000  loss         0.173311  avg_L1_norm_grad         0.000063  w[0]    0.074 bias    0.501\n",
      "iter 3601/1000000  loss         0.173310  avg_L1_norm_grad         0.000063  w[0]    0.074 bias    0.501\n",
      "iter 3700/1000000  loss         0.173202  avg_L1_norm_grad         0.000060  w[0]    0.074 bias    0.500\n",
      "iter 3701/1000000  loss         0.173201  avg_L1_norm_grad         0.000060  w[0]    0.074 bias    0.500\n",
      "iter 3800/1000000  loss         0.173101  avg_L1_norm_grad         0.000058  w[0]    0.074 bias    0.499\n",
      "iter 3801/1000000  loss         0.173100  avg_L1_norm_grad         0.000058  w[0]    0.074 bias    0.499\n",
      "iter 3900/1000000  loss         0.173008  avg_L1_norm_grad         0.000055  w[0]    0.074 bias    0.498\n",
      "iter 3901/1000000  loss         0.173007  avg_L1_norm_grad         0.000055  w[0]    0.074 bias    0.498\n",
      "iter 4000/1000000  loss         0.172922  avg_L1_norm_grad         0.000053  w[0]    0.075 bias    0.498\n",
      "iter 4001/1000000  loss         0.172922  avg_L1_norm_grad         0.000053  w[0]    0.075 bias    0.498\n",
      "iter 4100/1000000  loss         0.172844  avg_L1_norm_grad         0.000051  w[0]    0.075 bias    0.497\n",
      "iter 4101/1000000  loss         0.172843  avg_L1_norm_grad         0.000051  w[0]    0.075 bias    0.497\n",
      "iter 4200/1000000  loss         0.172771  avg_L1_norm_grad         0.000049  w[0]    0.075 bias    0.496\n",
      "iter 4201/1000000  loss         0.172770  avg_L1_norm_grad         0.000049  w[0]    0.075 bias    0.496\n",
      "iter 4300/1000000  loss         0.172704  avg_L1_norm_grad         0.000047  w[0]    0.075 bias    0.495\n",
      "iter 4301/1000000  loss         0.172703  avg_L1_norm_grad         0.000047  w[0]    0.075 bias    0.495\n",
      "iter 4400/1000000  loss         0.172642  avg_L1_norm_grad         0.000045  w[0]    0.076 bias    0.494\n",
      "iter 4401/1000000  loss         0.172641  avg_L1_norm_grad         0.000045  w[0]    0.076 bias    0.494\n",
      "iter 4500/1000000  loss         0.172584  avg_L1_norm_grad         0.000043  w[0]    0.076 bias    0.493\n",
      "iter 4501/1000000  loss         0.172584  avg_L1_norm_grad         0.000043  w[0]    0.076 bias    0.493\n",
      "iter 4600/1000000  loss         0.172531  avg_L1_norm_grad         0.000041  w[0]    0.076 bias    0.492\n",
      "iter 4601/1000000  loss         0.172531  avg_L1_norm_grad         0.000041  w[0]    0.076 bias    0.492\n",
      "iter 4700/1000000  loss         0.172482  avg_L1_norm_grad         0.000040  w[0]    0.076 bias    0.491\n",
      "iter 4701/1000000  loss         0.172482  avg_L1_norm_grad         0.000040  w[0]    0.076 bias    0.491\n",
      "iter 4800/1000000  loss         0.172437  avg_L1_norm_grad         0.000038  w[0]    0.076 bias    0.490\n",
      "iter 4801/1000000  loss         0.172436  avg_L1_norm_grad         0.000038  w[0]    0.076 bias    0.490\n",
      "iter 4900/1000000  loss         0.172394  avg_L1_norm_grad         0.000037  w[0]    0.077 bias    0.489\n",
      "iter 4901/1000000  loss         0.172394  avg_L1_norm_grad         0.000037  w[0]    0.077 bias    0.489\n",
      "iter 5000/1000000  loss         0.172355  avg_L1_norm_grad         0.000035  w[0]    0.077 bias    0.488\n",
      "iter 5001/1000000  loss         0.172355  avg_L1_norm_grad         0.000035  w[0]    0.077 bias    0.488\n",
      "iter 5100/1000000  loss         0.172319  avg_L1_norm_grad         0.000034  w[0]    0.077 bias    0.488\n",
      "iter 5101/1000000  loss         0.172318  avg_L1_norm_grad         0.000034  w[0]    0.077 bias    0.488\n",
      "iter 5200/1000000  loss         0.172285  avg_L1_norm_grad         0.000032  w[0]    0.077 bias    0.487\n",
      "iter 5201/1000000  loss         0.172285  avg_L1_norm_grad         0.000032  w[0]    0.077 bias    0.487\n",
      "iter 5300/1000000  loss         0.172254  avg_L1_norm_grad         0.000031  w[0]    0.077 bias    0.486\n",
      "iter 5301/1000000  loss         0.172253  avg_L1_norm_grad         0.000031  w[0]    0.077 bias    0.486\n",
      "iter 5400/1000000  loss         0.172225  avg_L1_norm_grad         0.000030  w[0]    0.077 bias    0.485\n",
      "iter 5401/1000000  loss         0.172224  avg_L1_norm_grad         0.000030  w[0]    0.077 bias    0.485\n",
      "iter 5500/1000000  loss         0.172197  avg_L1_norm_grad         0.000029  w[0]    0.078 bias    0.484\n",
      "iter 5501/1000000  loss         0.172197  avg_L1_norm_grad         0.000029  w[0]    0.078 bias    0.484\n",
      "iter 5600/1000000  loss         0.172172  avg_L1_norm_grad         0.000028  w[0]    0.078 bias    0.483\n",
      "iter 5601/1000000  loss         0.172172  avg_L1_norm_grad         0.000028  w[0]    0.078 bias    0.483\n",
      "iter 5700/1000000  loss         0.172149  avg_L1_norm_grad         0.000027  w[0]    0.078 bias    0.482\n",
      "iter 5701/1000000  loss         0.172148  avg_L1_norm_grad         0.000027  w[0]    0.078 bias    0.482\n",
      "iter 5800/1000000  loss         0.172127  avg_L1_norm_grad         0.000026  w[0]    0.078 bias    0.481\n",
      "iter 5801/1000000  loss         0.172127  avg_L1_norm_grad         0.000026  w[0]    0.078 bias    0.481\n",
      "iter 5900/1000000  loss         0.172106  avg_L1_norm_grad         0.000025  w[0]    0.078 bias    0.480\n",
      "iter 5901/1000000  loss         0.172106  avg_L1_norm_grad         0.000025  w[0]    0.078 bias    0.480\n",
      "iter 6000/1000000  loss         0.172087  avg_L1_norm_grad         0.000024  w[0]    0.078 bias    0.480\n",
      "iter 6001/1000000  loss         0.172087  avg_L1_norm_grad         0.000024  w[0]    0.078 bias    0.480\n",
      "iter 6100/1000000  loss         0.172070  avg_L1_norm_grad         0.000023  w[0]    0.078 bias    0.479\n",
      "iter 6101/1000000  loss         0.172069  avg_L1_norm_grad         0.000023  w[0]    0.078 bias    0.479\n",
      "iter 6200/1000000  loss         0.172053  avg_L1_norm_grad         0.000022  w[0]    0.078 bias    0.478\n",
      "iter 6201/1000000  loss         0.172053  avg_L1_norm_grad         0.000022  w[0]    0.078 bias    0.478\n",
      "iter 6300/1000000  loss         0.172037  avg_L1_norm_grad         0.000021  w[0]    0.078 bias    0.477\n",
      "iter 6301/1000000  loss         0.172037  avg_L1_norm_grad         0.000021  w[0]    0.078 bias    0.477\n",
      "iter 6400/1000000  loss         0.172023  avg_L1_norm_grad         0.000020  w[0]    0.079 bias    0.476\n",
      "iter 6401/1000000  loss         0.172023  avg_L1_norm_grad         0.000020  w[0]    0.079 bias    0.476\n",
      "iter 6500/1000000  loss         0.172009  avg_L1_norm_grad         0.000020  w[0]    0.079 bias    0.476\n",
      "iter 6501/1000000  loss         0.172009  avg_L1_norm_grad         0.000020  w[0]    0.079 bias    0.476\n",
      "iter 6600/1000000  loss         0.171997  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    0.475\n",
      "iter 6601/1000000  loss         0.171997  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    0.475\n",
      "iter 6700/1000000  loss         0.171985  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    0.474\n",
      "iter 6701/1000000  loss         0.171985  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    0.474\n",
      "iter 6800/1000000  loss         0.171974  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    0.473\n",
      "iter 6801/1000000  loss         0.171974  avg_L1_norm_grad         0.000018  w[0]    0.079 bias    0.473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.171964  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    0.472\n",
      "iter 6901/1000000  loss         0.171963  avg_L1_norm_grad         0.000017  w[0]    0.079 bias    0.472\n",
      "iter 7000/1000000  loss         0.171954  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    0.472\n",
      "iter 7001/1000000  loss         0.171954  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    0.472\n",
      "iter 7100/1000000  loss         0.171945  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    0.471\n",
      "iter 7101/1000000  loss         0.171945  avg_L1_norm_grad         0.000016  w[0]    0.079 bias    0.471\n",
      "iter 7200/1000000  loss         0.171936  avg_L1_norm_grad         0.000015  w[0]    0.079 bias    0.470\n",
      "iter 7201/1000000  loss         0.171936  avg_L1_norm_grad         0.000015  w[0]    0.079 bias    0.470\n",
      "iter 7300/1000000  loss         0.171928  avg_L1_norm_grad         0.000015  w[0]    0.079 bias    0.470\n",
      "iter 7301/1000000  loss         0.171928  avg_L1_norm_grad         0.000015  w[0]    0.079 bias    0.470\n",
      "iter 7400/1000000  loss         0.171921  avg_L1_norm_grad         0.000014  w[0]    0.079 bias    0.469\n",
      "iter 7401/1000000  loss         0.171921  avg_L1_norm_grad         0.000014  w[0]    0.079 bias    0.469\n",
      "iter 7500/1000000  loss         0.171914  avg_L1_norm_grad         0.000014  w[0]    0.079 bias    0.468\n",
      "iter 7501/1000000  loss         0.171914  avg_L1_norm_grad         0.000014  w[0]    0.079 bias    0.468\n",
      "iter 7600/1000000  loss         0.171907  avg_L1_norm_grad         0.000013  w[0]    0.079 bias    0.468\n",
      "iter 7601/1000000  loss         0.171907  avg_L1_norm_grad         0.000013  w[0]    0.079 bias    0.468\n",
      "iter 7700/1000000  loss         0.171901  avg_L1_norm_grad         0.000013  w[0]    0.079 bias    0.467\n",
      "iter 7701/1000000  loss         0.171901  avg_L1_norm_grad         0.000013  w[0]    0.079 bias    0.467\n",
      "iter 7800/1000000  loss         0.171895  avg_L1_norm_grad         0.000012  w[0]    0.080 bias    0.466\n",
      "iter 7801/1000000  loss         0.171895  avg_L1_norm_grad         0.000012  w[0]    0.080 bias    0.466\n",
      "iter 7900/1000000  loss         0.171889  avg_L1_norm_grad         0.000012  w[0]    0.080 bias    0.466\n",
      "iter 7901/1000000  loss         0.171889  avg_L1_norm_grad         0.000012  w[0]    0.080 bias    0.466\n",
      "iter 8000/1000000  loss         0.171884  avg_L1_norm_grad         0.000012  w[0]    0.080 bias    0.465\n",
      "iter 8001/1000000  loss         0.171884  avg_L1_norm_grad         0.000012  w[0]    0.080 bias    0.465\n",
      "iter 8100/1000000  loss         0.171879  avg_L1_norm_grad         0.000011  w[0]    0.080 bias    0.464\n",
      "iter 8101/1000000  loss         0.171879  avg_L1_norm_grad         0.000011  w[0]    0.080 bias    0.464\n",
      "iter 8200/1000000  loss         0.171875  avg_L1_norm_grad         0.000011  w[0]    0.080 bias    0.464\n",
      "iter 8201/1000000  loss         0.171875  avg_L1_norm_grad         0.000011  w[0]    0.080 bias    0.464\n",
      "iter 8300/1000000  loss         0.171871  avg_L1_norm_grad         0.000010  w[0]    0.080 bias    0.463\n",
      "iter 8301/1000000  loss         0.171871  avg_L1_norm_grad         0.000010  w[0]    0.080 bias    0.463\n",
      "iter 8400/1000000  loss         0.171867  avg_L1_norm_grad         0.000010  w[0]    0.080 bias    0.463\n",
      "iter 8401/1000000  loss         0.171867  avg_L1_norm_grad         0.000010  w[0]    0.080 bias    0.463\n",
      "iter 8500/1000000  loss         0.171863  avg_L1_norm_grad         0.000010  w[0]    0.080 bias    0.462\n",
      "iter 8501/1000000  loss         0.171863  avg_L1_norm_grad         0.000010  w[0]    0.080 bias    0.462\n",
      "iter 8600/1000000  loss         0.171859  avg_L1_norm_grad         0.000009  w[0]    0.080 bias    0.462\n",
      "iter 8601/1000000  loss         0.171859  avg_L1_norm_grad         0.000009  w[0]    0.080 bias    0.461\n",
      "iter 8700/1000000  loss         0.171856  avg_L1_norm_grad         0.000009  w[0]    0.080 bias    0.461\n",
      "iter 8701/1000000  loss         0.171856  avg_L1_norm_grad         0.000009  w[0]    0.080 bias    0.461\n",
      "iter 8800/1000000  loss         0.171853  avg_L1_norm_grad         0.000009  w[0]    0.080 bias    0.460\n",
      "iter 8801/1000000  loss         0.171853  avg_L1_norm_grad         0.000009  w[0]    0.080 bias    0.460\n",
      "iter 8900/1000000  loss         0.171850  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.460\n",
      "iter 8901/1000000  loss         0.171850  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.460\n",
      "iter 9000/1000000  loss         0.171847  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.459\n",
      "iter 9001/1000000  loss         0.171847  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.459\n",
      "iter 9100/1000000  loss         0.171844  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.459\n",
      "iter 9101/1000000  loss         0.171844  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.459\n",
      "iter 9200/1000000  loss         0.171842  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.458\n",
      "iter 9201/1000000  loss         0.171842  avg_L1_norm_grad         0.000008  w[0]    0.080 bias    0.458\n",
      "iter 9300/1000000  loss         0.171839  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.458\n",
      "iter 9301/1000000  loss         0.171839  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.458\n",
      "iter 9400/1000000  loss         0.171837  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.458\n",
      "iter 9401/1000000  loss         0.171837  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.458\n",
      "iter 9500/1000000  loss         0.171835  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.457\n",
      "iter 9501/1000000  loss         0.171835  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.457\n",
      "iter 9600/1000000  loss         0.171833  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.457\n",
      "iter 9601/1000000  loss         0.171833  avg_L1_norm_grad         0.000007  w[0]    0.080 bias    0.457\n",
      "iter 9700/1000000  loss         0.171831  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.456\n",
      "iter 9701/1000000  loss         0.171831  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.456\n",
      "iter 9800/1000000  loss         0.171829  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.456\n",
      "iter 9801/1000000  loss         0.171829  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.456\n",
      "iter 9900/1000000  loss         0.171827  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.455\n",
      "iter 9901/1000000  loss         0.171827  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.455\n",
      "iter 10000/1000000  loss         0.171826  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.455\n",
      "iter 10001/1000000  loss         0.171826  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.455\n",
      "iter 10100/1000000  loss         0.171824  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.455\n",
      "iter 10101/1000000  loss         0.171824  avg_L1_norm_grad         0.000006  w[0]    0.080 bias    0.455\n",
      "iter 10200/1000000  loss         0.171823  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.454\n",
      "iter 10201/1000000  loss         0.171823  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.454\n",
      "iter 10300/1000000  loss         0.171821  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.454\n",
      "iter 10301/1000000  loss         0.171821  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.454\n",
      "iter 10400/1000000  loss         0.171820  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.453\n",
      "iter 10401/1000000  loss         0.171820  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.453\n",
      "iter 10500/1000000  loss         0.171819  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.453\n",
      "iter 10501/1000000  loss         0.171819  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.453\n",
      "iter 10600/1000000  loss         0.171818  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.453\n",
      "iter 10601/1000000  loss         0.171818  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.453\n",
      "iter 10700/1000000  loss         0.171817  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.452\n",
      "iter 10701/1000000  loss         0.171817  avg_L1_norm_grad         0.000005  w[0]    0.080 bias    0.452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.171816  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.452\n",
      "iter 10801/1000000  loss         0.171816  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.452\n",
      "iter 10900/1000000  loss         0.171815  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.452\n",
      "iter 10901/1000000  loss         0.171815  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.452\n",
      "iter 11000/1000000  loss         0.171814  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.451\n",
      "iter 11001/1000000  loss         0.171814  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.451\n",
      "iter 11100/1000000  loss         0.171813  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.451\n",
      "iter 11101/1000000  loss         0.171813  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.451\n",
      "iter 11200/1000000  loss         0.171812  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.451\n",
      "iter 11201/1000000  loss         0.171812  avg_L1_norm_grad         0.000004  w[0]    0.080 bias    0.451\n",
      "iter 11300/1000000  loss         0.171811  avg_L1_norm_grad         0.000004  w[0]    0.081 bias    0.451\n",
      "iter 11301/1000000  loss         0.171811  avg_L1_norm_grad         0.000004  w[0]    0.081 bias    0.451\n",
      "iter 11400/1000000  loss         0.171810  avg_L1_norm_grad         0.000004  w[0]    0.081 bias    0.450\n",
      "iter 11401/1000000  loss         0.171810  avg_L1_norm_grad         0.000004  w[0]    0.081 bias    0.450\n",
      "iter 11500/1000000  loss         0.171810  avg_L1_norm_grad         0.000004  w[0]    0.081 bias    0.450\n",
      "iter 11501/1000000  loss         0.171810  avg_L1_norm_grad         0.000004  w[0]    0.081 bias    0.450\n",
      "iter 11600/1000000  loss         0.171809  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.450\n",
      "iter 11601/1000000  loss         0.171809  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.450\n",
      "iter 11700/1000000  loss         0.171808  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 11701/1000000  loss         0.171808  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 11800/1000000  loss         0.171808  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 11801/1000000  loss         0.171808  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 11900/1000000  loss         0.171807  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 11901/1000000  loss         0.171807  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 12000/1000000  loss         0.171807  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 12001/1000000  loss         0.171807  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.449\n",
      "iter 12100/1000000  loss         0.171806  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12101/1000000  loss         0.171806  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12200/1000000  loss         0.171806  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12201/1000000  loss         0.171806  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12300/1000000  loss         0.171805  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12301/1000000  loss         0.171805  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12400/1000000  loss         0.171805  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12401/1000000  loss         0.171805  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12500/1000000  loss         0.171804  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12501/1000000  loss         0.171804  avg_L1_norm_grad         0.000003  w[0]    0.081 bias    0.448\n",
      "iter 12600/1000000  loss         0.171804  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12601/1000000  loss         0.171804  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12700/1000000  loss         0.171803  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12701/1000000  loss         0.171803  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12800/1000000  loss         0.171803  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12801/1000000  loss         0.171803  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12900/1000000  loss         0.171803  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 12901/1000000  loss         0.171803  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 13000/1000000  loss         0.171802  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 13001/1000000  loss         0.171802  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.447\n",
      "iter 13100/1000000  loss         0.171802  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13101/1000000  loss         0.171802  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13200/1000000  loss         0.171802  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13201/1000000  loss         0.171802  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13300/1000000  loss         0.171801  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13301/1000000  loss         0.171801  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13400/1000000  loss         0.171801  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13401/1000000  loss         0.171801  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13500/1000000  loss         0.171801  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13501/1000000  loss         0.171801  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13600/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13601/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.446\n",
      "iter 13700/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 13701/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 13800/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 13801/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 13900/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 13901/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 14000/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 14001/1000000  loss         0.171800  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 14100/1000000  loss         0.171799  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 14101/1000000  loss         0.171799  avg_L1_norm_grad         0.000002  w[0]    0.081 bias    0.445\n",
      "iter 14200/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.445\n",
      "iter 14201/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.445\n",
      "iter 14300/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.445\n",
      "iter 14301/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.445\n",
      "iter 14400/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14401/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14500/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14501/1000000  loss         0.171799  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14600/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14601/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14701/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14800/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14801/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14900/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 14901/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 15000/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "iter 15001/1000000  loss         0.171798  avg_L1_norm_grad         0.000001  w[0]    0.081 bias    0.444\n",
      "Done. Converged after 15012 iterations.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028862  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.917236  avg_L1_norm_grad         0.029687  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.855355  avg_L1_norm_grad         0.020384  w[0]    0.001 bias    0.020\n",
      "iter    3/1000000  loss         0.810493  avg_L1_norm_grad         0.020836  w[0]    0.001 bias    0.022\n",
      "iter    4/1000000  loss         0.775279  avg_L1_norm_grad         0.014950  w[0]    0.002 bias    0.038\n",
      "iter    5/1000000  loss         0.747782  avg_L1_norm_grad         0.015119  w[0]    0.002 bias    0.043\n",
      "iter    6/1000000  loss         0.725180  avg_L1_norm_grad         0.012172  w[0]    0.003 bias    0.055\n",
      "iter    7/1000000  loss         0.706149  avg_L1_norm_grad         0.012072  w[0]    0.004 bias    0.062\n",
      "iter    8/1000000  loss         0.689590  avg_L1_norm_grad         0.010732  w[0]    0.004 bias    0.071\n",
      "iter    9/1000000  loss         0.674861  avg_L1_norm_grad         0.010421  w[0]    0.005 bias    0.079\n",
      "iter   10/1000000  loss         0.661530  avg_L1_norm_grad         0.009755  w[0]    0.005 bias    0.088\n",
      "iter   11/1000000  loss         0.649313  avg_L1_norm_grad         0.009397  w[0]    0.006 bias    0.095\n",
      "iter   12/1000000  loss         0.638013  avg_L1_norm_grad         0.008982  w[0]    0.006 bias    0.103\n",
      "iter   13/1000000  loss         0.627485  avg_L1_norm_grad         0.008665  w[0]    0.007 bias    0.111\n",
      "iter   14/1000000  loss         0.617620  avg_L1_norm_grad         0.008359  w[0]    0.007 bias    0.119\n",
      "iter   15/1000000  loss         0.608334  avg_L1_norm_grad         0.008092  w[0]    0.007 bias    0.126\n",
      "iter   16/1000000  loss         0.599559  avg_L1_norm_grad         0.007843  w[0]    0.008 bias    0.133\n",
      "iter   17/1000000  loss         0.591242  avg_L1_norm_grad         0.007616  w[0]    0.008 bias    0.140\n",
      "iter   18/1000000  loss         0.583338  avg_L1_norm_grad         0.007406  w[0]    0.009 bias    0.147\n",
      "iter   19/1000000  loss         0.575810  avg_L1_norm_grad         0.007210  w[0]    0.009 bias    0.154\n",
      "iter  100/1000000  loss         0.361368  avg_L1_norm_grad         0.002511  w[0]    0.031 bias    0.502\n",
      "iter  101/1000000  loss         0.360380  avg_L1_norm_grad         0.002492  w[0]    0.031 bias    0.505\n",
      "iter  200/1000000  loss         0.302946  avg_L1_norm_grad         0.001466  w[0]    0.047 bias    0.724\n",
      "iter  201/1000000  loss         0.302598  avg_L1_norm_grad         0.001460  w[0]    0.047 bias    0.726\n",
      "iter  300/1000000  loss         0.277599  avg_L1_norm_grad         0.001064  w[0]    0.059 bias    0.869\n",
      "iter  301/1000000  loss         0.277415  avg_L1_norm_grad         0.001062  w[0]    0.059 bias    0.870\n",
      "iter  400/1000000  loss         0.262953  avg_L1_norm_grad         0.000851  w[0]    0.068 bias    0.977\n",
      "iter  401/1000000  loss         0.262836  avg_L1_norm_grad         0.000849  w[0]    0.068 bias    0.977\n",
      "iter  500/1000000  loss         0.253269  avg_L1_norm_grad         0.000714  w[0]    0.076 bias    1.061\n",
      "iter  501/1000000  loss         0.253188  avg_L1_norm_grad         0.000713  w[0]    0.076 bias    1.062\n",
      "iter  600/1000000  loss         0.246336  avg_L1_norm_grad         0.000618  w[0]    0.083 bias    1.130\n",
      "iter  601/1000000  loss         0.246276  avg_L1_norm_grad         0.000617  w[0]    0.083 bias    1.131\n",
      "iter  700/1000000  loss         0.241103  avg_L1_norm_grad         0.000547  w[0]    0.089 bias    1.188\n",
      "iter  701/1000000  loss         0.241057  avg_L1_norm_grad         0.000546  w[0]    0.089 bias    1.189\n",
      "iter  800/1000000  loss         0.237003  avg_L1_norm_grad         0.000491  w[0]    0.094 bias    1.238\n",
      "iter  801/1000000  loss         0.236967  avg_L1_norm_grad         0.000491  w[0]    0.094 bias    1.239\n",
      "iter  900/1000000  loss         0.233702  avg_L1_norm_grad         0.000446  w[0]    0.099 bias    1.282\n",
      "iter  901/1000000  loss         0.233672  avg_L1_norm_grad         0.000446  w[0]    0.099 bias    1.282\n",
      "iter 1000/1000000  loss         0.230986  avg_L1_norm_grad         0.000408  w[0]    0.104 bias    1.321\n",
      "iter 1001/1000000  loss         0.230961  avg_L1_norm_grad         0.000408  w[0]    0.104 bias    1.321\n",
      "iter 1100/1000000  loss         0.228715  avg_L1_norm_grad         0.000377  w[0]    0.108 bias    1.355\n",
      "iter 1101/1000000  loss         0.228694  avg_L1_norm_grad         0.000377  w[0]    0.108 bias    1.356\n",
      "iter 1200/1000000  loss         0.226791  avg_L1_norm_grad         0.000350  w[0]    0.111 bias    1.387\n",
      "iter 1201/1000000  loss         0.226773  avg_L1_norm_grad         0.000349  w[0]    0.111 bias    1.387\n",
      "iter 1300/1000000  loss         0.225143  avg_L1_norm_grad         0.000326  w[0]    0.115 bias    1.415\n",
      "iter 1301/1000000  loss         0.225128  avg_L1_norm_grad         0.000325  w[0]    0.115 bias    1.415\n",
      "iter 1400/1000000  loss         0.223720  avg_L1_norm_grad         0.000305  w[0]    0.118 bias    1.441\n",
      "iter 1401/1000000  loss         0.223706  avg_L1_norm_grad         0.000304  w[0]    0.118 bias    1.442\n",
      "iter 1500/1000000  loss         0.222481  avg_L1_norm_grad         0.000286  w[0]    0.121 bias    1.466\n",
      "iter 1501/1000000  loss         0.222469  avg_L1_norm_grad         0.000286  w[0]    0.121 bias    1.466\n",
      "iter 1600/1000000  loss         0.221396  avg_L1_norm_grad         0.000269  w[0]    0.123 bias    1.488\n",
      "iter 1601/1000000  loss         0.221385  avg_L1_norm_grad         0.000269  w[0]    0.123 bias    1.488\n",
      "iter 1700/1000000  loss         0.220440  avg_L1_norm_grad         0.000253  w[0]    0.126 bias    1.509\n",
      "iter 1701/1000000  loss         0.220431  avg_L1_norm_grad         0.000253  w[0]    0.126 bias    1.509\n",
      "iter 1800/1000000  loss         0.219594  avg_L1_norm_grad         0.000239  w[0]    0.128 bias    1.529\n",
      "iter 1801/1000000  loss         0.219586  avg_L1_norm_grad         0.000239  w[0]    0.128 bias    1.529\n",
      "iter 1900/1000000  loss         0.218843  avg_L1_norm_grad         0.000226  w[0]    0.130 bias    1.547\n",
      "iter 1901/1000000  loss         0.218836  avg_L1_norm_grad         0.000226  w[0]    0.130 bias    1.547\n",
      "iter 2000/1000000  loss         0.218173  avg_L1_norm_grad         0.000214  w[0]    0.132 bias    1.565\n",
      "iter 2001/1000000  loss         0.218167  avg_L1_norm_grad         0.000214  w[0]    0.132 bias    1.565\n",
      "iter 2100/1000000  loss         0.217574  avg_L1_norm_grad         0.000203  w[0]    0.134 bias    1.581\n",
      "iter 2101/1000000  loss         0.217569  avg_L1_norm_grad         0.000203  w[0]    0.134 bias    1.581\n",
      "iter 2200/1000000  loss         0.217037  avg_L1_norm_grad         0.000193  w[0]    0.136 bias    1.596\n",
      "iter 2201/1000000  loss         0.217032  avg_L1_norm_grad         0.000193  w[0]    0.136 bias    1.597\n",
      "iter 2300/1000000  loss         0.216553  avg_L1_norm_grad         0.000184  w[0]    0.137 bias    1.611\n",
      "iter 2301/1000000  loss         0.216549  avg_L1_norm_grad         0.000184  w[0]    0.137 bias    1.611\n",
      "iter 2400/1000000  loss         0.216117  avg_L1_norm_grad         0.000175  w[0]    0.139 bias    1.625\n",
      "iter 2401/1000000  loss         0.216113  avg_L1_norm_grad         0.000175  w[0]    0.139 bias    1.625\n",
      "iter 2500/1000000  loss         0.215723  avg_L1_norm_grad         0.000166  w[0]    0.140 bias    1.639\n",
      "iter 2501/1000000  loss         0.215719  avg_L1_norm_grad         0.000166  w[0]    0.140 bias    1.639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2600/1000000  loss         0.215366  avg_L1_norm_grad         0.000159  w[0]    0.142 bias    1.651\n",
      "iter 2601/1000000  loss         0.215363  avg_L1_norm_grad         0.000159  w[0]    0.142 bias    1.651\n",
      "iter 2700/1000000  loss         0.215043  avg_L1_norm_grad         0.000151  w[0]    0.143 bias    1.663\n",
      "iter 2701/1000000  loss         0.215040  avg_L1_norm_grad         0.000151  w[0]    0.143 bias    1.663\n",
      "iter 2800/1000000  loss         0.214749  avg_L1_norm_grad         0.000145  w[0]    0.144 bias    1.675\n",
      "iter 2801/1000000  loss         0.214746  avg_L1_norm_grad         0.000144  w[0]    0.144 bias    1.675\n",
      "iter 2900/1000000  loss         0.214481  avg_L1_norm_grad         0.000138  w[0]    0.145 bias    1.686\n",
      "iter 2901/1000000  loss         0.214479  avg_L1_norm_grad         0.000138  w[0]    0.145 bias    1.686\n",
      "iter 3000/1000000  loss         0.214237  avg_L1_norm_grad         0.000132  w[0]    0.147 bias    1.697\n",
      "iter 3001/1000000  loss         0.214235  avg_L1_norm_grad         0.000132  w[0]    0.147 bias    1.697\n",
      "iter 3100/1000000  loss         0.214015  avg_L1_norm_grad         0.000126  w[0]    0.148 bias    1.707\n",
      "iter 3101/1000000  loss         0.214012  avg_L1_norm_grad         0.000126  w[0]    0.148 bias    1.707\n",
      "iter 3200/1000000  loss         0.213811  avg_L1_norm_grad         0.000121  w[0]    0.149 bias    1.717\n",
      "iter 3201/1000000  loss         0.213809  avg_L1_norm_grad         0.000121  w[0]    0.149 bias    1.717\n",
      "iter 3300/1000000  loss         0.213625  avg_L1_norm_grad         0.000116  w[0]    0.149 bias    1.726\n",
      "iter 3301/1000000  loss         0.213623  avg_L1_norm_grad         0.000116  w[0]    0.149 bias    1.726\n",
      "iter 3400/1000000  loss         0.213455  avg_L1_norm_grad         0.000111  w[0]    0.150 bias    1.735\n",
      "iter 3401/1000000  loss         0.213453  avg_L1_norm_grad         0.000111  w[0]    0.150 bias    1.735\n",
      "iter 3500/1000000  loss         0.213298  avg_L1_norm_grad         0.000106  w[0]    0.151 bias    1.744\n",
      "iter 3501/1000000  loss         0.213297  avg_L1_norm_grad         0.000106  w[0]    0.151 bias    1.744\n",
      "iter 3600/1000000  loss         0.213155  avg_L1_norm_grad         0.000102  w[0]    0.152 bias    1.752\n",
      "iter 3601/1000000  loss         0.213153  avg_L1_norm_grad         0.000102  w[0]    0.152 bias    1.752\n",
      "iter 3700/1000000  loss         0.213023  avg_L1_norm_grad         0.000098  w[0]    0.153 bias    1.760\n",
      "iter 3701/1000000  loss         0.213022  avg_L1_norm_grad         0.000098  w[0]    0.153 bias    1.760\n",
      "iter 3800/1000000  loss         0.212902  avg_L1_norm_grad         0.000094  w[0]    0.154 bias    1.768\n",
      "iter 3801/1000000  loss         0.212901  avg_L1_norm_grad         0.000094  w[0]    0.154 bias    1.768\n",
      "iter 3900/1000000  loss         0.212790  avg_L1_norm_grad         0.000090  w[0]    0.154 bias    1.775\n",
      "iter 3901/1000000  loss         0.212789  avg_L1_norm_grad         0.000090  w[0]    0.154 bias    1.775\n",
      "iter 4000/1000000  loss         0.212688  avg_L1_norm_grad         0.000086  w[0]    0.155 bias    1.782\n",
      "iter 4001/1000000  loss         0.212687  avg_L1_norm_grad         0.000086  w[0]    0.155 bias    1.782\n",
      "iter 4100/1000000  loss         0.212593  avg_L1_norm_grad         0.000083  w[0]    0.156 bias    1.789\n",
      "iter 4101/1000000  loss         0.212592  avg_L1_norm_grad         0.000083  w[0]    0.156 bias    1.789\n",
      "iter 4200/1000000  loss         0.212506  avg_L1_norm_grad         0.000080  w[0]    0.156 bias    1.796\n",
      "iter 4201/1000000  loss         0.212505  avg_L1_norm_grad         0.000080  w[0]    0.156 bias    1.796\n",
      "iter 4300/1000000  loss         0.212425  avg_L1_norm_grad         0.000077  w[0]    0.157 bias    1.802\n",
      "iter 4301/1000000  loss         0.212424  avg_L1_norm_grad         0.000077  w[0]    0.157 bias    1.802\n",
      "iter 4400/1000000  loss         0.212351  avg_L1_norm_grad         0.000074  w[0]    0.157 bias    1.808\n",
      "iter 4401/1000000  loss         0.212350  avg_L1_norm_grad         0.000074  w[0]    0.157 bias    1.808\n",
      "iter 4500/1000000  loss         0.212282  avg_L1_norm_grad         0.000071  w[0]    0.158 bias    1.814\n",
      "iter 4501/1000000  loss         0.212281  avg_L1_norm_grad         0.000071  w[0]    0.158 bias    1.814\n",
      "iter 4600/1000000  loss         0.212218  avg_L1_norm_grad         0.000068  w[0]    0.158 bias    1.820\n",
      "iter 4601/1000000  loss         0.212218  avg_L1_norm_grad         0.000068  w[0]    0.158 bias    1.820\n",
      "iter 4700/1000000  loss         0.212159  avg_L1_norm_grad         0.000066  w[0]    0.159 bias    1.825\n",
      "iter 4701/1000000  loss         0.212159  avg_L1_norm_grad         0.000066  w[0]    0.159 bias    1.825\n",
      "iter 4800/1000000  loss         0.212105  avg_L1_norm_grad         0.000063  w[0]    0.159 bias    1.830\n",
      "iter 4801/1000000  loss         0.212104  avg_L1_norm_grad         0.000063  w[0]    0.159 bias    1.831\n",
      "iter 4900/1000000  loss         0.212054  avg_L1_norm_grad         0.000061  w[0]    0.160 bias    1.836\n",
      "iter 4901/1000000  loss         0.212054  avg_L1_norm_grad         0.000061  w[0]    0.160 bias    1.836\n",
      "iter 5000/1000000  loss         0.212008  avg_L1_norm_grad         0.000059  w[0]    0.160 bias    1.841\n",
      "iter 5001/1000000  loss         0.212007  avg_L1_norm_grad         0.000058  w[0]    0.160 bias    1.841\n",
      "iter 5100/1000000  loss         0.211964  avg_L1_norm_grad         0.000056  w[0]    0.161 bias    1.845\n",
      "iter 5101/1000000  loss         0.211964  avg_L1_norm_grad         0.000056  w[0]    0.161 bias    1.845\n",
      "iter 5200/1000000  loss         0.211924  avg_L1_norm_grad         0.000054  w[0]    0.161 bias    1.850\n",
      "iter 5201/1000000  loss         0.211924  avg_L1_norm_grad         0.000054  w[0]    0.161 bias    1.850\n",
      "iter 5300/1000000  loss         0.211887  avg_L1_norm_grad         0.000052  w[0]    0.161 bias    1.854\n",
      "iter 5301/1000000  loss         0.211886  avg_L1_norm_grad         0.000052  w[0]    0.161 bias    1.854\n",
      "iter 5400/1000000  loss         0.211852  avg_L1_norm_grad         0.000050  w[0]    0.162 bias    1.859\n",
      "iter 5401/1000000  loss         0.211852  avg_L1_norm_grad         0.000050  w[0]    0.162 bias    1.859\n",
      "iter 5500/1000000  loss         0.211820  avg_L1_norm_grad         0.000049  w[0]    0.162 bias    1.863\n",
      "iter 5501/1000000  loss         0.211820  avg_L1_norm_grad         0.000049  w[0]    0.162 bias    1.863\n",
      "iter 5600/1000000  loss         0.211790  avg_L1_norm_grad         0.000047  w[0]    0.162 bias    1.867\n",
      "iter 5601/1000000  loss         0.211790  avg_L1_norm_grad         0.000047  w[0]    0.162 bias    1.867\n",
      "iter 5700/1000000  loss         0.211762  avg_L1_norm_grad         0.000045  w[0]    0.163 bias    1.871\n",
      "iter 5701/1000000  loss         0.211762  avg_L1_norm_grad         0.000045  w[0]    0.163 bias    1.871\n",
      "iter 5800/1000000  loss         0.211736  avg_L1_norm_grad         0.000044  w[0]    0.163 bias    1.874\n",
      "iter 5801/1000000  loss         0.211736  avg_L1_norm_grad         0.000044  w[0]    0.163 bias    1.874\n",
      "iter 5900/1000000  loss         0.211712  avg_L1_norm_grad         0.000042  w[0]    0.163 bias    1.878\n",
      "iter 5901/1000000  loss         0.211712  avg_L1_norm_grad         0.000042  w[0]    0.163 bias    1.878\n",
      "iter 6000/1000000  loss         0.211690  avg_L1_norm_grad         0.000041  w[0]    0.164 bias    1.881\n",
      "iter 6001/1000000  loss         0.211690  avg_L1_norm_grad         0.000041  w[0]    0.164 bias    1.881\n",
      "iter 6100/1000000  loss         0.211669  avg_L1_norm_grad         0.000039  w[0]    0.164 bias    1.885\n",
      "iter 6101/1000000  loss         0.211669  avg_L1_norm_grad         0.000039  w[0]    0.164 bias    1.885\n",
      "iter 6200/1000000  loss         0.211650  avg_L1_norm_grad         0.000038  w[0]    0.164 bias    1.888\n",
      "iter 6201/1000000  loss         0.211650  avg_L1_norm_grad         0.000038  w[0]    0.164 bias    1.888\n",
      "iter 6300/1000000  loss         0.211632  avg_L1_norm_grad         0.000036  w[0]    0.164 bias    1.891\n",
      "iter 6301/1000000  loss         0.211631  avg_L1_norm_grad         0.000036  w[0]    0.164 bias    1.891\n",
      "iter 6400/1000000  loss         0.211615  avg_L1_norm_grad         0.000035  w[0]    0.165 bias    1.894\n",
      "iter 6401/1000000  loss         0.211615  avg_L1_norm_grad         0.000035  w[0]    0.165 bias    1.894\n",
      "iter 6500/1000000  loss         0.211599  avg_L1_norm_grad         0.000034  w[0]    0.165 bias    1.897\n",
      "iter 6501/1000000  loss         0.211599  avg_L1_norm_grad         0.000034  w[0]    0.165 bias    1.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6600/1000000  loss         0.211584  avg_L1_norm_grad         0.000033  w[0]    0.165 bias    1.900\n",
      "iter 6601/1000000  loss         0.211584  avg_L1_norm_grad         0.000033  w[0]    0.165 bias    1.900\n",
      "iter 6700/1000000  loss         0.211571  avg_L1_norm_grad         0.000032  w[0]    0.165 bias    1.903\n",
      "iter 6701/1000000  loss         0.211571  avg_L1_norm_grad         0.000032  w[0]    0.165 bias    1.903\n",
      "iter 6800/1000000  loss         0.211558  avg_L1_norm_grad         0.000031  w[0]    0.165 bias    1.905\n",
      "iter 6801/1000000  loss         0.211558  avg_L1_norm_grad         0.000031  w[0]    0.165 bias    1.905\n",
      "iter 6900/1000000  loss         0.211546  avg_L1_norm_grad         0.000030  w[0]    0.166 bias    1.908\n",
      "iter 6901/1000000  loss         0.211546  avg_L1_norm_grad         0.000030  w[0]    0.166 bias    1.908\n",
      "iter 7000/1000000  loss         0.211535  avg_L1_norm_grad         0.000029  w[0]    0.166 bias    1.910\n",
      "iter 7001/1000000  loss         0.211535  avg_L1_norm_grad         0.000029  w[0]    0.166 bias    1.910\n",
      "iter 7100/1000000  loss         0.211525  avg_L1_norm_grad         0.000028  w[0]    0.166 bias    1.913\n",
      "iter 7101/1000000  loss         0.211525  avg_L1_norm_grad         0.000028  w[0]    0.166 bias    1.913\n",
      "iter 7200/1000000  loss         0.211515  avg_L1_norm_grad         0.000027  w[0]    0.166 bias    1.915\n",
      "iter 7201/1000000  loss         0.211515  avg_L1_norm_grad         0.000027  w[0]    0.166 bias    1.915\n",
      "iter 7300/1000000  loss         0.211506  avg_L1_norm_grad         0.000026  w[0]    0.166 bias    1.917\n",
      "iter 7301/1000000  loss         0.211506  avg_L1_norm_grad         0.000026  w[0]    0.166 bias    1.917\n",
      "iter 7400/1000000  loss         0.211498  avg_L1_norm_grad         0.000025  w[0]    0.167 bias    1.919\n",
      "iter 7401/1000000  loss         0.211498  avg_L1_norm_grad         0.000025  w[0]    0.167 bias    1.919\n",
      "iter 7500/1000000  loss         0.211490  avg_L1_norm_grad         0.000024  w[0]    0.167 bias    1.922\n",
      "iter 7501/1000000  loss         0.211490  avg_L1_norm_grad         0.000024  w[0]    0.167 bias    1.922\n",
      "iter 7600/1000000  loss         0.211482  avg_L1_norm_grad         0.000023  w[0]    0.167 bias    1.924\n",
      "iter 7601/1000000  loss         0.211482  avg_L1_norm_grad         0.000023  w[0]    0.167 bias    1.924\n",
      "iter 7700/1000000  loss         0.211476  avg_L1_norm_grad         0.000023  w[0]    0.167 bias    1.926\n",
      "iter 7701/1000000  loss         0.211475  avg_L1_norm_grad         0.000023  w[0]    0.167 bias    1.926\n",
      "iter 7800/1000000  loss         0.211469  avg_L1_norm_grad         0.000022  w[0]    0.167 bias    1.927\n",
      "iter 7801/1000000  loss         0.211469  avg_L1_norm_grad         0.000022  w[0]    0.167 bias    1.927\n",
      "iter 7900/1000000  loss         0.211463  avg_L1_norm_grad         0.000021  w[0]    0.167 bias    1.929\n",
      "iter 7901/1000000  loss         0.211463  avg_L1_norm_grad         0.000021  w[0]    0.167 bias    1.929\n",
      "iter 8000/1000000  loss         0.211457  avg_L1_norm_grad         0.000020  w[0]    0.167 bias    1.931\n",
      "iter 8001/1000000  loss         0.211457  avg_L1_norm_grad         0.000020  w[0]    0.167 bias    1.931\n",
      "iter 8100/1000000  loss         0.211452  avg_L1_norm_grad         0.000020  w[0]    0.168 bias    1.933\n",
      "iter 8101/1000000  loss         0.211452  avg_L1_norm_grad         0.000020  w[0]    0.168 bias    1.933\n",
      "iter 8200/1000000  loss         0.211447  avg_L1_norm_grad         0.000019  w[0]    0.168 bias    1.934\n",
      "iter 8201/1000000  loss         0.211447  avg_L1_norm_grad         0.000019  w[0]    0.168 bias    1.934\n",
      "iter 8300/1000000  loss         0.211443  avg_L1_norm_grad         0.000018  w[0]    0.168 bias    1.936\n",
      "iter 8301/1000000  loss         0.211443  avg_L1_norm_grad         0.000018  w[0]    0.168 bias    1.936\n",
      "iter 8400/1000000  loss         0.211438  avg_L1_norm_grad         0.000018  w[0]    0.168 bias    1.938\n",
      "iter 8401/1000000  loss         0.211438  avg_L1_norm_grad         0.000018  w[0]    0.168 bias    1.938\n",
      "iter 8500/1000000  loss         0.211434  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    1.939\n",
      "iter 8501/1000000  loss         0.211434  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    1.939\n",
      "iter 8600/1000000  loss         0.211430  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    1.940\n",
      "iter 8601/1000000  loss         0.211430  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    1.940\n",
      "iter 8700/1000000  loss         0.211427  avg_L1_norm_grad         0.000016  w[0]    0.168 bias    1.942\n",
      "iter 8701/1000000  loss         0.211427  avg_L1_norm_grad         0.000016  w[0]    0.168 bias    1.942\n",
      "iter 8800/1000000  loss         0.211424  avg_L1_norm_grad         0.000016  w[0]    0.168 bias    1.943\n",
      "iter 8801/1000000  loss         0.211424  avg_L1_norm_grad         0.000016  w[0]    0.168 bias    1.943\n",
      "iter 8900/1000000  loss         0.211420  avg_L1_norm_grad         0.000015  w[0]    0.168 bias    1.945\n",
      "iter 8901/1000000  loss         0.211420  avg_L1_norm_grad         0.000015  w[0]    0.168 bias    1.945\n",
      "iter 9000/1000000  loss         0.211418  avg_L1_norm_grad         0.000015  w[0]    0.168 bias    1.946\n",
      "iter 9001/1000000  loss         0.211418  avg_L1_norm_grad         0.000015  w[0]    0.168 bias    1.946\n",
      "iter 9100/1000000  loss         0.211415  avg_L1_norm_grad         0.000014  w[0]    0.169 bias    1.947\n",
      "iter 9101/1000000  loss         0.211415  avg_L1_norm_grad         0.000014  w[0]    0.169 bias    1.947\n",
      "iter 9200/1000000  loss         0.211412  avg_L1_norm_grad         0.000014  w[0]    0.169 bias    1.948\n",
      "iter 9201/1000000  loss         0.211412  avg_L1_norm_grad         0.000014  w[0]    0.169 bias    1.948\n",
      "iter 9300/1000000  loss         0.211410  avg_L1_norm_grad         0.000013  w[0]    0.169 bias    1.949\n",
      "iter 9301/1000000  loss         0.211410  avg_L1_norm_grad         0.000013  w[0]    0.169 bias    1.949\n",
      "iter 9400/1000000  loss         0.211408  avg_L1_norm_grad         0.000013  w[0]    0.169 bias    1.951\n",
      "iter 9401/1000000  loss         0.211408  avg_L1_norm_grad         0.000013  w[0]    0.169 bias    1.951\n",
      "iter 9500/1000000  loss         0.211406  avg_L1_norm_grad         0.000012  w[0]    0.169 bias    1.952\n",
      "iter 9501/1000000  loss         0.211405  avg_L1_norm_grad         0.000012  w[0]    0.169 bias    1.952\n",
      "iter 9600/1000000  loss         0.211404  avg_L1_norm_grad         0.000012  w[0]    0.169 bias    1.953\n",
      "iter 9601/1000000  loss         0.211404  avg_L1_norm_grad         0.000012  w[0]    0.169 bias    1.953\n",
      "iter 9700/1000000  loss         0.211402  avg_L1_norm_grad         0.000012  w[0]    0.169 bias    1.954\n",
      "iter 9701/1000000  loss         0.211402  avg_L1_norm_grad         0.000012  w[0]    0.169 bias    1.954\n",
      "Done. Converged after 9720 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9405555555555294\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "No Noise New 0.9424999999999738\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030466  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.909932  avg_L1_norm_grad         0.028286  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.845443  avg_L1_norm_grad         0.019556  w[0]    0.001 bias    0.021\n",
      "iter    3/1000000  loss         0.801142  avg_L1_norm_grad         0.018156  w[0]    0.001 bias    0.025\n",
      "iter    4/1000000  loss         0.767766  avg_L1_norm_grad         0.013917  w[0]    0.002 bias    0.040\n",
      "iter    5/1000000  loss         0.742020  avg_L1_norm_grad         0.013222  w[0]    0.002 bias    0.048\n",
      "iter    6/1000000  loss         0.720964  avg_L1_norm_grad         0.011387  w[0]    0.002 bias    0.060\n",
      "iter    7/1000000  loss         0.703085  avg_L1_norm_grad         0.010845  w[0]    0.003 bias    0.068\n",
      "iter    8/1000000  loss         0.687416  avg_L1_norm_grad         0.009974  w[0]    0.003 bias    0.078\n",
      "iter    9/1000000  loss         0.673384  avg_L1_norm_grad         0.009500  w[0]    0.003 bias    0.088\n",
      "iter   10/1000000  loss         0.660620  avg_L1_norm_grad         0.008992  w[0]    0.003 bias    0.097\n",
      "iter   11/1000000  loss         0.648873  avg_L1_norm_grad         0.008598  w[0]    0.004 bias    0.106\n",
      "iter   12/1000000  loss         0.637966  avg_L1_norm_grad         0.008237  w[0]    0.004 bias    0.115\n",
      "iter   13/1000000  loss         0.627770  avg_L1_norm_grad         0.007926  w[0]    0.004 bias    0.123\n",
      "iter   14/1000000  loss         0.618187  avg_L1_norm_grad         0.007649  w[0]    0.005 bias    0.132\n",
      "iter   15/1000000  loss         0.609140  avg_L1_norm_grad         0.007401  w[0]    0.005 bias    0.140\n",
      "iter   16/1000000  loss         0.600571  avg_L1_norm_grad         0.007174  w[0]    0.005 bias    0.149\n",
      "iter   17/1000000  loss         0.592430  avg_L1_norm_grad         0.006966  w[0]    0.005 bias    0.157\n",
      "iter   18/1000000  loss         0.584678  avg_L1_norm_grad         0.006774  w[0]    0.006 bias    0.165\n",
      "iter   19/1000000  loss         0.577283  avg_L1_norm_grad         0.006597  w[0]    0.006 bias    0.172\n",
      "iter  100/1000000  loss         0.362394  avg_L1_norm_grad         0.002292  w[0]    0.020 bias    0.592\n",
      "iter  101/1000000  loss         0.361380  avg_L1_norm_grad         0.002275  w[0]    0.020 bias    0.596\n",
      "iter  200/1000000  loss         0.302096  avg_L1_norm_grad         0.001332  w[0]    0.030 bias    0.882\n",
      "iter  201/1000000  loss         0.301736  avg_L1_norm_grad         0.001327  w[0]    0.030 bias    0.885\n",
      "iter  300/1000000  loss         0.275984  avg_L1_norm_grad         0.000962  w[0]    0.037 bias    1.082\n",
      "iter  301/1000000  loss         0.275796  avg_L1_norm_grad         0.000959  w[0]    0.037 bias    1.084\n",
      "iter  400/1000000  loss         0.261089  avg_L1_norm_grad         0.000763  w[0]    0.043 bias    1.234\n",
      "iter  401/1000000  loss         0.260972  avg_L1_norm_grad         0.000762  w[0]    0.043 bias    1.235\n",
      "iter  500/1000000  loss         0.251392  avg_L1_norm_grad         0.000639  w[0]    0.048 bias    1.356\n",
      "iter  501/1000000  loss         0.251312  avg_L1_norm_grad         0.000638  w[0]    0.048 bias    1.357\n",
      "iter  600/1000000  loss         0.244561  avg_L1_norm_grad         0.000553  w[0]    0.053 bias    1.457\n",
      "iter  601/1000000  loss         0.244502  avg_L1_norm_grad         0.000552  w[0]    0.053 bias    1.458\n",
      "iter  700/1000000  loss         0.239486  avg_L1_norm_grad         0.000488  w[0]    0.058 bias    1.542\n",
      "iter  701/1000000  loss         0.239441  avg_L1_norm_grad         0.000488  w[0]    0.058 bias    1.543\n",
      "iter  800/1000000  loss         0.235570  avg_L1_norm_grad         0.000438  w[0]    0.062 bias    1.616\n",
      "iter  801/1000000  loss         0.235536  avg_L1_norm_grad         0.000437  w[0]    0.062 bias    1.616\n",
      "iter  900/1000000  loss         0.232463  avg_L1_norm_grad         0.000397  w[0]    0.066 bias    1.680\n",
      "iter  901/1000000  loss         0.232435  avg_L1_norm_grad         0.000396  w[0]    0.066 bias    1.680\n",
      "iter 1000/1000000  loss         0.229943  avg_L1_norm_grad         0.000363  w[0]    0.070 bias    1.736\n",
      "iter 1001/1000000  loss         0.229921  avg_L1_norm_grad         0.000362  w[0]    0.070 bias    1.737\n",
      "iter 1100/1000000  loss         0.227865  avg_L1_norm_grad         0.000333  w[0]    0.073 bias    1.786\n",
      "iter 1101/1000000  loss         0.227846  avg_L1_norm_grad         0.000333  w[0]    0.073 bias    1.787\n",
      "iter 1200/1000000  loss         0.226126  avg_L1_norm_grad         0.000308  w[0]    0.077 bias    1.831\n",
      "iter 1201/1000000  loss         0.226110  avg_L1_norm_grad         0.000308  w[0]    0.077 bias    1.831\n",
      "iter 1300/1000000  loss         0.224654  avg_L1_norm_grad         0.000287  w[0]    0.080 bias    1.871\n",
      "iter 1301/1000000  loss         0.224640  avg_L1_norm_grad         0.000286  w[0]    0.080 bias    1.871\n",
      "iter 1400/1000000  loss         0.223397  avg_L1_norm_grad         0.000267  w[0]    0.083 bias    1.907\n",
      "iter 1401/1000000  loss         0.223385  avg_L1_norm_grad         0.000267  w[0]    0.083 bias    1.908\n",
      "iter 1500/1000000  loss         0.222314  avg_L1_norm_grad         0.000250  w[0]    0.086 bias    1.940\n",
      "iter 1501/1000000  loss         0.222304  avg_L1_norm_grad         0.000250  w[0]    0.086 bias    1.941\n",
      "iter 1600/1000000  loss         0.221375  avg_L1_norm_grad         0.000234  w[0]    0.089 bias    1.970\n",
      "iter 1601/1000000  loss         0.221366  avg_L1_norm_grad         0.000234  w[0]    0.089 bias    1.970\n",
      "iter 1700/1000000  loss         0.220556  avg_L1_norm_grad         0.000220  w[0]    0.092 bias    1.997\n",
      "iter 1701/1000000  loss         0.220548  avg_L1_norm_grad         0.000220  w[0]    0.092 bias    1.998\n",
      "iter 1800/1000000  loss         0.219837  avg_L1_norm_grad         0.000207  w[0]    0.095 bias    2.022\n",
      "iter 1801/1000000  loss         0.219830  avg_L1_norm_grad         0.000207  w[0]    0.095 bias    2.023\n",
      "iter 1900/1000000  loss         0.219203  avg_L1_norm_grad         0.000196  w[0]    0.098 bias    2.045\n",
      "iter 1901/1000000  loss         0.219197  avg_L1_norm_grad         0.000196  w[0]    0.098 bias    2.045\n",
      "iter 2000/1000000  loss         0.218643  avg_L1_norm_grad         0.000185  w[0]    0.100 bias    2.066\n",
      "iter 2001/1000000  loss         0.218638  avg_L1_norm_grad         0.000185  w[0]    0.100 bias    2.066\n",
      "iter 2100/1000000  loss         0.218145  avg_L1_norm_grad         0.000175  w[0]    0.103 bias    2.086\n",
      "iter 2101/1000000  loss         0.218140  avg_L1_norm_grad         0.000175  w[0]    0.103 bias    2.086\n",
      "iter 2200/1000000  loss         0.217701  avg_L1_norm_grad         0.000166  w[0]    0.105 bias    2.104\n",
      "iter 2201/1000000  loss         0.217697  avg_L1_norm_grad         0.000166  w[0]    0.105 bias    2.104\n",
      "iter 2300/1000000  loss         0.217305  avg_L1_norm_grad         0.000157  w[0]    0.108 bias    2.120\n",
      "iter 2301/1000000  loss         0.217301  avg_L1_norm_grad         0.000157  w[0]    0.108 bias    2.120\n",
      "iter 2400/1000000  loss         0.216949  avg_L1_norm_grad         0.000150  w[0]    0.110 bias    2.135\n",
      "iter 2401/1000000  loss         0.216946  avg_L1_norm_grad         0.000150  w[0]    0.110 bias    2.136\n",
      "iter 2500/1000000  loss         0.216629  avg_L1_norm_grad         0.000142  w[0]    0.112 bias    2.150\n",
      "iter 2501/1000000  loss         0.216626  avg_L1_norm_grad         0.000142  w[0]    0.112 bias    2.150\n",
      "iter 2600/1000000  loss         0.216341  avg_L1_norm_grad         0.000136  w[0]    0.114 bias    2.163\n",
      "iter 2601/1000000  loss         0.216339  avg_L1_norm_grad         0.000136  w[0]    0.114 bias    2.163\n",
      "iter 2700/1000000  loss         0.216082  avg_L1_norm_grad         0.000129  w[0]    0.116 bias    2.175\n",
      "iter 2701/1000000  loss         0.216079  avg_L1_norm_grad         0.000129  w[0]    0.117 bias    2.175\n",
      "iter 2800/1000000  loss         0.215846  avg_L1_norm_grad         0.000123  w[0]    0.119 bias    2.187\n",
      "iter 2801/1000000  loss         0.215844  avg_L1_norm_grad         0.000123  w[0]    0.119 bias    2.187\n",
      "iter 2900/1000000  loss         0.215633  avg_L1_norm_grad         0.000118  w[0]    0.120 bias    2.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.215631  avg_L1_norm_grad         0.000118  w[0]    0.120 bias    2.197\n",
      "iter 3000/1000000  loss         0.215440  avg_L1_norm_grad         0.000112  w[0]    0.122 bias    2.207\n",
      "iter 3001/1000000  loss         0.215438  avg_L1_norm_grad         0.000112  w[0]    0.122 bias    2.207\n",
      "iter 3100/1000000  loss         0.215264  avg_L1_norm_grad         0.000108  w[0]    0.124 bias    2.217\n",
      "iter 3101/1000000  loss         0.215263  avg_L1_norm_grad         0.000108  w[0]    0.124 bias    2.217\n",
      "iter 3200/1000000  loss         0.215104  avg_L1_norm_grad         0.000103  w[0]    0.126 bias    2.225\n",
      "iter 3201/1000000  loss         0.215103  avg_L1_norm_grad         0.000103  w[0]    0.126 bias    2.225\n",
      "iter 3300/1000000  loss         0.214958  avg_L1_norm_grad         0.000098  w[0]    0.128 bias    2.234\n",
      "iter 3301/1000000  loss         0.214957  avg_L1_norm_grad         0.000098  w[0]    0.128 bias    2.234\n",
      "iter 3400/1000000  loss         0.214825  avg_L1_norm_grad         0.000094  w[0]    0.129 bias    2.241\n",
      "iter 3401/1000000  loss         0.214824  avg_L1_norm_grad         0.000094  w[0]    0.129 bias    2.241\n",
      "iter 3500/1000000  loss         0.214703  avg_L1_norm_grad         0.000090  w[0]    0.131 bias    2.248\n",
      "iter 3501/1000000  loss         0.214702  avg_L1_norm_grad         0.000090  w[0]    0.131 bias    2.248\n",
      "iter 3600/1000000  loss         0.214591  avg_L1_norm_grad         0.000087  w[0]    0.132 bias    2.255\n",
      "iter 3601/1000000  loss         0.214590  avg_L1_norm_grad         0.000087  w[0]    0.132 bias    2.255\n",
      "iter 3700/1000000  loss         0.214489  avg_L1_norm_grad         0.000083  w[0]    0.134 bias    2.261\n",
      "iter 3701/1000000  loss         0.214488  avg_L1_norm_grad         0.000083  w[0]    0.134 bias    2.261\n",
      "iter 3800/1000000  loss         0.214396  avg_L1_norm_grad         0.000080  w[0]    0.135 bias    2.267\n",
      "iter 3801/1000000  loss         0.214395  avg_L1_norm_grad         0.000080  w[0]    0.135 bias    2.267\n",
      "iter 3900/1000000  loss         0.214310  avg_L1_norm_grad         0.000076  w[0]    0.137 bias    2.273\n",
      "iter 3901/1000000  loss         0.214309  avg_L1_norm_grad         0.000076  w[0]    0.137 bias    2.273\n",
      "iter 4000/1000000  loss         0.214231  avg_L1_norm_grad         0.000073  w[0]    0.138 bias    2.278\n",
      "iter 4001/1000000  loss         0.214230  avg_L1_norm_grad         0.000073  w[0]    0.138 bias    2.278\n",
      "iter 4100/1000000  loss         0.214158  avg_L1_norm_grad         0.000070  w[0]    0.140 bias    2.283\n",
      "iter 4101/1000000  loss         0.214157  avg_L1_norm_grad         0.000070  w[0]    0.140 bias    2.283\n",
      "iter 4200/1000000  loss         0.214091  avg_L1_norm_grad         0.000068  w[0]    0.141 bias    2.288\n",
      "iter 4201/1000000  loss         0.214090  avg_L1_norm_grad         0.000068  w[0]    0.141 bias    2.288\n",
      "iter 4300/1000000  loss         0.214029  avg_L1_norm_grad         0.000065  w[0]    0.142 bias    2.292\n",
      "iter 4301/1000000  loss         0.214029  avg_L1_norm_grad         0.000065  w[0]    0.142 bias    2.292\n",
      "iter 4400/1000000  loss         0.213973  avg_L1_norm_grad         0.000062  w[0]    0.143 bias    2.296\n",
      "iter 4401/1000000  loss         0.213972  avg_L1_norm_grad         0.000062  w[0]    0.143 bias    2.296\n",
      "iter 4500/1000000  loss         0.213920  avg_L1_norm_grad         0.000060  w[0]    0.144 bias    2.300\n",
      "iter 4501/1000000  loss         0.213920  avg_L1_norm_grad         0.000060  w[0]    0.144 bias    2.300\n",
      "iter 4600/1000000  loss         0.213872  avg_L1_norm_grad         0.000058  w[0]    0.145 bias    2.304\n",
      "iter 4601/1000000  loss         0.213871  avg_L1_norm_grad         0.000058  w[0]    0.145 bias    2.304\n",
      "iter 4700/1000000  loss         0.213827  avg_L1_norm_grad         0.000056  w[0]    0.147 bias    2.307\n",
      "iter 4701/1000000  loss         0.213827  avg_L1_norm_grad         0.000056  w[0]    0.147 bias    2.307\n",
      "iter 4800/1000000  loss         0.213786  avg_L1_norm_grad         0.000053  w[0]    0.148 bias    2.310\n",
      "iter 4801/1000000  loss         0.213786  avg_L1_norm_grad         0.000053  w[0]    0.148 bias    2.310\n",
      "iter 4900/1000000  loss         0.213748  avg_L1_norm_grad         0.000051  w[0]    0.149 bias    2.314\n",
      "iter 4901/1000000  loss         0.213747  avg_L1_norm_grad         0.000051  w[0]    0.149 bias    2.314\n",
      "iter 5000/1000000  loss         0.213712  avg_L1_norm_grad         0.000050  w[0]    0.150 bias    2.316\n",
      "iter 5001/1000000  loss         0.213712  avg_L1_norm_grad         0.000050  w[0]    0.150 bias    2.316\n",
      "iter 5100/1000000  loss         0.213680  avg_L1_norm_grad         0.000048  w[0]    0.150 bias    2.319\n",
      "iter 5101/1000000  loss         0.213679  avg_L1_norm_grad         0.000048  w[0]    0.151 bias    2.319\n",
      "iter 5200/1000000  loss         0.213649  avg_L1_norm_grad         0.000046  w[0]    0.151 bias    2.322\n",
      "iter 5201/1000000  loss         0.213649  avg_L1_norm_grad         0.000046  w[0]    0.151 bias    2.322\n",
      "iter 5300/1000000  loss         0.213621  avg_L1_norm_grad         0.000044  w[0]    0.152 bias    2.324\n",
      "iter 5301/1000000  loss         0.213621  avg_L1_norm_grad         0.000044  w[0]    0.152 bias    2.324\n",
      "iter 5400/1000000  loss         0.213595  avg_L1_norm_grad         0.000043  w[0]    0.153 bias    2.327\n",
      "iter 5401/1000000  loss         0.213595  avg_L1_norm_grad         0.000043  w[0]    0.153 bias    2.327\n",
      "iter 5500/1000000  loss         0.213571  avg_L1_norm_grad         0.000041  w[0]    0.154 bias    2.329\n",
      "iter 5501/1000000  loss         0.213571  avg_L1_norm_grad         0.000041  w[0]    0.154 bias    2.329\n",
      "iter 5600/1000000  loss         0.213549  avg_L1_norm_grad         0.000040  w[0]    0.155 bias    2.331\n",
      "iter 5601/1000000  loss         0.213549  avg_L1_norm_grad         0.000040  w[0]    0.155 bias    2.331\n",
      "iter 5700/1000000  loss         0.213528  avg_L1_norm_grad         0.000038  w[0]    0.156 bias    2.333\n",
      "iter 5701/1000000  loss         0.213528  avg_L1_norm_grad         0.000038  w[0]    0.156 bias    2.333\n",
      "iter 5800/1000000  loss         0.213509  avg_L1_norm_grad         0.000037  w[0]    0.156 bias    2.335\n",
      "iter 5801/1000000  loss         0.213509  avg_L1_norm_grad         0.000037  w[0]    0.156 bias    2.335\n",
      "iter 5900/1000000  loss         0.213491  avg_L1_norm_grad         0.000036  w[0]    0.157 bias    2.337\n",
      "iter 5901/1000000  loss         0.213491  avg_L1_norm_grad         0.000036  w[0]    0.157 bias    2.337\n",
      "iter 6000/1000000  loss         0.213474  avg_L1_norm_grad         0.000034  w[0]    0.158 bias    2.338\n",
      "iter 6001/1000000  loss         0.213474  avg_L1_norm_grad         0.000034  w[0]    0.158 bias    2.338\n",
      "iter 6100/1000000  loss         0.213459  avg_L1_norm_grad         0.000033  w[0]    0.158 bias    2.340\n",
      "iter 6101/1000000  loss         0.213458  avg_L1_norm_grad         0.000033  w[0]    0.158 bias    2.340\n",
      "iter 6200/1000000  loss         0.213444  avg_L1_norm_grad         0.000032  w[0]    0.159 bias    2.341\n",
      "iter 6201/1000000  loss         0.213444  avg_L1_norm_grad         0.000032  w[0]    0.159 bias    2.342\n",
      "iter 6300/1000000  loss         0.213431  avg_L1_norm_grad         0.000031  w[0]    0.160 bias    2.343\n",
      "iter 6301/1000000  loss         0.213431  avg_L1_norm_grad         0.000031  w[0]    0.160 bias    2.343\n",
      "iter 6400/1000000  loss         0.213418  avg_L1_norm_grad         0.000030  w[0]    0.160 bias    2.344\n",
      "iter 6401/1000000  loss         0.213418  avg_L1_norm_grad         0.000030  w[0]    0.160 bias    2.344\n",
      "iter 6500/1000000  loss         0.213407  avg_L1_norm_grad         0.000029  w[0]    0.161 bias    2.346\n",
      "iter 6501/1000000  loss         0.213407  avg_L1_norm_grad         0.000029  w[0]    0.161 bias    2.346\n",
      "iter 6600/1000000  loss         0.213396  avg_L1_norm_grad         0.000028  w[0]    0.161 bias    2.347\n",
      "iter 6601/1000000  loss         0.213396  avg_L1_norm_grad         0.000028  w[0]    0.161 bias    2.347\n",
      "iter 6700/1000000  loss         0.213386  avg_L1_norm_grad         0.000027  w[0]    0.162 bias    2.348\n",
      "iter 6701/1000000  loss         0.213386  avg_L1_norm_grad         0.000027  w[0]    0.162 bias    2.348\n",
      "iter 6800/1000000  loss         0.213376  avg_L1_norm_grad         0.000026  w[0]    0.162 bias    2.349\n",
      "iter 6801/1000000  loss         0.213376  avg_L1_norm_grad         0.000026  w[0]    0.162 bias    2.349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.213368  avg_L1_norm_grad         0.000025  w[0]    0.163 bias    2.350\n",
      "iter 6901/1000000  loss         0.213367  avg_L1_norm_grad         0.000025  w[0]    0.163 bias    2.350\n",
      "iter 7000/1000000  loss         0.213359  avg_L1_norm_grad         0.000024  w[0]    0.163 bias    2.351\n",
      "iter 7001/1000000  loss         0.213359  avg_L1_norm_grad         0.000024  w[0]    0.163 bias    2.351\n",
      "iter 7100/1000000  loss         0.213352  avg_L1_norm_grad         0.000023  w[0]    0.164 bias    2.352\n",
      "iter 7101/1000000  loss         0.213352  avg_L1_norm_grad         0.000023  w[0]    0.164 bias    2.352\n",
      "iter 7200/1000000  loss         0.213345  avg_L1_norm_grad         0.000022  w[0]    0.164 bias    2.353\n",
      "iter 7201/1000000  loss         0.213345  avg_L1_norm_grad         0.000022  w[0]    0.164 bias    2.353\n",
      "iter 7300/1000000  loss         0.213338  avg_L1_norm_grad         0.000022  w[0]    0.165 bias    2.354\n",
      "iter 7301/1000000  loss         0.213338  avg_L1_norm_grad         0.000022  w[0]    0.165 bias    2.354\n",
      "iter 7400/1000000  loss         0.213332  avg_L1_norm_grad         0.000021  w[0]    0.165 bias    2.355\n",
      "iter 7401/1000000  loss         0.213332  avg_L1_norm_grad         0.000021  w[0]    0.165 bias    2.355\n",
      "iter 7500/1000000  loss         0.213326  avg_L1_norm_grad         0.000020  w[0]    0.166 bias    2.356\n",
      "iter 7501/1000000  loss         0.213326  avg_L1_norm_grad         0.000020  w[0]    0.166 bias    2.356\n",
      "iter 7600/1000000  loss         0.213321  avg_L1_norm_grad         0.000020  w[0]    0.166 bias    2.357\n",
      "iter 7601/1000000  loss         0.213321  avg_L1_norm_grad         0.000020  w[0]    0.166 bias    2.357\n",
      "iter 7700/1000000  loss         0.213316  avg_L1_norm_grad         0.000019  w[0]    0.166 bias    2.357\n",
      "iter 7701/1000000  loss         0.213316  avg_L1_norm_grad         0.000019  w[0]    0.166 bias    2.357\n",
      "iter 7800/1000000  loss         0.213311  avg_L1_norm_grad         0.000018  w[0]    0.167 bias    2.358\n",
      "iter 7801/1000000  loss         0.213311  avg_L1_norm_grad         0.000018  w[0]    0.167 bias    2.358\n",
      "iter 7900/1000000  loss         0.213306  avg_L1_norm_grad         0.000018  w[0]    0.167 bias    2.359\n",
      "iter 7901/1000000  loss         0.213306  avg_L1_norm_grad         0.000018  w[0]    0.167 bias    2.359\n",
      "iter 8000/1000000  loss         0.213302  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    2.359\n",
      "iter 8001/1000000  loss         0.213302  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    2.359\n",
      "iter 8100/1000000  loss         0.213299  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    2.360\n",
      "iter 8101/1000000  loss         0.213298  avg_L1_norm_grad         0.000017  w[0]    0.168 bias    2.360\n",
      "iter 8200/1000000  loss         0.213295  avg_L1_norm_grad         0.000016  w[0]    0.168 bias    2.360\n",
      "iter 8201/1000000  loss         0.213295  avg_L1_norm_grad         0.000016  w[0]    0.168 bias    2.360\n",
      "iter 8300/1000000  loss         0.213292  avg_L1_norm_grad         0.000015  w[0]    0.169 bias    2.361\n",
      "iter 8301/1000000  loss         0.213292  avg_L1_norm_grad         0.000015  w[0]    0.169 bias    2.361\n",
      "Done. Converged after 8319 iterations.\n",
      "Origin Accuracy 0.9578055555555528\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr22 = LRGD(alpha=100.0, step_size=0.1)\n",
    "orig_lr22.fit(x_te, y_te)\n",
    "y_hat_Origin=np.asarray(orig_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 1572 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030593  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.945402  avg_L1_norm_grad         0.053469  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         1.193625  avg_L1_norm_grad         0.082112  w[0]    0.001 bias    0.036\n",
      "iter    3/1000000  loss         1.782658  avg_L1_norm_grad         0.096247  w[0]    0.000 bias   -0.003\n",
      "iter    4/1000000  loss         0.956486  avg_L1_norm_grad         0.067067  w[0]    0.002 bias    0.058\n",
      "iter    5/1000000  loss         1.235686  avg_L1_norm_grad         0.074687  w[0]    0.001 bias    0.028\n",
      "iter    6/1000000  loss         0.846800  avg_L1_norm_grad         0.058168  w[0]    0.003 bias    0.077\n",
      "iter    7/1000000  loss         0.988616  avg_L1_norm_grad         0.060480  w[0]    0.002 bias    0.052\n",
      "iter    8/1000000  loss         0.740667  avg_L1_norm_grad         0.048358  w[0]    0.003 bias    0.094\n",
      "iter    9/1000000  loss         0.814129  avg_L1_norm_grad         0.049390  w[0]    0.003 bias    0.074\n",
      "iter   10/1000000  loss         0.668988  avg_L1_norm_grad         0.042128  w[0]    0.004 bias    0.108\n",
      "iter   11/1000000  loss         0.714707  avg_L1_norm_grad         0.042734  w[0]    0.003 bias    0.092\n",
      "iter   12/1000000  loss         0.615327  avg_L1_norm_grad         0.037628  w[0]    0.004 bias    0.122\n",
      "iter   13/1000000  loss         0.646372  avg_L1_norm_grad         0.037994  w[0]    0.004 bias    0.108\n",
      "iter   14/1000000  loss         0.572307  avg_L1_norm_grad         0.033891  w[0]    0.005 bias    0.135\n",
      "iter   15/1000000  loss         0.593316  avg_L1_norm_grad         0.034025  w[0]    0.004 bias    0.123\n",
      "iter   16/1000000  loss         0.536087  avg_L1_norm_grad         0.030405  w[0]    0.005 bias    0.147\n",
      "iter   17/1000000  loss         0.549083  avg_L1_norm_grad         0.030288  w[0]    0.005 bias    0.136\n",
      "iter   18/1000000  loss         0.504719  avg_L1_norm_grad         0.026934  w[0]    0.005 bias    0.159\n",
      "iter   19/1000000  loss         0.511121  avg_L1_norm_grad         0.026566  w[0]    0.005 bias    0.149\n",
      "iter  100/1000000  loss         0.280145  avg_L1_norm_grad         0.001219  w[0]    0.013 bias    0.396\n",
      "iter  101/1000000  loss         0.279466  avg_L1_norm_grad         0.001211  w[0]    0.013 bias    0.397\n",
      "iter  200/1000000  loss         0.240314  avg_L1_norm_grad         0.000758  w[0]    0.019 bias    0.531\n",
      "iter  201/1000000  loss         0.240079  avg_L1_norm_grad         0.000756  w[0]    0.019 bias    0.532\n",
      "iter  300/1000000  loss         0.223258  avg_L1_norm_grad         0.000575  w[0]    0.023 bias    0.614\n",
      "iter  301/1000000  loss         0.223134  avg_L1_norm_grad         0.000573  w[0]    0.023 bias    0.614\n",
      "iter  400/1000000  loss         0.213424  avg_L1_norm_grad         0.000472  w[0]    0.027 bias    0.670\n",
      "iter  401/1000000  loss         0.213346  avg_L1_norm_grad         0.000471  w[0]    0.027 bias    0.671\n",
      "iter  500/1000000  loss         0.206904  avg_L1_norm_grad         0.000403  w[0]    0.031 bias    0.712\n",
      "iter  501/1000000  loss         0.206850  avg_L1_norm_grad         0.000402  w[0]    0.031 bias    0.712\n",
      "iter  600/1000000  loss         0.202222  avg_L1_norm_grad         0.000352  w[0]    0.034 bias    0.744\n",
      "iter  601/1000000  loss         0.202182  avg_L1_norm_grad         0.000351  w[0]    0.034 bias    0.745\n",
      "iter  700/1000000  loss         0.198684  avg_L1_norm_grad         0.000313  w[0]    0.037 bias    0.770\n",
      "iter  701/1000000  loss         0.198653  avg_L1_norm_grad         0.000312  w[0]    0.037 bias    0.770\n",
      "iter  800/1000000  loss         0.195916  avg_L1_norm_grad         0.000281  w[0]    0.040 bias    0.791\n",
      "iter  801/1000000  loss         0.195891  avg_L1_norm_grad         0.000281  w[0]    0.040 bias    0.791\n",
      "iter  900/1000000  loss         0.193693  avg_L1_norm_grad         0.000255  w[0]    0.043 bias    0.808\n",
      "iter  901/1000000  loss         0.193673  avg_L1_norm_grad         0.000255  w[0]    0.043 bias    0.808\n",
      "iter 1000/1000000  loss         0.191873  avg_L1_norm_grad         0.000233  w[0]    0.046 bias    0.822\n",
      "iter 1001/1000000  loss         0.191857  avg_L1_norm_grad         0.000233  w[0]    0.046 bias    0.822\n",
      "iter 1100/1000000  loss         0.190361  avg_L1_norm_grad         0.000214  w[0]    0.049 bias    0.834\n",
      "iter 1101/1000000  loss         0.190347  avg_L1_norm_grad         0.000214  w[0]    0.049 bias    0.834\n",
      "iter 1200/1000000  loss         0.189088  avg_L1_norm_grad         0.000197  w[0]    0.051 bias    0.845\n",
      "iter 1201/1000000  loss         0.189076  avg_L1_norm_grad         0.000197  w[0]    0.051 bias    0.845\n",
      "iter 1300/1000000  loss         0.188006  avg_L1_norm_grad         0.000183  w[0]    0.054 bias    0.853\n",
      "iter 1301/1000000  loss         0.187996  avg_L1_norm_grad         0.000183  w[0]    0.054 bias    0.854\n",
      "iter 1400/1000000  loss         0.187078  avg_L1_norm_grad         0.000170  w[0]    0.056 bias    0.861\n",
      "iter 1401/1000000  loss         0.187070  avg_L1_norm_grad         0.000170  w[0]    0.056 bias    0.861\n",
      "iter 1500/1000000  loss         0.186277  avg_L1_norm_grad         0.000158  w[0]    0.058 bias    0.868\n",
      "iter 1501/1000000  loss         0.186269  avg_L1_norm_grad         0.000158  w[0]    0.058 bias    0.868\n",
      "iter 1600/1000000  loss         0.185580  avg_L1_norm_grad         0.000148  w[0]    0.060 bias    0.873\n",
      "iter 1601/1000000  loss         0.185573  avg_L1_norm_grad         0.000148  w[0]    0.060 bias    0.874\n",
      "iter 1700/1000000  loss         0.184970  avg_L1_norm_grad         0.000139  w[0]    0.063 bias    0.878\n",
      "iter 1701/1000000  loss         0.184965  avg_L1_norm_grad         0.000139  w[0]    0.063 bias    0.879\n",
      "iter 1800/1000000  loss         0.184435  avg_L1_norm_grad         0.000130  w[0]    0.065 bias    0.883\n",
      "iter 1801/1000000  loss         0.184430  avg_L1_norm_grad         0.000130  w[0]    0.065 bias    0.883\n",
      "iter 1900/1000000  loss         0.183963  avg_L1_norm_grad         0.000123  w[0]    0.066 bias    0.887\n",
      "iter 1901/1000000  loss         0.183958  avg_L1_norm_grad         0.000122  w[0]    0.067 bias    0.887\n",
      "iter 2000/1000000  loss         0.183544  avg_L1_norm_grad         0.000116  w[0]    0.068 bias    0.890\n",
      "iter 2001/1000000  loss         0.183540  avg_L1_norm_grad         0.000115  w[0]    0.068 bias    0.890\n",
      "iter 2100/1000000  loss         0.183172  avg_L1_norm_grad         0.000109  w[0]    0.070 bias    0.893\n",
      "iter 2101/1000000  loss         0.183169  avg_L1_norm_grad         0.000109  w[0]    0.070 bias    0.893\n",
      "iter 2200/1000000  loss         0.182841  avg_L1_norm_grad         0.000103  w[0]    0.072 bias    0.896\n",
      "iter 2201/1000000  loss         0.182838  avg_L1_norm_grad         0.000103  w[0]    0.072 bias    0.896\n",
      "iter 2300/1000000  loss         0.182544  avg_L1_norm_grad         0.000098  w[0]    0.073 bias    0.898\n",
      "iter 2301/1000000  loss         0.182541  avg_L1_norm_grad         0.000098  w[0]    0.073 bias    0.898\n",
      "iter 2400/1000000  loss         0.182278  avg_L1_norm_grad         0.000093  w[0]    0.075 bias    0.900\n",
      "iter 2401/1000000  loss         0.182276  avg_L1_norm_grad         0.000092  w[0]    0.075 bias    0.900\n",
      "iter 2500/1000000  loss         0.182039  avg_L1_norm_grad         0.000088  w[0]    0.076 bias    0.902\n",
      "iter 2501/1000000  loss         0.182037  avg_L1_norm_grad         0.000088  w[0]    0.077 bias    0.902\n",
      "iter 2600/1000000  loss         0.181823  avg_L1_norm_grad         0.000083  w[0]    0.078 bias    0.903\n",
      "iter 2601/1000000  loss         0.181821  avg_L1_norm_grad         0.000083  w[0]    0.078 bias    0.903\n",
      "iter 2700/1000000  loss         0.181629  avg_L1_norm_grad         0.000079  w[0]    0.079 bias    0.905\n",
      "iter 2701/1000000  loss         0.181627  avg_L1_norm_grad         0.000079  w[0]    0.079 bias    0.905\n",
      "iter 2800/1000000  loss         0.181453  avg_L1_norm_grad         0.000075  w[0]    0.081 bias    0.906\n",
      "iter 2801/1000000  loss         0.181451  avg_L1_norm_grad         0.000075  w[0]    0.081 bias    0.906\n",
      "iter 2900/1000000  loss         0.181293  avg_L1_norm_grad         0.000072  w[0]    0.082 bias    0.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.181292  avg_L1_norm_grad         0.000071  w[0]    0.082 bias    0.907\n",
      "iter 3000/1000000  loss         0.181149  avg_L1_norm_grad         0.000068  w[0]    0.083 bias    0.908\n",
      "iter 3001/1000000  loss         0.181147  avg_L1_norm_grad         0.000068  w[0]    0.083 bias    0.908\n",
      "iter 3100/1000000  loss         0.181017  avg_L1_norm_grad         0.000065  w[0]    0.084 bias    0.908\n",
      "iter 3101/1000000  loss         0.181016  avg_L1_norm_grad         0.000065  w[0]    0.084 bias    0.908\n",
      "iter 3200/1000000  loss         0.180897  avg_L1_norm_grad         0.000062  w[0]    0.085 bias    0.909\n",
      "iter 3201/1000000  loss         0.180896  avg_L1_norm_grad         0.000062  w[0]    0.085 bias    0.909\n",
      "iter 3300/1000000  loss         0.180788  avg_L1_norm_grad         0.000059  w[0]    0.086 bias    0.910\n",
      "iter 3301/1000000  loss         0.180787  avg_L1_norm_grad         0.000059  w[0]    0.086 bias    0.910\n",
      "iter 3400/1000000  loss         0.180688  avg_L1_norm_grad         0.000056  w[0]    0.087 bias    0.910\n",
      "iter 3401/1000000  loss         0.180687  avg_L1_norm_grad         0.000056  w[0]    0.087 bias    0.910\n",
      "iter 3500/1000000  loss         0.180597  avg_L1_norm_grad         0.000054  w[0]    0.088 bias    0.911\n",
      "iter 3501/1000000  loss         0.180596  avg_L1_norm_grad         0.000054  w[0]    0.088 bias    0.911\n",
      "iter 3600/1000000  loss         0.180514  avg_L1_norm_grad         0.000051  w[0]    0.089 bias    0.911\n",
      "iter 3601/1000000  loss         0.180513  avg_L1_norm_grad         0.000051  w[0]    0.089 bias    0.911\n",
      "iter 3700/1000000  loss         0.180437  avg_L1_norm_grad         0.000049  w[0]    0.090 bias    0.911\n",
      "iter 3701/1000000  loss         0.180437  avg_L1_norm_grad         0.000049  w[0]    0.090 bias    0.911\n",
      "iter 3800/1000000  loss         0.180367  avg_L1_norm_grad         0.000047  w[0]    0.091 bias    0.911\n",
      "iter 3801/1000000  loss         0.180366  avg_L1_norm_grad         0.000047  w[0]    0.091 bias    0.911\n",
      "iter 3900/1000000  loss         0.180303  avg_L1_norm_grad         0.000045  w[0]    0.092 bias    0.912\n",
      "iter 3901/1000000  loss         0.180302  avg_L1_norm_grad         0.000045  w[0]    0.092 bias    0.912\n",
      "iter 4000/1000000  loss         0.180243  avg_L1_norm_grad         0.000043  w[0]    0.093 bias    0.912\n",
      "iter 4001/1000000  loss         0.180243  avg_L1_norm_grad         0.000043  w[0]    0.093 bias    0.912\n",
      "iter 4100/1000000  loss         0.180189  avg_L1_norm_grad         0.000041  w[0]    0.093 bias    0.912\n",
      "iter 4101/1000000  loss         0.180188  avg_L1_norm_grad         0.000041  w[0]    0.093 bias    0.912\n",
      "iter 4200/1000000  loss         0.180139  avg_L1_norm_grad         0.000039  w[0]    0.094 bias    0.912\n",
      "iter 4201/1000000  loss         0.180138  avg_L1_norm_grad         0.000039  w[0]    0.094 bias    0.912\n",
      "iter 4300/1000000  loss         0.180093  avg_L1_norm_grad         0.000038  w[0]    0.095 bias    0.912\n",
      "iter 4301/1000000  loss         0.180092  avg_L1_norm_grad         0.000038  w[0]    0.095 bias    0.912\n",
      "iter 4400/1000000  loss         0.180050  avg_L1_norm_grad         0.000036  w[0]    0.095 bias    0.912\n",
      "iter 4401/1000000  loss         0.180049  avg_L1_norm_grad         0.000036  w[0]    0.095 bias    0.912\n",
      "iter 4500/1000000  loss         0.180010  avg_L1_norm_grad         0.000034  w[0]    0.096 bias    0.912\n",
      "iter 4501/1000000  loss         0.180010  avg_L1_norm_grad         0.000034  w[0]    0.096 bias    0.912\n",
      "iter 4600/1000000  loss         0.179974  avg_L1_norm_grad         0.000033  w[0]    0.097 bias    0.912\n",
      "iter 4601/1000000  loss         0.179974  avg_L1_norm_grad         0.000033  w[0]    0.097 bias    0.912\n",
      "iter 4700/1000000  loss         0.179940  avg_L1_norm_grad         0.000032  w[0]    0.097 bias    0.912\n",
      "iter 4701/1000000  loss         0.179940  avg_L1_norm_grad         0.000032  w[0]    0.097 bias    0.912\n",
      "iter 4800/1000000  loss         0.179909  avg_L1_norm_grad         0.000030  w[0]    0.098 bias    0.912\n",
      "iter 4801/1000000  loss         0.179909  avg_L1_norm_grad         0.000030  w[0]    0.098 bias    0.912\n",
      "iter 4900/1000000  loss         0.179880  avg_L1_norm_grad         0.000029  w[0]    0.098 bias    0.912\n",
      "iter 4901/1000000  loss         0.179880  avg_L1_norm_grad         0.000029  w[0]    0.098 bias    0.912\n",
      "iter 5000/1000000  loss         0.179853  avg_L1_norm_grad         0.000028  w[0]    0.099 bias    0.912\n",
      "iter 5001/1000000  loss         0.179853  avg_L1_norm_grad         0.000028  w[0]    0.099 bias    0.912\n",
      "iter 5100/1000000  loss         0.179829  avg_L1_norm_grad         0.000027  w[0]    0.099 bias    0.912\n",
      "iter 5101/1000000  loss         0.179828  avg_L1_norm_grad         0.000027  w[0]    0.099 bias    0.912\n",
      "iter 5200/1000000  loss         0.179806  avg_L1_norm_grad         0.000026  w[0]    0.100 bias    0.912\n",
      "iter 5201/1000000  loss         0.179805  avg_L1_norm_grad         0.000026  w[0]    0.100 bias    0.912\n",
      "iter 5300/1000000  loss         0.179784  avg_L1_norm_grad         0.000025  w[0]    0.100 bias    0.912\n",
      "iter 5301/1000000  loss         0.179784  avg_L1_norm_grad         0.000025  w[0]    0.100 bias    0.912\n",
      "iter 5400/1000000  loss         0.179764  avg_L1_norm_grad         0.000024  w[0]    0.101 bias    0.912\n",
      "iter 5401/1000000  loss         0.179764  avg_L1_norm_grad         0.000024  w[0]    0.101 bias    0.912\n",
      "iter 5500/1000000  loss         0.179746  avg_L1_norm_grad         0.000023  w[0]    0.101 bias    0.912\n",
      "iter 5501/1000000  loss         0.179745  avg_L1_norm_grad         0.000023  w[0]    0.101 bias    0.912\n",
      "iter 5600/1000000  loss         0.179728  avg_L1_norm_grad         0.000022  w[0]    0.102 bias    0.911\n",
      "iter 5601/1000000  loss         0.179728  avg_L1_norm_grad         0.000022  w[0]    0.102 bias    0.911\n",
      "iter 5700/1000000  loss         0.179712  avg_L1_norm_grad         0.000021  w[0]    0.102 bias    0.911\n",
      "iter 5701/1000000  loss         0.179712  avg_L1_norm_grad         0.000021  w[0]    0.102 bias    0.911\n",
      "iter 5800/1000000  loss         0.179697  avg_L1_norm_grad         0.000020  w[0]    0.102 bias    0.911\n",
      "iter 5801/1000000  loss         0.179697  avg_L1_norm_grad         0.000020  w[0]    0.102 bias    0.911\n",
      "iter 5900/1000000  loss         0.179683  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    0.911\n",
      "iter 5901/1000000  loss         0.179683  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    0.911\n",
      "iter 6000/1000000  loss         0.179670  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    0.911\n",
      "iter 6001/1000000  loss         0.179670  avg_L1_norm_grad         0.000019  w[0]    0.103 bias    0.911\n",
      "iter 6100/1000000  loss         0.179658  avg_L1_norm_grad         0.000018  w[0]    0.103 bias    0.911\n",
      "iter 6101/1000000  loss         0.179658  avg_L1_norm_grad         0.000018  w[0]    0.103 bias    0.911\n",
      "iter 6200/1000000  loss         0.179647  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    0.911\n",
      "iter 6201/1000000  loss         0.179647  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    0.911\n",
      "iter 6300/1000000  loss         0.179636  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    0.911\n",
      "iter 6301/1000000  loss         0.179636  avg_L1_norm_grad         0.000017  w[0]    0.104 bias    0.911\n",
      "iter 6400/1000000  loss         0.179626  avg_L1_norm_grad         0.000016  w[0]    0.104 bias    0.911\n",
      "iter 6401/1000000  loss         0.179626  avg_L1_norm_grad         0.000016  w[0]    0.104 bias    0.911\n",
      "iter 6500/1000000  loss         0.179617  avg_L1_norm_grad         0.000015  w[0]    0.104 bias    0.911\n",
      "iter 6501/1000000  loss         0.179617  avg_L1_norm_grad         0.000015  w[0]    0.104 bias    0.911\n",
      "iter 6600/1000000  loss         0.179608  avg_L1_norm_grad         0.000015  w[0]    0.105 bias    0.911\n",
      "iter 6601/1000000  loss         0.179608  avg_L1_norm_grad         0.000015  w[0]    0.105 bias    0.911\n",
      "iter 6700/1000000  loss         0.179600  avg_L1_norm_grad         0.000014  w[0]    0.105 bias    0.911\n",
      "iter 6701/1000000  loss         0.179600  avg_L1_norm_grad         0.000014  w[0]    0.105 bias    0.911\n",
      "iter 6800/1000000  loss         0.179592  avg_L1_norm_grad         0.000014  w[0]    0.105 bias    0.911\n",
      "iter 6801/1000000  loss         0.179592  avg_L1_norm_grad         0.000014  w[0]    0.105 bias    0.911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.179585  avg_L1_norm_grad         0.000013  w[0]    0.105 bias    0.910\n",
      "iter 6901/1000000  loss         0.179585  avg_L1_norm_grad         0.000013  w[0]    0.105 bias    0.910\n",
      "iter 7000/1000000  loss         0.179578  avg_L1_norm_grad         0.000013  w[0]    0.106 bias    0.910\n",
      "iter 7001/1000000  loss         0.179578  avg_L1_norm_grad         0.000013  w[0]    0.106 bias    0.910\n",
      "iter 7100/1000000  loss         0.179572  avg_L1_norm_grad         0.000012  w[0]    0.106 bias    0.910\n",
      "iter 7101/1000000  loss         0.179572  avg_L1_norm_grad         0.000012  w[0]    0.106 bias    0.910\n",
      "iter 7200/1000000  loss         0.179566  avg_L1_norm_grad         0.000012  w[0]    0.106 bias    0.910\n",
      "iter 7201/1000000  loss         0.179566  avg_L1_norm_grad         0.000012  w[0]    0.106 bias    0.910\n",
      "iter 7300/1000000  loss         0.179560  avg_L1_norm_grad         0.000011  w[0]    0.106 bias    0.910\n",
      "iter 7301/1000000  loss         0.179560  avg_L1_norm_grad         0.000011  w[0]    0.106 bias    0.910\n",
      "iter 7400/1000000  loss         0.179555  avg_L1_norm_grad         0.000011  w[0]    0.106 bias    0.910\n",
      "iter 7401/1000000  loss         0.179555  avg_L1_norm_grad         0.000011  w[0]    0.106 bias    0.910\n",
      "iter 7500/1000000  loss         0.179550  avg_L1_norm_grad         0.000011  w[0]    0.107 bias    0.910\n",
      "iter 7501/1000000  loss         0.179550  avg_L1_norm_grad         0.000011  w[0]    0.107 bias    0.910\n",
      "iter 7600/1000000  loss         0.179545  avg_L1_norm_grad         0.000010  w[0]    0.107 bias    0.910\n",
      "iter 7601/1000000  loss         0.179545  avg_L1_norm_grad         0.000010  w[0]    0.107 bias    0.910\n",
      "iter 7700/1000000  loss         0.179541  avg_L1_norm_grad         0.000010  w[0]    0.107 bias    0.910\n",
      "iter 7701/1000000  loss         0.179541  avg_L1_norm_grad         0.000010  w[0]    0.107 bias    0.910\n",
      "iter 7800/1000000  loss         0.179537  avg_L1_norm_grad         0.000009  w[0]    0.107 bias    0.910\n",
      "iter 7801/1000000  loss         0.179537  avg_L1_norm_grad         0.000009  w[0]    0.107 bias    0.910\n",
      "iter 7900/1000000  loss         0.179533  avg_L1_norm_grad         0.000009  w[0]    0.107 bias    0.910\n",
      "iter 7901/1000000  loss         0.179533  avg_L1_norm_grad         0.000009  w[0]    0.107 bias    0.910\n",
      "iter 8000/1000000  loss         0.179529  avg_L1_norm_grad         0.000009  w[0]    0.107 bias    0.910\n",
      "iter 8001/1000000  loss         0.179529  avg_L1_norm_grad         0.000009  w[0]    0.107 bias    0.910\n",
      "iter 8100/1000000  loss         0.179526  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8101/1000000  loss         0.179526  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8200/1000000  loss         0.179522  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8201/1000000  loss         0.179522  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8300/1000000  loss         0.179519  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8301/1000000  loss         0.179519  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8400/1000000  loss         0.179516  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8401/1000000  loss         0.179516  avg_L1_norm_grad         0.000008  w[0]    0.108 bias    0.910\n",
      "iter 8500/1000000  loss         0.179514  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8501/1000000  loss         0.179514  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8600/1000000  loss         0.179511  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8601/1000000  loss         0.179511  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8700/1000000  loss         0.179509  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8701/1000000  loss         0.179508  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8800/1000000  loss         0.179506  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8801/1000000  loss         0.179506  avg_L1_norm_grad         0.000007  w[0]    0.108 bias    0.910\n",
      "iter 8900/1000000  loss         0.179504  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 8901/1000000  loss         0.179504  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9000/1000000  loss         0.179502  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9001/1000000  loss         0.179502  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9100/1000000  loss         0.179500  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9101/1000000  loss         0.179500  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9200/1000000  loss         0.179498  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9201/1000000  loss         0.179498  avg_L1_norm_grad         0.000006  w[0]    0.109 bias    0.910\n",
      "iter 9300/1000000  loss         0.179496  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9301/1000000  loss         0.179496  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9400/1000000  loss         0.179494  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9401/1000000  loss         0.179494  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9500/1000000  loss         0.179493  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9501/1000000  loss         0.179493  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9600/1000000  loss         0.179491  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9601/1000000  loss         0.179491  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9700/1000000  loss         0.179490  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9701/1000000  loss         0.179490  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9800/1000000  loss         0.179488  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9801/1000000  loss         0.179488  avg_L1_norm_grad         0.000005  w[0]    0.109 bias    0.911\n",
      "iter 9900/1000000  loss         0.179487  avg_L1_norm_grad         0.000004  w[0]    0.109 bias    0.911\n",
      "iter 9901/1000000  loss         0.179487  avg_L1_norm_grad         0.000004  w[0]    0.109 bias    0.911\n",
      "iter 10000/1000000  loss         0.179486  avg_L1_norm_grad         0.000004  w[0]    0.109 bias    0.911\n",
      "iter 10001/1000000  loss         0.179486  avg_L1_norm_grad         0.000004  w[0]    0.109 bias    0.911\n",
      "iter 10100/1000000  loss         0.179485  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10101/1000000  loss         0.179485  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10200/1000000  loss         0.179483  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10201/1000000  loss         0.179483  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10300/1000000  loss         0.179482  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10301/1000000  loss         0.179482  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10400/1000000  loss         0.179481  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10401/1000000  loss         0.179481  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10500/1000000  loss         0.179480  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10501/1000000  loss         0.179480  avg_L1_norm_grad         0.000004  w[0]    0.110 bias    0.911\n",
      "iter 10600/1000000  loss         0.179479  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.911\n",
      "iter 10601/1000000  loss         0.179479  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.911\n",
      "iter 10700/1000000  loss         0.179479  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.911\n",
      "iter 10701/1000000  loss         0.179479  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.179478  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.911\n",
      "iter 10801/1000000  loss         0.179478  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.911\n",
      "iter 10900/1000000  loss         0.179477  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 10901/1000000  loss         0.179477  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11000/1000000  loss         0.179476  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11001/1000000  loss         0.179476  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11100/1000000  loss         0.179475  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11101/1000000  loss         0.179475  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11200/1000000  loss         0.179475  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11201/1000000  loss         0.179475  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11300/1000000  loss         0.179474  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11301/1000000  loss         0.179474  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11400/1000000  loss         0.179473  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11401/1000000  loss         0.179473  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11500/1000000  loss         0.179473  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11501/1000000  loss         0.179473  avg_L1_norm_grad         0.000003  w[0]    0.110 bias    0.912\n",
      "iter 11600/1000000  loss         0.179472  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11601/1000000  loss         0.179472  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11700/1000000  loss         0.179472  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11701/1000000  loss         0.179472  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11800/1000000  loss         0.179471  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11801/1000000  loss         0.179471  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11900/1000000  loss         0.179471  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 11901/1000000  loss         0.179471  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.912\n",
      "iter 12000/1000000  loss         0.179470  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.913\n",
      "iter 12001/1000000  loss         0.179470  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.913\n",
      "iter 12100/1000000  loss         0.179470  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.913\n",
      "iter 12101/1000000  loss         0.179470  avg_L1_norm_grad         0.000002  w[0]    0.110 bias    0.913\n",
      "iter 12200/1000000  loss         0.179469  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12201/1000000  loss         0.179469  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12300/1000000  loss         0.179469  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12301/1000000  loss         0.179469  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12400/1000000  loss         0.179468  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12401/1000000  loss         0.179468  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12500/1000000  loss         0.179468  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12501/1000000  loss         0.179468  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12600/1000000  loss         0.179467  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12601/1000000  loss         0.179467  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12700/1000000  loss         0.179467  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12701/1000000  loss         0.179467  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12800/1000000  loss         0.179467  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12801/1000000  loss         0.179467  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12900/1000000  loss         0.179466  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 12901/1000000  loss         0.179466  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.913\n",
      "iter 13000/1000000  loss         0.179466  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.914\n",
      "iter 13001/1000000  loss         0.179466  avg_L1_norm_grad         0.000002  w[0]    0.111 bias    0.914\n",
      "iter 13100/1000000  loss         0.179466  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13101/1000000  loss         0.179466  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13200/1000000  loss         0.179466  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13201/1000000  loss         0.179466  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13300/1000000  loss         0.179465  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13301/1000000  loss         0.179465  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13400/1000000  loss         0.179465  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13401/1000000  loss         0.179465  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13500/1000000  loss         0.179465  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13501/1000000  loss         0.179465  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13600/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13601/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13700/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13701/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13800/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13801/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13900/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 13901/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.914\n",
      "iter 14000/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14001/1000000  loss         0.179464  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14100/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14101/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14200/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14201/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14300/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14301/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14400/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14401/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14500/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14501/1000000  loss         0.179463  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14600/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14601/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14701/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14800/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14801/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14900/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 14901/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.915\n",
      "iter 15000/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15001/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15100/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15101/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15200/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15201/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15300/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15301/1000000  loss         0.179462  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15400/1000000  loss         0.179461  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15401/1000000  loss         0.179461  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15500/1000000  loss         0.179461  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "iter 15501/1000000  loss         0.179461  avg_L1_norm_grad         0.000001  w[0]    0.111 bias    0.916\n",
      "Done. Converged after 15526 iterations.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_lr22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6029c1cc7546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnew_lr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRGDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnew_lr2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_hat_New\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_lr22\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_va\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalc_TP_TN_FP_FN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat_New\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_lr22' is not defined"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr2 = LRGDF(alpha=100.0, step_size=0.1)\n",
    "new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n",
      "New Accuracy 0.9622499999999973\n"
     ]
    }
   ],
   "source": [
    "y_hat_New=np.asarray(new_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Alpha=100\n",
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "plt.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "plt.plot(fpr3te,tpr3te, label=\"MinMax Trans. Adding noise, counting bright blocks with 1 bright NB and with 2\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "plt.plot(fpr1Tte,tpr1Tte, label=\"No added Feature No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "plt.plot(fprTte,tprTte, label=\"MinMax Trans. Adding ave and counting bright blocks with 1 bright NB and with 2\")\n",
    "\n",
    "plt.xlim([-0.0, 1.0]);\n",
    "plt.ylim([-0.0, 1.0]);\n",
    "plt.legend();\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "Ave Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr2.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Err: 0.425 0.992655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEKCAYAAAAIFwCwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHVFJREFUeJzt3XtwnNWZJvDn7db9Ykuy5JvkC74DJsaJTAAHB5JACPFymeESZ8J4MpmYqU1SSS1bOwlbqcluKlVMKiGb2slk1gkOsEsIUwsESNgZCGFiCGB8AYzBF8AIfLdkW7buUne/+4faU8LxeVrH0Gp15vlVUZb06O3vqNV66e7vfOeYu0NEZLQShR6AiBQXNQ0RiaKmISJR1DREJIqahohEUdMQkShqGiISRU1DRKKoaYhIlJJCD2A06ic1evPMmcG8ov8AvwGzcFZWQUszSZ4nMETzQa8JH9qP01rkmq3Lfi4ASKd5XloWPrQl+aE9w2/7+FGeT2zgOT04/7kdPM9xrwFDfcEoXTKBlia7DvHbrg4/HoaPPUhjLw8fP9fPnbDw46Gt7SA6Ojpz3jVAgZqGmV0J4IcAkgB+6u63s+9vnjkTDz71+2A+b+e3+QFLwj+mnbWQlvZNOJfmlRnesPYNLQ9mzYO/prVI8QcQSst53p3jD7dpVjBKl9bR0mS6m+b+q/tpbitvojlVwht52vnDOmG84dn+V4NZZ+MVtHbi+h/w2269mOZ++B2aD869PJilnD8eqkrCj4cLln2R1o405i9PzCwJ4EcAPgXgHACrzOycsR6HiJyZQryncQGAN9x9t7sPAvgFgGsKMA4ROQOFaBrNAPaM+Hxv9mvvYmZrzGyTmW061tExZoMTEa4QTeN0b7b8wTt+7r7W3VvdvbW+sXEMhiUio1GIprEXwIwRn7cA2F+AcYjIGShE09gIYL6ZnWVmZQA+A+CRAoxDRM7AmJ9ydfeUmX0ZwL9g+JTrOncPn+MCUIFOzMfDwbxjyX+kx2w89EAwO1YdPiUKAHXP/Yjm6eXX0rw5sz4cVvPTmhga4Lnl6PmVfA4Jek8Eo66qs2lpHbbT3K64mh+7j89R2ZNYGcxmZJ6itemSKTRPZo7RfHBqazDr6ecvles+cT3Ne1K8PlP9QZrXdr8YzMre3kVrbcbccJjr9P4IBZmn4e6PAXisEMcWkfdG08hFJIqahohEUdMQkShqGiISRU1DRKKoaYhIlKJYTwOWAErCaz+weRgAgJnnB6P6IT7fALNm0PjYYPjycgCY9Oq/BDP7AJ8jMlQxneal7VtpjinkvDyA7tTUYFZpnfy2E/zy9BePr6D50v6f0fz/HQpfb3TV2R+ltdXg1yqVDfHLz3f2XxTMptXw+6VzkD9eShJ87k1tgo8NyfCfbPq88GXzAFBy4u1wOKqVNIbpmYaIRFHTEJEoahoiEkVNQ0SiqGmISBQ1DRGJUhSnXD1RiqHKlmBeOiV8OhYAkCaX/ab45ePHpvBLvBs7w6dUAcAb6oNZ5pFf0Nqeld+keV2OS+t9x/M0r6lvCoeN/FQyBvmpw6X1z9Lct/LTomsWhseeqspxWjPDV0ofLAlvhwEAzWTV7uf3V9HaWRPDv28AOHciv1+OZ2bTfGLF7mBW0t9Oa719TziMuDRezzREJIqahohEUdMQkShqGiISRU1DRKKoaYhIFDUNEYlSFPM0bKAbJW1PB3NPpWh9Zv5HgtlgGZmrAKD+2O/44Cbxc/429Ho4+/AyWjtx70P82FP5pe825wM09+d/G65NJHntkYP82PPD2wAAgC3gY0MyPPcmuT08bgDoms+3Eai1Npp3pxYHsxUz3qK1Q5lKmrOfCwAmengeBgCgvyucHT9MS20aebyU8qUORtIzDRGJoqYhIlHUNEQkipqGiERR0xCRKGoaIhJFTUNEohRknoaZtQHoApAGkHJ3elI/UzYRPTOvCuY1JXzOwGC6OphV7ssxD6Oihsa+8wWa2/Q54XFNmE9rjw8207yp9zc0R2k5jW1xeGuHVP08WttTF577AgATk3tpjsqJNPbtG4KZTedrfdRmwnNjAORcO6JvKBPM3uzja3nMnMDXCdlwMHyfA8D02lKaTyjfF84GHqG1VkXu8xzzckYq5OSuy9yd38MiMu7o5YmIRClU03AAj5vZZjNbU6AxiMgZKNTLk+Xuvt/MJgN4wsx2uPv6kd+QbSZrAGDGzGmFGKOInEZBnmm4+/7sv4cBPATggtN8z1p3b3X31sbGhrEeoogEjHnTMLNqM6s9+TGAKwBsG+txiMiZKcTLkykAHjKzk8f/ubv/cwHGISJnYMybhrvvBrAkpibRdxTV2+4P3+be/bS+8uJLwmFZjvUPcuyLYlP5ehp9tWcHs8reN2htRXktzdHAj42j75xxffKFR2nprllfp3nPUCPN6yv4fIS5i8L73FTvepDWDv76AZqX33QZzSdPDa9L8dNn+ePhM8v43JqWCTnWfnEa0/U6rCI8HwkAcIDMXxnq57Uj6JSriERR0xCRKGoaIhJFTUNEoqhpiEgUNQ0RiVIUWxh41USkl3wymHedw6eZs0vjp1Q+xw/efYTnEybTuOKtJ8JhE7/MuvbYMzT33fyUbfeH/oLmIGcPK5f9B1o6e6CH5kf7+aXvdeW8vjxBluo/2klrM50DNE8/xZczqL86fLp41bJraG1fymg+P8GnJHVV8NkIu4+HT+nWTeO1JUZOq2oLAxHJFzUNEYmipiEiUdQ0RCSKmoaIRFHTEJEoahoiEqU45ml4Ev3pumBe3/c8re+uJuevO4/xg1fmuDw9h/Tc8FL/7GcCgOqJfLF2m8y3GagF39rh6MBZwexwF19q/8ld7TTfvZfPIblw8VSaX7M/vBRCx4/53JqGK8M/FwB0rfo2zdv7wvN+ppbx5QaGMnx7BdQ20XhggF/evqh+VzA72HserW1J/DYcZtK0diQ90xCRKGoaIhJFTUNEoqhpiEgUNQ0RiaKmISJR1DREJEpRzNNIYBA1tjeYpybwc+M1Fl6SvrvuDzZ3e3ctcmwD0MXnUpSUHA/fdtfLtPbopKto3lD+Js0H0nxNiwll+4LZ1nY+P+XjC/h8g/JS/v+ji1v4Wv19P9oazH5zP1/j5LpzJtH8pcP1NG+d2hbMHt3F59asnP82zf23v6R543l8fgyqwr/T2jL+O+tNhOevZKycH3cEPdMQkShqGiISRU1DRKKoaYhIFDUNEYmipiEiUdQ0RCRK3uZpmNk6ACsBHHb3xdmvNQC4H8BsAG0AbnT3HAtaAMik6HyIkkq+z0VvSXgeR82JTfzY6RSNfR8/L4/evmBkc+bS0prS8PwSAOhNh/fnAIDKt/geG+gO7z1y6fwDtHQvrqD56ulkvxcAyDGHxL8Uvv0/PWsDrS27/hM0XzyJ77ny8I7w2GbUV9LalPPHoi06m+YHa66m+ZQXvxvMJl7M56e81fXhYDaUGR/7ntwF4MpTvvZ1AE+6+3wAT2Y/F5Eikrem4e7rARw95cvXALg7+/HdAK7N1/FFJD/G+j2NKe5+AACy//I9DUVk3Bm3b4Sa2Roz22Rmm9o7yL6eIjKmxrppHDKzaQCQ/Tf4Tp+7r3X3VndvbWp8b4v7isj7Z6ybxiMAVmc/Xg3g4TE+voi8R3lrGmZ2H4DnACw0s71m9gUAtwO43MxeB3B59nMRKSJ5m6fh7qsC0cfP4MaG52qE9HfT8spX7gnfdIrPw7CPruT5ED8v/2LpnwWzzXv4FJVP1vJ5GC3Vm2luM/icgMH/dVcwK63h+29MmfMazdEZnp8CIOc+G1YXfo+8bNWn+W33naDxloN8rsWCptJgVlPG/2QmlIbXKAGA/XWfoXlV8tQTjqcOIPx78X8O7xUDADsXLQ5m/SnteyIieaKmISJR1DREJIqahohEUdMQkShqGiISpSi2MMgkq9A7Mby0e1Uqx+XpLdPP/OA5Lo3fXvnnNH/0hfDY3nydL8V/8xK+PYId56f3cp2KTnzl1vBt7+Onc0uPvEpzfzvH7yTXqe6Lw5fG+5bf8dtu5JeIXz64lh+7KXyq2vfu5LUV/DT39OQzNO9LtvDbrwtftu9tfLuND0xOBrOqUqO1I+mZhohEUdMQkShqGiISRU1DRKKoaYhIFDUNEYmipiEiUYpinoZZBmUJMucgET7/DADY9WYwGnx5Ly0tW36Q5l/8QTvNz74ivE3B3X/J52FgPz/v7kNDvH5wkMbJQ2SbgYYGfuxjfHsF1E3g9a+9zuu3/j4Y2Wy+9QMS/GHtO7bxPBGei+HbdtBaq+JLCnhnJ80r9vNtJwbI4/XoM3zezvSrXwxmpYleWjuSnmmISBQ1DRGJoqYhIlHoiz8z+08sd/c73t/hiMh4l+uNUO0dICLvQpuGu/+3sRqIiBSHUb2nYWYtZvaQmR02s0Nm9oCZ8Wt4ReSP0mjnafwMwM8B3JD9/HPZr12ej0Gdqi9ViVc6wsuvt9TydSMqfvndYFbWnOMV2GS+jUDfHr5c/oL54fqNXefS2vlT+DyOur4XaD5UO4fm9tPvB7PO1eEMABqPPUZzP8zntyQubM1RT+aBVNfT2r4K/nNnchx7b/dZwWzhheHtMAAAk3Ksh5Hjsdp713qar18XXqekgd8taPppeB0R7+DzjUYa7dmTJnf/mbunsv/dBaBp1EcRkT8ao20aHWb2OTNLZv/7HAC+7JSI/FEabdP4SwA3AjgI4ACA6wF8Pl+DEpHxa7TvaXwbwGp3PwYAZtYA4HsYbiYi8u/IaJ9pfOBkwwAAdz8KYGl+hiQi49lom0bCzP7tvdnsM42iuEJWRN5fo/3D/z6AZ83s/wJwDL+/8Z28jUpExq1RNQ13v8fMNgH4GAAD8Cfu/hqrMbN1AFYCOOzui7Nf+xaALwI4eVL4NnfnJ/wBVCQHsKA+vLZE9YEn+fivDK+/cPzhXbS2fN4FfHDg+3+0H+sLZkMZp7XbOviaFk1VV9N8YW+OuRSLwnMKGtObaC0q+XoZNoOvK4F6vheNTZoZDssqaW3lQBvNH2+/hOZHeo4Hs4UN/Ml5p51D87rjj9K88rMX0nzR5kPBbNY3+M9ll34qnP1jjt/3CKN+iZFtErRRnOIuAH8P4NTZMD9w9+9F3I6IjCN5uzTe3dcDOJqv2xeRwijEehpfNrOtZrZu5JurIlIcxrpp/BjAXADnY3iSWPACBzNbY2abzGxTR8ex0LeJyBgb06bh7ofcPe3uGQA/ARB8l9Hd17p7q7u3NjbqCYnIeDGmTcPMpo349DoAfFloERl38jZBy8zuA3ApgEYz2wvgbwFcambnY3iuRxuAW/J1fBHJj7w1DXdfdZov33kmt5XoPoKqZ9eFv2HZR2l9pmdjMCurKaO1/h6fjD35d8+EM4QzAPiHX/4ZzWfUhvdzAQB/8SWa91z8V8Gs5uiztBZVE3nufA6K73ie5jY5PIfEX36O1u4975s0P5svkYJkE9lHp5/PEXmijc9PuWHOIn7wwa00nnXnnwezodn87yDl5cEsA/53MJJWIxeRKGoaIhJFTUNEoqhpiEgUNQ0RiaKmISJRimMhneoaWCu57DfVT8s3/tfwUv+Lr2qmtft7l9C8fOpbNP/g9eFLpc8/ZzKtLUnwnt4zxM8dlm7kl/1XZ/53MPMafurQ5k2l+cv9V9J8yeSHaO7rnwof+1K+c8Zg2mg+qeIAzesGtwSzQ5X85zqnsYrm//1Jcsk/gK9ctpDmd21IB7OlZXwpheXTXw9mZhlaO5KeaYhIFDUNEYmipiEiUdQ0RCSKmoaIRFHTEJEoahoiEqU45mkAgJH+doyfd1/wkfAG99V/tYLWDpXwtZE/+zd82fh508JL/R/pHqC1LbV8/knTwO9ojhs+zfMKMhfj+GFee5jPT5nRdB6vT/I5Kqmr/zqYlXbupLVz7WF+7F5+2T7qwtsrTOn8DS3d2RfeJgAAblnB59ZUJPfRHKgNJ2X8z7nk+QeCmXWPfklNPdMQkShqGiISRU1DRKKoaYhIFDUNEYmipiEiUdQ0RCRKcczTyKSA7iPBeKjlIlre8B2yvkIlX4q/ItlJ8+c3nKB57WVzgtnH5vH1DyZXbqY5wNdueKTj4zS/ZEJPMDtczedRLDj8P2m+r5v/bPUb+HoaiePhsXlTHa21Sz5JcxziWz9gYvhn9xd+T0sv+sRcmvem+W6BR/rn0fzeb/yfYPa12/nPteurjwaz/nf443wkPdMQkShqGiISRU1DRKKoaYhIFDUNEYmipiEiUdQ0RCRK3uZpmNkMAPcAmAogA2Ctu//QzBoA3A9gNoA2ADe6O7+Y3xJAaUUwZusEAACWLg9n7XxdiMqKN2j+J5/m63G8fbArmL1+lK+nUVlyFs1faj+X5p+c3UbzrR3hOQHnN22jtYO1fO+RxS9/l+Z21U00T7wUXivkxZvuo7VnX89/Z8mGSpqX3RLeC8cuv47WDqb5fjE1JXydksO9LTS/8K9bg5nN53/OMy57OZiV7d9Pa0fK5zONFIBb3f1sABcC+JKZnQPg6wCedPf5AJ7Mfi4iRSJvTcPdD7j7luzHXQC2A2gGcA2Au7PfdjeAa/M1BhF5/43JexpmNhvAUgAbAExx9wPAcGMBwOcri8i4kvemYWY1AB4A8DV35xdqvLtujZltMrNN7R3h9wVEZGzltWmYWSmGG8a97v5g9suHzGxaNp8G4LTvDLn7WndvdffWpsbwYqoiMrby1jTMzADcCWC7u98xInoEwOrsx6sB5Fg6WkTGk3xeGr8cwM0AXjGzl7Jfuw3A7QD+ycy+AOAdADfkuiE/2omh+8KXUg+t+SatrzpELmdunJXr8NSy6TU0XzgpfArusa38NNeJAX7Z/vIWfsrWLEPzhsrSYNaT4kvt58qbF3+Y5jn/f9XbF4yWPnAzLe37yb/SvPSiGTT317YEs6eb/wutncevfMdgCT8l21LDt2e48bLwaXI/9gStrVwZPkWf+FX4dOyp8tY03P0ZAKGFLPhCDyIybmlGqIhEUdMQkShqGiISRU1DRKKoaYhIFDUNEYlSFFsYWOMklH5hdTAvHeSXt3tveBo62dwAAJCatiTHbfO+e9bE14PZh+fz+QLLprbRvHtoCs0H0nwm7dyq9cFsKJFjwkEuQ3wOSYfz+7Vx0eJg5s8+TWsrr+FLBiDBf2cnln4+mC0veZXWJjvbaL59gF9a397Lt2eYUxf+k7UG/nP3zLs6mGVqHqe1I+mZhohEUdMQkShqGiISRU1DRKKoaYhIFDUNEYmipiEiUYpingb6euCvbQzH5/0pLS+fOymYJY/tprW55kL0DfE1K6oy28O3PTiN1pYlemjeVLGD5vt6P0RzhJfTwI52PrZFDQdo7q+Ef18A0DiH71qBmvDvzD6VYwmWZI6H9QC/XyceeSqY9TZdRGv3GJ+HUVvKHy/PtfGlLatLw/M4jiQ+RmuXZO4PZokUv0/e9b2j/k4REahpiEgkNQ0RiaKmISJR1DREJIqahohEUdMQkSjFMU/DHUilgnFV4rSbtP2bNCqCWXvVJ2ht/xDfeyQnMt9gQhe/+zcdCq8pAQDnNvJ9U0qMr2kxlKkMZosb+fyVyvQ+mvcu+yzNc40t5eXBrCrZQWvReZDnpeHbBoCj9VcGs2SGj7u+gs+zeKrNaX75gvDjBQBSmfD/55urt9HaPX2rgtlg8ke0diQ90xCRKGoaIhJFTUNEoqhpiEgUNQ0RiaKmISJR1DREJEre5mmY2QwA9wCYCiADYK27/9DMvgXgiwDas996m7s/Rm8skQCqwnMKfOcLtDw5IbyHR9Mkvq6Dt4f3LQEAm76A5m91fzyYlSb4Of+SHPtzpDJ8vkFDOd8P5tjgrGB2pH8erd3WzvdsOW8yWawDwPSql2meTpOH5mAvre2q/iDNa3u20LzByeOpvJrWIsc8jpsW8r1oetJTae7k//Nlzh/LLRu+Ea7t4fNuRsrn5K4UgFvdfYuZ1QLYbGZPZLMfuPv38nhsEcmTvDUNdz8A4ED24y4z2w6gOV/HE5GxMSbvaZjZbABLAWzIfunLZrbVzNaZ2WlfO5jZGjPbZGab2o+NfikyEcmvvDcNM6sB8ACAr7n7CQA/BjAXwPkYfiby/dPVuftad29199am+hyvI0VkzOS1aZhZKYYbxr3u/iAAuPshd0+7ewbATwBckM8xiMj7K29Nw8wMwJ0Atrv7HSO+PnKZ6+sA8EvzRGRcyefZk+UAbgbwipm9lP3abQBWmdn5ABxAG4Bbct5SaRls6sxg7O+8yevnhE8PDiT5Uv0npvJtAO55jr/fcuu8dcGss+xmWru07Nc0P5y5jOZvH51M8+bazmDWUvY0rd3qy/htD/Gz6L6eb3FQuWJluHbjM7S29sIamqNuOo39tfDt20L+xLirdA7NK3GU5tUlfJmHoQx5qZ7gf862bHk4rPoVrR0pn2dPngFgp4n4o0lExjXNCBWRKGoaIhJFTUNEoqhpiEgUNQ0RiaKmISJRimILAy+pwFDdwmBecvAdfgMH3whG5Ql++XhjyxKa3/qhQ/zYpeH5JQtL2nhtN49zWVL5OM27EueGwyF+ifeSKWU093f4/Zq+5HqaZzLhS+vLPrSC1sLD210AADr442X/7K8Es+bkZlpbs4fPKMjM4vM89vbw+S8t3b8Ih1Pm01r0kzlFnua1I+iZhohEUdMQkShqGiISRU1DRKKoaYhIFDUNEYmipiEiUczdCz2GnMysHcDbI77UCKCjQMPJZbyObbyOC9DYztT7ObZZ7t40mm8siqZxKjPb5O6thR7H6YzXsY3XcQEa25kq1Nj08kREoqhpiEiUYm0aaws9AGK8jm28jgvQ2M5UQcZWlO9piEjhFOszDREpkKJqGmZ2pZntNLM3zOzrhR7PSGbWZmavmNlLZrapwGNZZ2aHzWzbiK81mNkTZvZ69t/TbodZoLF9y8z2Ze+7l8zsqgKNbYaZPWVm283sVTP7avbrBb3vyLgKcr8VzcsTM0sC2AXgcgB7AWwEsMrdXyvowLLMrA1Aq7sX/Jy+ma3A8Goc97j74uzXvgvgqLvfnm249e7+N+NkbN8C0O3u3xvr8ZwytmkAprn7FjOrBbAZwLUA/gIFvO/IuG5EAe63YnqmcQGAN9x9t7sPAvgFgGsKPKZxyd3XA3+wK881AO7Ofnw3hh90Yy4wtnHB3Q+4+5bsx10AtgNoRoHvOzKugiimptEMYM+Iz/eigHfcaTiAx81ss5mtKfRgTmOKux8Ahh+EAPj2a2Pvy2a2NfvypSAvnUYys9kAlgLYgHF0350yLqAA91sxNY3T7dY2nl5bLXf3DwL4FIAvZZ+Gy+j8GMBcAOcDOADg+4UcjJnVYHjj8q+5+4lCjmWk04yrIPdbMTWNvQBGbsraAmB/gcbyB9x9f/bfwwAewvDLqfHk0MnNt7P/8k1Dx5C7H3L3tLtnAPwEBbzvzKwUw3+Y97r7g9kvF/y+O924CnW/FVPT2AhgvpmdZWZlAD4D4JECjwkAYGbV2TeoYGbVAK4AsI1XjblHAKzOfrwawMMFHMu7nPyDzLoOBbrvzMwA3Algu7vfMSIq6H0XGleh7reiOXsCANlTSv8DQBLAOnf/ToGHBAAwszkYfnYBDK/w/vNCjs3M7gNwKYavgjwE4G8B/BLAPwGYCeAdADe4+5i/IRkY26UYfortANoA3HLyPYQxHttHADwN4BUAmeyXb8Pw+wcFu+/IuFahAPdbUTUNESm8Ynp5IiLjgJqGiERR0xCRKGoaIhJFTUNEoqhpiEgUNQ15T2yYHkf/juiXLdHMbHZ2bYd/ALAFwM3ZtUS2mdnfZb/nRjO7I/vxV81sd/bjuWb2TOFGL++VmoacqYUA7gHwaQDfBvAxDM9OXGZm1wJYD+CS7PdeAuCImTUDODm7UYqUmoacqbfd/XkAywD8q7u3u3sKwL0AVrj7QQA12WtyZgD4OYAVGG4gahpFTE1DzlRP9t/TLVlw0nMAPg9gJ4YbxSUALgLw+/wOTfJJTUPeqw0APmpmjdklGVcB+F02Ww/gP2f/fRHAZQAG3P14QUYq74uSQg9Aipu7HzCzbwB4CsPPOh5z95OXjj+N4Zcm6909bWZ7AOwo0FDlfaKrXEUkil6eiEgUNQ0RiaKmISJR1DREJIqahohEUdMQkShqGiISRU1DRKL8f0P++/jUGODvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1=new_lr2.w_G[0:784]\n",
    "plt.imshow(w1.reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.xlabel('row');\n",
    "plt.ylabel('col');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1=new_lr2.w_G[784:2*784]\n",
    "plt.imshow(w1.reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.xlabel('row');\n",
    "plt.ylabel('col');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0161138   2.05783374 -0.00750544]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "w1=new_lr2.w_G[2*784:-1]\n",
    "print(w1)\n",
    "print(w1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
