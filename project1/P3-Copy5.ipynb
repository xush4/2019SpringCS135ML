{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9f10b64b896a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_sneaker_vs_sandal/x_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_sneaker_vs_sandal/y_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\comp135_env\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   2045\u001b[0m         rows = list(\n\u001b[0;32m   2046\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[1;32m-> 2047\u001b[1;33m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[0;32m   2048\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m         rows = list(\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\comp135_env\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2045\u001b[0m         rows = list(\n\u001b[0;32m   2046\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[1;32m-> 2047\u001b[1;33m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[0;32m   2048\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m         rows = list(\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\comp135_env\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m         rows = list(\n\u001b[1;32m-> 2046\u001b[1;33m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0m\u001b[0;32m   2047\u001b[0m                   for (i, conv) in enumerate(converters)]))\n\u001b[0;32m   2048\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(0,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "#orig_lr2 = LRGD(alpha=10.0, step_size=0.1)\n",
    "#orig_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_Origin=np.asarray(orig_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "#tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "#acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "#print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "#new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "#new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr22 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr22.fit(x_te, y_te)\n",
    "y_hat_Origin=np.asarray(orig_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "new_lr2.fit(x_te, y_te)\n",
    "y_hat_New=np.asarray(new_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changed Alpha\n",
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "plt.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "plt.plot(fpr3te,tpr3te, label=\"MinMax Trans. Adding noise, counting bright blocks with 1 bright NB and with 2\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "plt.plot(fpr1Tte,tpr1Tte, label=\"No added Feature No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "plt.plot(fprTte,tprTte, label=\"MinMax Trans. Adding ave and counting bright blocks with 1 bright NB and with 2\")\n",
    "\n",
    "plt.xlim([-0.0, 1.0]);\n",
    "plt.ylim([-0.0, 1.0]);\n",
    "plt.legend();\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
