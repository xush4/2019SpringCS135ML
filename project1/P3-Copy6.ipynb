{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                FN_id.append(i)      \n",
    "    return TP, TN, FP, FN , FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(0,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "#orig_lr2 = LRGD(alpha=10.0, step_size=0.1)\n",
    "#orig_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_Origin=np.asarray(orig_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "#tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "#acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "#print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "#new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "#new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Rate Loaded\n",
      "TurnOnOnce Rate Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 1575 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028836  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.943019  avg_L1_norm_grad         0.056446  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         1.264147  avg_L1_norm_grad         0.094548  w[0]    0.001 bias    0.034\n",
      "iter    3/1000000  loss         2.039439  avg_L1_norm_grad         0.112421  w[0]   -0.000 bias   -0.008\n",
      "iter    4/1000000  loss         1.065893  avg_L1_norm_grad         0.084125  w[0]    0.002 bias    0.056\n",
      "iter    5/1000000  loss         1.508234  avg_L1_norm_grad         0.094314  w[0]    0.001 bias    0.019\n",
      "iter    6/1000000  loss         0.920049  avg_L1_norm_grad         0.071819  w[0]    0.003 bias    0.073\n",
      "iter    7/1000000  loss         1.147488  avg_L1_norm_grad         0.075679  w[0]    0.001 bias    0.043\n",
      "iter    8/1000000  loss         0.790012  avg_L1_norm_grad         0.059174  w[0]    0.003 bias    0.088\n",
      "iter    9/1000000  loss         0.909344  avg_L1_norm_grad         0.061301  w[0]    0.002 bias    0.063\n",
      "iter   10/1000000  loss         0.705690  avg_L1_norm_grad         0.051193  w[0]    0.004 bias    0.101\n",
      "iter   11/1000000  loss         0.779821  avg_L1_norm_grad         0.052677  w[0]    0.003 bias    0.080\n",
      "iter   12/1000000  loss         0.644068  avg_L1_norm_grad         0.045578  w[0]    0.004 bias    0.113\n",
      "iter   13/1000000  loss         0.694973  avg_L1_norm_grad         0.046720  w[0]    0.003 bias    0.094\n",
      "iter   14/1000000  loss         0.595706  avg_L1_norm_grad         0.041145  w[0]    0.004 bias    0.124\n",
      "iter   15/1000000  loss         0.631852  avg_L1_norm_grad         0.041996  w[0]    0.003 bias    0.107\n",
      "iter   16/1000000  loss         0.555682  avg_L1_norm_grad         0.037224  w[0]    0.004 bias    0.134\n",
      "iter   17/1000000  loss         0.580743  avg_L1_norm_grad         0.037781  w[0]    0.004 bias    0.119\n",
      "iter   18/1000000  loss         0.521367  avg_L1_norm_grad         0.033466  w[0]    0.005 bias    0.143\n",
      "iter   19/1000000  loss         0.537420  avg_L1_norm_grad         0.033728  w[0]    0.004 bias    0.130\n",
      "iter  100/1000000  loss         0.283333  avg_L1_norm_grad         0.001286  w[0]    0.001 bias    0.310\n",
      "iter  101/1000000  loss         0.282692  avg_L1_norm_grad         0.001278  w[0]    0.001 bias    0.311\n",
      "iter  200/1000000  loss         0.245061  avg_L1_norm_grad         0.000807  w[0]   -0.007 bias    0.391\n",
      "iter  201/1000000  loss         0.244829  avg_L1_norm_grad         0.000805  w[0]   -0.007 bias    0.392\n",
      "iter  300/1000000  loss         0.227916  avg_L1_norm_grad         0.000620  w[0]   -0.014 bias    0.435\n",
      "iter  301/1000000  loss         0.227789  avg_L1_norm_grad         0.000619  w[0]   -0.014 bias    0.436\n",
      "iter  400/1000000  loss         0.217624  avg_L1_norm_grad         0.000516  w[0]   -0.020 bias    0.464\n",
      "iter  401/1000000  loss         0.217541  avg_L1_norm_grad         0.000515  w[0]   -0.020 bias    0.464\n",
      "iter  500/1000000  loss         0.210551  avg_L1_norm_grad         0.000446  w[0]   -0.026 bias    0.483\n",
      "iter  501/1000000  loss         0.210491  avg_L1_norm_grad         0.000445  w[0]   -0.026 bias    0.483\n",
      "iter  600/1000000  loss         0.205306  avg_L1_norm_grad         0.000394  w[0]   -0.031 bias    0.498\n",
      "iter  601/1000000  loss         0.205261  avg_L1_norm_grad         0.000393  w[0]   -0.031 bias    0.498\n",
      "iter  700/1000000  loss         0.201229  avg_L1_norm_grad         0.000353  w[0]   -0.036 bias    0.509\n",
      "iter  701/1000000  loss         0.201193  avg_L1_norm_grad         0.000352  w[0]   -0.036 bias    0.509\n",
      "iter  800/1000000  loss         0.197956  avg_L1_norm_grad         0.000319  w[0]   -0.040 bias    0.519\n",
      "iter  801/1000000  loss         0.197926  avg_L1_norm_grad         0.000319  w[0]   -0.040 bias    0.519\n",
      "iter  900/1000000  loss         0.195269  avg_L1_norm_grad         0.000292  w[0]   -0.044 bias    0.526\n",
      "iter  901/1000000  loss         0.195244  avg_L1_norm_grad         0.000291  w[0]   -0.044 bias    0.526\n",
      "iter 1000/1000000  loss         0.193024  avg_L1_norm_grad         0.000268  w[0]   -0.047 bias    0.533\n",
      "iter 1001/1000000  loss         0.193004  avg_L1_norm_grad         0.000268  w[0]   -0.047 bias    0.533\n",
      "iter 1100/1000000  loss         0.191126  avg_L1_norm_grad         0.000247  w[0]   -0.050 bias    0.538\n",
      "iter 1101/1000000  loss         0.191108  avg_L1_norm_grad         0.000247  w[0]   -0.050 bias    0.538\n",
      "iter 1200/1000000  loss         0.189503  avg_L1_norm_grad         0.000229  w[0]   -0.053 bias    0.543\n",
      "iter 1201/1000000  loss         0.189488  avg_L1_norm_grad         0.000229  w[0]   -0.053 bias    0.543\n",
      "iter 1300/1000000  loss         0.188103  avg_L1_norm_grad         0.000213  w[0]   -0.056 bias    0.547\n",
      "iter 1301/1000000  loss         0.188090  avg_L1_norm_grad         0.000213  w[0]   -0.056 bias    0.547\n",
      "iter 1400/1000000  loss         0.186887  avg_L1_norm_grad         0.000199  w[0]   -0.058 bias    0.551\n",
      "iter 1401/1000000  loss         0.186876  avg_L1_norm_grad         0.000199  w[0]   -0.058 bias    0.551\n",
      "iter 1500/1000000  loss         0.185825  avg_L1_norm_grad         0.000186  w[0]   -0.060 bias    0.554\n",
      "iter 1501/1000000  loss         0.185815  avg_L1_norm_grad         0.000186  w[0]   -0.060 bias    0.554\n",
      "iter 1600/1000000  loss         0.184892  avg_L1_norm_grad         0.000174  w[0]   -0.062 bias    0.557\n",
      "iter 1601/1000000  loss         0.184883  avg_L1_norm_grad         0.000174  w[0]   -0.062 bias    0.557\n",
      "iter 1700/1000000  loss         0.184068  avg_L1_norm_grad         0.000163  w[0]   -0.064 bias    0.559\n",
      "iter 1701/1000000  loss         0.184060  avg_L1_norm_grad         0.000163  w[0]   -0.064 bias    0.559\n",
      "iter 1800/1000000  loss         0.183338  avg_L1_norm_grad         0.000154  w[0]   -0.065 bias    0.561\n",
      "iter 1801/1000000  loss         0.183331  avg_L1_norm_grad         0.000154  w[0]   -0.065 bias    0.561\n",
      "iter 1900/1000000  loss         0.182688  avg_L1_norm_grad         0.000145  w[0]   -0.067 bias    0.563\n",
      "iter 1901/1000000  loss         0.182682  avg_L1_norm_grad         0.000145  w[0]   -0.067 bias    0.563\n",
      "iter 2000/1000000  loss         0.182109  avg_L1_norm_grad         0.000137  w[0]   -0.068 bias    0.565\n",
      "iter 2001/1000000  loss         0.182103  avg_L1_norm_grad         0.000137  w[0]   -0.068 bias    0.565\n",
      "iter 2100/1000000  loss         0.181590  avg_L1_norm_grad         0.000129  w[0]   -0.070 bias    0.566\n",
      "iter 2101/1000000  loss         0.181585  avg_L1_norm_grad         0.000129  w[0]   -0.070 bias    0.566\n",
      "iter 2200/1000000  loss         0.181124  avg_L1_norm_grad         0.000122  w[0]   -0.071 bias    0.567\n",
      "iter 2201/1000000  loss         0.181120  avg_L1_norm_grad         0.000122  w[0]   -0.071 bias    0.568\n",
      "iter 2300/1000000  loss         0.180705  avg_L1_norm_grad         0.000116  w[0]   -0.072 bias    0.569\n",
      "iter 2301/1000000  loss         0.180701  avg_L1_norm_grad         0.000116  w[0]   -0.072 bias    0.569\n",
      "iter 2400/1000000  loss         0.180327  avg_L1_norm_grad         0.000110  w[0]   -0.073 bias    0.569\n",
      "iter 2401/1000000  loss         0.180323  avg_L1_norm_grad         0.000110  w[0]   -0.073 bias    0.569\n",
      "iter 2500/1000000  loss         0.179985  avg_L1_norm_grad         0.000104  w[0]   -0.074 bias    0.570\n",
      "iter 2501/1000000  loss         0.179981  avg_L1_norm_grad         0.000104  w[0]   -0.074 bias    0.570\n",
      "iter 2600/1000000  loss         0.179675  avg_L1_norm_grad         0.000099  w[0]   -0.075 bias    0.571\n",
      "iter 2601/1000000  loss         0.179672  avg_L1_norm_grad         0.000099  w[0]   -0.075 bias    0.571\n",
      "iter 2700/1000000  loss         0.179394  avg_L1_norm_grad         0.000094  w[0]   -0.075 bias    0.571\n",
      "iter 2701/1000000  loss         0.179391  avg_L1_norm_grad         0.000094  w[0]   -0.075 bias    0.571\n",
      "iter 2800/1000000  loss         0.179138  avg_L1_norm_grad         0.000089  w[0]   -0.076 bias    0.572\n",
      "iter 2801/1000000  loss         0.179136  avg_L1_norm_grad         0.000089  w[0]   -0.076 bias    0.572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.178906  avg_L1_norm_grad         0.000085  w[0]   -0.077 bias    0.572\n",
      "iter 2901/1000000  loss         0.178904  avg_L1_norm_grad         0.000085  w[0]   -0.077 bias    0.572\n",
      "iter 3000/1000000  loss         0.178694  avg_L1_norm_grad         0.000081  w[0]   -0.078 bias    0.572\n",
      "iter 3001/1000000  loss         0.178692  avg_L1_norm_grad         0.000081  w[0]   -0.078 bias    0.572\n",
      "iter 3100/1000000  loss         0.178500  avg_L1_norm_grad         0.000077  w[0]   -0.078 bias    0.572\n",
      "iter 3101/1000000  loss         0.178498  avg_L1_norm_grad         0.000077  w[0]   -0.078 bias    0.572\n",
      "iter 3200/1000000  loss         0.178323  avg_L1_norm_grad         0.000073  w[0]   -0.079 bias    0.572\n",
      "iter 3201/1000000  loss         0.178321  avg_L1_norm_grad         0.000073  w[0]   -0.079 bias    0.572\n",
      "iter 3300/1000000  loss         0.178160  avg_L1_norm_grad         0.000070  w[0]   -0.079 bias    0.572\n",
      "iter 3301/1000000  loss         0.178159  avg_L1_norm_grad         0.000070  w[0]   -0.079 bias    0.572\n",
      "iter 3400/1000000  loss         0.178011  avg_L1_norm_grad         0.000067  w[0]   -0.080 bias    0.572\n",
      "iter 3401/1000000  loss         0.178010  avg_L1_norm_grad         0.000067  w[0]   -0.080 bias    0.572\n",
      "iter 3500/1000000  loss         0.177875  avg_L1_norm_grad         0.000064  w[0]   -0.080 bias    0.572\n",
      "iter 3501/1000000  loss         0.177874  avg_L1_norm_grad         0.000064  w[0]   -0.080 bias    0.572\n",
      "iter 3600/1000000  loss         0.177749  avg_L1_norm_grad         0.000061  w[0]   -0.081 bias    0.571\n",
      "iter 3601/1000000  loss         0.177748  avg_L1_norm_grad         0.000061  w[0]   -0.081 bias    0.571\n",
      "iter 3700/1000000  loss         0.177634  avg_L1_norm_grad         0.000058  w[0]   -0.081 bias    0.571\n",
      "iter 3701/1000000  loss         0.177633  avg_L1_norm_grad         0.000058  w[0]   -0.081 bias    0.571\n",
      "iter 3800/1000000  loss         0.177527  avg_L1_norm_grad         0.000056  w[0]   -0.081 bias    0.571\n",
      "iter 3801/1000000  loss         0.177526  avg_L1_norm_grad         0.000056  w[0]   -0.081 bias    0.571\n",
      "iter 3900/1000000  loss         0.177429  avg_L1_norm_grad         0.000053  w[0]   -0.082 bias    0.570\n",
      "iter 3901/1000000  loss         0.177428  avg_L1_norm_grad         0.000053  w[0]   -0.082 bias    0.570\n",
      "iter 4000/1000000  loss         0.177339  avg_L1_norm_grad         0.000051  w[0]   -0.082 bias    0.570\n",
      "iter 4001/1000000  loss         0.177338  avg_L1_norm_grad         0.000051  w[0]   -0.082 bias    0.570\n",
      "iter 4100/1000000  loss         0.177255  avg_L1_norm_grad         0.000049  w[0]   -0.082 bias    0.569\n",
      "iter 4101/1000000  loss         0.177255  avg_L1_norm_grad         0.000049  w[0]   -0.082 bias    0.569\n",
      "iter 4200/1000000  loss         0.177178  avg_L1_norm_grad         0.000047  w[0]   -0.083 bias    0.569\n",
      "iter 4201/1000000  loss         0.177177  avg_L1_norm_grad         0.000047  w[0]   -0.083 bias    0.569\n",
      "iter 4300/1000000  loss         0.177107  avg_L1_norm_grad         0.000045  w[0]   -0.083 bias    0.569\n",
      "iter 4301/1000000  loss         0.177106  avg_L1_norm_grad         0.000045  w[0]   -0.083 bias    0.569\n",
      "iter 4400/1000000  loss         0.177041  avg_L1_norm_grad         0.000043  w[0]   -0.083 bias    0.568\n",
      "iter 4401/1000000  loss         0.177040  avg_L1_norm_grad         0.000043  w[0]   -0.083 bias    0.568\n",
      "iter 4500/1000000  loss         0.176979  avg_L1_norm_grad         0.000041  w[0]   -0.083 bias    0.568\n",
      "iter 4501/1000000  loss         0.176979  avg_L1_norm_grad         0.000041  w[0]   -0.083 bias    0.568\n",
      "iter 4600/1000000  loss         0.176922  avg_L1_norm_grad         0.000040  w[0]   -0.084 bias    0.567\n",
      "iter 4601/1000000  loss         0.176922  avg_L1_norm_grad         0.000040  w[0]   -0.084 bias    0.567\n",
      "iter 4700/1000000  loss         0.176870  avg_L1_norm_grad         0.000038  w[0]   -0.084 bias    0.566\n",
      "iter 4701/1000000  loss         0.176869  avg_L1_norm_grad         0.000038  w[0]   -0.084 bias    0.566\n",
      "iter 4800/1000000  loss         0.176821  avg_L1_norm_grad         0.000037  w[0]   -0.084 bias    0.566\n",
      "iter 4801/1000000  loss         0.176820  avg_L1_norm_grad         0.000037  w[0]   -0.084 bias    0.566\n",
      "iter 4900/1000000  loss         0.176775  avg_L1_norm_grad         0.000035  w[0]   -0.084 bias    0.565\n",
      "iter 4901/1000000  loss         0.176775  avg_L1_norm_grad         0.000035  w[0]   -0.084 bias    0.565\n",
      "iter 5000/1000000  loss         0.176733  avg_L1_norm_grad         0.000034  w[0]   -0.084 bias    0.565\n",
      "iter 5001/1000000  loss         0.176733  avg_L1_norm_grad         0.000034  w[0]   -0.084 bias    0.565\n",
      "iter 5100/1000000  loss         0.176694  avg_L1_norm_grad         0.000033  w[0]   -0.085 bias    0.564\n",
      "iter 5101/1000000  loss         0.176693  avg_L1_norm_grad         0.000033  w[0]   -0.085 bias    0.564\n",
      "iter 5200/1000000  loss         0.176657  avg_L1_norm_grad         0.000031  w[0]   -0.085 bias    0.564\n",
      "iter 5201/1000000  loss         0.176657  avg_L1_norm_grad         0.000031  w[0]   -0.085 bias    0.564\n",
      "iter 5300/1000000  loss         0.176623  avg_L1_norm_grad         0.000030  w[0]   -0.085 bias    0.563\n",
      "iter 5301/1000000  loss         0.176623  avg_L1_norm_grad         0.000030  w[0]   -0.085 bias    0.563\n",
      "iter 5400/1000000  loss         0.176591  avg_L1_norm_grad         0.000029  w[0]   -0.085 bias    0.562\n",
      "iter 5401/1000000  loss         0.176591  avg_L1_norm_grad         0.000029  w[0]   -0.085 bias    0.562\n",
      "iter 5500/1000000  loss         0.176561  avg_L1_norm_grad         0.000028  w[0]   -0.085 bias    0.562\n",
      "iter 5501/1000000  loss         0.176561  avg_L1_norm_grad         0.000028  w[0]   -0.085 bias    0.562\n",
      "iter 5600/1000000  loss         0.176534  avg_L1_norm_grad         0.000027  w[0]   -0.085 bias    0.561\n",
      "iter 5601/1000000  loss         0.176533  avg_L1_norm_grad         0.000027  w[0]   -0.085 bias    0.561\n",
      "iter 5700/1000000  loss         0.176508  avg_L1_norm_grad         0.000026  w[0]   -0.085 bias    0.561\n",
      "iter 5701/1000000  loss         0.176508  avg_L1_norm_grad         0.000026  w[0]   -0.085 bias    0.561\n",
      "iter 5800/1000000  loss         0.176484  avg_L1_norm_grad         0.000025  w[0]   -0.086 bias    0.560\n",
      "iter 5801/1000000  loss         0.176483  avg_L1_norm_grad         0.000025  w[0]   -0.086 bias    0.560\n",
      "iter 5900/1000000  loss         0.176461  avg_L1_norm_grad         0.000024  w[0]   -0.086 bias    0.560\n",
      "iter 5901/1000000  loss         0.176461  avg_L1_norm_grad         0.000024  w[0]   -0.086 bias    0.560\n",
      "iter 6000/1000000  loss         0.176440  avg_L1_norm_grad         0.000023  w[0]   -0.086 bias    0.559\n",
      "iter 6001/1000000  loss         0.176440  avg_L1_norm_grad         0.000023  w[0]   -0.086 bias    0.559\n",
      "iter 6100/1000000  loss         0.176420  avg_L1_norm_grad         0.000022  w[0]   -0.086 bias    0.559\n",
      "iter 6101/1000000  loss         0.176420  avg_L1_norm_grad         0.000022  w[0]   -0.086 bias    0.559\n",
      "iter 6200/1000000  loss         0.176402  avg_L1_norm_grad         0.000021  w[0]   -0.086 bias    0.558\n",
      "iter 6201/1000000  loss         0.176401  avg_L1_norm_grad         0.000021  w[0]   -0.086 bias    0.558\n",
      "iter 6300/1000000  loss         0.176384  avg_L1_norm_grad         0.000020  w[0]   -0.086 bias    0.557\n",
      "iter 6301/1000000  loss         0.176384  avg_L1_norm_grad         0.000020  w[0]   -0.086 bias    0.557\n",
      "iter 6400/1000000  loss         0.176368  avg_L1_norm_grad         0.000020  w[0]   -0.086 bias    0.557\n",
      "iter 6401/1000000  loss         0.176368  avg_L1_norm_grad         0.000020  w[0]   -0.086 bias    0.557\n",
      "iter 6500/1000000  loss         0.176353  avg_L1_norm_grad         0.000019  w[0]   -0.086 bias    0.556\n",
      "iter 6501/1000000  loss         0.176353  avg_L1_norm_grad         0.000019  w[0]   -0.086 bias    0.556\n",
      "iter 6600/1000000  loss         0.176339  avg_L1_norm_grad         0.000018  w[0]   -0.086 bias    0.556\n",
      "iter 6601/1000000  loss         0.176338  avg_L1_norm_grad         0.000018  w[0]   -0.086 bias    0.556\n",
      "iter 6700/1000000  loss         0.176325  avg_L1_norm_grad         0.000018  w[0]   -0.086 bias    0.555\n",
      "iter 6701/1000000  loss         0.176325  avg_L1_norm_grad         0.000018  w[0]   -0.086 bias    0.555\n",
      "iter 6800/1000000  loss         0.176313  avg_L1_norm_grad         0.000017  w[0]   -0.086 bias    0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6801/1000000  loss         0.176312  avg_L1_norm_grad         0.000017  w[0]   -0.086 bias    0.555\n",
      "iter 6900/1000000  loss         0.176301  avg_L1_norm_grad         0.000016  w[0]   -0.087 bias    0.554\n",
      "iter 6901/1000000  loss         0.176301  avg_L1_norm_grad         0.000016  w[0]   -0.087 bias    0.554\n",
      "iter 7000/1000000  loss         0.176290  avg_L1_norm_grad         0.000016  w[0]   -0.087 bias    0.554\n",
      "iter 7001/1000000  loss         0.176290  avg_L1_norm_grad         0.000016  w[0]   -0.087 bias    0.554\n",
      "iter 7100/1000000  loss         0.176279  avg_L1_norm_grad         0.000015  w[0]   -0.087 bias    0.553\n",
      "iter 7101/1000000  loss         0.176279  avg_L1_norm_grad         0.000015  w[0]   -0.087 bias    0.553\n",
      "iter 7200/1000000  loss         0.176269  avg_L1_norm_grad         0.000015  w[0]   -0.087 bias    0.553\n",
      "iter 7201/1000000  loss         0.176269  avg_L1_norm_grad         0.000015  w[0]   -0.087 bias    0.553\n",
      "iter 7300/1000000  loss         0.176260  avg_L1_norm_grad         0.000014  w[0]   -0.087 bias    0.553\n",
      "iter 7301/1000000  loss         0.176260  avg_L1_norm_grad         0.000014  w[0]   -0.087 bias    0.553\n",
      "iter 7400/1000000  loss         0.176252  avg_L1_norm_grad         0.000014  w[0]   -0.087 bias    0.552\n",
      "iter 7401/1000000  loss         0.176251  avg_L1_norm_grad         0.000014  w[0]   -0.087 bias    0.552\n",
      "iter 7500/1000000  loss         0.176243  avg_L1_norm_grad         0.000013  w[0]   -0.087 bias    0.552\n",
      "iter 7501/1000000  loss         0.176243  avg_L1_norm_grad         0.000013  w[0]   -0.087 bias    0.552\n",
      "iter 7600/1000000  loss         0.176236  avg_L1_norm_grad         0.000013  w[0]   -0.087 bias    0.551\n",
      "iter 7601/1000000  loss         0.176236  avg_L1_norm_grad         0.000013  w[0]   -0.087 bias    0.551\n",
      "iter 7700/1000000  loss         0.176228  avg_L1_norm_grad         0.000012  w[0]   -0.087 bias    0.551\n",
      "iter 7701/1000000  loss         0.176228  avg_L1_norm_grad         0.000012  w[0]   -0.087 bias    0.551\n",
      "iter 7800/1000000  loss         0.176222  avg_L1_norm_grad         0.000012  w[0]   -0.087 bias    0.550\n",
      "iter 7801/1000000  loss         0.176221  avg_L1_norm_grad         0.000012  w[0]   -0.087 bias    0.550\n",
      "iter 7900/1000000  loss         0.176215  avg_L1_norm_grad         0.000011  w[0]   -0.087 bias    0.550\n",
      "iter 7901/1000000  loss         0.176215  avg_L1_norm_grad         0.000011  w[0]   -0.087 bias    0.550\n",
      "iter 8000/1000000  loss         0.176209  avg_L1_norm_grad         0.000011  w[0]   -0.087 bias    0.550\n",
      "iter 8001/1000000  loss         0.176209  avg_L1_norm_grad         0.000011  w[0]   -0.087 bias    0.550\n",
      "iter 8100/1000000  loss         0.176203  avg_L1_norm_grad         0.000011  w[0]   -0.087 bias    0.549\n",
      "iter 8101/1000000  loss         0.176203  avg_L1_norm_grad         0.000011  w[0]   -0.087 bias    0.549\n",
      "iter 8200/1000000  loss         0.176198  avg_L1_norm_grad         0.000010  w[0]   -0.087 bias    0.549\n",
      "iter 8201/1000000  loss         0.176198  avg_L1_norm_grad         0.000010  w[0]   -0.087 bias    0.549\n",
      "iter 8300/1000000  loss         0.176193  avg_L1_norm_grad         0.000010  w[0]   -0.087 bias    0.549\n",
      "iter 8301/1000000  loss         0.176193  avg_L1_norm_grad         0.000010  w[0]   -0.087 bias    0.549\n",
      "iter 8400/1000000  loss         0.176188  avg_L1_norm_grad         0.000010  w[0]   -0.087 bias    0.548\n",
      "iter 8401/1000000  loss         0.176188  avg_L1_norm_grad         0.000010  w[0]   -0.087 bias    0.548\n",
      "iter 8500/1000000  loss         0.176183  avg_L1_norm_grad         0.000009  w[0]   -0.087 bias    0.548\n",
      "iter 8501/1000000  loss         0.176183  avg_L1_norm_grad         0.000009  w[0]   -0.087 bias    0.548\n",
      "iter 8600/1000000  loss         0.176179  avg_L1_norm_grad         0.000009  w[0]   -0.087 bias    0.548\n",
      "iter 8601/1000000  loss         0.176179  avg_L1_norm_grad         0.000009  w[0]   -0.087 bias    0.548\n",
      "iter 8700/1000000  loss         0.176175  avg_L1_norm_grad         0.000009  w[0]   -0.087 bias    0.547\n",
      "iter 8701/1000000  loss         0.176175  avg_L1_norm_grad         0.000009  w[0]   -0.087 bias    0.547\n",
      "iter 8800/1000000  loss         0.176171  avg_L1_norm_grad         0.000008  w[0]   -0.087 bias    0.547\n",
      "iter 8801/1000000  loss         0.176171  avg_L1_norm_grad         0.000008  w[0]   -0.087 bias    0.547\n",
      "iter 8900/1000000  loss         0.176167  avg_L1_norm_grad         0.000008  w[0]   -0.087 bias    0.547\n",
      "iter 8901/1000000  loss         0.176167  avg_L1_norm_grad         0.000008  w[0]   -0.087 bias    0.547\n",
      "iter 9000/1000000  loss         0.176164  avg_L1_norm_grad         0.000008  w[0]   -0.087 bias    0.546\n",
      "iter 9001/1000000  loss         0.176164  avg_L1_norm_grad         0.000008  w[0]   -0.087 bias    0.546\n",
      "iter 9100/1000000  loss         0.176161  avg_L1_norm_grad         0.000007  w[0]   -0.087 bias    0.546\n",
      "iter 9101/1000000  loss         0.176161  avg_L1_norm_grad         0.000007  w[0]   -0.087 bias    0.546\n",
      "iter 9200/1000000  loss         0.176157  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.546\n",
      "iter 9201/1000000  loss         0.176157  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.546\n",
      "iter 9300/1000000  loss         0.176154  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.546\n",
      "iter 9301/1000000  loss         0.176154  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.546\n",
      "iter 9400/1000000  loss         0.176152  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.545\n",
      "iter 9401/1000000  loss         0.176152  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.545\n",
      "iter 9500/1000000  loss         0.176149  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.545\n",
      "iter 9501/1000000  loss         0.176149  avg_L1_norm_grad         0.000007  w[0]   -0.088 bias    0.545\n",
      "iter 9600/1000000  loss         0.176147  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.545\n",
      "iter 9601/1000000  loss         0.176147  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.545\n",
      "iter 9700/1000000  loss         0.176144  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.545\n",
      "iter 9701/1000000  loss         0.176144  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.545\n",
      "iter 9800/1000000  loss         0.176142  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.544\n",
      "iter 9801/1000000  loss         0.176142  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.544\n",
      "iter 9900/1000000  loss         0.176140  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.544\n",
      "iter 9901/1000000  loss         0.176140  avg_L1_norm_grad         0.000006  w[0]   -0.088 bias    0.544\n",
      "iter 10000/1000000  loss         0.176138  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.544\n",
      "iter 10001/1000000  loss         0.176138  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.544\n",
      "iter 10100/1000000  loss         0.176136  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.544\n",
      "iter 10101/1000000  loss         0.176136  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.544\n",
      "iter 10200/1000000  loss         0.176134  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.544\n",
      "iter 10201/1000000  loss         0.176134  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.544\n",
      "iter 10300/1000000  loss         0.176132  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.543\n",
      "iter 10301/1000000  loss         0.176132  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.543\n",
      "iter 10400/1000000  loss         0.176131  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.543\n",
      "iter 10401/1000000  loss         0.176131  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.543\n",
      "iter 10500/1000000  loss         0.176129  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.543\n",
      "iter 10501/1000000  loss         0.176129  avg_L1_norm_grad         0.000005  w[0]   -0.088 bias    0.543\n",
      "iter 10600/1000000  loss         0.176127  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.543\n",
      "iter 10601/1000000  loss         0.176127  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.543\n",
      "iter 10700/1000000  loss         0.176126  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10701/1000000  loss         0.176126  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.543\n",
      "iter 10800/1000000  loss         0.176125  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.543\n",
      "iter 10801/1000000  loss         0.176125  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.543\n",
      "iter 10900/1000000  loss         0.176123  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 10901/1000000  loss         0.176123  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11000/1000000  loss         0.176122  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11001/1000000  loss         0.176122  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11100/1000000  loss         0.176121  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11101/1000000  loss         0.176121  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11200/1000000  loss         0.176120  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11201/1000000  loss         0.176120  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11300/1000000  loss         0.176119  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11301/1000000  loss         0.176119  avg_L1_norm_grad         0.000004  w[0]   -0.088 bias    0.542\n",
      "iter 11400/1000000  loss         0.176118  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11401/1000000  loss         0.176118  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11500/1000000  loss         0.176117  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11501/1000000  loss         0.176117  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11600/1000000  loss         0.176116  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11601/1000000  loss         0.176116  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11700/1000000  loss         0.176115  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11701/1000000  loss         0.176115  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.542\n",
      "iter 11800/1000000  loss         0.176114  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 11801/1000000  loss         0.176114  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 11900/1000000  loss         0.176113  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 11901/1000000  loss         0.176113  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12000/1000000  loss         0.176113  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12001/1000000  loss         0.176113  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12100/1000000  loss         0.176112  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12101/1000000  loss         0.176112  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12200/1000000  loss         0.176111  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12201/1000000  loss         0.176111  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12300/1000000  loss         0.176111  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12301/1000000  loss         0.176111  avg_L1_norm_grad         0.000003  w[0]   -0.088 bias    0.541\n",
      "iter 12400/1000000  loss         0.176110  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12401/1000000  loss         0.176110  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12500/1000000  loss         0.176109  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12501/1000000  loss         0.176109  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12600/1000000  loss         0.176109  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12601/1000000  loss         0.176109  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12700/1000000  loss         0.176108  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12701/1000000  loss         0.176108  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12800/1000000  loss         0.176108  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12801/1000000  loss         0.176108  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12900/1000000  loss         0.176107  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 12901/1000000  loss         0.176107  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13000/1000000  loss         0.176107  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13001/1000000  loss         0.176107  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13100/1000000  loss         0.176106  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13101/1000000  loss         0.176106  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13200/1000000  loss         0.176106  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13201/1000000  loss         0.176106  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13300/1000000  loss         0.176105  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13301/1000000  loss         0.176105  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13400/1000000  loss         0.176105  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13401/1000000  loss         0.176105  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13500/1000000  loss         0.176105  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13501/1000000  loss         0.176105  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.541\n",
      "iter 13600/1000000  loss         0.176104  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13601/1000000  loss         0.176104  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13700/1000000  loss         0.176104  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13701/1000000  loss         0.176104  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13800/1000000  loss         0.176104  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13801/1000000  loss         0.176104  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13900/1000000  loss         0.176103  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 13901/1000000  loss         0.176103  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 14000/1000000  loss         0.176103  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 14001/1000000  loss         0.176103  avg_L1_norm_grad         0.000002  w[0]   -0.088 bias    0.540\n",
      "iter 14100/1000000  loss         0.176103  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14101/1000000  loss         0.176103  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14200/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14201/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14300/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14301/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14400/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14401/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14500/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "iter 14501/1000000  loss         0.176102  avg_L1_norm_grad         0.000001  w[0]   -0.088 bias    0.540\n",
      "Done. Converged after 14545 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028764  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.917699  avg_L1_norm_grad         0.029631  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.856183  avg_L1_norm_grad         0.020332  w[0]    0.001 bias    0.020\n",
      "iter    3/1000000  loss         0.811599  avg_L1_norm_grad         0.020819  w[0]    0.001 bias    0.022\n",
      "iter    4/1000000  loss         0.776603  avg_L1_norm_grad         0.014885  w[0]    0.002 bias    0.038\n",
      "iter    5/1000000  loss         0.749295  avg_L1_norm_grad         0.015086  w[0]    0.002 bias    0.043\n",
      "iter    6/1000000  loss         0.726856  avg_L1_norm_grad         0.012120  w[0]    0.002 bias    0.055\n",
      "iter    7/1000000  loss         0.707978  avg_L1_norm_grad         0.012026  w[0]    0.002 bias    0.062\n",
      "iter    8/1000000  loss         0.691562  avg_L1_norm_grad         0.010651  w[0]    0.003 bias    0.071\n",
      "iter    9/1000000  loss         0.676965  avg_L1_norm_grad         0.010341  w[0]    0.003 bias    0.079\n",
      "iter   10/1000000  loss         0.663757  avg_L1_norm_grad         0.009665  w[0]    0.003 bias    0.087\n",
      "iter   11/1000000  loss         0.651654  avg_L1_norm_grad         0.009312  w[0]    0.003 bias    0.095\n",
      "iter   12/1000000  loss         0.640458  avg_L1_norm_grad         0.008897  w[0]    0.004 bias    0.103\n",
      "iter   13/1000000  loss         0.630027  avg_L1_norm_grad         0.008581  w[0]    0.004 bias    0.111\n",
      "iter   14/1000000  loss         0.620251  avg_L1_norm_grad         0.008277  w[0]    0.004 bias    0.118\n",
      "iter   15/1000000  loss         0.611047  avg_L1_norm_grad         0.008013  w[0]    0.004 bias    0.126\n",
      "iter   16/1000000  loss         0.602349  avg_L1_norm_grad         0.007767  w[0]    0.005 bias    0.133\n",
      "iter   17/1000000  loss         0.594103  avg_L1_norm_grad         0.007542  w[0]    0.005 bias    0.140\n",
      "iter   18/1000000  loss         0.586265  avg_L1_norm_grad         0.007334  w[0]    0.005 bias    0.147\n",
      "iter   19/1000000  loss         0.578798  avg_L1_norm_grad         0.007140  w[0]    0.005 bias    0.154\n",
      "iter  100/1000000  loss         0.365424  avg_L1_norm_grad         0.002497  w[0]    0.010 bias    0.503\n",
      "iter  101/1000000  loss         0.364438  avg_L1_norm_grad         0.002478  w[0]    0.010 bias    0.506\n",
      "iter  200/1000000  loss         0.307171  avg_L1_norm_grad         0.001454  w[0]    0.007 bias    0.727\n",
      "iter  201/1000000  loss         0.306824  avg_L1_norm_grad         0.001448  w[0]    0.007 bias    0.729\n",
      "iter  300/1000000  loss         0.282000  avg_L1_norm_grad         0.001053  w[0]    0.003 bias    0.875\n",
      "iter  301/1000000  loss         0.281817  avg_L1_norm_grad         0.001051  w[0]    0.003 bias    0.876\n",
      "iter  400/1000000  loss         0.267547  avg_L1_norm_grad         0.000839  w[0]   -0.002 bias    0.985\n",
      "iter  401/1000000  loss         0.267433  avg_L1_norm_grad         0.000838  w[0]   -0.002 bias    0.985\n",
      "iter  500/1000000  loss         0.258053  avg_L1_norm_grad         0.000704  w[0]   -0.007 bias    1.072\n",
      "iter  501/1000000  loss         0.257974  avg_L1_norm_grad         0.000703  w[0]   -0.007 bias    1.072\n",
      "iter  600/1000000  loss         0.251294  avg_L1_norm_grad         0.000610  w[0]   -0.012 bias    1.144\n",
      "iter  601/1000000  loss         0.251236  avg_L1_norm_grad         0.000609  w[0]   -0.012 bias    1.144\n",
      "iter  700/1000000  loss         0.246215  avg_L1_norm_grad         0.000539  w[0]   -0.017 bias    1.205\n",
      "iter  701/1000000  loss         0.246171  avg_L1_norm_grad         0.000539  w[0]   -0.017 bias    1.205\n",
      "iter  800/1000000  loss         0.242250  avg_L1_norm_grad         0.000484  w[0]   -0.021 bias    1.258\n",
      "iter  801/1000000  loss         0.242215  avg_L1_norm_grad         0.000484  w[0]   -0.021 bias    1.258\n",
      "iter  900/1000000  loss         0.239064  avg_L1_norm_grad         0.000440  w[0]   -0.026 bias    1.304\n",
      "iter  901/1000000  loss         0.239035  avg_L1_norm_grad         0.000440  w[0]   -0.026 bias    1.305\n",
      "iter 1000/1000000  loss         0.236447  avg_L1_norm_grad         0.000403  w[0]   -0.030 bias    1.346\n",
      "iter 1001/1000000  loss         0.236423  avg_L1_norm_grad         0.000403  w[0]   -0.030 bias    1.347\n",
      "iter 1100/1000000  loss         0.234260  avg_L1_norm_grad         0.000372  w[0]   -0.033 bias    1.384\n",
      "iter 1101/1000000  loss         0.234240  avg_L1_norm_grad         0.000372  w[0]   -0.033 bias    1.384\n",
      "iter 1200/1000000  loss         0.232407  avg_L1_norm_grad         0.000345  w[0]   -0.037 bias    1.418\n",
      "iter 1201/1000000  loss         0.232390  avg_L1_norm_grad         0.000345  w[0]   -0.037 bias    1.418\n",
      "iter 1300/1000000  loss         0.230820  avg_L1_norm_grad         0.000321  w[0]   -0.040 bias    1.449\n",
      "iter 1301/1000000  loss         0.230805  avg_L1_norm_grad         0.000321  w[0]   -0.040 bias    1.450\n",
      "iter 1400/1000000  loss         0.229447  avg_L1_norm_grad         0.000301  w[0]   -0.044 bias    1.478\n",
      "iter 1401/1000000  loss         0.229434  avg_L1_norm_grad         0.000300  w[0]   -0.044 bias    1.479\n",
      "iter 1500/1000000  loss         0.228251  avg_L1_norm_grad         0.000282  w[0]   -0.047 bias    1.505\n",
      "iter 1501/1000000  loss         0.228240  avg_L1_norm_grad         0.000282  w[0]   -0.047 bias    1.505\n",
      "iter 1600/1000000  loss         0.227202  avg_L1_norm_grad         0.000265  w[0]   -0.049 bias    1.530\n",
      "iter 1601/1000000  loss         0.227192  avg_L1_norm_grad         0.000265  w[0]   -0.049 bias    1.530\n",
      "iter 1700/1000000  loss         0.226277  avg_L1_norm_grad         0.000250  w[0]   -0.052 bias    1.553\n",
      "iter 1701/1000000  loss         0.226268  avg_L1_norm_grad         0.000250  w[0]   -0.052 bias    1.554\n",
      "iter 1800/1000000  loss         0.225458  avg_L1_norm_grad         0.000236  w[0]   -0.055 bias    1.575\n",
      "iter 1801/1000000  loss         0.225450  avg_L1_norm_grad         0.000236  w[0]   -0.055 bias    1.576\n",
      "iter 1900/1000000  loss         0.224729  avg_L1_norm_grad         0.000223  w[0]   -0.057 bias    1.596\n",
      "iter 1901/1000000  loss         0.224722  avg_L1_norm_grad         0.000223  w[0]   -0.057 bias    1.596\n",
      "iter 2000/1000000  loss         0.224078  avg_L1_norm_grad         0.000211  w[0]   -0.059 bias    1.615\n",
      "iter 2001/1000000  loss         0.224072  avg_L1_norm_grad         0.000211  w[0]   -0.059 bias    1.616\n",
      "iter 2100/1000000  loss         0.223495  avg_L1_norm_grad         0.000201  w[0]   -0.061 bias    1.634\n",
      "iter 2101/1000000  loss         0.223489  avg_L1_norm_grad         0.000200  w[0]   -0.061 bias    1.634\n",
      "iter 2200/1000000  loss         0.222971  avg_L1_norm_grad         0.000190  w[0]   -0.063 bias    1.651\n",
      "iter 2201/1000000  loss         0.222966  avg_L1_norm_grad         0.000190  w[0]   -0.063 bias    1.651\n",
      "iter 2300/1000000  loss         0.222500  avg_L1_norm_grad         0.000181  w[0]   -0.065 bias    1.668\n",
      "iter 2301/1000000  loss         0.222495  avg_L1_norm_grad         0.000181  w[0]   -0.065 bias    1.668\n",
      "iter 2400/1000000  loss         0.222074  avg_L1_norm_grad         0.000172  w[0]   -0.067 bias    1.683\n",
      "iter 2401/1000000  loss         0.222070  avg_L1_norm_grad         0.000172  w[0]   -0.067 bias    1.683\n",
      "iter 2500/1000000  loss         0.221689  avg_L1_norm_grad         0.000164  w[0]   -0.069 bias    1.698\n",
      "iter 2501/1000000  loss         0.221686  avg_L1_norm_grad         0.000163  w[0]   -0.069 bias    1.698\n",
      "iter 2600/1000000  loss         0.221340  avg_L1_norm_grad         0.000156  w[0]   -0.070 bias    1.712\n",
      "iter 2601/1000000  loss         0.221337  avg_L1_norm_grad         0.000156  w[0]   -0.070 bias    1.713\n",
      "iter 2700/1000000  loss         0.221024  avg_L1_norm_grad         0.000149  w[0]   -0.072 bias    1.726\n",
      "iter 2701/1000000  loss         0.221021  avg_L1_norm_grad         0.000148  w[0]   -0.072 bias    1.726\n",
      "iter 2800/1000000  loss         0.220736  avg_L1_norm_grad         0.000142  w[0]   -0.073 bias    1.739\n",
      "iter 2801/1000000  loss         0.220733  avg_L1_norm_grad         0.000142  w[0]   -0.073 bias    1.739\n",
      "iter 2900/1000000  loss         0.220474  avg_L1_norm_grad         0.000135  w[0]   -0.074 bias    1.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.220472  avg_L1_norm_grad         0.000135  w[0]   -0.074 bias    1.751\n",
      "iter 3000/1000000  loss         0.220235  avg_L1_norm_grad         0.000129  w[0]   -0.076 bias    1.763\n",
      "iter 3001/1000000  loss         0.220233  avg_L1_norm_grad         0.000129  w[0]   -0.076 bias    1.763\n",
      "iter 3100/1000000  loss         0.220017  avg_L1_norm_grad         0.000123  w[0]   -0.077 bias    1.774\n",
      "iter 3101/1000000  loss         0.220015  avg_L1_norm_grad         0.000123  w[0]   -0.077 bias    1.774\n",
      "iter 3200/1000000  loss         0.219818  avg_L1_norm_grad         0.000118  w[0]   -0.078 bias    1.785\n",
      "iter 3201/1000000  loss         0.219816  avg_L1_norm_grad         0.000118  w[0]   -0.078 bias    1.785\n",
      "iter 3300/1000000  loss         0.219636  avg_L1_norm_grad         0.000113  w[0]   -0.079 bias    1.795\n",
      "iter 3301/1000000  loss         0.219634  avg_L1_norm_grad         0.000113  w[0]   -0.079 bias    1.795\n",
      "iter 3400/1000000  loss         0.219469  avg_L1_norm_grad         0.000108  w[0]   -0.080 bias    1.805\n",
      "iter 3401/1000000  loss         0.219467  avg_L1_norm_grad         0.000108  w[0]   -0.080 bias    1.805\n",
      "iter 3500/1000000  loss         0.219316  avg_L1_norm_grad         0.000104  w[0]   -0.081 bias    1.815\n",
      "iter 3501/1000000  loss         0.219314  avg_L1_norm_grad         0.000104  w[0]   -0.081 bias    1.815\n",
      "iter 3600/1000000  loss         0.219175  avg_L1_norm_grad         0.000099  w[0]   -0.082 bias    1.824\n",
      "iter 3601/1000000  loss         0.219174  avg_L1_norm_grad         0.000099  w[0]   -0.082 bias    1.824\n",
      "iter 3700/1000000  loss         0.219046  avg_L1_norm_grad         0.000095  w[0]   -0.082 bias    1.832\n",
      "iter 3701/1000000  loss         0.219045  avg_L1_norm_grad         0.000095  w[0]   -0.082 bias    1.832\n",
      "iter 3800/1000000  loss         0.218928  avg_L1_norm_grad         0.000091  w[0]   -0.083 bias    1.841\n",
      "iter 3801/1000000  loss         0.218926  avg_L1_norm_grad         0.000091  w[0]   -0.083 bias    1.841\n",
      "iter 3900/1000000  loss         0.218819  avg_L1_norm_grad         0.000088  w[0]   -0.084 bias    1.849\n",
      "iter 3901/1000000  loss         0.218818  avg_L1_norm_grad         0.000088  w[0]   -0.084 bias    1.849\n",
      "iter 4000/1000000  loss         0.218718  avg_L1_norm_grad         0.000084  w[0]   -0.085 bias    1.856\n",
      "iter 4001/1000000  loss         0.218717  avg_L1_norm_grad         0.000084  w[0]   -0.085 bias    1.857\n",
      "iter 4100/1000000  loss         0.218626  avg_L1_norm_grad         0.000081  w[0]   -0.085 bias    1.864\n",
      "iter 4101/1000000  loss         0.218625  avg_L1_norm_grad         0.000081  w[0]   -0.085 bias    1.864\n",
      "iter 4200/1000000  loss         0.218541  avg_L1_norm_grad         0.000078  w[0]   -0.086 bias    1.871\n",
      "iter 4201/1000000  loss         0.218540  avg_L1_norm_grad         0.000077  w[0]   -0.086 bias    1.871\n",
      "iter 4300/1000000  loss         0.218462  avg_L1_norm_grad         0.000074  w[0]   -0.086 bias    1.878\n",
      "iter 4301/1000000  loss         0.218461  avg_L1_norm_grad         0.000074  w[0]   -0.086 bias    1.878\n",
      "iter 4400/1000000  loss         0.218389  avg_L1_norm_grad         0.000072  w[0]   -0.087 bias    1.884\n",
      "iter 4401/1000000  loss         0.218389  avg_L1_norm_grad         0.000072  w[0]   -0.087 bias    1.884\n",
      "iter 4500/1000000  loss         0.218322  avg_L1_norm_grad         0.000069  w[0]   -0.087 bias    1.891\n",
      "iter 4501/1000000  loss         0.218322  avg_L1_norm_grad         0.000069  w[0]   -0.087 bias    1.891\n",
      "iter 4600/1000000  loss         0.218261  avg_L1_norm_grad         0.000066  w[0]   -0.088 bias    1.897\n",
      "iter 4601/1000000  loss         0.218260  avg_L1_norm_grad         0.000066  w[0]   -0.088 bias    1.897\n",
      "iter 4700/1000000  loss         0.218203  avg_L1_norm_grad         0.000064  w[0]   -0.088 bias    1.903\n",
      "iter 4701/1000000  loss         0.218203  avg_L1_norm_grad         0.000064  w[0]   -0.088 bias    1.903\n",
      "iter 4800/1000000  loss         0.218150  avg_L1_norm_grad         0.000061  w[0]   -0.089 bias    1.908\n",
      "iter 4801/1000000  loss         0.218150  avg_L1_norm_grad         0.000061  w[0]   -0.089 bias    1.908\n",
      "iter 4900/1000000  loss         0.218101  avg_L1_norm_grad         0.000059  w[0]   -0.089 bias    1.914\n",
      "iter 4901/1000000  loss         0.218101  avg_L1_norm_grad         0.000059  w[0]   -0.089 bias    1.914\n",
      "iter 5000/1000000  loss         0.218056  avg_L1_norm_grad         0.000057  w[0]   -0.089 bias    1.919\n",
      "iter 5001/1000000  loss         0.218056  avg_L1_norm_grad         0.000057  w[0]   -0.089 bias    1.919\n",
      "iter 5100/1000000  loss         0.218014  avg_L1_norm_grad         0.000055  w[0]   -0.090 bias    1.924\n",
      "iter 5101/1000000  loss         0.218014  avg_L1_norm_grad         0.000055  w[0]   -0.090 bias    1.924\n",
      "iter 5200/1000000  loss         0.217975  avg_L1_norm_grad         0.000053  w[0]   -0.090 bias    1.929\n",
      "iter 5201/1000000  loss         0.217975  avg_L1_norm_grad         0.000053  w[0]   -0.090 bias    1.929\n",
      "iter 5300/1000000  loss         0.217939  avg_L1_norm_grad         0.000051  w[0]   -0.090 bias    1.933\n",
      "iter 5301/1000000  loss         0.217939  avg_L1_norm_grad         0.000051  w[0]   -0.090 bias    1.933\n",
      "iter 5400/1000000  loss         0.217906  avg_L1_norm_grad         0.000049  w[0]   -0.091 bias    1.938\n",
      "iter 5401/1000000  loss         0.217905  avg_L1_norm_grad         0.000049  w[0]   -0.091 bias    1.938\n",
      "iter 5500/1000000  loss         0.217875  avg_L1_norm_grad         0.000047  w[0]   -0.091 bias    1.942\n",
      "iter 5501/1000000  loss         0.217874  avg_L1_norm_grad         0.000047  w[0]   -0.091 bias    1.942\n",
      "iter 5600/1000000  loss         0.217846  avg_L1_norm_grad         0.000045  w[0]   -0.091 bias    1.946\n",
      "iter 5601/1000000  loss         0.217845  avg_L1_norm_grad         0.000045  w[0]   -0.091 bias    1.946\n",
      "iter 5700/1000000  loss         0.217819  avg_L1_norm_grad         0.000044  w[0]   -0.092 bias    1.950\n",
      "iter 5701/1000000  loss         0.217819  avg_L1_norm_grad         0.000044  w[0]   -0.092 bias    1.950\n",
      "iter 5800/1000000  loss         0.217794  avg_L1_norm_grad         0.000042  w[0]   -0.092 bias    1.954\n",
      "iter 5801/1000000  loss         0.217794  avg_L1_norm_grad         0.000042  w[0]   -0.092 bias    1.954\n",
      "iter 5900/1000000  loss         0.217771  avg_L1_norm_grad         0.000041  w[0]   -0.092 bias    1.958\n",
      "iter 5901/1000000  loss         0.217771  avg_L1_norm_grad         0.000041  w[0]   -0.092 bias    1.958\n",
      "iter 6000/1000000  loss         0.217750  avg_L1_norm_grad         0.000039  w[0]   -0.092 bias    1.961\n",
      "iter 6001/1000000  loss         0.217749  avg_L1_norm_grad         0.000039  w[0]   -0.092 bias    1.961\n",
      "iter 6100/1000000  loss         0.217730  avg_L1_norm_grad         0.000038  w[0]   -0.092 bias    1.965\n",
      "iter 6101/1000000  loss         0.217729  avg_L1_norm_grad         0.000038  w[0]   -0.092 bias    1.965\n",
      "iter 6200/1000000  loss         0.217711  avg_L1_norm_grad         0.000037  w[0]   -0.093 bias    1.968\n",
      "iter 6201/1000000  loss         0.217711  avg_L1_norm_grad         0.000037  w[0]   -0.093 bias    1.968\n",
      "iter 6300/1000000  loss         0.217694  avg_L1_norm_grad         0.000035  w[0]   -0.093 bias    1.971\n",
      "iter 6301/1000000  loss         0.217694  avg_L1_norm_grad         0.000035  w[0]   -0.093 bias    1.971\n",
      "iter 6400/1000000  loss         0.217678  avg_L1_norm_grad         0.000034  w[0]   -0.093 bias    1.974\n",
      "iter 6401/1000000  loss         0.217678  avg_L1_norm_grad         0.000034  w[0]   -0.093 bias    1.974\n",
      "iter 6500/1000000  loss         0.217663  avg_L1_norm_grad         0.000033  w[0]   -0.093 bias    1.977\n",
      "iter 6501/1000000  loss         0.217663  avg_L1_norm_grad         0.000033  w[0]   -0.093 bias    1.977\n",
      "iter 6600/1000000  loss         0.217649  avg_L1_norm_grad         0.000032  w[0]   -0.093 bias    1.980\n",
      "iter 6601/1000000  loss         0.217649  avg_L1_norm_grad         0.000032  w[0]   -0.093 bias    1.980\n",
      "iter 6700/1000000  loss         0.217636  avg_L1_norm_grad         0.000031  w[0]   -0.093 bias    1.983\n",
      "iter 6701/1000000  loss         0.217636  avg_L1_norm_grad         0.000031  w[0]   -0.093 bias    1.983\n",
      "iter 6800/1000000  loss         0.217624  avg_L1_norm_grad         0.000029  w[0]   -0.093 bias    1.985\n",
      "iter 6801/1000000  loss         0.217624  avg_L1_norm_grad         0.000029  w[0]   -0.093 bias    1.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.217613  avg_L1_norm_grad         0.000028  w[0]   -0.094 bias    1.988\n",
      "iter 6901/1000000  loss         0.217612  avg_L1_norm_grad         0.000028  w[0]   -0.094 bias    1.988\n",
      "iter 7000/1000000  loss         0.217602  avg_L1_norm_grad         0.000027  w[0]   -0.094 bias    1.990\n",
      "iter 7001/1000000  loss         0.217602  avg_L1_norm_grad         0.000027  w[0]   -0.094 bias    1.990\n",
      "iter 7100/1000000  loss         0.217592  avg_L1_norm_grad         0.000027  w[0]   -0.094 bias    1.993\n",
      "iter 7101/1000000  loss         0.217592  avg_L1_norm_grad         0.000027  w[0]   -0.094 bias    1.993\n",
      "iter 7200/1000000  loss         0.217583  avg_L1_norm_grad         0.000026  w[0]   -0.094 bias    1.995\n",
      "iter 7201/1000000  loss         0.217583  avg_L1_norm_grad         0.000026  w[0]   -0.094 bias    1.995\n",
      "iter 7300/1000000  loss         0.217575  avg_L1_norm_grad         0.000025  w[0]   -0.094 bias    1.997\n",
      "iter 7301/1000000  loss         0.217575  avg_L1_norm_grad         0.000025  w[0]   -0.094 bias    1.997\n",
      "iter 7400/1000000  loss         0.217567  avg_L1_norm_grad         0.000024  w[0]   -0.094 bias    1.999\n",
      "iter 7401/1000000  loss         0.217567  avg_L1_norm_grad         0.000024  w[0]   -0.094 bias    1.999\n",
      "iter 7500/1000000  loss         0.217559  avg_L1_norm_grad         0.000023  w[0]   -0.094 bias    2.001\n",
      "iter 7501/1000000  loss         0.217559  avg_L1_norm_grad         0.000023  w[0]   -0.094 bias    2.001\n",
      "iter 7600/1000000  loss         0.217552  avg_L1_norm_grad         0.000022  w[0]   -0.094 bias    2.003\n",
      "iter 7601/1000000  loss         0.217552  avg_L1_norm_grad         0.000022  w[0]   -0.094 bias    2.003\n",
      "iter 7700/1000000  loss         0.217546  avg_L1_norm_grad         0.000022  w[0]   -0.094 bias    2.005\n",
      "iter 7701/1000000  loss         0.217546  avg_L1_norm_grad         0.000022  w[0]   -0.094 bias    2.005\n",
      "iter 7800/1000000  loss         0.217540  avg_L1_norm_grad         0.000021  w[0]   -0.094 bias    2.007\n",
      "iter 7801/1000000  loss         0.217540  avg_L1_norm_grad         0.000021  w[0]   -0.094 bias    2.007\n",
      "iter 7900/1000000  loss         0.217534  avg_L1_norm_grad         0.000020  w[0]   -0.095 bias    2.009\n",
      "iter 7901/1000000  loss         0.217534  avg_L1_norm_grad         0.000020  w[0]   -0.095 bias    2.009\n",
      "iter 8000/1000000  loss         0.217529  avg_L1_norm_grad         0.000019  w[0]   -0.095 bias    2.011\n",
      "iter 8001/1000000  loss         0.217529  avg_L1_norm_grad         0.000019  w[0]   -0.095 bias    2.011\n",
      "iter 8100/1000000  loss         0.217524  avg_L1_norm_grad         0.000019  w[0]   -0.095 bias    2.012\n",
      "iter 8101/1000000  loss         0.217524  avg_L1_norm_grad         0.000019  w[0]   -0.095 bias    2.012\n",
      "iter 8200/1000000  loss         0.217519  avg_L1_norm_grad         0.000018  w[0]   -0.095 bias    2.014\n",
      "iter 8201/1000000  loss         0.217519  avg_L1_norm_grad         0.000018  w[0]   -0.095 bias    2.014\n",
      "iter 8300/1000000  loss         0.217515  avg_L1_norm_grad         0.000018  w[0]   -0.095 bias    2.016\n",
      "iter 8301/1000000  loss         0.217515  avg_L1_norm_grad         0.000018  w[0]   -0.095 bias    2.016\n",
      "iter 8400/1000000  loss         0.217511  avg_L1_norm_grad         0.000017  w[0]   -0.095 bias    2.017\n",
      "iter 8401/1000000  loss         0.217511  avg_L1_norm_grad         0.000017  w[0]   -0.095 bias    2.017\n",
      "iter 8500/1000000  loss         0.217507  avg_L1_norm_grad         0.000016  w[0]   -0.095 bias    2.019\n",
      "iter 8501/1000000  loss         0.217507  avg_L1_norm_grad         0.000016  w[0]   -0.095 bias    2.019\n",
      "iter 8600/1000000  loss         0.217504  avg_L1_norm_grad         0.000016  w[0]   -0.095 bias    2.020\n",
      "iter 8601/1000000  loss         0.217504  avg_L1_norm_grad         0.000016  w[0]   -0.095 bias    2.020\n",
      "iter 8700/1000000  loss         0.217501  avg_L1_norm_grad         0.000015  w[0]   -0.095 bias    2.021\n",
      "iter 8701/1000000  loss         0.217501  avg_L1_norm_grad         0.000015  w[0]   -0.095 bias    2.021\n",
      "iter 8800/1000000  loss         0.217497  avg_L1_norm_grad         0.000015  w[0]   -0.095 bias    2.023\n",
      "iter 8801/1000000  loss         0.217497  avg_L1_norm_grad         0.000015  w[0]   -0.095 bias    2.023\n",
      "iter 8900/1000000  loss         0.217495  avg_L1_norm_grad         0.000014  w[0]   -0.095 bias    2.024\n",
      "iter 8901/1000000  loss         0.217495  avg_L1_norm_grad         0.000014  w[0]   -0.095 bias    2.024\n",
      "iter 9000/1000000  loss         0.217492  avg_L1_norm_grad         0.000014  w[0]   -0.095 bias    2.025\n",
      "iter 9001/1000000  loss         0.217492  avg_L1_norm_grad         0.000014  w[0]   -0.095 bias    2.025\n",
      "iter 9100/1000000  loss         0.217489  avg_L1_norm_grad         0.000013  w[0]   -0.095 bias    2.026\n",
      "iter 9101/1000000  loss         0.217489  avg_L1_norm_grad         0.000013  w[0]   -0.095 bias    2.026\n",
      "iter 9200/1000000  loss         0.217487  avg_L1_norm_grad         0.000013  w[0]   -0.095 bias    2.027\n",
      "iter 9201/1000000  loss         0.217487  avg_L1_norm_grad         0.000013  w[0]   -0.095 bias    2.027\n",
      "iter 9300/1000000  loss         0.217485  avg_L1_norm_grad         0.000013  w[0]   -0.095 bias    2.028\n",
      "iter 9301/1000000  loss         0.217485  avg_L1_norm_grad         0.000013  w[0]   -0.095 bias    2.028\n",
      "iter 9400/1000000  loss         0.217483  avg_L1_norm_grad         0.000012  w[0]   -0.095 bias    2.030\n",
      "iter 9401/1000000  loss         0.217483  avg_L1_norm_grad         0.000012  w[0]   -0.095 bias    2.030\n",
      "iter 9500/1000000  loss         0.217481  avg_L1_norm_grad         0.000012  w[0]   -0.095 bias    2.031\n",
      "iter 9501/1000000  loss         0.217481  avg_L1_norm_grad         0.000012  w[0]   -0.095 bias    2.031\n",
      "Done. Converged after 9538 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9424999999999738\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Rate Loaded\n",
      "TurnOnOnce Rate Loaded\n",
      "Ave Loaded\n",
      "No Noise New 0.9452777777777515\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030387  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.910350  avg_L1_norm_grad         0.028370  w[0]    0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.846025  avg_L1_norm_grad         0.019582  w[0]    0.001 bias    0.021\n",
      "iter    3/1000000  loss         0.801791  avg_L1_norm_grad         0.018246  w[0]    0.001 bias    0.025\n",
      "iter    4/1000000  loss         0.768414  avg_L1_norm_grad         0.013900  w[0]    0.001 bias    0.040\n",
      "iter    5/1000000  loss         0.742700  avg_L1_norm_grad         0.013230  w[0]    0.001 bias    0.047\n",
      "iter    6/1000000  loss         0.721689  avg_L1_norm_grad         0.011335  w[0]    0.002 bias    0.059\n",
      "iter    7/1000000  loss         0.703867  avg_L1_norm_grad         0.010809  w[0]    0.002 bias    0.068\n",
      "iter    8/1000000  loss         0.688256  avg_L1_norm_grad         0.009934  w[0]    0.002 bias    0.078\n",
      "iter    9/1000000  loss         0.674280  avg_L1_norm_grad         0.009465  w[0]    0.002 bias    0.087\n",
      "iter   10/1000000  loss         0.661566  avg_L1_norm_grad         0.008960  w[0]    0.002 bias    0.096\n",
      "iter   11/1000000  loss         0.649865  avg_L1_norm_grad         0.008570  w[0]    0.003 bias    0.105\n",
      "iter   12/1000000  loss         0.639001  avg_L1_norm_grad         0.008211  w[0]    0.003 bias    0.114\n",
      "iter   13/1000000  loss         0.628844  avg_L1_norm_grad         0.007905  w[0]    0.003 bias    0.123\n",
      "iter   14/1000000  loss         0.619296  avg_L1_norm_grad         0.007630  w[0]    0.003 bias    0.131\n",
      "iter   15/1000000  loss         0.610282  avg_L1_norm_grad         0.007383  w[0]    0.003 bias    0.140\n",
      "iter   16/1000000  loss         0.601743  avg_L1_norm_grad         0.007159  w[0]    0.004 bias    0.148\n",
      "iter   17/1000000  loss         0.593631  avg_L1_norm_grad         0.006952  w[0]    0.004 bias    0.156\n",
      "iter   18/1000000  loss         0.585906  avg_L1_norm_grad         0.006761  w[0]    0.004 bias    0.164\n",
      "iter   19/1000000  loss         0.578535  avg_L1_norm_grad         0.006582  w[0]    0.004 bias    0.172\n",
      "iter  100/1000000  loss         0.363839  avg_L1_norm_grad         0.002283  w[0]    0.009 bias    0.593\n",
      "iter  101/1000000  loss         0.362821  avg_L1_norm_grad         0.002265  w[0]    0.009 bias    0.597\n",
      "iter  200/1000000  loss         0.303196  avg_L1_norm_grad         0.001330  w[0]    0.009 bias    0.887\n",
      "iter  201/1000000  loss         0.302833  avg_L1_norm_grad         0.001325  w[0]    0.009 bias    0.889\n",
      "iter  300/1000000  loss         0.276864  avg_L1_norm_grad         0.000967  w[0]    0.006 bias    1.090\n",
      "iter  301/1000000  loss         0.276673  avg_L1_norm_grad         0.000964  w[0]    0.006 bias    1.092\n",
      "iter  400/1000000  loss         0.261839  avg_L1_norm_grad         0.000769  w[0]    0.004 bias    1.245\n",
      "iter  401/1000000  loss         0.261721  avg_L1_norm_grad         0.000767  w[0]    0.003 bias    1.246\n",
      "iter  500/1000000  loss         0.252066  avg_L1_norm_grad         0.000643  w[0]    0.000 bias    1.369\n",
      "iter  501/1000000  loss         0.251986  avg_L1_norm_grad         0.000642  w[0]    0.000 bias    1.371\n",
      "iter  600/1000000  loss         0.245189  avg_L1_norm_grad         0.000556  w[0]   -0.003 bias    1.473\n",
      "iter  601/1000000  loss         0.245131  avg_L1_norm_grad         0.000555  w[0]   -0.003 bias    1.474\n",
      "iter  700/1000000  loss         0.240087  avg_L1_norm_grad         0.000491  w[0]   -0.006 bias    1.561\n",
      "iter  701/1000000  loss         0.240042  avg_L1_norm_grad         0.000491  w[0]   -0.006 bias    1.561\n",
      "iter  800/1000000  loss         0.236155  avg_L1_norm_grad         0.000441  w[0]   -0.009 bias    1.636\n",
      "iter  801/1000000  loss         0.236120  avg_L1_norm_grad         0.000441  w[0]   -0.009 bias    1.637\n",
      "iter  900/1000000  loss         0.233038  avg_L1_norm_grad         0.000401  w[0]   -0.012 bias    1.702\n",
      "iter  901/1000000  loss         0.233010  avg_L1_norm_grad         0.000400  w[0]   -0.012 bias    1.703\n",
      "iter 1000/1000000  loss         0.230512  avg_L1_norm_grad         0.000367  w[0]   -0.015 bias    1.760\n",
      "iter 1001/1000000  loss         0.230489  avg_L1_norm_grad         0.000366  w[0]   -0.015 bias    1.761\n",
      "iter 1100/1000000  loss         0.228430  avg_L1_norm_grad         0.000338  w[0]   -0.018 bias    1.812\n",
      "iter 1101/1000000  loss         0.228411  avg_L1_norm_grad         0.000337  w[0]   -0.018 bias    1.812\n",
      "iter 1200/1000000  loss         0.226688  avg_L1_norm_grad         0.000312  w[0]   -0.021 bias    1.858\n",
      "iter 1201/1000000  loss         0.226672  avg_L1_norm_grad         0.000312  w[0]   -0.021 bias    1.859\n",
      "iter 1300/1000000  loss         0.225215  avg_L1_norm_grad         0.000291  w[0]   -0.023 bias    1.900\n",
      "iter 1301/1000000  loss         0.225201  avg_L1_norm_grad         0.000290  w[0]   -0.023 bias    1.900\n",
      "iter 1400/1000000  loss         0.223956  avg_L1_norm_grad         0.000271  w[0]   -0.026 bias    1.937\n",
      "iter 1401/1000000  loss         0.223945  avg_L1_norm_grad         0.000271  w[0]   -0.026 bias    1.938\n",
      "iter 1500/1000000  loss         0.222872  avg_L1_norm_grad         0.000254  w[0]   -0.028 bias    1.971\n",
      "iter 1501/1000000  loss         0.222862  avg_L1_norm_grad         0.000254  w[0]   -0.028 bias    1.972\n",
      "iter 1600/1000000  loss         0.221932  avg_L1_norm_grad         0.000239  w[0]   -0.031 bias    2.002\n",
      "iter 1601/1000000  loss         0.221923  avg_L1_norm_grad         0.000239  w[0]   -0.031 bias    2.003\n",
      "iter 1700/1000000  loss         0.221112  avg_L1_norm_grad         0.000225  w[0]   -0.033 bias    2.031\n",
      "iter 1701/1000000  loss         0.221104  avg_L1_norm_grad         0.000225  w[0]   -0.033 bias    2.031\n",
      "iter 1800/1000000  loss         0.220391  avg_L1_norm_grad         0.000212  w[0]   -0.035 bias    2.057\n",
      "iter 1801/1000000  loss         0.220385  avg_L1_norm_grad         0.000212  w[0]   -0.035 bias    2.057\n",
      "iter 1900/1000000  loss         0.219756  avg_L1_norm_grad         0.000200  w[0]   -0.037 bias    2.080\n",
      "iter 1901/1000000  loss         0.219751  avg_L1_norm_grad         0.000200  w[0]   -0.037 bias    2.081\n",
      "iter 2000/1000000  loss         0.219194  avg_L1_norm_grad         0.000189  w[0]   -0.039 bias    2.102\n",
      "iter 2001/1000000  loss         0.219189  avg_L1_norm_grad         0.000189  w[0]   -0.039 bias    2.103\n",
      "iter 2100/1000000  loss         0.218694  avg_L1_norm_grad         0.000180  w[0]   -0.041 bias    2.123\n",
      "iter 2101/1000000  loss         0.218690  avg_L1_norm_grad         0.000179  w[0]   -0.041 bias    2.123\n",
      "iter 2200/1000000  loss         0.218249  avg_L1_norm_grad         0.000170  w[0]   -0.043 bias    2.141\n",
      "iter 2201/1000000  loss         0.218244  avg_L1_norm_grad         0.000170  w[0]   -0.043 bias    2.141\n",
      "iter 2300/1000000  loss         0.217850  avg_L1_norm_grad         0.000162  w[0]   -0.044 bias    2.159\n",
      "iter 2301/1000000  loss         0.217846  avg_L1_norm_grad         0.000162  w[0]   -0.044 bias    2.159\n",
      "iter 2400/1000000  loss         0.217492  avg_L1_norm_grad         0.000154  w[0]   -0.046 bias    2.175\n",
      "iter 2401/1000000  loss         0.217488  avg_L1_norm_grad         0.000154  w[0]   -0.046 bias    2.175\n",
      "iter 2500/1000000  loss         0.217170  avg_L1_norm_grad         0.000147  w[0]   -0.048 bias    2.189\n",
      "iter 2501/1000000  loss         0.217166  avg_L1_norm_grad         0.000146  w[0]   -0.048 bias    2.190\n",
      "iter 2600/1000000  loss         0.216879  avg_L1_norm_grad         0.000140  w[0]   -0.049 bias    2.203\n",
      "iter 2601/1000000  loss         0.216876  avg_L1_norm_grad         0.000139  w[0]   -0.049 bias    2.203\n",
      "iter 2700/1000000  loss         0.216616  avg_L1_norm_grad         0.000133  w[0]   -0.051 bias    2.216\n",
      "iter 2701/1000000  loss         0.216614  avg_L1_norm_grad         0.000133  w[0]   -0.051 bias    2.216\n",
      "iter 2800/1000000  loss         0.216379  avg_L1_norm_grad         0.000127  w[0]   -0.052 bias    2.228\n",
      "iter 2801/1000000  loss         0.216376  avg_L1_norm_grad         0.000127  w[0]   -0.052 bias    2.228\n",
      "iter 2900/1000000  loss         0.216163  avg_L1_norm_grad         0.000121  w[0]   -0.053 bias    2.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.216161  avg_L1_norm_grad         0.000121  w[0]   -0.053 bias    2.239\n",
      "iter 3000/1000000  loss         0.215966  avg_L1_norm_grad         0.000116  w[0]   -0.055 bias    2.249\n",
      "iter 3001/1000000  loss         0.215964  avg_L1_norm_grad         0.000116  w[0]   -0.055 bias    2.250\n",
      "iter 3100/1000000  loss         0.215788  avg_L1_norm_grad         0.000111  w[0]   -0.056 bias    2.259\n",
      "iter 3101/1000000  loss         0.215786  avg_L1_norm_grad         0.000111  w[0]   -0.056 bias    2.259\n",
      "iter 3200/1000000  loss         0.215625  avg_L1_norm_grad         0.000106  w[0]   -0.057 bias    2.268\n",
      "iter 3201/1000000  loss         0.215623  avg_L1_norm_grad         0.000106  w[0]   -0.057 bias    2.268\n",
      "iter 3300/1000000  loss         0.215476  avg_L1_norm_grad         0.000102  w[0]   -0.058 bias    2.277\n",
      "iter 3301/1000000  loss         0.215474  avg_L1_norm_grad         0.000102  w[0]   -0.058 bias    2.277\n",
      "iter 3400/1000000  loss         0.215339  avg_L1_norm_grad         0.000098  w[0]   -0.059 bias    2.285\n",
      "iter 3401/1000000  loss         0.215338  avg_L1_norm_grad         0.000098  w[0]   -0.059 bias    2.285\n",
      "iter 3500/1000000  loss         0.215215  avg_L1_norm_grad         0.000094  w[0]   -0.060 bias    2.292\n",
      "iter 3501/1000000  loss         0.215213  avg_L1_norm_grad         0.000094  w[0]   -0.060 bias    2.292\n",
      "iter 3600/1000000  loss         0.215100  avg_L1_norm_grad         0.000090  w[0]   -0.061 bias    2.299\n",
      "iter 3601/1000000  loss         0.215099  avg_L1_norm_grad         0.000090  w[0]   -0.061 bias    2.299\n",
      "iter 3700/1000000  loss         0.214995  avg_L1_norm_grad         0.000087  w[0]   -0.062 bias    2.306\n",
      "iter 3701/1000000  loss         0.214994  avg_L1_norm_grad         0.000087  w[0]   -0.062 bias    2.306\n",
      "iter 3800/1000000  loss         0.214899  avg_L1_norm_grad         0.000083  w[0]   -0.063 bias    2.312\n",
      "iter 3801/1000000  loss         0.214898  avg_L1_norm_grad         0.000083  w[0]   -0.063 bias    2.312\n",
      "iter 3900/1000000  loss         0.214810  avg_L1_norm_grad         0.000080  w[0]   -0.064 bias    2.318\n",
      "iter 3901/1000000  loss         0.214809  avg_L1_norm_grad         0.000080  w[0]   -0.064 bias    2.318\n",
      "iter 4000/1000000  loss         0.214728  avg_L1_norm_grad         0.000077  w[0]   -0.065 bias    2.323\n",
      "iter 4001/1000000  loss         0.214727  avg_L1_norm_grad         0.000077  w[0]   -0.065 bias    2.323\n",
      "iter 4100/1000000  loss         0.214653  avg_L1_norm_grad         0.000074  w[0]   -0.066 bias    2.328\n",
      "iter 4101/1000000  loss         0.214652  avg_L1_norm_grad         0.000074  w[0]   -0.066 bias    2.328\n",
      "iter 4200/1000000  loss         0.214583  avg_L1_norm_grad         0.000071  w[0]   -0.066 bias    2.333\n",
      "iter 4201/1000000  loss         0.214582  avg_L1_norm_grad         0.000071  w[0]   -0.066 bias    2.333\n",
      "iter 4300/1000000  loss         0.214519  avg_L1_norm_grad         0.000068  w[0]   -0.067 bias    2.337\n",
      "iter 4301/1000000  loss         0.214518  avg_L1_norm_grad         0.000068  w[0]   -0.067 bias    2.337\n",
      "iter 4400/1000000  loss         0.214460  avg_L1_norm_grad         0.000066  w[0]   -0.068 bias    2.342\n",
      "iter 4401/1000000  loss         0.214459  avg_L1_norm_grad         0.000066  w[0]   -0.068 bias    2.342\n",
      "iter 4500/1000000  loss         0.214405  avg_L1_norm_grad         0.000063  w[0]   -0.068 bias    2.346\n",
      "iter 4501/1000000  loss         0.214404  avg_L1_norm_grad         0.000063  w[0]   -0.068 bias    2.346\n",
      "iter 4600/1000000  loss         0.214354  avg_L1_norm_grad         0.000061  w[0]   -0.069 bias    2.349\n",
      "iter 4601/1000000  loss         0.214354  avg_L1_norm_grad         0.000061  w[0]   -0.069 bias    2.349\n",
      "iter 4700/1000000  loss         0.214307  avg_L1_norm_grad         0.000059  w[0]   -0.070 bias    2.353\n",
      "iter 4701/1000000  loss         0.214307  avg_L1_norm_grad         0.000059  w[0]   -0.070 bias    2.353\n",
      "iter 4800/1000000  loss         0.214264  avg_L1_norm_grad         0.000057  w[0]   -0.070 bias    2.356\n",
      "iter 4801/1000000  loss         0.214263  avg_L1_norm_grad         0.000057  w[0]   -0.070 bias    2.356\n",
      "iter 4900/1000000  loss         0.214223  avg_L1_norm_grad         0.000055  w[0]   -0.071 bias    2.359\n",
      "iter 4901/1000000  loss         0.214223  avg_L1_norm_grad         0.000055  w[0]   -0.071 bias    2.359\n",
      "iter 5000/1000000  loss         0.214186  avg_L1_norm_grad         0.000053  w[0]   -0.071 bias    2.362\n",
      "iter 5001/1000000  loss         0.214186  avg_L1_norm_grad         0.000053  w[0]   -0.071 bias    2.362\n",
      "iter 5100/1000000  loss         0.214151  avg_L1_norm_grad         0.000051  w[0]   -0.072 bias    2.365\n",
      "iter 5101/1000000  loss         0.214151  avg_L1_norm_grad         0.000051  w[0]   -0.072 bias    2.365\n",
      "iter 5200/1000000  loss         0.214119  avg_L1_norm_grad         0.000049  w[0]   -0.072 bias    2.368\n",
      "iter 5201/1000000  loss         0.214119  avg_L1_norm_grad         0.000049  w[0]   -0.072 bias    2.368\n",
      "iter 5300/1000000  loss         0.214089  avg_L1_norm_grad         0.000047  w[0]   -0.073 bias    2.370\n",
      "iter 5301/1000000  loss         0.214089  avg_L1_norm_grad         0.000047  w[0]   -0.073 bias    2.370\n",
      "iter 5400/1000000  loss         0.214061  avg_L1_norm_grad         0.000046  w[0]   -0.073 bias    2.373\n",
      "iter 5401/1000000  loss         0.214061  avg_L1_norm_grad         0.000046  w[0]   -0.073 bias    2.373\n",
      "iter 5500/1000000  loss         0.214035  avg_L1_norm_grad         0.000044  w[0]   -0.074 bias    2.375\n",
      "iter 5501/1000000  loss         0.214035  avg_L1_norm_grad         0.000044  w[0]   -0.074 bias    2.375\n",
      "iter 5600/1000000  loss         0.214011  avg_L1_norm_grad         0.000042  w[0]   -0.074 bias    2.377\n",
      "iter 5601/1000000  loss         0.214011  avg_L1_norm_grad         0.000042  w[0]   -0.074 bias    2.377\n",
      "iter 5700/1000000  loss         0.213989  avg_L1_norm_grad         0.000041  w[0]   -0.075 bias    2.379\n",
      "iter 5701/1000000  loss         0.213989  avg_L1_norm_grad         0.000041  w[0]   -0.075 bias    2.379\n",
      "iter 5800/1000000  loss         0.213968  avg_L1_norm_grad         0.000039  w[0]   -0.075 bias    2.381\n",
      "iter 5801/1000000  loss         0.213968  avg_L1_norm_grad         0.000039  w[0]   -0.075 bias    2.381\n",
      "iter 5900/1000000  loss         0.213949  avg_L1_norm_grad         0.000038  w[0]   -0.075 bias    2.383\n",
      "iter 5901/1000000  loss         0.213948  avg_L1_norm_grad         0.000038  w[0]   -0.075 bias    2.383\n",
      "iter 6000/1000000  loss         0.213930  avg_L1_norm_grad         0.000037  w[0]   -0.076 bias    2.385\n",
      "iter 6001/1000000  loss         0.213930  avg_L1_norm_grad         0.000037  w[0]   -0.076 bias    2.385\n",
      "iter 6100/1000000  loss         0.213914  avg_L1_norm_grad         0.000036  w[0]   -0.076 bias    2.386\n",
      "iter 6101/1000000  loss         0.213913  avg_L1_norm_grad         0.000036  w[0]   -0.076 bias    2.386\n",
      "iter 6200/1000000  loss         0.213898  avg_L1_norm_grad         0.000034  w[0]   -0.076 bias    2.388\n",
      "iter 6201/1000000  loss         0.213898  avg_L1_norm_grad         0.000034  w[0]   -0.076 bias    2.388\n",
      "iter 6300/1000000  loss         0.213883  avg_L1_norm_grad         0.000033  w[0]   -0.077 bias    2.389\n",
      "iter 6301/1000000  loss         0.213883  avg_L1_norm_grad         0.000033  w[0]   -0.077 bias    2.389\n",
      "iter 6400/1000000  loss         0.213870  avg_L1_norm_grad         0.000032  w[0]   -0.077 bias    2.391\n",
      "iter 6401/1000000  loss         0.213869  avg_L1_norm_grad         0.000032  w[0]   -0.077 bias    2.391\n",
      "iter 6500/1000000  loss         0.213857  avg_L1_norm_grad         0.000031  w[0]   -0.077 bias    2.392\n",
      "iter 6501/1000000  loss         0.213857  avg_L1_norm_grad         0.000031  w[0]   -0.077 bias    2.392\n",
      "iter 6600/1000000  loss         0.213845  avg_L1_norm_grad         0.000030  w[0]   -0.078 bias    2.393\n",
      "iter 6601/1000000  loss         0.213845  avg_L1_norm_grad         0.000030  w[0]   -0.078 bias    2.393\n",
      "iter 6700/1000000  loss         0.213834  avg_L1_norm_grad         0.000029  w[0]   -0.078 bias    2.394\n",
      "iter 6701/1000000  loss         0.213834  avg_L1_norm_grad         0.000029  w[0]   -0.078 bias    2.394\n",
      "iter 6800/1000000  loss         0.213823  avg_L1_norm_grad         0.000028  w[0]   -0.078 bias    2.396\n",
      "iter 6801/1000000  loss         0.213823  avg_L1_norm_grad         0.000028  w[0]   -0.078 bias    2.396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.213814  avg_L1_norm_grad         0.000027  w[0]   -0.078 bias    2.397\n",
      "iter 6901/1000000  loss         0.213814  avg_L1_norm_grad         0.000027  w[0]   -0.078 bias    2.397\n",
      "iter 7000/1000000  loss         0.213805  avg_L1_norm_grad         0.000026  w[0]   -0.079 bias    2.398\n",
      "iter 7001/1000000  loss         0.213805  avg_L1_norm_grad         0.000026  w[0]   -0.079 bias    2.398\n",
      "iter 7100/1000000  loss         0.213796  avg_L1_norm_grad         0.000025  w[0]   -0.079 bias    2.399\n",
      "iter 7101/1000000  loss         0.213796  avg_L1_norm_grad         0.000025  w[0]   -0.079 bias    2.399\n",
      "iter 7200/1000000  loss         0.213788  avg_L1_norm_grad         0.000024  w[0]   -0.079 bias    2.399\n",
      "iter 7201/1000000  loss         0.213788  avg_L1_norm_grad         0.000024  w[0]   -0.079 bias    2.399\n",
      "iter 7300/1000000  loss         0.213781  avg_L1_norm_grad         0.000024  w[0]   -0.079 bias    2.400\n",
      "iter 7301/1000000  loss         0.213781  avg_L1_norm_grad         0.000024  w[0]   -0.079 bias    2.400\n",
      "iter 7400/1000000  loss         0.213774  avg_L1_norm_grad         0.000023  w[0]   -0.079 bias    2.401\n",
      "iter 7401/1000000  loss         0.213774  avg_L1_norm_grad         0.000023  w[0]   -0.079 bias    2.401\n",
      "iter 7500/1000000  loss         0.213767  avg_L1_norm_grad         0.000022  w[0]   -0.080 bias    2.402\n",
      "iter 7501/1000000  loss         0.213767  avg_L1_norm_grad         0.000022  w[0]   -0.080 bias    2.402\n",
      "iter 7600/1000000  loss         0.213761  avg_L1_norm_grad         0.000021  w[0]   -0.080 bias    2.403\n",
      "iter 7601/1000000  loss         0.213761  avg_L1_norm_grad         0.000021  w[0]   -0.080 bias    2.403\n",
      "Done. Converged after 7637 iterations.\n",
      "Origin Accuracy 0.9565833333333307\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr22 = LRGD(alpha=100.0, step_size=0.1)\n",
    "orig_lr22.fit(x_te, y_te)\n",
    "y_hat_Origin=np.asarray(orig_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax\n",
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Rate Loaded\n",
      "TurnOnOnce Rate Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 1575 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030458  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.946976  avg_L1_norm_grad         0.053736  w[0]    0.000 bias   -0.000\n",
      "iter    2/1000000  loss         1.205726  avg_L1_norm_grad         0.083119  w[0]    0.001 bias    0.035\n",
      "iter    3/1000000  loss         1.810786  avg_L1_norm_grad         0.096997  w[0]   -0.000 bias   -0.003\n",
      "iter    4/1000000  loss         0.960261  avg_L1_norm_grad         0.067396  w[0]    0.002 bias    0.058\n",
      "iter    5/1000000  loss         1.246339  avg_L1_norm_grad         0.075331  w[0]    0.001 bias    0.028\n",
      "iter    6/1000000  loss         0.857910  avg_L1_norm_grad         0.059290  w[0]    0.002 bias    0.077\n",
      "iter    7/1000000  loss         1.006298  avg_L1_norm_grad         0.061599  w[0]    0.001 bias    0.052\n",
      "iter    8/1000000  loss         0.749038  avg_L1_norm_grad         0.049262  w[0]    0.002 bias    0.093\n",
      "iter    9/1000000  loss         0.825706  avg_L1_norm_grad         0.050251  w[0]    0.002 bias    0.073\n",
      "iter   10/1000000  loss         0.675974  avg_L1_norm_grad         0.042948  w[0]    0.003 bias    0.108\n",
      "iter   11/1000000  loss         0.723974  avg_L1_norm_grad         0.043515  w[0]    0.002 bias    0.091\n",
      "iter   12/1000000  loss         0.621492  avg_L1_norm_grad         0.038431  w[0]    0.003 bias    0.122\n",
      "iter   13/1000000  loss         0.654540  avg_L1_norm_grad         0.038773  w[0]    0.002 bias    0.107\n",
      "iter   14/1000000  loss         0.577973  avg_L1_norm_grad         0.034719  w[0]    0.003 bias    0.135\n",
      "iter   15/1000000  loss         0.600863  avg_L1_norm_grad         0.034844  w[0]    0.003 bias    0.122\n",
      "iter   16/1000000  loss         0.541386  avg_L1_norm_grad         0.031276  w[0]    0.003 bias    0.147\n",
      "iter   17/1000000  loss         0.556115  avg_L1_norm_grad         0.031162  w[0]    0.003 bias    0.136\n",
      "iter   18/1000000  loss         0.509667  avg_L1_norm_grad         0.027852  w[0]    0.003 bias    0.159\n",
      "iter   19/1000000  loss         0.517576  avg_L1_norm_grad         0.027492  w[0]    0.003 bias    0.149\n",
      "iter  100/1000000  loss         0.281180  avg_L1_norm_grad         0.001223  w[0]    0.002 bias    0.397\n",
      "iter  101/1000000  loss         0.280499  avg_L1_norm_grad         0.001215  w[0]    0.002 bias    0.398\n",
      "iter  200/1000000  loss         0.241199  avg_L1_norm_grad         0.000765  w[0]   -0.002 bias    0.535\n",
      "iter  201/1000000  loss         0.240963  avg_L1_norm_grad         0.000762  w[0]   -0.003 bias    0.536\n",
      "iter  300/1000000  loss         0.224077  avg_L1_norm_grad         0.000580  w[0]   -0.007 bias    0.619\n",
      "iter  301/1000000  loss         0.223952  avg_L1_norm_grad         0.000579  w[0]   -0.007 bias    0.620\n",
      "iter  400/1000000  loss         0.214203  avg_L1_norm_grad         0.000476  w[0]   -0.012 bias    0.678\n",
      "iter  401/1000000  loss         0.214124  avg_L1_norm_grad         0.000475  w[0]   -0.012 bias    0.679\n",
      "iter  500/1000000  loss         0.207649  avg_L1_norm_grad         0.000407  w[0]   -0.016 bias    0.723\n",
      "iter  501/1000000  loss         0.207595  avg_L1_norm_grad         0.000407  w[0]   -0.016 bias    0.723\n",
      "iter  600/1000000  loss         0.202935  avg_L1_norm_grad         0.000357  w[0]   -0.020 bias    0.758\n",
      "iter  601/1000000  loss         0.202894  avg_L1_norm_grad         0.000357  w[0]   -0.020 bias    0.758\n",
      "iter  700/1000000  loss         0.199364  avg_L1_norm_grad         0.000318  w[0]   -0.023 bias    0.786\n",
      "iter  701/1000000  loss         0.199333  avg_L1_norm_grad         0.000318  w[0]   -0.023 bias    0.786\n",
      "iter  800/1000000  loss         0.196562  avg_L1_norm_grad         0.000287  w[0]   -0.027 bias    0.809\n",
      "iter  801/1000000  loss         0.196537  avg_L1_norm_grad         0.000286  w[0]   -0.027 bias    0.809\n",
      "iter  900/1000000  loss         0.194306  avg_L1_norm_grad         0.000260  w[0]   -0.030 bias    0.829\n",
      "iter  901/1000000  loss         0.194285  avg_L1_norm_grad         0.000260  w[0]   -0.030 bias    0.829\n",
      "iter 1000/1000000  loss         0.192453  avg_L1_norm_grad         0.000238  w[0]   -0.032 bias    0.846\n",
      "iter 1001/1000000  loss         0.192436  avg_L1_norm_grad         0.000238  w[0]   -0.032 bias    0.846\n",
      "iter 1100/1000000  loss         0.190907  avg_L1_norm_grad         0.000219  w[0]   -0.035 bias    0.860\n",
      "iter 1101/1000000  loss         0.190893  avg_L1_norm_grad         0.000219  w[0]   -0.035 bias    0.860\n",
      "iter 1200/1000000  loss         0.189602  avg_L1_norm_grad         0.000202  w[0]   -0.037 bias    0.873\n",
      "iter 1201/1000000  loss         0.189590  avg_L1_norm_grad         0.000202  w[0]   -0.037 bias    0.873\n",
      "iter 1300/1000000  loss         0.188488  avg_L1_norm_grad         0.000188  w[0]   -0.040 bias    0.884\n",
      "iter 1301/1000000  loss         0.188477  avg_L1_norm_grad         0.000188  w[0]   -0.040 bias    0.884\n",
      "iter 1400/1000000  loss         0.187528  avg_L1_norm_grad         0.000175  w[0]   -0.042 bias    0.894\n",
      "iter 1401/1000000  loss         0.187519  avg_L1_norm_grad         0.000175  w[0]   -0.042 bias    0.894\n",
      "iter 1500/1000000  loss         0.186696  avg_L1_norm_grad         0.000163  w[0]   -0.044 bias    0.903\n",
      "iter 1501/1000000  loss         0.186688  avg_L1_norm_grad         0.000163  w[0]   -0.044 bias    0.903\n",
      "iter 1600/1000000  loss         0.185969  avg_L1_norm_grad         0.000153  w[0]   -0.045 bias    0.911\n",
      "iter 1601/1000000  loss         0.185962  avg_L1_norm_grad         0.000153  w[0]   -0.045 bias    0.911\n",
      "iter 1700/1000000  loss         0.185330  avg_L1_norm_grad         0.000144  w[0]   -0.047 bias    0.919\n",
      "iter 1701/1000000  loss         0.185324  avg_L1_norm_grad         0.000143  w[0]   -0.047 bias    0.919\n",
      "iter 1800/1000000  loss         0.184766  avg_L1_norm_grad         0.000135  w[0]   -0.049 bias    0.925\n",
      "iter 1801/1000000  loss         0.184761  avg_L1_norm_grad         0.000135  w[0]   -0.049 bias    0.925\n",
      "iter 1900/1000000  loss         0.184266  avg_L1_norm_grad         0.000128  w[0]   -0.050 bias    0.931\n",
      "iter 1901/1000000  loss         0.184261  avg_L1_norm_grad         0.000127  w[0]   -0.050 bias    0.931\n",
      "iter 2000/1000000  loss         0.183821  avg_L1_norm_grad         0.000120  w[0]   -0.051 bias    0.937\n",
      "iter 2001/1000000  loss         0.183817  avg_L1_norm_grad         0.000120  w[0]   -0.051 bias    0.937\n",
      "iter 2100/1000000  loss         0.183423  avg_L1_norm_grad         0.000114  w[0]   -0.053 bias    0.942\n",
      "iter 2101/1000000  loss         0.183419  avg_L1_norm_grad         0.000114  w[0]   -0.053 bias    0.942\n",
      "iter 2200/1000000  loss         0.183066  avg_L1_norm_grad         0.000108  w[0]   -0.054 bias    0.946\n",
      "iter 2201/1000000  loss         0.183063  avg_L1_norm_grad         0.000108  w[0]   -0.054 bias    0.946\n",
      "iter 2300/1000000  loss         0.182745  avg_L1_norm_grad         0.000102  w[0]   -0.055 bias    0.950\n",
      "iter 2301/1000000  loss         0.182742  avg_L1_norm_grad         0.000102  w[0]   -0.055 bias    0.950\n",
      "iter 2400/1000000  loss         0.182456  avg_L1_norm_grad         0.000097  w[0]   -0.056 bias    0.954\n",
      "iter 2401/1000000  loss         0.182453  avg_L1_norm_grad         0.000097  w[0]   -0.056 bias    0.954\n",
      "iter 2500/1000000  loss         0.182195  avg_L1_norm_grad         0.000092  w[0]   -0.057 bias    0.958\n",
      "iter 2501/1000000  loss         0.182192  avg_L1_norm_grad         0.000092  w[0]   -0.057 bias    0.958\n",
      "iter 2600/1000000  loss         0.181958  avg_L1_norm_grad         0.000088  w[0]   -0.058 bias    0.961\n",
      "iter 2601/1000000  loss         0.181955  avg_L1_norm_grad         0.000088  w[0]   -0.058 bias    0.961\n",
      "iter 2700/1000000  loss         0.181742  avg_L1_norm_grad         0.000083  w[0]   -0.058 bias    0.964\n",
      "iter 2701/1000000  loss         0.181740  avg_L1_norm_grad         0.000083  w[0]   -0.058 bias    0.964\n",
      "iter 2800/1000000  loss         0.181547  avg_L1_norm_grad         0.000079  w[0]   -0.059 bias    0.967\n",
      "iter 2801/1000000  loss         0.181545  avg_L1_norm_grad         0.000079  w[0]   -0.059 bias    0.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.181368  avg_L1_norm_grad         0.000076  w[0]   -0.060 bias    0.970\n",
      "iter 2901/1000000  loss         0.181367  avg_L1_norm_grad         0.000076  w[0]   -0.060 bias    0.970\n",
      "iter 3000/1000000  loss         0.181206  avg_L1_norm_grad         0.000072  w[0]   -0.061 bias    0.973\n",
      "iter 3001/1000000  loss         0.181204  avg_L1_norm_grad         0.000072  w[0]   -0.061 bias    0.973\n",
      "iter 3100/1000000  loss         0.181057  avg_L1_norm_grad         0.000069  w[0]   -0.061 bias    0.975\n",
      "iter 3101/1000000  loss         0.181055  avg_L1_norm_grad         0.000069  w[0]   -0.061 bias    0.975\n",
      "iter 3200/1000000  loss         0.180921  avg_L1_norm_grad         0.000066  w[0]   -0.062 bias    0.977\n",
      "iter 3201/1000000  loss         0.180919  avg_L1_norm_grad         0.000066  w[0]   -0.062 bias    0.978\n",
      "iter 3300/1000000  loss         0.180796  avg_L1_norm_grad         0.000063  w[0]   -0.062 bias    0.980\n",
      "iter 3301/1000000  loss         0.180794  avg_L1_norm_grad         0.000063  w[0]   -0.062 bias    0.980\n",
      "iter 3400/1000000  loss         0.180681  avg_L1_norm_grad         0.000060  w[0]   -0.063 bias    0.982\n",
      "iter 3401/1000000  loss         0.180680  avg_L1_norm_grad         0.000060  w[0]   -0.063 bias    0.982\n",
      "iter 3500/1000000  loss         0.180575  avg_L1_norm_grad         0.000057  w[0]   -0.063 bias    0.984\n",
      "iter 3501/1000000  loss         0.180574  avg_L1_norm_grad         0.000057  w[0]   -0.063 bias    0.984\n",
      "iter 3600/1000000  loss         0.180478  avg_L1_norm_grad         0.000055  w[0]   -0.064 bias    0.986\n",
      "iter 3601/1000000  loss         0.180477  avg_L1_norm_grad         0.000055  w[0]   -0.064 bias    0.986\n",
      "iter 3700/1000000  loss         0.180389  avg_L1_norm_grad         0.000052  w[0]   -0.064 bias    0.987\n",
      "iter 3701/1000000  loss         0.180388  avg_L1_norm_grad         0.000052  w[0]   -0.064 bias    0.987\n",
      "iter 3800/1000000  loss         0.180306  avg_L1_norm_grad         0.000050  w[0]   -0.065 bias    0.989\n",
      "iter 3801/1000000  loss         0.180306  avg_L1_norm_grad         0.000050  w[0]   -0.065 bias    0.989\n",
      "iter 3900/1000000  loss         0.180230  avg_L1_norm_grad         0.000048  w[0]   -0.065 bias    0.991\n",
      "iter 3901/1000000  loss         0.180229  avg_L1_norm_grad         0.000048  w[0]   -0.065 bias    0.991\n",
      "iter 4000/1000000  loss         0.180160  avg_L1_norm_grad         0.000046  w[0]   -0.065 bias    0.992\n",
      "iter 4001/1000000  loss         0.180159  avg_L1_norm_grad         0.000046  w[0]   -0.065 bias    0.992\n",
      "iter 4100/1000000  loss         0.180095  avg_L1_norm_grad         0.000044  w[0]   -0.066 bias    0.994\n",
      "iter 4101/1000000  loss         0.180094  avg_L1_norm_grad         0.000044  w[0]   -0.066 bias    0.994\n",
      "iter 4200/1000000  loss         0.180034  avg_L1_norm_grad         0.000042  w[0]   -0.066 bias    0.995\n",
      "iter 4201/1000000  loss         0.180034  avg_L1_norm_grad         0.000042  w[0]   -0.066 bias    0.995\n",
      "iter 4300/1000000  loss         0.179979  avg_L1_norm_grad         0.000040  w[0]   -0.066 bias    0.997\n",
      "iter 4301/1000000  loss         0.179978  avg_L1_norm_grad         0.000040  w[0]   -0.066 bias    0.997\n",
      "iter 4400/1000000  loss         0.179927  avg_L1_norm_grad         0.000039  w[0]   -0.067 bias    0.998\n",
      "iter 4401/1000000  loss         0.179926  avg_L1_norm_grad         0.000039  w[0]   -0.067 bias    0.998\n",
      "iter 4500/1000000  loss         0.179879  avg_L1_norm_grad         0.000037  w[0]   -0.067 bias    0.999\n",
      "iter 4501/1000000  loss         0.179878  avg_L1_norm_grad         0.000037  w[0]   -0.067 bias    0.999\n",
      "iter 4600/1000000  loss         0.179834  avg_L1_norm_grad         0.000036  w[0]   -0.067 bias    1.001\n",
      "iter 4601/1000000  loss         0.179834  avg_L1_norm_grad         0.000036  w[0]   -0.067 bias    1.001\n",
      "iter 4700/1000000  loss         0.179793  avg_L1_norm_grad         0.000034  w[0]   -0.067 bias    1.002\n",
      "iter 4701/1000000  loss         0.179792  avg_L1_norm_grad         0.000034  w[0]   -0.067 bias    1.002\n",
      "iter 4800/1000000  loss         0.179754  avg_L1_norm_grad         0.000033  w[0]   -0.068 bias    1.003\n",
      "iter 4801/1000000  loss         0.179754  avg_L1_norm_grad         0.000033  w[0]   -0.068 bias    1.003\n",
      "iter 4900/1000000  loss         0.179718  avg_L1_norm_grad         0.000031  w[0]   -0.068 bias    1.004\n",
      "iter 4901/1000000  loss         0.179718  avg_L1_norm_grad         0.000031  w[0]   -0.068 bias    1.004\n",
      "iter 5000/1000000  loss         0.179685  avg_L1_norm_grad         0.000030  w[0]   -0.068 bias    1.005\n",
      "iter 5001/1000000  loss         0.179684  avg_L1_norm_grad         0.000030  w[0]   -0.068 bias    1.005\n",
      "iter 5100/1000000  loss         0.179653  avg_L1_norm_grad         0.000029  w[0]   -0.068 bias    1.006\n",
      "iter 5101/1000000  loss         0.179653  avg_L1_norm_grad         0.000029  w[0]   -0.068 bias    1.006\n",
      "iter 5200/1000000  loss         0.179624  avg_L1_norm_grad         0.000028  w[0]   -0.068 bias    1.007\n",
      "iter 5201/1000000  loss         0.179624  avg_L1_norm_grad         0.000028  w[0]   -0.068 bias    1.007\n",
      "iter 5300/1000000  loss         0.179597  avg_L1_norm_grad         0.000027  w[0]   -0.068 bias    1.009\n",
      "iter 5301/1000000  loss         0.179597  avg_L1_norm_grad         0.000027  w[0]   -0.068 bias    1.009\n",
      "iter 5400/1000000  loss         0.179572  avg_L1_norm_grad         0.000026  w[0]   -0.069 bias    1.010\n",
      "iter 5401/1000000  loss         0.179571  avg_L1_norm_grad         0.000026  w[0]   -0.069 bias    1.010\n",
      "iter 5500/1000000  loss         0.179548  avg_L1_norm_grad         0.000025  w[0]   -0.069 bias    1.011\n",
      "iter 5501/1000000  loss         0.179548  avg_L1_norm_grad         0.000025  w[0]   -0.069 bias    1.011\n",
      "iter 5600/1000000  loss         0.179526  avg_L1_norm_grad         0.000024  w[0]   -0.069 bias    1.012\n",
      "iter 5601/1000000  loss         0.179525  avg_L1_norm_grad         0.000024  w[0]   -0.069 bias    1.012\n",
      "iter 5700/1000000  loss         0.179505  avg_L1_norm_grad         0.000023  w[0]   -0.069 bias    1.013\n",
      "iter 5701/1000000  loss         0.179505  avg_L1_norm_grad         0.000023  w[0]   -0.069 bias    1.013\n",
      "iter 5800/1000000  loss         0.179486  avg_L1_norm_grad         0.000022  w[0]   -0.069 bias    1.014\n",
      "iter 5801/1000000  loss         0.179485  avg_L1_norm_grad         0.000022  w[0]   -0.069 bias    1.014\n",
      "iter 5900/1000000  loss         0.179467  avg_L1_norm_grad         0.000021  w[0]   -0.069 bias    1.014\n",
      "iter 5901/1000000  loss         0.179467  avg_L1_norm_grad         0.000021  w[0]   -0.069 bias    1.014\n",
      "iter 6000/1000000  loss         0.179450  avg_L1_norm_grad         0.000020  w[0]   -0.069 bias    1.015\n",
      "iter 6001/1000000  loss         0.179450  avg_L1_norm_grad         0.000020  w[0]   -0.069 bias    1.015\n",
      "iter 6100/1000000  loss         0.179434  avg_L1_norm_grad         0.000020  w[0]   -0.069 bias    1.016\n",
      "iter 6101/1000000  loss         0.179434  avg_L1_norm_grad         0.000020  w[0]   -0.069 bias    1.016\n",
      "iter 6200/1000000  loss         0.179419  avg_L1_norm_grad         0.000019  w[0]   -0.070 bias    1.017\n",
      "iter 6201/1000000  loss         0.179419  avg_L1_norm_grad         0.000019  w[0]   -0.070 bias    1.017\n",
      "iter 6300/1000000  loss         0.179405  avg_L1_norm_grad         0.000018  w[0]   -0.070 bias    1.018\n",
      "iter 6301/1000000  loss         0.179405  avg_L1_norm_grad         0.000018  w[0]   -0.070 bias    1.018\n",
      "iter 6400/1000000  loss         0.179392  avg_L1_norm_grad         0.000017  w[0]   -0.070 bias    1.019\n",
      "iter 6401/1000000  loss         0.179392  avg_L1_norm_grad         0.000017  w[0]   -0.070 bias    1.019\n",
      "iter 6500/1000000  loss         0.179379  avg_L1_norm_grad         0.000017  w[0]   -0.070 bias    1.020\n",
      "iter 6501/1000000  loss         0.179379  avg_L1_norm_grad         0.000017  w[0]   -0.070 bias    1.020\n",
      "iter 6600/1000000  loss         0.179368  avg_L1_norm_grad         0.000016  w[0]   -0.070 bias    1.021\n",
      "iter 6601/1000000  loss         0.179368  avg_L1_norm_grad         0.000016  w[0]   -0.070 bias    1.021\n",
      "iter 6700/1000000  loss         0.179357  avg_L1_norm_grad         0.000016  w[0]   -0.070 bias    1.022\n",
      "iter 6701/1000000  loss         0.179357  avg_L1_norm_grad         0.000016  w[0]   -0.070 bias    1.022\n",
      "iter 6800/1000000  loss         0.179346  avg_L1_norm_grad         0.000015  w[0]   -0.070 bias    1.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6801/1000000  loss         0.179346  avg_L1_norm_grad         0.000015  w[0]   -0.070 bias    1.022\n",
      "iter 6900/1000000  loss         0.179337  avg_L1_norm_grad         0.000014  w[0]   -0.070 bias    1.023\n",
      "iter 6901/1000000  loss         0.179337  avg_L1_norm_grad         0.000014  w[0]   -0.070 bias    1.023\n",
      "iter 7000/1000000  loss         0.179327  avg_L1_norm_grad         0.000014  w[0]   -0.070 bias    1.024\n",
      "iter 7001/1000000  loss         0.179327  avg_L1_norm_grad         0.000014  w[0]   -0.070 bias    1.024\n",
      "iter 7100/1000000  loss         0.179319  avg_L1_norm_grad         0.000013  w[0]   -0.070 bias    1.025\n",
      "iter 7101/1000000  loss         0.179319  avg_L1_norm_grad         0.000013  w[0]   -0.070 bias    1.025\n",
      "iter 7200/1000000  loss         0.179311  avg_L1_norm_grad         0.000013  w[0]   -0.070 bias    1.026\n",
      "iter 7201/1000000  loss         0.179311  avg_L1_norm_grad         0.000013  w[0]   -0.070 bias    1.026\n",
      "iter 7300/1000000  loss         0.179303  avg_L1_norm_grad         0.000012  w[0]   -0.070 bias    1.026\n",
      "iter 7301/1000000  loss         0.179303  avg_L1_norm_grad         0.000012  w[0]   -0.070 bias    1.026\n",
      "iter 7400/1000000  loss         0.179296  avg_L1_norm_grad         0.000012  w[0]   -0.070 bias    1.027\n",
      "iter 7401/1000000  loss         0.179296  avg_L1_norm_grad         0.000012  w[0]   -0.070 bias    1.027\n",
      "iter 7500/1000000  loss         0.179289  avg_L1_norm_grad         0.000011  w[0]   -0.070 bias    1.028\n",
      "iter 7501/1000000  loss         0.179289  avg_L1_norm_grad         0.000011  w[0]   -0.070 bias    1.028\n",
      "iter 7600/1000000  loss         0.179282  avg_L1_norm_grad         0.000011  w[0]   -0.070 bias    1.029\n",
      "iter 7601/1000000  loss         0.179282  avg_L1_norm_grad         0.000011  w[0]   -0.070 bias    1.029\n",
      "iter 7700/1000000  loss         0.179276  avg_L1_norm_grad         0.000011  w[0]   -0.070 bias    1.029\n",
      "iter 7701/1000000  loss         0.179276  avg_L1_norm_grad         0.000011  w[0]   -0.070 bias    1.029\n",
      "iter 7800/1000000  loss         0.179270  avg_L1_norm_grad         0.000010  w[0]   -0.070 bias    1.030\n",
      "iter 7801/1000000  loss         0.179270  avg_L1_norm_grad         0.000010  w[0]   -0.070 bias    1.030\n",
      "iter 7900/1000000  loss         0.179265  avg_L1_norm_grad         0.000010  w[0]   -0.070 bias    1.031\n",
      "iter 7901/1000000  loss         0.179265  avg_L1_norm_grad         0.000010  w[0]   -0.070 bias    1.031\n",
      "iter 8000/1000000  loss         0.179260  avg_L1_norm_grad         0.000010  w[0]   -0.070 bias    1.031\n",
      "iter 8001/1000000  loss         0.179260  avg_L1_norm_grad         0.000010  w[0]   -0.070 bias    1.031\n",
      "iter 8100/1000000  loss         0.179255  avg_L1_norm_grad         0.000009  w[0]   -0.070 bias    1.032\n",
      "iter 8101/1000000  loss         0.179255  avg_L1_norm_grad         0.000009  w[0]   -0.070 bias    1.032\n",
      "iter 8200/1000000  loss         0.179250  avg_L1_norm_grad         0.000009  w[0]   -0.070 bias    1.033\n",
      "iter 8201/1000000  loss         0.179250  avg_L1_norm_grad         0.000009  w[0]   -0.070 bias    1.033\n",
      "iter 8300/1000000  loss         0.179246  avg_L1_norm_grad         0.000009  w[0]   -0.071 bias    1.034\n",
      "iter 8301/1000000  loss         0.179246  avg_L1_norm_grad         0.000009  w[0]   -0.071 bias    1.034\n",
      "iter 8400/1000000  loss         0.179242  avg_L1_norm_grad         0.000008  w[0]   -0.071 bias    1.034\n",
      "iter 8401/1000000  loss         0.179242  avg_L1_norm_grad         0.000008  w[0]   -0.071 bias    1.034\n",
      "iter 8500/1000000  loss         0.179238  avg_L1_norm_grad         0.000008  w[0]   -0.071 bias    1.035\n",
      "iter 8501/1000000  loss         0.179238  avg_L1_norm_grad         0.000008  w[0]   -0.071 bias    1.035\n",
      "iter 8600/1000000  loss         0.179234  avg_L1_norm_grad         0.000008  w[0]   -0.071 bias    1.036\n",
      "iter 8601/1000000  loss         0.179234  avg_L1_norm_grad         0.000008  w[0]   -0.071 bias    1.036\n",
      "iter 8700/1000000  loss         0.179230  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.036\n",
      "iter 8701/1000000  loss         0.179230  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.036\n",
      "iter 8800/1000000  loss         0.179227  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.037\n",
      "iter 8801/1000000  loss         0.179227  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.037\n",
      "iter 8900/1000000  loss         0.179224  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.037\n",
      "iter 8901/1000000  loss         0.179224  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.037\n",
      "iter 9000/1000000  loss         0.179221  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.038\n",
      "iter 9001/1000000  loss         0.179221  avg_L1_norm_grad         0.000007  w[0]   -0.071 bias    1.038\n",
      "iter 9100/1000000  loss         0.179218  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.039\n",
      "iter 9101/1000000  loss         0.179218  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.039\n",
      "iter 9200/1000000  loss         0.179215  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.039\n",
      "iter 9201/1000000  loss         0.179215  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.039\n",
      "iter 9300/1000000  loss         0.179212  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.040\n",
      "iter 9301/1000000  loss         0.179212  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.040\n",
      "iter 9400/1000000  loss         0.179210  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.041\n",
      "iter 9401/1000000  loss         0.179210  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.041\n",
      "iter 9500/1000000  loss         0.179208  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.041\n",
      "iter 9501/1000000  loss         0.179208  avg_L1_norm_grad         0.000006  w[0]   -0.071 bias    1.041\n",
      "iter 9600/1000000  loss         0.179205  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.042\n",
      "iter 9601/1000000  loss         0.179205  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.042\n",
      "iter 9700/1000000  loss         0.179203  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.042\n",
      "iter 9701/1000000  loss         0.179203  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.042\n",
      "iter 9800/1000000  loss         0.179201  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.043\n",
      "iter 9801/1000000  loss         0.179201  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.043\n",
      "iter 9900/1000000  loss         0.179199  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.044\n",
      "iter 9901/1000000  loss         0.179199  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.044\n",
      "iter 10000/1000000  loss         0.179197  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.044\n",
      "iter 10001/1000000  loss         0.179197  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.044\n",
      "iter 10100/1000000  loss         0.179196  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.045\n",
      "iter 10101/1000000  loss         0.179196  avg_L1_norm_grad         0.000005  w[0]   -0.071 bias    1.045\n",
      "iter 10200/1000000  loss         0.179194  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.045\n",
      "iter 10201/1000000  loss         0.179194  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.045\n",
      "iter 10300/1000000  loss         0.179192  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.046\n",
      "iter 10301/1000000  loss         0.179192  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.046\n",
      "iter 10400/1000000  loss         0.179191  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.046\n",
      "iter 10401/1000000  loss         0.179191  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.046\n",
      "iter 10500/1000000  loss         0.179189  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.047\n",
      "iter 10501/1000000  loss         0.179189  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.047\n",
      "iter 10600/1000000  loss         0.179188  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.047\n",
      "iter 10601/1000000  loss         0.179188  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.047\n",
      "iter 10700/1000000  loss         0.179187  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10701/1000000  loss         0.179187  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.048\n",
      "iter 10800/1000000  loss         0.179185  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.048\n",
      "iter 10801/1000000  loss         0.179185  avg_L1_norm_grad         0.000004  w[0]   -0.071 bias    1.048\n",
      "iter 10900/1000000  loss         0.179184  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.049\n",
      "iter 10901/1000000  loss         0.179184  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.049\n",
      "iter 11000/1000000  loss         0.179183  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.049\n",
      "iter 11001/1000000  loss         0.179183  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.049\n",
      "iter 11100/1000000  loss         0.179182  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.050\n",
      "iter 11101/1000000  loss         0.179182  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.050\n",
      "iter 11200/1000000  loss         0.179181  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.050\n",
      "iter 11201/1000000  loss         0.179181  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.050\n",
      "iter 11300/1000000  loss         0.179180  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.051\n",
      "iter 11301/1000000  loss         0.179180  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.051\n",
      "iter 11400/1000000  loss         0.179179  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.051\n",
      "iter 11401/1000000  loss         0.179179  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.051\n",
      "iter 11500/1000000  loss         0.179178  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.052\n",
      "iter 11501/1000000  loss         0.179178  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.052\n",
      "iter 11600/1000000  loss         0.179177  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.052\n",
      "iter 11601/1000000  loss         0.179177  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.052\n",
      "iter 11700/1000000  loss         0.179176  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.053\n",
      "iter 11701/1000000  loss         0.179176  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.053\n",
      "iter 11800/1000000  loss         0.179176  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.053\n",
      "iter 11801/1000000  loss         0.179176  avg_L1_norm_grad         0.000003  w[0]   -0.071 bias    1.053\n",
      "iter 11900/1000000  loss         0.179175  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.054\n",
      "iter 11901/1000000  loss         0.179175  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.054\n",
      "iter 12000/1000000  loss         0.179174  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.054\n",
      "iter 12001/1000000  loss         0.179174  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.054\n",
      "iter 12100/1000000  loss         0.179173  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.054\n",
      "iter 12101/1000000  loss         0.179173  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.054\n",
      "iter 12200/1000000  loss         0.179173  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.055\n",
      "iter 12201/1000000  loss         0.179173  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.055\n",
      "iter 12300/1000000  loss         0.179172  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.055\n",
      "iter 12301/1000000  loss         0.179172  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.055\n",
      "iter 12400/1000000  loss         0.179171  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.056\n",
      "iter 12401/1000000  loss         0.179171  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.056\n",
      "iter 12500/1000000  loss         0.179171  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.056\n",
      "iter 12501/1000000  loss         0.179171  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.056\n",
      "iter 12600/1000000  loss         0.179170  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.057\n",
      "iter 12601/1000000  loss         0.179170  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.057\n",
      "iter 12700/1000000  loss         0.179170  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.057\n",
      "iter 12701/1000000  loss         0.179170  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.057\n",
      "iter 12800/1000000  loss         0.179169  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.057\n",
      "iter 12801/1000000  loss         0.179169  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.057\n",
      "iter 12900/1000000  loss         0.179169  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.058\n",
      "iter 12901/1000000  loss         0.179169  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.058\n",
      "iter 13000/1000000  loss         0.179168  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.058\n",
      "iter 13001/1000000  loss         0.179168  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.058\n",
      "iter 13100/1000000  loss         0.179168  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.059\n",
      "iter 13101/1000000  loss         0.179168  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.059\n",
      "iter 13200/1000000  loss         0.179167  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.059\n",
      "iter 13201/1000000  loss         0.179167  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.059\n",
      "iter 13300/1000000  loss         0.179167  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.059\n",
      "iter 13301/1000000  loss         0.179167  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.059\n",
      "iter 13400/1000000  loss         0.179167  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.060\n",
      "iter 13401/1000000  loss         0.179167  avg_L1_norm_grad         0.000002  w[0]   -0.071 bias    1.060\n",
      "iter 13500/1000000  loss         0.179166  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.060\n",
      "iter 13501/1000000  loss         0.179166  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.060\n",
      "iter 13600/1000000  loss         0.179166  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.060\n",
      "iter 13601/1000000  loss         0.179166  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.060\n",
      "iter 13700/1000000  loss         0.179165  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.061\n",
      "iter 13701/1000000  loss         0.179165  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.061\n",
      "iter 13800/1000000  loss         0.179165  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.061\n",
      "iter 13801/1000000  loss         0.179165  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.061\n",
      "iter 13900/1000000  loss         0.179165  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.061\n",
      "iter 13901/1000000  loss         0.179165  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.061\n",
      "iter 14000/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.062\n",
      "iter 14001/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.062\n",
      "iter 14100/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.062\n",
      "iter 14101/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.062\n",
      "iter 14200/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.062\n",
      "iter 14201/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.062\n",
      "iter 14300/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.063\n",
      "iter 14301/1000000  loss         0.179164  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.063\n",
      "iter 14400/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.063\n",
      "iter 14401/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.063\n",
      "iter 14500/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.063\n",
      "iter 14501/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.063\n",
      "iter 14600/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14601/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.064\n",
      "iter 14700/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.064\n",
      "iter 14701/1000000  loss         0.179163  avg_L1_norm_grad         0.000001  w[0]   -0.071 bias    1.064\n",
      "Done. Converged after 14702 iterations.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_lr22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6029c1cc7546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnew_lr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRGDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnew_lr2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_hat_New\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_lr22\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_va\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalc_TP_TN_FP_FN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat_New\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_lr22' is not defined"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr2 = LRGDF(alpha=100.0, step_size=0.1)\n",
    "new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Rate Loaded\n",
      "TurnOnOnce Rate Loaded\n",
      "Ave Loaded\n",
      "New Accuracy 0.9623888888888862\n"
     ]
    }
   ],
   "source": [
    "y_hat_New=np.asarray(new_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn, FPSample, FNSample=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Alpha=100\n",
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "plt.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "plt.plot(fpr3te,tpr3te, label=\"MinMax Trans. Adding noise, None Linear\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "plt.plot(fpr1Tte,tpr1Tte, label=\"No added Feature No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "plt.plot(fprTte,tprTte, label=\"MinMax Trans. Adding ave and None Linear\")\n",
    "\n",
    "plt.xlim([-0.0, 1.0]);\n",
    "plt.ylim([-0.0, 1.0]);\n",
    "plt.legend();\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_FP_9=x_va[FPSample]\n",
    "y_FP_9=y_va[FPSample]\n",
    "x_FN_9=x_va[FNSample]\n",
    "y_FN_9=y_va[FNSample]\n",
    "print(\"False Positive\")\n",
    "show_images(x_FP_9, y_FP_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"False Negative\")\n",
    "show_images(x_FN_9, y_FN_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Rate Loaded\n",
      "TurnOnOnce Rate Loaded\n",
      "Ave Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr2.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.455 0.9926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.57578915e-01  2.07896230e+00 -8.09081608e-01 -3.56350488e-04\n",
      "  1.71354425e-03 -7.48898383e-03]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "w1=new_lr2.w_G[-7:-1]\n",
    "print(w1)\n",
    "print(w1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
