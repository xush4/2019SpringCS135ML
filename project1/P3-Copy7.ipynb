{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x.copy()\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(0,10)):\n",
    "                pos=randint(0,N)\n",
    "                if (x[i,pos]<0.1):\n",
    "                    x_j[i, pos]=0.0098\n",
    "                else:\n",
    "                    x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "#orig_lr2 = LRGD(alpha=10.0, step_size=0.1)\n",
    "#orig_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_Origin=np.asarray(orig_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "#tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "#acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "#print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "#new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "#new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## self.feature_transform\n",
    " self.feature_transform_pipeline = sklearn.pipeline.Pipeline(\n",
    "                    [('rescaler', sklearn.preprocessing.MinMaxScaler()),  \n",
    "                    ('feature_transform', sklearn.pipeline.FeatureUnion(transformer_list=[  \n",
    "                    ('original_x', sklearn.preprocessing.PolynomialFeatures(degree=1, include_bias=False)),  \n",
    "                    ('TurnOn_x', TurnOnFeatureExtractor()),  \n",
    "                    ('TurnOn_x2', TurnOnOnceFeatureExtractor()),  \n",
    "                    ('TurnOn_all', TurnOnAllFeatureExtractor()),  \n",
    "                    ('TurnOn_y', TurnOnFeatureExtractorY()),  \n",
    "                    ('TurnOn_y2', TurnOnOnceFeatureExtractorY()),  \n",
    "                    ]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax Transform\n",
      "TurnOn Loaded 0.0\n",
      "(1, 8400)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Shape of Transformed Data (8400, 790)\n",
      "Initializing w_G with 790 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.031943  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.897277  avg_L1_norm_grad         0.025909  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.826493  avg_L1_norm_grad         0.019039  w[0]    0.000 bias    0.021\n",
      "iter    3/1000000  loss         0.779316  avg_L1_norm_grad         0.015787  w[0]    0.000 bias    0.026\n",
      "iter    4/1000000  loss         0.744214  avg_L1_norm_grad         0.012704  w[0]    0.000 bias    0.041\n",
      "iter    5/1000000  loss         0.717152  avg_L1_norm_grad         0.011393  w[0]    0.000 bias    0.049\n",
      "iter    6/1000000  loss         0.694871  avg_L1_norm_grad         0.010064  w[0]    0.000 bias    0.060\n",
      "iter    7/1000000  loss         0.675765  avg_L1_norm_grad         0.009375  w[0]    0.000 bias    0.069\n",
      "iter    8/1000000  loss         0.658876  avg_L1_norm_grad         0.008703  w[0]    0.000 bias    0.079\n",
      "iter    9/1000000  loss         0.643646  avg_L1_norm_grad         0.008229  w[0]    0.000 bias    0.088\n",
      "iter   10/1000000  loss         0.629721  avg_L1_norm_grad         0.007799  w[0]    0.000 bias    0.097\n",
      "iter   11/1000000  loss         0.616861  avg_L1_norm_grad         0.007447  w[0]    0.000 bias    0.105\n",
      "iter   12/1000000  loss         0.604897  avg_L1_norm_grad         0.007137  w[0]    0.000 bias    0.114\n",
      "iter   13/1000000  loss         0.593703  avg_L1_norm_grad         0.006862  w[0]    0.000 bias    0.122\n",
      "iter   14/1000000  loss         0.583183  avg_L1_norm_grad         0.006620  w[0]    0.000 bias    0.130\n",
      "iter   15/1000000  loss         0.573261  avg_L1_norm_grad         0.006400  w[0]    0.000 bias    0.138\n",
      "iter   16/1000000  loss         0.563877  avg_L1_norm_grad         0.006200  w[0]    0.000 bias    0.146\n",
      "iter   17/1000000  loss         0.554981  avg_L1_norm_grad         0.006016  w[0]    0.000 bias    0.153\n",
      "iter   18/1000000  loss         0.546529  avg_L1_norm_grad         0.005845  w[0]    0.000 bias    0.161\n",
      "iter   19/1000000  loss         0.538488  avg_L1_norm_grad         0.005686  w[0]    0.000 bias    0.168\n",
      "iter  100/1000000  loss         0.318718  avg_L1_norm_grad         0.001938  w[0]    0.000 bias    0.504\n",
      "iter  101/1000000  loss         0.317756  avg_L1_norm_grad         0.001923  w[0]    0.000 bias    0.507\n",
      "iter  200/1000000  loss         0.263269  avg_L1_norm_grad         0.001145  w[0]    0.000 bias    0.685\n",
      "iter  201/1000000  loss         0.262949  avg_L1_norm_grad         0.001141  w[0]    0.000 bias    0.687\n",
      "iter  300/1000000  loss         0.240456  avg_L1_norm_grad         0.000834  w[0]    0.000 bias    0.785\n",
      "iter  301/1000000  loss         0.240293  avg_L1_norm_grad         0.000832  w[0]    0.000 bias    0.786\n",
      "iter  400/1000000  loss         0.227712  avg_L1_norm_grad         0.000665  w[0]    0.000 bias    0.848\n",
      "iter  401/1000000  loss         0.227612  avg_L1_norm_grad         0.000664  w[0]    0.000 bias    0.849\n",
      "iter  500/1000000  loss         0.219456  avg_L1_norm_grad         0.000557  w[0]    0.000 bias    0.890\n",
      "iter  501/1000000  loss         0.219387  avg_L1_norm_grad         0.000556  w[0]    0.000 bias    0.890\n",
      "iter  600/1000000  loss         0.213612  avg_L1_norm_grad         0.000482  w[0]    0.000 bias    0.918\n",
      "iter  601/1000000  loss         0.213562  avg_L1_norm_grad         0.000481  w[0]    0.000 bias    0.918\n",
      "iter  700/1000000  loss         0.209227  avg_L1_norm_grad         0.000426  w[0]    0.000 bias    0.938\n",
      "iter  701/1000000  loss         0.209188  avg_L1_norm_grad         0.000425  w[0]    0.000 bias    0.938\n",
      "iter  800/1000000  loss         0.205798  avg_L1_norm_grad         0.000382  w[0]    0.000 bias    0.952\n",
      "iter  801/1000000  loss         0.205768  avg_L1_norm_grad         0.000382  w[0]    0.000 bias    0.952\n",
      "iter  900/1000000  loss         0.203036  avg_L1_norm_grad         0.000347  w[0]    0.000 bias    0.962\n",
      "iter  901/1000000  loss         0.203011  avg_L1_norm_grad         0.000346  w[0]    0.000 bias    0.962\n",
      "iter 1000/1000000  loss         0.200758  avg_L1_norm_grad         0.000317  w[0]    0.000 bias    0.969\n",
      "iter 1001/1000000  loss         0.200737  avg_L1_norm_grad         0.000317  w[0]    0.000 bias    0.969\n",
      "iter 1100/1000000  loss         0.198846  avg_L1_norm_grad         0.000293  w[0]    0.000 bias    0.974\n",
      "iter 1101/1000000  loss         0.198829  avg_L1_norm_grad         0.000292  w[0]    0.000 bias    0.974\n",
      "iter 1200/1000000  loss         0.197219  avg_L1_norm_grad         0.000271  w[0]    0.000 bias    0.977\n",
      "iter 1201/1000000  loss         0.197204  avg_L1_norm_grad         0.000271  w[0]    0.000 bias    0.977\n",
      "iter 1300/1000000  loss         0.195818  avg_L1_norm_grad         0.000253  w[0]    0.000 bias    0.979\n",
      "iter 1301/1000000  loss         0.195804  avg_L1_norm_grad         0.000253  w[0]    0.000 bias    0.979\n",
      "iter 1400/1000000  loss         0.194599  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    0.980\n",
      "iter 1401/1000000  loss         0.194588  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    0.980\n",
      "iter 1500/1000000  loss         0.193532  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    0.981\n",
      "iter 1501/1000000  loss         0.193522  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    0.981\n",
      "iter 1600/1000000  loss         0.192590  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    0.981\n",
      "iter 1601/1000000  loss         0.192582  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    0.981\n",
      "iter 1700/1000000  loss         0.191755  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    0.981\n",
      "iter 1701/1000000  loss         0.191747  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    0.981\n",
      "iter 1800/1000000  loss         0.191010  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    0.981\n",
      "iter 1801/1000000  loss         0.191003  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    0.981\n",
      "iter 1900/1000000  loss         0.190343  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    0.981\n",
      "iter 1901/1000000  loss         0.190336  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    0.981\n",
      "iter 2000/1000000  loss         0.189742  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    0.981\n",
      "iter 2001/1000000  loss         0.189737  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    0.981\n",
      "iter 2100/1000000  loss         0.189201  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    0.981\n",
      "iter 2101/1000000  loss         0.189196  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    0.981\n",
      "iter 2200/1000000  loss         0.188710  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    0.980\n",
      "iter 2201/1000000  loss         0.188706  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    0.980\n",
      "iter 2300/1000000  loss         0.188265  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    0.980\n",
      "iter 2301/1000000  loss         0.188261  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    0.980\n",
      "iter 2400/1000000  loss         0.187860  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    0.980\n",
      "iter 2401/1000000  loss         0.187856  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    0.980\n",
      "iter 2500/1000000  loss         0.187490  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    0.980\n",
      "iter 2501/1000000  loss         0.187486  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    0.980\n",
      "iter 2600/1000000  loss         0.187151  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    0.980\n",
      "iter 2601/1000000  loss         0.187148  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    0.980\n",
      "iter 2700/1000000  loss         0.186841  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    0.980\n",
      "iter 2701/1000000  loss         0.186838  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    0.980\n",
      "iter 2800/1000000  loss         0.186556  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    0.981\n",
      "iter 2801/1000000  loss         0.186553  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    0.981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.186294  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    0.981\n",
      "iter 2901/1000000  loss         0.186291  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    0.981\n",
      "iter 3000/1000000  loss         0.186052  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    0.981\n",
      "iter 3001/1000000  loss         0.186050  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    0.981\n",
      "iter 3100/1000000  loss         0.185829  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    0.982\n",
      "iter 3101/1000000  loss         0.185827  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    0.982\n",
      "iter 3200/1000000  loss         0.185623  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    0.982\n",
      "iter 3201/1000000  loss         0.185621  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    0.982\n",
      "iter 3300/1000000  loss         0.185432  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    0.983\n",
      "iter 3301/1000000  loss         0.185430  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    0.983\n",
      "iter 3400/1000000  loss         0.185255  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    0.984\n",
      "iter 3401/1000000  loss         0.185253  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    0.984\n",
      "iter 3500/1000000  loss         0.185090  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    0.985\n",
      "iter 3501/1000000  loss         0.185089  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    0.985\n",
      "iter 3600/1000000  loss         0.184937  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    0.985\n",
      "iter 3601/1000000  loss         0.184936  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    0.985\n",
      "iter 3700/1000000  loss         0.184795  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    0.986\n",
      "iter 3701/1000000  loss         0.184794  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    0.986\n",
      "iter 3800/1000000  loss         0.184663  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    0.987\n",
      "iter 3801/1000000  loss         0.184661  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    0.987\n",
      "iter 3900/1000000  loss         0.184539  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    0.988\n",
      "iter 3901/1000000  loss         0.184538  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    0.988\n",
      "iter 4000/1000000  loss         0.184424  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    0.989\n",
      "iter 4001/1000000  loss         0.184423  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    0.989\n",
      "iter 4100/1000000  loss         0.184316  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    0.990\n",
      "iter 4101/1000000  loss         0.184315  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    0.990\n",
      "iter 4200/1000000  loss         0.184216  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    0.992\n",
      "iter 4201/1000000  loss         0.184215  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    0.992\n",
      "iter 4300/1000000  loss         0.184121  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    0.993\n",
      "iter 4301/1000000  loss         0.184120  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    0.993\n",
      "iter 4400/1000000  loss         0.184033  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    0.994\n",
      "iter 4401/1000000  loss         0.184032  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    0.994\n",
      "iter 4500/1000000  loss         0.183950  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    0.995\n",
      "iter 4501/1000000  loss         0.183950  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    0.995\n",
      "iter 4600/1000000  loss         0.183873  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    0.996\n",
      "iter 4601/1000000  loss         0.183872  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    0.996\n",
      "iter 4700/1000000  loss         0.183800  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    0.998\n",
      "iter 4701/1000000  loss         0.183799  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    0.998\n",
      "iter 4800/1000000  loss         0.183732  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    0.999\n",
      "iter 4801/1000000  loss         0.183731  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    0.999\n",
      "iter 4900/1000000  loss         0.183667  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    1.000\n",
      "iter 4901/1000000  loss         0.183667  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    1.000\n",
      "iter 5000/1000000  loss         0.183607  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    1.001\n",
      "iter 5001/1000000  loss         0.183606  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    1.001\n",
      "iter 5100/1000000  loss         0.183550  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    1.003\n",
      "iter 5101/1000000  loss         0.183549  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    1.003\n",
      "iter 5200/1000000  loss         0.183496  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    1.004\n",
      "iter 5201/1000000  loss         0.183496  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    1.004\n",
      "iter 5300/1000000  loss         0.183446  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    1.005\n",
      "iter 5301/1000000  loss         0.183445  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    1.005\n",
      "iter 5400/1000000  loss         0.183398  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    1.007\n",
      "iter 5401/1000000  loss         0.183398  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    1.007\n",
      "iter 5500/1000000  loss         0.183353  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    1.008\n",
      "iter 5501/1000000  loss         0.183353  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    1.008\n",
      "iter 5600/1000000  loss         0.183311  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    1.009\n",
      "iter 5601/1000000  loss         0.183311  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    1.009\n",
      "iter 5700/1000000  loss         0.183271  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    1.011\n",
      "iter 5701/1000000  loss         0.183271  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    1.011\n",
      "iter 5800/1000000  loss         0.183233  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    1.012\n",
      "iter 5801/1000000  loss         0.183233  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    1.012\n",
      "iter 5900/1000000  loss         0.183198  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    1.013\n",
      "iter 5901/1000000  loss         0.183198  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    1.013\n",
      "iter 6000/1000000  loss         0.183164  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    1.015\n",
      "iter 6001/1000000  loss         0.183164  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    1.015\n",
      "iter 6100/1000000  loss         0.183132  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    1.016\n",
      "iter 6101/1000000  loss         0.183132  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    1.016\n",
      "iter 6200/1000000  loss         0.183102  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    1.017\n",
      "iter 6201/1000000  loss         0.183102  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    1.017\n",
      "iter 6300/1000000  loss         0.183074  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    1.019\n",
      "iter 6301/1000000  loss         0.183073  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    1.019\n",
      "iter 6400/1000000  loss         0.183047  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    1.020\n",
      "iter 6401/1000000  loss         0.183047  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    1.020\n",
      "iter 6500/1000000  loss         0.183021  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    1.021\n",
      "iter 6501/1000000  loss         0.183021  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    1.021\n",
      "iter 6600/1000000  loss         0.182997  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    1.022\n",
      "iter 6601/1000000  loss         0.182997  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    1.022\n",
      "iter 6700/1000000  loss         0.182974  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    1.024\n",
      "iter 6701/1000000  loss         0.182974  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    1.024\n",
      "iter 6800/1000000  loss         0.182953  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.025\n",
      "iter 6801/1000000  loss         0.182952  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.182932  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.026\n",
      "iter 6901/1000000  loss         0.182932  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.026\n",
      "iter 7000/1000000  loss         0.182912  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    1.027\n",
      "iter 7001/1000000  loss         0.182912  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    1.027\n",
      "iter 7100/1000000  loss         0.182894  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.029\n",
      "iter 7101/1000000  loss         0.182894  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.029\n",
      "iter 7200/1000000  loss         0.182876  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    1.030\n",
      "iter 7201/1000000  loss         0.182876  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    1.030\n",
      "iter 7300/1000000  loss         0.182860  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.031\n",
      "iter 7301/1000000  loss         0.182860  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.031\n",
      "iter 7400/1000000  loss         0.182844  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.032\n",
      "iter 7401/1000000  loss         0.182844  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.032\n",
      "iter 7500/1000000  loss         0.182829  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.033\n",
      "iter 7501/1000000  loss         0.182829  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.033\n",
      "iter 7600/1000000  loss         0.182815  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.035\n",
      "iter 7601/1000000  loss         0.182815  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.035\n",
      "iter 7700/1000000  loss         0.182801  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.036\n",
      "iter 7701/1000000  loss         0.182801  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.036\n",
      "iter 7800/1000000  loss         0.182788  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.037\n",
      "iter 7801/1000000  loss         0.182788  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.037\n",
      "iter 7900/1000000  loss         0.182776  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.038\n",
      "iter 7901/1000000  loss         0.182776  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.038\n",
      "iter 8000/1000000  loss         0.182764  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.039\n",
      "iter 8001/1000000  loss         0.182764  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.039\n",
      "iter 8100/1000000  loss         0.182753  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.040\n",
      "iter 8101/1000000  loss         0.182753  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.040\n",
      "iter 8200/1000000  loss         0.182743  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.041\n",
      "iter 8201/1000000  loss         0.182743  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.041\n",
      "iter 8300/1000000  loss         0.182733  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.042\n",
      "iter 8301/1000000  loss         0.182733  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.042\n",
      "iter 8400/1000000  loss         0.182723  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.043\n",
      "iter 8401/1000000  loss         0.182723  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.043\n",
      "iter 8500/1000000  loss         0.182714  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.044\n",
      "iter 8501/1000000  loss         0.182714  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.044\n",
      "iter 8600/1000000  loss         0.182706  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.045\n",
      "iter 8601/1000000  loss         0.182706  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.045\n",
      "iter 8700/1000000  loss         0.182698  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.046\n",
      "iter 8701/1000000  loss         0.182698  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.046\n",
      "iter 8800/1000000  loss         0.182690  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.047\n",
      "iter 8801/1000000  loss         0.182690  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.047\n",
      "iter 8900/1000000  loss         0.182682  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.048\n",
      "iter 8901/1000000  loss         0.182682  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.048\n",
      "iter 9000/1000000  loss         0.182675  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.049\n",
      "iter 9001/1000000  loss         0.182675  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.049\n",
      "iter 9100/1000000  loss         0.182669  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.050\n",
      "iter 9101/1000000  loss         0.182669  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.050\n",
      "iter 9200/1000000  loss         0.182662  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.051\n",
      "iter 9201/1000000  loss         0.182662  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.051\n",
      "iter 9300/1000000  loss         0.182656  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.052\n",
      "iter 9301/1000000  loss         0.182656  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.052\n",
      "iter 9400/1000000  loss         0.182650  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.053\n",
      "iter 9401/1000000  loss         0.182650  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.053\n",
      "iter 9500/1000000  loss         0.182645  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.054\n",
      "iter 9501/1000000  loss         0.182645  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.054\n",
      "iter 9600/1000000  loss         0.182639  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.055\n",
      "iter 9601/1000000  loss         0.182639  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.055\n",
      "iter 9700/1000000  loss         0.182634  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.056\n",
      "iter 9701/1000000  loss         0.182634  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.056\n",
      "iter 9800/1000000  loss         0.182630  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.056\n",
      "iter 9801/1000000  loss         0.182630  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.056\n",
      "iter 9900/1000000  loss         0.182625  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.057\n",
      "iter 9901/1000000  loss         0.182625  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.057\n",
      "iter 10000/1000000  loss         0.182621  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.058\n",
      "iter 10001/1000000  loss         0.182621  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.058\n",
      "iter 10100/1000000  loss         0.182616  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.059\n",
      "iter 10101/1000000  loss         0.182616  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.059\n",
      "iter 10200/1000000  loss         0.182612  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.060\n",
      "iter 10201/1000000  loss         0.182612  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.060\n",
      "iter 10300/1000000  loss         0.182609  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.060\n",
      "iter 10301/1000000  loss         0.182609  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.060\n",
      "iter 10400/1000000  loss         0.182605  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.061\n",
      "iter 10401/1000000  loss         0.182605  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.061\n",
      "iter 10500/1000000  loss         0.182602  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.062\n",
      "iter 10501/1000000  loss         0.182602  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.062\n",
      "iter 10600/1000000  loss         0.182598  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.063\n",
      "iter 10601/1000000  loss         0.182598  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.063\n",
      "iter 10700/1000000  loss         0.182595  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.063\n",
      "iter 10701/1000000  loss         0.182595  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.182592  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.064\n",
      "iter 10801/1000000  loss         0.182592  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.064\n",
      "iter 10900/1000000  loss         0.182589  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.065\n",
      "iter 10901/1000000  loss         0.182589  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.065\n",
      "iter 11000/1000000  loss         0.182587  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.066\n",
      "iter 11001/1000000  loss         0.182587  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.066\n",
      "iter 11100/1000000  loss         0.182584  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.066\n",
      "iter 11101/1000000  loss         0.182584  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.066\n",
      "iter 11200/1000000  loss         0.182581  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.067\n",
      "iter 11201/1000000  loss         0.182581  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.067\n",
      "iter 11300/1000000  loss         0.182579  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.068\n",
      "iter 11301/1000000  loss         0.182579  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.068\n",
      "iter 11400/1000000  loss         0.182577  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.068\n",
      "iter 11401/1000000  loss         0.182577  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.068\n",
      "iter 11500/1000000  loss         0.182575  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.069\n",
      "iter 11501/1000000  loss         0.182575  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.069\n",
      "iter 11600/1000000  loss         0.182573  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.070\n",
      "iter 11601/1000000  loss         0.182573  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.070\n",
      "iter 11700/1000000  loss         0.182571  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.070\n",
      "iter 11701/1000000  loss         0.182571  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.070\n",
      "iter 11800/1000000  loss         0.182569  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.071\n",
      "iter 11801/1000000  loss         0.182569  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.071\n",
      "iter 11900/1000000  loss         0.182567  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.071\n",
      "iter 11901/1000000  loss         0.182567  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.071\n",
      "iter 12000/1000000  loss         0.182565  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.072\n",
      "iter 12001/1000000  loss         0.182565  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.072\n",
      "iter 12100/1000000  loss         0.182563  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.073\n",
      "iter 12101/1000000  loss         0.182563  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.073\n",
      "iter 12200/1000000  loss         0.182562  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.073\n",
      "iter 12201/1000000  loss         0.182562  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.073\n",
      "iter 12300/1000000  loss         0.182560  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.074\n",
      "iter 12301/1000000  loss         0.182560  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.074\n",
      "iter 12400/1000000  loss         0.182559  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.074\n",
      "iter 12401/1000000  loss         0.182559  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.074\n",
      "iter 12500/1000000  loss         0.182558  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.075\n",
      "iter 12501/1000000  loss         0.182558  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.075\n",
      "iter 12600/1000000  loss         0.182556  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.075\n",
      "iter 12601/1000000  loss         0.182556  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.075\n",
      "iter 12700/1000000  loss         0.182555  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.076\n",
      "iter 12701/1000000  loss         0.182555  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.076\n",
      "iter 12800/1000000  loss         0.182554  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.076\n",
      "iter 12801/1000000  loss         0.182554  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.076\n",
      "iter 12900/1000000  loss         0.182553  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.077\n",
      "iter 12901/1000000  loss         0.182553  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.077\n",
      "iter 13000/1000000  loss         0.182552  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.077\n",
      "iter 13001/1000000  loss         0.182551  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.077\n",
      "iter 13100/1000000  loss         0.182550  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.078\n",
      "iter 13101/1000000  loss         0.182550  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.078\n",
      "iter 13200/1000000  loss         0.182549  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.078\n",
      "iter 13201/1000000  loss         0.182549  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.078\n",
      "iter 13300/1000000  loss         0.182548  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.079\n",
      "iter 13301/1000000  loss         0.182548  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.079\n",
      "iter 13400/1000000  loss         0.182548  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.079\n",
      "iter 13401/1000000  loss         0.182548  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.079\n",
      "iter 13500/1000000  loss         0.182547  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.080\n",
      "iter 13501/1000000  loss         0.182547  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.080\n",
      "iter 13600/1000000  loss         0.182546  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.080\n",
      "iter 13601/1000000  loss         0.182546  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.080\n",
      "iter 13700/1000000  loss         0.182545  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.081\n",
      "iter 13701/1000000  loss         0.182545  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    1.081\n",
      "Done. Converged after 13756 iterations.\n",
      "Shape of Transformed Data (8400, 785)\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.031696  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.906516  avg_L1_norm_grad         0.026254  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.841723  avg_L1_norm_grad         0.018679  w[0]    0.000 bias    0.022\n",
      "iter    3/1000000  loss         0.799066  avg_L1_norm_grad         0.015978  w[0]    0.000 bias    0.028\n",
      "iter    4/1000000  loss         0.767438  avg_L1_norm_grad         0.012957  w[0]    0.000 bias    0.044\n",
      "iter    5/1000000  loss         0.742949  avg_L1_norm_grad         0.011840  w[0]    0.000 bias    0.053\n",
      "iter    6/1000000  loss         0.722724  avg_L1_norm_grad         0.010529  w[0]    0.000 bias    0.065\n",
      "iter    7/1000000  loss         0.705326  avg_L1_norm_grad         0.009883  w[0]    0.000 bias    0.075\n",
      "iter    8/1000000  loss         0.689909  avg_L1_norm_grad         0.009207  w[0]    0.000 bias    0.086\n",
      "iter    9/1000000  loss         0.675973  avg_L1_norm_grad         0.008730  w[0]    0.000 bias    0.096\n",
      "iter   10/1000000  loss         0.663194  avg_L1_norm_grad         0.008304  w[0]    0.000 bias    0.107\n",
      "iter   11/1000000  loss         0.651355  avg_L1_norm_grad         0.007948  w[0]    0.000 bias    0.117\n",
      "iter   12/1000000  loss         0.640302  avg_L1_norm_grad         0.007636  w[0]    0.000 bias    0.126\n",
      "iter   13/1000000  loss         0.629922  avg_L1_norm_grad         0.007362  w[0]    0.000 bias    0.136\n",
      "iter   14/1000000  loss         0.620128  avg_L1_norm_grad         0.007119  w[0]    0.000 bias    0.145\n",
      "iter   15/1000000  loss         0.610855  avg_L1_norm_grad         0.006899  w[0]    0.000 bias    0.155\n",
      "iter   16/1000000  loss         0.602049  avg_L1_norm_grad         0.006696  w[0]    0.000 bias    0.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   17/1000000  loss         0.593667  avg_L1_norm_grad         0.006510  w[0]    0.000 bias    0.173\n",
      "iter   18/1000000  loss         0.585674  avg_L1_norm_grad         0.006337  w[0]    0.000 bias    0.182\n",
      "iter   19/1000000  loss         0.578038  avg_L1_norm_grad         0.006178  w[0]    0.000 bias    0.191\n",
      "iter  100/1000000  loss         0.354226  avg_L1_norm_grad         0.002238  w[0]    0.000 bias    0.660\n",
      "iter  101/1000000  loss         0.353157  avg_L1_norm_grad         0.002221  w[0]    0.000 bias    0.664\n",
      "iter  200/1000000  loss         0.290401  avg_L1_norm_grad         0.001318  w[0]    0.000 bias    0.983\n",
      "iter  201/1000000  loss         0.290019  avg_L1_norm_grad         0.001313  w[0]    0.000 bias    0.985\n",
      "iter  300/1000000  loss         0.262677  avg_L1_norm_grad         0.000950  w[0]    0.000 bias    1.201\n",
      "iter  301/1000000  loss         0.262477  avg_L1_norm_grad         0.000947  w[0]    0.000 bias    1.202\n",
      "iter  400/1000000  loss         0.246901  avg_L1_norm_grad         0.000749  w[0]    0.000 bias    1.363\n",
      "iter  401/1000000  loss         0.246778  avg_L1_norm_grad         0.000748  w[0]    0.000 bias    1.364\n",
      "iter  500/1000000  loss         0.236682  avg_L1_norm_grad         0.000622  w[0]    0.000 bias    1.490\n",
      "iter  501/1000000  loss         0.236598  avg_L1_norm_grad         0.000621  w[0]    0.000 bias    1.491\n",
      "iter  600/1000000  loss         0.229525  avg_L1_norm_grad         0.000534  w[0]    0.000 bias    1.593\n",
      "iter  601/1000000  loss         0.229464  avg_L1_norm_grad         0.000533  w[0]    0.000 bias    1.593\n",
      "iter  700/1000000  loss         0.224242  avg_L1_norm_grad         0.000468  w[0]    0.000 bias    1.677\n",
      "iter  701/1000000  loss         0.224196  avg_L1_norm_grad         0.000467  w[0]    0.000 bias    1.678\n",
      "iter  800/1000000  loss         0.220191  avg_L1_norm_grad         0.000416  w[0]    0.000 bias    1.748\n",
      "iter  801/1000000  loss         0.220156  avg_L1_norm_grad         0.000416  w[0]    0.000 bias    1.749\n",
      "iter  900/1000000  loss         0.216996  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.808\n",
      "iter  901/1000000  loss         0.216967  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.809\n",
      "iter 1000/1000000  loss         0.214417  avg_L1_norm_grad         0.000340  w[0]    0.000 bias    1.859\n",
      "iter 1001/1000000  loss         0.214394  avg_L1_norm_grad         0.000339  w[0]    0.000 bias    1.860\n",
      "iter 1100/1000000  loss         0.212299  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    1.903\n",
      "iter 1101/1000000  loss         0.212280  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    1.904\n",
      "iter 1200/1000000  loss         0.210533  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    1.942\n",
      "iter 1201/1000000  loss         0.210517  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    1.942\n",
      "iter 1300/1000000  loss         0.209043  avg_L1_norm_grad         0.000264  w[0]    0.000 bias    1.975\n",
      "iter 1301/1000000  loss         0.209029  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    1.975\n",
      "iter 1400/1000000  loss         0.207772  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.004\n",
      "iter 1401/1000000  loss         0.207760  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.004\n",
      "iter 1500/1000000  loss         0.206677  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.029\n",
      "iter 1501/1000000  loss         0.206667  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.030\n",
      "iter 1600/1000000  loss         0.205728  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.052\n",
      "iter 1601/1000000  loss         0.205719  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.052\n",
      "iter 1700/1000000  loss         0.204899  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.071\n",
      "iter 1701/1000000  loss         0.204892  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.071\n",
      "iter 1800/1000000  loss         0.204171  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.089\n",
      "iter 1801/1000000  loss         0.204164  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.089\n",
      "iter 1900/1000000  loss         0.203527  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.104\n",
      "iter 1901/1000000  loss         0.203521  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.104\n",
      "iter 2000/1000000  loss         0.202956  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.117\n",
      "iter 2001/1000000  loss         0.202951  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    2.117\n",
      "iter 2100/1000000  loss         0.202447  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.129\n",
      "iter 2101/1000000  loss         0.202442  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.129\n",
      "iter 2200/1000000  loss         0.201991  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.140\n",
      "iter 2201/1000000  loss         0.201987  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.140\n",
      "iter 2300/1000000  loss         0.201581  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.149\n",
      "iter 2301/1000000  loss         0.201577  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.149\n",
      "iter 2400/1000000  loss         0.201212  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.157\n",
      "iter 2401/1000000  loss         0.201208  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    2.158\n",
      "iter 2500/1000000  loss         0.200878  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.165\n",
      "iter 2501/1000000  loss         0.200875  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.165\n",
      "iter 2600/1000000  loss         0.200575  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.171\n",
      "iter 2601/1000000  loss         0.200572  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.171\n",
      "iter 2700/1000000  loss         0.200300  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.177\n",
      "iter 2701/1000000  loss         0.200297  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.177\n",
      "iter 2800/1000000  loss         0.200049  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    2.182\n",
      "iter 2801/1000000  loss         0.200046  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    2.182\n",
      "iter 2900/1000000  loss         0.199820  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.186\n",
      "iter 2901/1000000  loss         0.199817  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.187\n",
      "iter 3000/1000000  loss         0.199610  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.190\n",
      "iter 3001/1000000  loss         0.199608  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.190\n",
      "iter 3100/1000000  loss         0.199417  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.194\n",
      "iter 3101/1000000  loss         0.199416  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.194\n",
      "iter 3200/1000000  loss         0.199241  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.197\n",
      "iter 3201/1000000  loss         0.199239  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.197\n",
      "iter 3300/1000000  loss         0.199078  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.200\n",
      "iter 3301/1000000  loss         0.199076  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.200\n",
      "iter 3400/1000000  loss         0.198928  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.202\n",
      "iter 3401/1000000  loss         0.198927  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.202\n",
      "iter 3500/1000000  loss         0.198790  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.204\n",
      "iter 3501/1000000  loss         0.198788  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.204\n",
      "iter 3600/1000000  loss         0.198662  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.206\n",
      "iter 3601/1000000  loss         0.198660  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.206\n",
      "iter 3700/1000000  loss         0.198543  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.207\n",
      "iter 3701/1000000  loss         0.198542  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.207\n",
      "iter 3800/1000000  loss         0.198433  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.208\n",
      "iter 3801/1000000  loss         0.198432  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3900/1000000  loss         0.198331  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.210\n",
      "iter 3901/1000000  loss         0.198330  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.210\n",
      "iter 4000/1000000  loss         0.198237  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.210\n",
      "iter 4001/1000000  loss         0.198236  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.210\n",
      "iter 4100/1000000  loss         0.198148  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.211\n",
      "iter 4101/1000000  loss         0.198148  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.211\n",
      "iter 4200/1000000  loss         0.198066  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.212\n",
      "iter 4201/1000000  loss         0.198066  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.212\n",
      "iter 4300/1000000  loss         0.197990  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.212\n",
      "iter 4301/1000000  loss         0.197989  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.212\n",
      "iter 4400/1000000  loss         0.197919  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.213\n",
      "iter 4401/1000000  loss         0.197918  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.213\n",
      "iter 4500/1000000  loss         0.197852  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.213\n",
      "iter 4501/1000000  loss         0.197851  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.213\n",
      "iter 4600/1000000  loss         0.197790  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.213\n",
      "iter 4601/1000000  loss         0.197789  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.213\n",
      "iter 4700/1000000  loss         0.197731  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.213\n",
      "iter 4701/1000000  loss         0.197731  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.213\n",
      "iter 4800/1000000  loss         0.197677  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.213\n",
      "iter 4801/1000000  loss         0.197676  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.213\n",
      "iter 4900/1000000  loss         0.197626  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.213\n",
      "iter 4901/1000000  loss         0.197625  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.213\n",
      "iter 5000/1000000  loss         0.197578  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.213\n",
      "iter 5001/1000000  loss         0.197578  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.213\n",
      "iter 5100/1000000  loss         0.197533  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.213\n",
      "iter 5101/1000000  loss         0.197533  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.213\n",
      "iter 5200/1000000  loss         0.197491  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.213\n",
      "iter 5201/1000000  loss         0.197491  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.213\n",
      "iter 5300/1000000  loss         0.197452  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.213\n",
      "iter 5301/1000000  loss         0.197451  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.213\n",
      "iter 5400/1000000  loss         0.197415  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.213\n",
      "iter 5401/1000000  loss         0.197414  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.213\n",
      "iter 5500/1000000  loss         0.197380  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.212\n",
      "iter 5501/1000000  loss         0.197379  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.212\n",
      "iter 5600/1000000  loss         0.197347  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.212\n",
      "iter 5601/1000000  loss         0.197346  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.212\n",
      "iter 5700/1000000  loss         0.197316  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.212\n",
      "iter 5701/1000000  loss         0.197316  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.212\n",
      "iter 5800/1000000  loss         0.197287  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.212\n",
      "iter 5801/1000000  loss         0.197287  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.212\n",
      "iter 5900/1000000  loss         0.197259  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.211\n",
      "iter 5901/1000000  loss         0.197259  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.211\n",
      "iter 6000/1000000  loss         0.197234  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.211\n",
      "iter 6001/1000000  loss         0.197233  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.211\n",
      "iter 6100/1000000  loss         0.197209  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.211\n",
      "iter 6101/1000000  loss         0.197209  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.211\n",
      "iter 6200/1000000  loss         0.197186  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.210\n",
      "iter 6201/1000000  loss         0.197186  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.210\n",
      "iter 6300/1000000  loss         0.197165  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.210\n",
      "iter 6301/1000000  loss         0.197164  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.210\n",
      "iter 6400/1000000  loss         0.197144  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.210\n",
      "iter 6401/1000000  loss         0.197144  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.210\n",
      "iter 6500/1000000  loss         0.197125  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.209\n",
      "iter 6501/1000000  loss         0.197125  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.209\n",
      "iter 6600/1000000  loss         0.197107  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.209\n",
      "iter 6601/1000000  loss         0.197106  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.209\n",
      "iter 6700/1000000  loss         0.197089  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.208\n",
      "iter 6701/1000000  loss         0.197089  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.208\n",
      "iter 6800/1000000  loss         0.197073  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.208\n",
      "iter 6801/1000000  loss         0.197073  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.208\n",
      "iter 6900/1000000  loss         0.197058  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.208\n",
      "iter 6901/1000000  loss         0.197057  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.208\n",
      "iter 7000/1000000  loss         0.197043  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7001/1000000  loss         0.197043  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7100/1000000  loss         0.197029  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7101/1000000  loss         0.197029  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7200/1000000  loss         0.197016  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.207\n",
      "iter 7201/1000000  loss         0.197016  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.207\n",
      "iter 7300/1000000  loss         0.197004  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7301/1000000  loss         0.197004  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7400/1000000  loss         0.196992  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7401/1000000  loss         0.196992  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7500/1000000  loss         0.196981  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.206\n",
      "iter 7501/1000000  loss         0.196981  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.206\n",
      "iter 7600/1000000  loss         0.196971  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7601/1000000  loss         0.196970  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7700/1000000  loss         0.196961  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7701/1000000  loss         0.196960  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7800/1000000  loss         0.196951  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.205\n",
      "iter 7801/1000000  loss         0.196951  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7900/1000000  loss         0.196942  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 7901/1000000  loss         0.196942  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 8000/1000000  loss         0.196934  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 8001/1000000  loss         0.196934  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 8100/1000000  loss         0.196926  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.204\n",
      "iter 8101/1000000  loss         0.196926  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.204\n",
      "iter 8200/1000000  loss         0.196918  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.203\n",
      "iter 8201/1000000  loss         0.196918  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.203\n",
      "iter 8300/1000000  loss         0.196911  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8301/1000000  loss         0.196911  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8400/1000000  loss         0.196904  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8401/1000000  loss         0.196904  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8500/1000000  loss         0.196897  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.203\n",
      "iter 8501/1000000  loss         0.196897  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.203\n",
      "iter 8600/1000000  loss         0.196891  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.202\n",
      "iter 8601/1000000  loss         0.196891  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.202\n",
      "iter 8700/1000000  loss         0.196885  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8701/1000000  loss         0.196885  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8800/1000000  loss         0.196880  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8801/1000000  loss         0.196880  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8900/1000000  loss         0.196874  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.202\n",
      "iter 8901/1000000  loss         0.196874  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.202\n",
      "iter 9000/1000000  loss         0.196869  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9001/1000000  loss         0.196869  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9100/1000000  loss         0.196864  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9101/1000000  loss         0.196864  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9200/1000000  loss         0.196860  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9201/1000000  loss         0.196860  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9300/1000000  loss         0.196855  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9301/1000000  loss         0.196855  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9400/1000000  loss         0.196851  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9401/1000000  loss         0.196851  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9500/1000000  loss         0.196847  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9501/1000000  loss         0.196847  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9600/1000000  loss         0.196844  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9601/1000000  loss         0.196844  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9700/1000000  loss         0.196840  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.200\n",
      "iter 9701/1000000  loss         0.196840  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.200\n",
      "Done. Converged after 9785 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9577777777777512\n",
      "TurnOn Loaded 0.0\n",
      "(1, 3600)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "No Noise New 0.9605555555555289\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformed Data (84000, 785)\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.031835  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.905375  avg_L1_norm_grad         0.026133  w[0]   -0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.840303  avg_L1_norm_grad         0.018502  w[0]    0.000 bias    0.022\n",
      "iter    3/1000000  loss         0.797717  avg_L1_norm_grad         0.015796  w[0]    0.000 bias    0.028\n",
      "iter    4/1000000  loss         0.766286  avg_L1_norm_grad         0.012888  w[0]    0.000 bias    0.043\n",
      "iter    5/1000000  loss         0.741943  avg_L1_norm_grad         0.011766  w[0]    0.000 bias    0.053\n",
      "iter    6/1000000  loss         0.721849  avg_L1_norm_grad         0.010498  w[0]    0.000 bias    0.065\n",
      "iter    7/1000000  loss         0.704570  avg_L1_norm_grad         0.009830  w[0]    0.000 bias    0.075\n",
      "iter    8/1000000  loss         0.689271  avg_L1_norm_grad         0.009168  w[0]    0.000 bias    0.086\n",
      "iter    9/1000000  loss         0.675449  avg_L1_norm_grad         0.008688  w[0]    0.000 bias    0.096\n",
      "iter   10/1000000  loss         0.662782  avg_L1_norm_grad         0.008260  w[0]    0.000 bias    0.106\n",
      "iter   11/1000000  loss         0.651050  avg_L1_norm_grad         0.007903  w[0]    0.000 bias    0.116\n",
      "iter   12/1000000  loss         0.640096  avg_L1_norm_grad         0.007590  w[0]    0.000 bias    0.126\n",
      "iter   13/1000000  loss         0.629808  avg_L1_norm_grad         0.007318  w[0]    0.000 bias    0.136\n",
      "iter   14/1000000  loss         0.620099  avg_L1_norm_grad         0.007076  w[0]    0.000 bias    0.145\n",
      "iter   15/1000000  loss         0.610904  avg_L1_norm_grad         0.006858  w[0]    0.000 bias    0.154\n",
      "iter   16/1000000  loss         0.602168  avg_L1_norm_grad         0.006659  w[0]    0.000 bias    0.163\n",
      "iter   17/1000000  loss         0.593850  avg_L1_norm_grad         0.006476  w[0]    0.000 bias    0.172\n",
      "iter   18/1000000  loss         0.585914  avg_L1_norm_grad         0.006306  w[0]    0.000 bias    0.181\n",
      "iter   19/1000000  loss         0.578329  avg_L1_norm_grad         0.006148  w[0]    0.000 bias    0.190\n",
      "iter  100/1000000  loss         0.354386  avg_L1_norm_grad         0.002238  w[0]    0.000 bias    0.662\n",
      "iter  101/1000000  loss         0.353312  avg_L1_norm_grad         0.002221  w[0]    0.000 bias    0.666\n",
      "iter  200/1000000  loss         0.290354  avg_L1_norm_grad         0.001309  w[0]    0.000 bias    0.990\n",
      "iter  201/1000000  loss         0.289972  avg_L1_norm_grad         0.001304  w[0]    0.000 bias    0.992\n",
      "iter  300/1000000  loss         0.262736  avg_L1_norm_grad         0.000936  w[0]    0.000 bias    1.212\n",
      "iter  301/1000000  loss         0.262537  avg_L1_norm_grad         0.000934  w[0]    0.000 bias    1.214\n",
      "iter  400/1000000  loss         0.247118  avg_L1_norm_grad         0.000732  w[0]    0.000 bias    1.378\n",
      "iter  401/1000000  loss         0.246996  avg_L1_norm_grad         0.000731  w[0]    0.000 bias    1.380\n",
      "iter  500/1000000  loss         0.237049  avg_L1_norm_grad         0.000607  w[0]    0.000 bias    1.509\n",
      "iter  501/1000000  loss         0.236966  avg_L1_norm_grad         0.000606  w[0]    0.000 bias    1.510\n",
      "iter  600/1000000  loss         0.230021  avg_L1_norm_grad         0.000519  w[0]    0.000 bias    1.615\n",
      "iter  601/1000000  loss         0.229961  avg_L1_norm_grad         0.000518  w[0]    0.000 bias    1.616\n",
      "iter  700/1000000  loss         0.224847  avg_L1_norm_grad         0.000453  w[0]    0.000 bias    1.703\n",
      "iter  701/1000000  loss         0.224802  avg_L1_norm_grad         0.000453  w[0]    0.000 bias    1.704\n",
      "iter  800/1000000  loss         0.220889  avg_L1_norm_grad         0.000402  w[0]    0.000 bias    1.777\n",
      "iter  801/1000000  loss         0.220854  avg_L1_norm_grad         0.000402  w[0]    0.000 bias    1.778\n",
      "iter  900/1000000  loss         0.217771  avg_L1_norm_grad         0.000361  w[0]   -0.000 bias    1.840\n",
      "iter  901/1000000  loss         0.217743  avg_L1_norm_grad         0.000361  w[0]   -0.000 bias    1.841\n",
      "iter 1000/1000000  loss         0.215259  avg_L1_norm_grad         0.000327  w[0]   -0.000 bias    1.894\n",
      "iter 1001/1000000  loss         0.215237  avg_L1_norm_grad         0.000327  w[0]   -0.000 bias    1.895\n",
      "iter 1100/1000000  loss         0.213199  avg_L1_norm_grad         0.000299  w[0]   -0.000 bias    1.941\n",
      "iter 1101/1000000  loss         0.213180  avg_L1_norm_grad         0.000299  w[0]   -0.000 bias    1.942\n",
      "iter 1200/1000000  loss         0.211484  avg_L1_norm_grad         0.000275  w[0]   -0.000 bias    1.982\n",
      "iter 1201/1000000  loss         0.211468  avg_L1_norm_grad         0.000275  w[0]   -0.000 bias    1.983\n",
      "iter 1300/1000000  loss         0.210038  avg_L1_norm_grad         0.000254  w[0]   -0.000 bias    2.018\n",
      "iter 1301/1000000  loss         0.210025  avg_L1_norm_grad         0.000254  w[0]   -0.000 bias    2.018\n",
      "iter 1400/1000000  loss         0.208806  avg_L1_norm_grad         0.000236  w[0]   -0.000 bias    2.049\n",
      "iter 1401/1000000  loss         0.208795  avg_L1_norm_grad         0.000236  w[0]   -0.000 bias    2.050\n",
      "iter 1500/1000000  loss         0.207748  avg_L1_norm_grad         0.000220  w[0]   -0.000 bias    2.077\n",
      "iter 1501/1000000  loss         0.207738  avg_L1_norm_grad         0.000220  w[0]   -0.000 bias    2.077\n",
      "iter 1600/1000000  loss         0.206831  avg_L1_norm_grad         0.000206  w[0]   -0.000 bias    2.101\n",
      "iter 1601/1000000  loss         0.206823  avg_L1_norm_grad         0.000206  w[0]   -0.000 bias    2.102\n",
      "iter 1700/1000000  loss         0.206032  avg_L1_norm_grad         0.000193  w[0]   -0.000 bias    2.123\n",
      "iter 1701/1000000  loss         0.206024  avg_L1_norm_grad         0.000193  w[0]   -0.000 bias    2.123\n",
      "iter 1800/1000000  loss         0.205330  avg_L1_norm_grad         0.000182  w[0]   -0.000 bias    2.142\n",
      "iter 1801/1000000  loss         0.205324  avg_L1_norm_grad         0.000182  w[0]   -0.000 bias    2.143\n",
      "iter 1900/1000000  loss         0.204712  avg_L1_norm_grad         0.000172  w[0]   -0.000 bias    2.159\n",
      "iter 1901/1000000  loss         0.204706  avg_L1_norm_grad         0.000172  w[0]   -0.000 bias    2.160\n",
      "iter 2000/1000000  loss         0.204163  avg_L1_norm_grad         0.000163  w[0]   -0.000 bias    2.175\n",
      "iter 2001/1000000  loss         0.204158  avg_L1_norm_grad         0.000163  w[0]   -0.000 bias    2.175\n",
      "iter 2100/1000000  loss         0.203675  avg_L1_norm_grad         0.000154  w[0]   -0.000 bias    2.188\n",
      "iter 2101/1000000  loss         0.203670  avg_L1_norm_grad         0.000154  w[0]   -0.000 bias    2.188\n",
      "iter 2200/1000000  loss         0.203239  avg_L1_norm_grad         0.000146  w[0]   -0.000 bias    2.200\n",
      "iter 2201/1000000  loss         0.203235  avg_L1_norm_grad         0.000146  w[0]   -0.000 bias    2.200\n",
      "iter 2300/1000000  loss         0.202847  avg_L1_norm_grad         0.000139  w[0]   -0.000 bias    2.211\n",
      "iter 2301/1000000  loss         0.202844  avg_L1_norm_grad         0.000139  w[0]   -0.000 bias    2.211\n",
      "iter 2400/1000000  loss         0.202495  avg_L1_norm_grad         0.000133  w[0]   -0.000 bias    2.221\n",
      "iter 2401/1000000  loss         0.202492  avg_L1_norm_grad         0.000133  w[0]   -0.000 bias    2.221\n",
      "iter 2500/1000000  loss         0.202177  avg_L1_norm_grad         0.000127  w[0]   -0.000 bias    2.229\n",
      "iter 2501/1000000  loss         0.202174  avg_L1_norm_grad         0.000127  w[0]   -0.000 bias    2.229\n",
      "iter 2600/1000000  loss         0.201889  avg_L1_norm_grad         0.000121  w[0]   -0.000 bias    2.237\n",
      "iter 2601/1000000  loss         0.201887  avg_L1_norm_grad         0.000121  w[0]   -0.000 bias    2.237\n",
      "iter 2700/1000000  loss         0.201628  avg_L1_norm_grad         0.000116  w[0]   -0.000 bias    2.244\n",
      "iter 2701/1000000  loss         0.201626  avg_L1_norm_grad         0.000116  w[0]   -0.000 bias    2.244\n",
      "iter 2800/1000000  loss         0.201390  avg_L1_norm_grad         0.000111  w[0]   -0.000 bias    2.250\n",
      "iter 2801/1000000  loss         0.201388  avg_L1_norm_grad         0.000111  w[0]   -0.000 bias    2.250\n",
      "iter 2900/1000000  loss         0.201173  avg_L1_norm_grad         0.000106  w[0]   -0.000 bias    2.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2901/1000000  loss         0.201171  avg_L1_norm_grad         0.000106  w[0]   -0.000 bias    2.256\n",
      "iter 3000/1000000  loss         0.200975  avg_L1_norm_grad         0.000102  w[0]   -0.000 bias    2.260\n",
      "iter 3001/1000000  loss         0.200973  avg_L1_norm_grad         0.000102  w[0]   -0.000 bias    2.260\n",
      "iter 3100/1000000  loss         0.200794  avg_L1_norm_grad         0.000098  w[0]   -0.000 bias    2.265\n",
      "iter 3101/1000000  loss         0.200792  avg_L1_norm_grad         0.000098  w[0]   -0.000 bias    2.265\n",
      "iter 3200/1000000  loss         0.200627  avg_L1_norm_grad         0.000094  w[0]   -0.000 bias    2.269\n",
      "iter 3201/1000000  loss         0.200626  avg_L1_norm_grad         0.000094  w[0]   -0.000 bias    2.269\n",
      "iter 3300/1000000  loss         0.200475  avg_L1_norm_grad         0.000090  w[0]   -0.000 bias    2.272\n",
      "iter 3301/1000000  loss         0.200473  avg_L1_norm_grad         0.000090  w[0]   -0.000 bias    2.272\n",
      "iter 3400/1000000  loss         0.200334  avg_L1_norm_grad         0.000087  w[0]   -0.000 bias    2.275\n",
      "iter 3401/1000000  loss         0.200332  avg_L1_norm_grad         0.000087  w[0]   -0.000 bias    2.275\n",
      "iter 3500/1000000  loss         0.200204  avg_L1_norm_grad         0.000083  w[0]   -0.000 bias    2.278\n",
      "iter 3501/1000000  loss         0.200203  avg_L1_norm_grad         0.000083  w[0]   -0.000 bias    2.278\n",
      "iter 3600/1000000  loss         0.200084  avg_L1_norm_grad         0.000080  w[0]   -0.000 bias    2.280\n",
      "iter 3601/1000000  loss         0.200083  avg_L1_norm_grad         0.000080  w[0]   -0.000 bias    2.280\n",
      "iter 3700/1000000  loss         0.199973  avg_L1_norm_grad         0.000077  w[0]   -0.000 bias    2.282\n",
      "iter 3701/1000000  loss         0.199972  avg_L1_norm_grad         0.000077  w[0]   -0.000 bias    2.282\n",
      "iter 3800/1000000  loss         0.199871  avg_L1_norm_grad         0.000074  w[0]   -0.000 bias    2.284\n",
      "iter 3801/1000000  loss         0.199870  avg_L1_norm_grad         0.000074  w[0]   -0.000 bias    2.284\n",
      "iter 3900/1000000  loss         0.199776  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    2.286\n",
      "iter 3901/1000000  loss         0.199775  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    2.286\n",
      "iter 4000/1000000  loss         0.199688  avg_L1_norm_grad         0.000069  w[0]   -0.000 bias    2.287\n",
      "iter 4001/1000000  loss         0.199687  avg_L1_norm_grad         0.000069  w[0]   -0.000 bias    2.287\n",
      "iter 4100/1000000  loss         0.199606  avg_L1_norm_grad         0.000067  w[0]   -0.000 bias    2.289\n",
      "iter 4101/1000000  loss         0.199605  avg_L1_norm_grad         0.000067  w[0]   -0.000 bias    2.289\n",
      "iter 4200/1000000  loss         0.199529  avg_L1_norm_grad         0.000065  w[0]   -0.000 bias    2.290\n",
      "iter 4201/1000000  loss         0.199529  avg_L1_norm_grad         0.000065  w[0]   -0.000 bias    2.290\n",
      "iter 4300/1000000  loss         0.199459  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.290\n",
      "iter 4301/1000000  loss         0.199458  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.291\n",
      "iter 4400/1000000  loss         0.199392  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.291\n",
      "iter 4401/1000000  loss         0.199392  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.291\n",
      "iter 4500/1000000  loss         0.199331  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.292\n",
      "iter 4501/1000000  loss         0.199330  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.292\n",
      "iter 4600/1000000  loss         0.199273  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.292\n",
      "iter 4601/1000000  loss         0.199273  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    2.292\n",
      "iter 4700/1000000  loss         0.199220  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.293\n",
      "iter 4701/1000000  loss         0.199219  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.293\n",
      "iter 4800/1000000  loss         0.199169  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.293\n",
      "iter 4801/1000000  loss         0.199169  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.293\n",
      "iter 4900/1000000  loss         0.199122  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.294\n",
      "iter 4901/1000000  loss         0.199122  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.294\n",
      "iter 5000/1000000  loss         0.199078  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.294\n",
      "iter 5001/1000000  loss         0.199078  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.294\n",
      "iter 5100/1000000  loss         0.199037  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.294\n",
      "iter 5101/1000000  loss         0.199037  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.294\n",
      "iter 5200/1000000  loss         0.198999  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.294\n",
      "iter 5201/1000000  loss         0.198998  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.294\n",
      "iter 5300/1000000  loss         0.198962  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.294\n",
      "iter 5301/1000000  loss         0.198962  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.294\n",
      "iter 5400/1000000  loss         0.198928  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.294\n",
      "iter 5401/1000000  loss         0.198928  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.294\n",
      "iter 5500/1000000  loss         0.198896  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.294\n",
      "iter 5501/1000000  loss         0.198896  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.294\n",
      "iter 5600/1000000  loss         0.198866  avg_L1_norm_grad         0.000041  w[0]   -0.000 bias    2.294\n",
      "iter 5601/1000000  loss         0.198866  avg_L1_norm_grad         0.000041  w[0]   -0.000 bias    2.294\n",
      "iter 5700/1000000  loss         0.198838  avg_L1_norm_grad         0.000040  w[0]   -0.000 bias    2.294\n",
      "iter 5701/1000000  loss         0.198838  avg_L1_norm_grad         0.000040  w[0]   -0.000 bias    2.294\n",
      "iter 5800/1000000  loss         0.198812  avg_L1_norm_grad         0.000039  w[0]   -0.000 bias    2.294\n",
      "iter 5801/1000000  loss         0.198811  avg_L1_norm_grad         0.000039  w[0]   -0.000 bias    2.294\n",
      "iter 5900/1000000  loss         0.198787  avg_L1_norm_grad         0.000038  w[0]   -0.000 bias    2.293\n",
      "iter 5901/1000000  loss         0.198786  avg_L1_norm_grad         0.000038  w[0]   -0.000 bias    2.293\n",
      "iter 6000/1000000  loss         0.198763  avg_L1_norm_grad         0.000037  w[0]   -0.000 bias    2.293\n",
      "iter 6001/1000000  loss         0.198763  avg_L1_norm_grad         0.000037  w[0]   -0.000 bias    2.293\n",
      "iter 6100/1000000  loss         0.198741  avg_L1_norm_grad         0.000036  w[0]   -0.000 bias    2.293\n",
      "iter 6101/1000000  loss         0.198741  avg_L1_norm_grad         0.000036  w[0]   -0.000 bias    2.293\n",
      "iter 6200/1000000  loss         0.198720  avg_L1_norm_grad         0.000035  w[0]   -0.000 bias    2.293\n",
      "iter 6201/1000000  loss         0.198720  avg_L1_norm_grad         0.000035  w[0]   -0.000 bias    2.293\n",
      "iter 6300/1000000  loss         0.198700  avg_L1_norm_grad         0.000034  w[0]   -0.000 bias    2.292\n",
      "iter 6301/1000000  loss         0.198700  avg_L1_norm_grad         0.000034  w[0]   -0.000 bias    2.292\n",
      "iter 6400/1000000  loss         0.198682  avg_L1_norm_grad         0.000033  w[0]   -0.000 bias    2.292\n",
      "iter 6401/1000000  loss         0.198682  avg_L1_norm_grad         0.000033  w[0]   -0.000 bias    2.292\n",
      "iter 6500/1000000  loss         0.198664  avg_L1_norm_grad         0.000032  w[0]   -0.000 bias    2.292\n",
      "iter 6501/1000000  loss         0.198664  avg_L1_norm_grad         0.000032  w[0]   -0.000 bias    2.292\n",
      "iter 6600/1000000  loss         0.198648  avg_L1_norm_grad         0.000031  w[0]   -0.000 bias    2.292\n",
      "iter 6601/1000000  loss         0.198647  avg_L1_norm_grad         0.000031  w[0]   -0.000 bias    2.292\n",
      "iter 6700/1000000  loss         0.198632  avg_L1_norm_grad         0.000030  w[0]   -0.000 bias    2.291\n",
      "iter 6701/1000000  loss         0.198632  avg_L1_norm_grad         0.000030  w[0]   -0.000 bias    2.291\n",
      "iter 6800/1000000  loss         0.198617  avg_L1_norm_grad         0.000029  w[0]   -0.000 bias    2.291\n",
      "iter 6801/1000000  loss         0.198617  avg_L1_norm_grad         0.000029  w[0]   -0.000 bias    2.291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.198603  avg_L1_norm_grad         0.000029  w[0]   -0.000 bias    2.291\n",
      "iter 6901/1000000  loss         0.198603  avg_L1_norm_grad         0.000029  w[0]   -0.000 bias    2.291\n",
      "iter 7000/1000000  loss         0.198590  avg_L1_norm_grad         0.000028  w[0]   -0.000 bias    2.290\n",
      "iter 7001/1000000  loss         0.198590  avg_L1_norm_grad         0.000028  w[0]   -0.000 bias    2.290\n",
      "iter 7100/1000000  loss         0.198578  avg_L1_norm_grad         0.000027  w[0]   -0.000 bias    2.290\n",
      "iter 7101/1000000  loss         0.198578  avg_L1_norm_grad         0.000027  w[0]   -0.000 bias    2.290\n",
      "iter 7200/1000000  loss         0.198566  avg_L1_norm_grad         0.000026  w[0]   -0.000 bias    2.290\n",
      "iter 7201/1000000  loss         0.198566  avg_L1_norm_grad         0.000026  w[0]   -0.000 bias    2.290\n",
      "iter 7300/1000000  loss         0.198555  avg_L1_norm_grad         0.000026  w[0]   -0.000 bias    2.290\n",
      "iter 7301/1000000  loss         0.198555  avg_L1_norm_grad         0.000026  w[0]   -0.000 bias    2.290\n",
      "iter 7400/1000000  loss         0.198544  avg_L1_norm_grad         0.000025  w[0]   -0.000 bias    2.289\n",
      "iter 7401/1000000  loss         0.198544  avg_L1_norm_grad         0.000025  w[0]   -0.000 bias    2.289\n",
      "iter 7500/1000000  loss         0.198534  avg_L1_norm_grad         0.000024  w[0]   -0.000 bias    2.289\n",
      "iter 7501/1000000  loss         0.198534  avg_L1_norm_grad         0.000024  w[0]   -0.000 bias    2.289\n",
      "iter 7600/1000000  loss         0.198525  avg_L1_norm_grad         0.000024  w[0]   -0.000 bias    2.289\n",
      "iter 7601/1000000  loss         0.198525  avg_L1_norm_grad         0.000024  w[0]   -0.000 bias    2.289\n",
      "iter 7700/1000000  loss         0.198516  avg_L1_norm_grad         0.000023  w[0]   -0.000 bias    2.288\n",
      "iter 7701/1000000  loss         0.198516  avg_L1_norm_grad         0.000023  w[0]   -0.000 bias    2.288\n",
      "iter 7800/1000000  loss         0.198507  avg_L1_norm_grad         0.000022  w[0]   -0.000 bias    2.288\n",
      "iter 7801/1000000  loss         0.198507  avg_L1_norm_grad         0.000022  w[0]   -0.000 bias    2.288\n",
      "iter 7900/1000000  loss         0.198499  avg_L1_norm_grad         0.000022  w[0]   -0.000 bias    2.288\n",
      "iter 7901/1000000  loss         0.198499  avg_L1_norm_grad         0.000022  w[0]   -0.000 bias    2.288\n",
      "iter 8000/1000000  loss         0.198492  avg_L1_norm_grad         0.000021  w[0]   -0.000 bias    2.287\n",
      "iter 8001/1000000  loss         0.198492  avg_L1_norm_grad         0.000021  w[0]   -0.000 bias    2.287\n",
      "iter 8100/1000000  loss         0.198484  avg_L1_norm_grad         0.000021  w[0]   -0.000 bias    2.287\n",
      "iter 8101/1000000  loss         0.198484  avg_L1_norm_grad         0.000021  w[0]   -0.000 bias    2.287\n",
      "iter 8200/1000000  loss         0.198477  avg_L1_norm_grad         0.000020  w[0]   -0.000 bias    2.287\n",
      "iter 8201/1000000  loss         0.198477  avg_L1_norm_grad         0.000020  w[0]   -0.000 bias    2.287\n",
      "iter 8300/1000000  loss         0.198471  avg_L1_norm_grad         0.000020  w[0]   -0.000 bias    2.287\n",
      "iter 8301/1000000  loss         0.198471  avg_L1_norm_grad         0.000020  w[0]   -0.000 bias    2.287\n",
      "iter 8400/1000000  loss         0.198465  avg_L1_norm_grad         0.000019  w[0]   -0.000 bias    2.286\n",
      "iter 8401/1000000  loss         0.198465  avg_L1_norm_grad         0.000019  w[0]   -0.000 bias    2.286\n",
      "iter 8500/1000000  loss         0.198459  avg_L1_norm_grad         0.000019  w[0]   -0.000 bias    2.286\n",
      "iter 8501/1000000  loss         0.198459  avg_L1_norm_grad         0.000019  w[0]   -0.000 bias    2.286\n",
      "iter 8600/1000000  loss         0.198453  avg_L1_norm_grad         0.000018  w[0]   -0.000 bias    2.286\n",
      "iter 8601/1000000  loss         0.198453  avg_L1_norm_grad         0.000018  w[0]   -0.000 bias    2.286\n",
      "iter 8700/1000000  loss         0.198448  avg_L1_norm_grad         0.000018  w[0]   -0.000 bias    2.286\n",
      "iter 8701/1000000  loss         0.198448  avg_L1_norm_grad         0.000018  w[0]   -0.000 bias    2.286\n",
      "iter 8800/1000000  loss         0.198443  avg_L1_norm_grad         0.000017  w[0]   -0.000 bias    2.285\n",
      "iter 8801/1000000  loss         0.198443  avg_L1_norm_grad         0.000017  w[0]   -0.000 bias    2.285\n",
      "iter 8900/1000000  loss         0.198438  avg_L1_norm_grad         0.000017  w[0]   -0.000 bias    2.285\n",
      "iter 8901/1000000  loss         0.198438  avg_L1_norm_grad         0.000017  w[0]   -0.000 bias    2.285\n",
      "iter 9000/1000000  loss         0.198434  avg_L1_norm_grad         0.000016  w[0]   -0.000 bias    2.285\n",
      "iter 9001/1000000  loss         0.198434  avg_L1_norm_grad         0.000016  w[0]   -0.000 bias    2.285\n",
      "iter 9100/1000000  loss         0.198430  avg_L1_norm_grad         0.000016  w[0]   -0.000 bias    2.285\n",
      "iter 9101/1000000  loss         0.198429  avg_L1_norm_grad         0.000016  w[0]   -0.000 bias    2.285\n",
      "iter 9200/1000000  loss         0.198425  avg_L1_norm_grad         0.000016  w[0]   -0.000 bias    2.284\n",
      "iter 9201/1000000  loss         0.198425  avg_L1_norm_grad         0.000016  w[0]   -0.000 bias    2.284\n",
      "iter 9300/1000000  loss         0.198422  avg_L1_norm_grad         0.000015  w[0]   -0.000 bias    2.284\n",
      "iter 9301/1000000  loss         0.198422  avg_L1_norm_grad         0.000015  w[0]   -0.000 bias    2.284\n",
      "iter 9400/1000000  loss         0.198418  avg_L1_norm_grad         0.000015  w[0]   -0.000 bias    2.284\n",
      "iter 9401/1000000  loss         0.198418  avg_L1_norm_grad         0.000015  w[0]   -0.000 bias    2.284\n",
      "iter 9500/1000000  loss         0.198414  avg_L1_norm_grad         0.000015  w[0]   -0.000 bias    2.284\n",
      "iter 9501/1000000  loss         0.198414  avg_L1_norm_grad         0.000014  w[0]   -0.000 bias    2.284\n",
      "iter 9600/1000000  loss         0.198411  avg_L1_norm_grad         0.000014  w[0]   -0.000 bias    2.283\n",
      "iter 9601/1000000  loss         0.198411  avg_L1_norm_grad         0.000014  w[0]   -0.000 bias    2.283\n",
      "Done. Converged after 9671 iterations.\n",
      "Origin Accuracy 0.9619722222222195\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr22 = LRGD(alpha=100.0, step_size=0.1)\n",
    "orig_lr22.fit(x_te, y_te)\n",
    "y_hat_Origin=np.asarray(orig_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax Transform\n",
      "TurnOn Loaded 0.0\n",
      "(1, 84000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Shape of Transformed Data (84000, 790)\n",
      "Initializing w_G with 790 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.032054  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.895911  avg_L1_norm_grad         0.025763  w[0]   -0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.824460  avg_L1_norm_grad         0.018877  w[0]    0.000 bias    0.021\n",
      "iter    3/1000000  loss         0.777021  avg_L1_norm_grad         0.015585  w[0]    0.000 bias    0.026\n",
      "iter    4/1000000  loss         0.741881  avg_L1_norm_grad         0.012608  w[0]    0.000 bias    0.041\n",
      "iter    5/1000000  loss         0.714789  avg_L1_norm_grad         0.011288  w[0]    0.000 bias    0.048\n",
      "iter    6/1000000  loss         0.692493  avg_L1_norm_grad         0.010008  w[0]    0.000 bias    0.060\n",
      "iter    7/1000000  loss         0.673377  avg_L1_norm_grad         0.009299  w[0]    0.000 bias    0.068\n",
      "iter    8/1000000  loss         0.656489  avg_L1_norm_grad         0.008645  w[0]    0.000 bias    0.078\n",
      "iter    9/1000000  loss         0.641270  avg_L1_norm_grad         0.008162  w[0]    0.000 bias    0.087\n",
      "iter   10/1000000  loss         0.627363  avg_L1_norm_grad         0.007737  w[0]    0.000 bias    0.096\n",
      "iter   11/1000000  loss         0.614525  avg_L1_norm_grad         0.007383  w[0]    0.000 bias    0.104\n",
      "iter   12/1000000  loss         0.602584  avg_L1_norm_grad         0.007072  w[0]    0.000 bias    0.113\n",
      "iter   13/1000000  loss         0.591413  avg_L1_norm_grad         0.006802  w[0]    0.000 bias    0.121\n",
      "iter   14/1000000  loss         0.580915  avg_L1_norm_grad         0.006560  w[0]    0.000 bias    0.129\n",
      "iter   15/1000000  loss         0.571014  avg_L1_norm_grad         0.006344  w[0]    0.000 bias    0.137\n",
      "iter   16/1000000  loss         0.561649  avg_L1_norm_grad         0.006147  w[0]    0.000 bias    0.144\n",
      "iter   17/1000000  loss         0.552770  avg_L1_norm_grad         0.005964  w[0]    0.001 bias    0.152\n",
      "iter   18/1000000  loss         0.544335  avg_L1_norm_grad         0.005795  w[0]    0.001 bias    0.159\n",
      "iter   19/1000000  loss         0.536306  avg_L1_norm_grad         0.005637  w[0]    0.001 bias    0.166\n",
      "iter  100/1000000  loss         0.316450  avg_L1_norm_grad         0.001927  w[0]    0.001 bias    0.498\n",
      "iter  101/1000000  loss         0.315488  avg_L1_norm_grad         0.001913  w[0]    0.001 bias    0.501\n",
      "iter  200/1000000  loss         0.261193  avg_L1_norm_grad         0.001130  w[0]    0.001 bias    0.677\n",
      "iter  201/1000000  loss         0.260877  avg_L1_norm_grad         0.001126  w[0]    0.001 bias    0.678\n",
      "iter  300/1000000  loss         0.238700  avg_L1_norm_grad         0.000820  w[0]    0.000 bias    0.775\n",
      "iter  301/1000000  loss         0.238541  avg_L1_norm_grad         0.000818  w[0]    0.000 bias    0.776\n",
      "iter  400/1000000  loss         0.226260  avg_L1_norm_grad         0.000652  w[0]   -0.001 bias    0.836\n",
      "iter  401/1000000  loss         0.226163  avg_L1_norm_grad         0.000650  w[0]   -0.001 bias    0.837\n",
      "iter  500/1000000  loss         0.218265  avg_L1_norm_grad         0.000546  w[0]   -0.002 bias    0.877\n",
      "iter  501/1000000  loss         0.218199  avg_L1_norm_grad         0.000545  w[0]   -0.002 bias    0.877\n",
      "iter  600/1000000  loss         0.212642  avg_L1_norm_grad         0.000472  w[0]   -0.003 bias    0.904\n",
      "iter  601/1000000  loss         0.212594  avg_L1_norm_grad         0.000471  w[0]   -0.003 bias    0.904\n",
      "iter  700/1000000  loss         0.208443  avg_L1_norm_grad         0.000416  w[0]   -0.004 bias    0.923\n",
      "iter  701/1000000  loss         0.208406  avg_L1_norm_grad         0.000415  w[0]   -0.004 bias    0.923\n",
      "iter  800/1000000  loss         0.205171  avg_L1_norm_grad         0.000372  w[0]   -0.004 bias    0.936\n",
      "iter  801/1000000  loss         0.205142  avg_L1_norm_grad         0.000372  w[0]   -0.004 bias    0.936\n",
      "iter  900/1000000  loss         0.202542  avg_L1_norm_grad         0.000337  w[0]   -0.005 bias    0.945\n",
      "iter  901/1000000  loss         0.202518  avg_L1_norm_grad         0.000336  w[0]   -0.005 bias    0.945\n",
      "iter 1000/1000000  loss         0.200378  avg_L1_norm_grad         0.000308  w[0]   -0.006 bias    0.951\n",
      "iter 1001/1000000  loss         0.200358  avg_L1_norm_grad         0.000308  w[0]   -0.006 bias    0.951\n",
      "iter 1100/1000000  loss         0.198564  avg_L1_norm_grad         0.000283  w[0]   -0.007 bias    0.955\n",
      "iter 1101/1000000  loss         0.198547  avg_L1_norm_grad         0.000283  w[0]   -0.007 bias    0.955\n",
      "iter 1200/1000000  loss         0.197021  avg_L1_norm_grad         0.000262  w[0]   -0.008 bias    0.958\n",
      "iter 1201/1000000  loss         0.197007  avg_L1_norm_grad         0.000262  w[0]   -0.008 bias    0.958\n",
      "iter 1300/1000000  loss         0.195693  avg_L1_norm_grad         0.000244  w[0]   -0.009 bias    0.959\n",
      "iter 1301/1000000  loss         0.195681  avg_L1_norm_grad         0.000244  w[0]   -0.009 bias    0.959\n",
      "iter 1400/1000000  loss         0.194540  avg_L1_norm_grad         0.000228  w[0]   -0.010 bias    0.960\n",
      "iter 1401/1000000  loss         0.194529  avg_L1_norm_grad         0.000228  w[0]   -0.010 bias    0.960\n",
      "iter 1500/1000000  loss         0.193529  avg_L1_norm_grad         0.000214  w[0]   -0.011 bias    0.960\n",
      "iter 1501/1000000  loss         0.193520  avg_L1_norm_grad         0.000214  w[0]   -0.011 bias    0.960\n",
      "iter 1600/1000000  loss         0.192638  avg_L1_norm_grad         0.000201  w[0]   -0.011 bias    0.960\n",
      "iter 1601/1000000  loss         0.192630  avg_L1_norm_grad         0.000201  w[0]   -0.011 bias    0.960\n",
      "iter 1700/1000000  loss         0.191847  avg_L1_norm_grad         0.000190  w[0]   -0.012 bias    0.960\n",
      "iter 1701/1000000  loss         0.191840  avg_L1_norm_grad         0.000190  w[0]   -0.012 bias    0.960\n",
      "iter 1800/1000000  loss         0.191142  avg_L1_norm_grad         0.000180  w[0]   -0.013 bias    0.959\n",
      "iter 1801/1000000  loss         0.191136  avg_L1_norm_grad         0.000180  w[0]   -0.013 bias    0.959\n",
      "iter 1900/1000000  loss         0.190511  avg_L1_norm_grad         0.000170  w[0]   -0.014 bias    0.959\n",
      "iter 1901/1000000  loss         0.190505  avg_L1_norm_grad         0.000170  w[0]   -0.014 bias    0.959\n",
      "iter 2000/1000000  loss         0.189944  avg_L1_norm_grad         0.000162  w[0]   -0.014 bias    0.958\n",
      "iter 2001/1000000  loss         0.189938  avg_L1_norm_grad         0.000162  w[0]   -0.014 bias    0.958\n",
      "iter 2100/1000000  loss         0.189432  avg_L1_norm_grad         0.000154  w[0]   -0.015 bias    0.958\n",
      "iter 2101/1000000  loss         0.189427  avg_L1_norm_grad         0.000154  w[0]   -0.015 bias    0.958\n",
      "iter 2200/1000000  loss         0.188968  avg_L1_norm_grad         0.000147  w[0]   -0.016 bias    0.957\n",
      "iter 2201/1000000  loss         0.188964  avg_L1_norm_grad         0.000147  w[0]   -0.016 bias    0.957\n",
      "iter 2300/1000000  loss         0.188547  avg_L1_norm_grad         0.000140  w[0]   -0.016 bias    0.957\n",
      "iter 2301/1000000  loss         0.188543  avg_L1_norm_grad         0.000140  w[0]   -0.016 bias    0.957\n",
      "iter 2400/1000000  loss         0.188165  avg_L1_norm_grad         0.000134  w[0]   -0.017 bias    0.956\n",
      "iter 2401/1000000  loss         0.188161  avg_L1_norm_grad         0.000134  w[0]   -0.017 bias    0.956\n",
      "iter 2500/1000000  loss         0.187815  avg_L1_norm_grad         0.000128  w[0]   -0.017 bias    0.956\n",
      "iter 2501/1000000  loss         0.187812  avg_L1_norm_grad         0.000128  w[0]   -0.017 bias    0.956\n",
      "iter 2600/1000000  loss         0.187496  avg_L1_norm_grad         0.000122  w[0]   -0.018 bias    0.956\n",
      "iter 2601/1000000  loss         0.187493  avg_L1_norm_grad         0.000122  w[0]   -0.018 bias    0.956\n",
      "iter 2700/1000000  loss         0.187204  avg_L1_norm_grad         0.000117  w[0]   -0.019 bias    0.956\n",
      "iter 2701/1000000  loss         0.187201  avg_L1_norm_grad         0.000117  w[0]   -0.019 bias    0.956\n",
      "iter 2800/1000000  loss         0.186935  avg_L1_norm_grad         0.000112  w[0]   -0.019 bias    0.956\n",
      "iter 2801/1000000  loss         0.186933  avg_L1_norm_grad         0.000112  w[0]   -0.019 bias    0.956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.186689  avg_L1_norm_grad         0.000108  w[0]   -0.020 bias    0.956\n",
      "iter 2901/1000000  loss         0.186686  avg_L1_norm_grad         0.000108  w[0]   -0.020 bias    0.956\n",
      "iter 3000/1000000  loss         0.186461  avg_L1_norm_grad         0.000103  w[0]   -0.020 bias    0.956\n",
      "iter 3001/1000000  loss         0.186459  avg_L1_norm_grad         0.000103  w[0]   -0.020 bias    0.956\n",
      "iter 3100/1000000  loss         0.186252  avg_L1_norm_grad         0.000099  w[0]   -0.021 bias    0.956\n",
      "iter 3101/1000000  loss         0.186250  avg_L1_norm_grad         0.000099  w[0]   -0.021 bias    0.956\n",
      "iter 3200/1000000  loss         0.186058  avg_L1_norm_grad         0.000096  w[0]   -0.021 bias    0.957\n",
      "iter 3201/1000000  loss         0.186056  avg_L1_norm_grad         0.000096  w[0]   -0.021 bias    0.957\n",
      "iter 3300/1000000  loss         0.185879  avg_L1_norm_grad         0.000092  w[0]   -0.022 bias    0.957\n",
      "iter 3301/1000000  loss         0.185877  avg_L1_norm_grad         0.000092  w[0]   -0.022 bias    0.957\n",
      "iter 3400/1000000  loss         0.185713  avg_L1_norm_grad         0.000089  w[0]   -0.022 bias    0.958\n",
      "iter 3401/1000000  loss         0.185711  avg_L1_norm_grad         0.000089  w[0]   -0.022 bias    0.958\n",
      "iter 3500/1000000  loss         0.185559  avg_L1_norm_grad         0.000085  w[0]   -0.023 bias    0.959\n",
      "iter 3501/1000000  loss         0.185557  avg_L1_norm_grad         0.000085  w[0]   -0.023 bias    0.959\n",
      "iter 3600/1000000  loss         0.185416  avg_L1_norm_grad         0.000082  w[0]   -0.023 bias    0.959\n",
      "iter 3601/1000000  loss         0.185414  avg_L1_norm_grad         0.000082  w[0]   -0.023 bias    0.959\n",
      "iter 3700/1000000  loss         0.185283  avg_L1_norm_grad         0.000080  w[0]   -0.024 bias    0.960\n",
      "iter 3701/1000000  loss         0.185281  avg_L1_norm_grad         0.000079  w[0]   -0.024 bias    0.960\n",
      "iter 3800/1000000  loss         0.185159  avg_L1_norm_grad         0.000077  w[0]   -0.024 bias    0.961\n",
      "iter 3801/1000000  loss         0.185158  avg_L1_norm_grad         0.000077  w[0]   -0.024 bias    0.961\n",
      "iter 3900/1000000  loss         0.185044  avg_L1_norm_grad         0.000074  w[0]   -0.025 bias    0.962\n",
      "iter 3901/1000000  loss         0.185043  avg_L1_norm_grad         0.000074  w[0]   -0.025 bias    0.962\n",
      "iter 4000/1000000  loss         0.184936  avg_L1_norm_grad         0.000072  w[0]   -0.025 bias    0.963\n",
      "iter 4001/1000000  loss         0.184935  avg_L1_norm_grad         0.000072  w[0]   -0.025 bias    0.963\n",
      "iter 4100/1000000  loss         0.184836  avg_L1_norm_grad         0.000069  w[0]   -0.025 bias    0.964\n",
      "iter 4101/1000000  loss         0.184835  avg_L1_norm_grad         0.000069  w[0]   -0.025 bias    0.964\n",
      "iter 4200/1000000  loss         0.184742  avg_L1_norm_grad         0.000067  w[0]   -0.026 bias    0.965\n",
      "iter 4201/1000000  loss         0.184741  avg_L1_norm_grad         0.000067  w[0]   -0.026 bias    0.965\n",
      "iter 4300/1000000  loss         0.184655  avg_L1_norm_grad         0.000065  w[0]   -0.026 bias    0.966\n",
      "iter 4301/1000000  loss         0.184654  avg_L1_norm_grad         0.000065  w[0]   -0.026 bias    0.966\n",
      "iter 4400/1000000  loss         0.184573  avg_L1_norm_grad         0.000063  w[0]   -0.027 bias    0.967\n",
      "iter 4401/1000000  loss         0.184572  avg_L1_norm_grad         0.000063  w[0]   -0.027 bias    0.967\n",
      "iter 4500/1000000  loss         0.184496  avg_L1_norm_grad         0.000061  w[0]   -0.027 bias    0.968\n",
      "iter 4501/1000000  loss         0.184495  avg_L1_norm_grad         0.000061  w[0]   -0.027 bias    0.968\n",
      "iter 4600/1000000  loss         0.184424  avg_L1_norm_grad         0.000059  w[0]   -0.027 bias    0.970\n",
      "iter 4601/1000000  loss         0.184423  avg_L1_norm_grad         0.000059  w[0]   -0.027 bias    0.970\n",
      "iter 4700/1000000  loss         0.184357  avg_L1_norm_grad         0.000057  w[0]   -0.028 bias    0.971\n",
      "iter 4701/1000000  loss         0.184356  avg_L1_norm_grad         0.000057  w[0]   -0.028 bias    0.971\n",
      "iter 4800/1000000  loss         0.184293  avg_L1_norm_grad         0.000055  w[0]   -0.028 bias    0.972\n",
      "iter 4801/1000000  loss         0.184293  avg_L1_norm_grad         0.000055  w[0]   -0.028 bias    0.972\n",
      "iter 4900/1000000  loss         0.184234  avg_L1_norm_grad         0.000053  w[0]   -0.028 bias    0.973\n",
      "iter 4901/1000000  loss         0.184233  avg_L1_norm_grad         0.000053  w[0]   -0.028 bias    0.973\n",
      "iter 5000/1000000  loss         0.184178  avg_L1_norm_grad         0.000052  w[0]   -0.029 bias    0.975\n",
      "iter 5001/1000000  loss         0.184177  avg_L1_norm_grad         0.000052  w[0]   -0.029 bias    0.975\n",
      "iter 5100/1000000  loss         0.184125  avg_L1_norm_grad         0.000050  w[0]   -0.029 bias    0.976\n",
      "iter 5101/1000000  loss         0.184125  avg_L1_norm_grad         0.000050  w[0]   -0.029 bias    0.976\n",
      "iter 5200/1000000  loss         0.184076  avg_L1_norm_grad         0.000048  w[0]   -0.029 bias    0.977\n",
      "iter 5201/1000000  loss         0.184075  avg_L1_norm_grad         0.000048  w[0]   -0.029 bias    0.977\n",
      "iter 5300/1000000  loss         0.184029  avg_L1_norm_grad         0.000047  w[0]   -0.030 bias    0.979\n",
      "iter 5301/1000000  loss         0.184029  avg_L1_norm_grad         0.000047  w[0]   -0.030 bias    0.979\n",
      "iter 5400/1000000  loss         0.183986  avg_L1_norm_grad         0.000046  w[0]   -0.030 bias    0.980\n",
      "iter 5401/1000000  loss         0.183985  avg_L1_norm_grad         0.000046  w[0]   -0.030 bias    0.980\n",
      "iter 5500/1000000  loss         0.183944  avg_L1_norm_grad         0.000044  w[0]   -0.030 bias    0.981\n",
      "iter 5501/1000000  loss         0.183944  avg_L1_norm_grad         0.000044  w[0]   -0.030 bias    0.981\n",
      "iter 5600/1000000  loss         0.183906  avg_L1_norm_grad         0.000043  w[0]   -0.030 bias    0.983\n",
      "iter 5601/1000000  loss         0.183905  avg_L1_norm_grad         0.000043  w[0]   -0.030 bias    0.983\n",
      "iter 5700/1000000  loss         0.183869  avg_L1_norm_grad         0.000042  w[0]   -0.031 bias    0.984\n",
      "iter 5701/1000000  loss         0.183868  avg_L1_norm_grad         0.000042  w[0]   -0.031 bias    0.984\n",
      "iter 5800/1000000  loss         0.183834  avg_L1_norm_grad         0.000040  w[0]   -0.031 bias    0.985\n",
      "iter 5801/1000000  loss         0.183834  avg_L1_norm_grad         0.000040  w[0]   -0.031 bias    0.985\n",
      "iter 5900/1000000  loss         0.183802  avg_L1_norm_grad         0.000039  w[0]   -0.031 bias    0.987\n",
      "iter 5901/1000000  loss         0.183801  avg_L1_norm_grad         0.000039  w[0]   -0.031 bias    0.987\n",
      "iter 6000/1000000  loss         0.183771  avg_L1_norm_grad         0.000038  w[0]   -0.031 bias    0.988\n",
      "iter 6001/1000000  loss         0.183770  avg_L1_norm_grad         0.000038  w[0]   -0.031 bias    0.988\n",
      "iter 6100/1000000  loss         0.183742  avg_L1_norm_grad         0.000037  w[0]   -0.032 bias    0.989\n",
      "iter 6101/1000000  loss         0.183741  avg_L1_norm_grad         0.000037  w[0]   -0.032 bias    0.989\n",
      "iter 6200/1000000  loss         0.183714  avg_L1_norm_grad         0.000036  w[0]   -0.032 bias    0.991\n",
      "iter 6201/1000000  loss         0.183714  avg_L1_norm_grad         0.000036  w[0]   -0.032 bias    0.991\n",
      "iter 6300/1000000  loss         0.183688  avg_L1_norm_grad         0.000035  w[0]   -0.032 bias    0.992\n",
      "iter 6301/1000000  loss         0.183688  avg_L1_norm_grad         0.000035  w[0]   -0.032 bias    0.992\n",
      "iter 6400/1000000  loss         0.183663  avg_L1_norm_grad         0.000034  w[0]   -0.032 bias    0.993\n",
      "iter 6401/1000000  loss         0.183663  avg_L1_norm_grad         0.000034  w[0]   -0.032 bias    0.993\n",
      "iter 6500/1000000  loss         0.183640  avg_L1_norm_grad         0.000033  w[0]   -0.033 bias    0.995\n",
      "iter 6501/1000000  loss         0.183640  avg_L1_norm_grad         0.000033  w[0]   -0.033 bias    0.995\n",
      "iter 6600/1000000  loss         0.183618  avg_L1_norm_grad         0.000032  w[0]   -0.033 bias    0.996\n",
      "iter 6601/1000000  loss         0.183618  avg_L1_norm_grad         0.000032  w[0]   -0.033 bias    0.996\n",
      "iter 6700/1000000  loss         0.183597  avg_L1_norm_grad         0.000031  w[0]   -0.033 bias    0.997\n",
      "iter 6701/1000000  loss         0.183597  avg_L1_norm_grad         0.000031  w[0]   -0.033 bias    0.997\n",
      "iter 6800/1000000  loss         0.183577  avg_L1_norm_grad         0.000030  w[0]   -0.033 bias    0.998\n",
      "iter 6801/1000000  loss         0.183577  avg_L1_norm_grad         0.000030  w[0]   -0.033 bias    0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.183559  avg_L1_norm_grad         0.000030  w[0]   -0.033 bias    1.000\n",
      "iter 6901/1000000  loss         0.183558  avg_L1_norm_grad         0.000030  w[0]   -0.033 bias    1.000\n",
      "iter 7000/1000000  loss         0.183541  avg_L1_norm_grad         0.000029  w[0]   -0.034 bias    1.001\n",
      "iter 7001/1000000  loss         0.183541  avg_L1_norm_grad         0.000029  w[0]   -0.034 bias    1.001\n",
      "iter 7100/1000000  loss         0.183524  avg_L1_norm_grad         0.000028  w[0]   -0.034 bias    1.002\n",
      "iter 7101/1000000  loss         0.183524  avg_L1_norm_grad         0.000028  w[0]   -0.034 bias    1.002\n",
      "iter 7200/1000000  loss         0.183508  avg_L1_norm_grad         0.000027  w[0]   -0.034 bias    1.004\n",
      "iter 7201/1000000  loss         0.183508  avg_L1_norm_grad         0.000027  w[0]   -0.034 bias    1.004\n",
      "iter 7300/1000000  loss         0.183493  avg_L1_norm_grad         0.000026  w[0]   -0.034 bias    1.005\n",
      "iter 7301/1000000  loss         0.183493  avg_L1_norm_grad         0.000026  w[0]   -0.034 bias    1.005\n",
      "iter 7400/1000000  loss         0.183479  avg_L1_norm_grad         0.000026  w[0]   -0.034 bias    1.006\n",
      "iter 7401/1000000  loss         0.183479  avg_L1_norm_grad         0.000026  w[0]   -0.034 bias    1.006\n",
      "iter 7500/1000000  loss         0.183465  avg_L1_norm_grad         0.000025  w[0]   -0.035 bias    1.007\n",
      "iter 7501/1000000  loss         0.183465  avg_L1_norm_grad         0.000025  w[0]   -0.035 bias    1.007\n",
      "iter 7600/1000000  loss         0.183452  avg_L1_norm_grad         0.000024  w[0]   -0.035 bias    1.008\n",
      "iter 7601/1000000  loss         0.183452  avg_L1_norm_grad         0.000024  w[0]   -0.035 bias    1.008\n",
      "iter 7700/1000000  loss         0.183440  avg_L1_norm_grad         0.000024  w[0]   -0.035 bias    1.010\n",
      "iter 7701/1000000  loss         0.183440  avg_L1_norm_grad         0.000024  w[0]   -0.035 bias    1.010\n",
      "iter 7800/1000000  loss         0.183428  avg_L1_norm_grad         0.000023  w[0]   -0.035 bias    1.011\n",
      "iter 7801/1000000  loss         0.183428  avg_L1_norm_grad         0.000023  w[0]   -0.035 bias    1.011\n",
      "iter 7900/1000000  loss         0.183417  avg_L1_norm_grad         0.000023  w[0]   -0.035 bias    1.012\n",
      "iter 7901/1000000  loss         0.183417  avg_L1_norm_grad         0.000023  w[0]   -0.035 bias    1.012\n",
      "iter 8000/1000000  loss         0.183407  avg_L1_norm_grad         0.000022  w[0]   -0.035 bias    1.013\n",
      "iter 8001/1000000  loss         0.183407  avg_L1_norm_grad         0.000022  w[0]   -0.035 bias    1.013\n",
      "iter 8100/1000000  loss         0.183397  avg_L1_norm_grad         0.000021  w[0]   -0.036 bias    1.014\n",
      "iter 8101/1000000  loss         0.183397  avg_L1_norm_grad         0.000021  w[0]   -0.036 bias    1.014\n",
      "iter 8200/1000000  loss         0.183387  avg_L1_norm_grad         0.000021  w[0]   -0.036 bias    1.015\n",
      "iter 8201/1000000  loss         0.183387  avg_L1_norm_grad         0.000021  w[0]   -0.036 bias    1.015\n",
      "iter 8300/1000000  loss         0.183378  avg_L1_norm_grad         0.000020  w[0]   -0.036 bias    1.016\n",
      "iter 8301/1000000  loss         0.183378  avg_L1_norm_grad         0.000020  w[0]   -0.036 bias    1.016\n",
      "iter 8400/1000000  loss         0.183370  avg_L1_norm_grad         0.000020  w[0]   -0.036 bias    1.017\n",
      "iter 8401/1000000  loss         0.183370  avg_L1_norm_grad         0.000020  w[0]   -0.036 bias    1.017\n",
      "iter 8500/1000000  loss         0.183361  avg_L1_norm_grad         0.000019  w[0]   -0.036 bias    1.018\n",
      "iter 8501/1000000  loss         0.183361  avg_L1_norm_grad         0.000019  w[0]   -0.036 bias    1.018\n",
      "iter 8600/1000000  loss         0.183354  avg_L1_norm_grad         0.000019  w[0]   -0.036 bias    1.020\n",
      "iter 8601/1000000  loss         0.183354  avg_L1_norm_grad         0.000019  w[0]   -0.036 bias    1.020\n",
      "iter 8700/1000000  loss         0.183346  avg_L1_norm_grad         0.000018  w[0]   -0.036 bias    1.021\n",
      "iter 8701/1000000  loss         0.183346  avg_L1_norm_grad         0.000018  w[0]   -0.036 bias    1.021\n",
      "iter 8800/1000000  loss         0.183339  avg_L1_norm_grad         0.000018  w[0]   -0.037 bias    1.022\n",
      "iter 8801/1000000  loss         0.183339  avg_L1_norm_grad         0.000018  w[0]   -0.037 bias    1.022\n",
      "iter 8900/1000000  loss         0.183333  avg_L1_norm_grad         0.000017  w[0]   -0.037 bias    1.023\n",
      "iter 8901/1000000  loss         0.183333  avg_L1_norm_grad         0.000017  w[0]   -0.037 bias    1.023\n",
      "iter 9000/1000000  loss         0.183326  avg_L1_norm_grad         0.000017  w[0]   -0.037 bias    1.024\n",
      "iter 9001/1000000  loss         0.183326  avg_L1_norm_grad         0.000017  w[0]   -0.037 bias    1.024\n",
      "iter 9100/1000000  loss         0.183320  avg_L1_norm_grad         0.000017  w[0]   -0.037 bias    1.025\n",
      "iter 9101/1000000  loss         0.183320  avg_L1_norm_grad         0.000017  w[0]   -0.037 bias    1.025\n",
      "iter 9200/1000000  loss         0.183314  avg_L1_norm_grad         0.000016  w[0]   -0.037 bias    1.026\n",
      "iter 9201/1000000  loss         0.183314  avg_L1_norm_grad         0.000016  w[0]   -0.037 bias    1.026\n",
      "iter 9300/1000000  loss         0.183309  avg_L1_norm_grad         0.000016  w[0]   -0.037 bias    1.026\n",
      "iter 9301/1000000  loss         0.183309  avg_L1_norm_grad         0.000016  w[0]   -0.037 bias    1.026\n",
      "iter 9400/1000000  loss         0.183304  avg_L1_norm_grad         0.000015  w[0]   -0.037 bias    1.027\n",
      "iter 9401/1000000  loss         0.183304  avg_L1_norm_grad         0.000015  w[0]   -0.037 bias    1.027\n",
      "iter 9500/1000000  loss         0.183299  avg_L1_norm_grad         0.000015  w[0]   -0.037 bias    1.028\n",
      "iter 9501/1000000  loss         0.183299  avg_L1_norm_grad         0.000015  w[0]   -0.037 bias    1.028\n",
      "iter 9600/1000000  loss         0.183294  avg_L1_norm_grad         0.000015  w[0]   -0.038 bias    1.029\n",
      "iter 9601/1000000  loss         0.183294  avg_L1_norm_grad         0.000015  w[0]   -0.038 bias    1.029\n",
      "iter 9700/1000000  loss         0.183290  avg_L1_norm_grad         0.000014  w[0]   -0.038 bias    1.030\n",
      "iter 9701/1000000  loss         0.183289  avg_L1_norm_grad         0.000014  w[0]   -0.038 bias    1.030\n",
      "iter 9800/1000000  loss         0.183285  avg_L1_norm_grad         0.000014  w[0]   -0.038 bias    1.031\n",
      "iter 9801/1000000  loss         0.183285  avg_L1_norm_grad         0.000014  w[0]   -0.038 bias    1.031\n",
      "iter 9900/1000000  loss         0.183281  avg_L1_norm_grad         0.000014  w[0]   -0.038 bias    1.032\n",
      "iter 9901/1000000  loss         0.183281  avg_L1_norm_grad         0.000014  w[0]   -0.038 bias    1.032\n",
      "iter 10000/1000000  loss         0.183277  avg_L1_norm_grad         0.000013  w[0]   -0.038 bias    1.033\n",
      "iter 10001/1000000  loss         0.183277  avg_L1_norm_grad         0.000013  w[0]   -0.038 bias    1.033\n",
      "iter 10100/1000000  loss         0.183273  avg_L1_norm_grad         0.000013  w[0]   -0.038 bias    1.034\n",
      "iter 10101/1000000  loss         0.183273  avg_L1_norm_grad         0.000013  w[0]   -0.038 bias    1.034\n",
      "iter 10200/1000000  loss         0.183270  avg_L1_norm_grad         0.000013  w[0]   -0.038 bias    1.034\n",
      "iter 10201/1000000  loss         0.183270  avg_L1_norm_grad         0.000013  w[0]   -0.038 bias    1.034\n",
      "iter 10300/1000000  loss         0.183267  avg_L1_norm_grad         0.000012  w[0]   -0.038 bias    1.035\n",
      "iter 10301/1000000  loss         0.183267  avg_L1_norm_grad         0.000012  w[0]   -0.038 bias    1.035\n",
      "iter 10400/1000000  loss         0.183263  avg_L1_norm_grad         0.000012  w[0]   -0.038 bias    1.036\n",
      "iter 10401/1000000  loss         0.183263  avg_L1_norm_grad         0.000012  w[0]   -0.038 bias    1.036\n",
      "iter 10500/1000000  loss         0.183260  avg_L1_norm_grad         0.000012  w[0]   -0.038 bias    1.037\n",
      "iter 10501/1000000  loss         0.183260  avg_L1_norm_grad         0.000012  w[0]   -0.038 bias    1.037\n",
      "iter 10600/1000000  loss         0.183257  avg_L1_norm_grad         0.000011  w[0]   -0.038 bias    1.038\n",
      "iter 10601/1000000  loss         0.183257  avg_L1_norm_grad         0.000011  w[0]   -0.038 bias    1.038\n",
      "iter 10700/1000000  loss         0.183254  avg_L1_norm_grad         0.000011  w[0]   -0.039 bias    1.038\n",
      "iter 10701/1000000  loss         0.183254  avg_L1_norm_grad         0.000011  w[0]   -0.039 bias    1.038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.183252  avg_L1_norm_grad         0.000011  w[0]   -0.039 bias    1.039\n",
      "iter 10801/1000000  loss         0.183252  avg_L1_norm_grad         0.000011  w[0]   -0.039 bias    1.039\n",
      "iter 10900/1000000  loss         0.183249  avg_L1_norm_grad         0.000011  w[0]   -0.039 bias    1.040\n",
      "iter 10901/1000000  loss         0.183249  avg_L1_norm_grad         0.000011  w[0]   -0.039 bias    1.040\n",
      "iter 11000/1000000  loss         0.183247  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.041\n",
      "iter 11001/1000000  loss         0.183247  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.041\n",
      "iter 11100/1000000  loss         0.183244  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.041\n",
      "iter 11101/1000000  loss         0.183244  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.041\n",
      "iter 11200/1000000  loss         0.183242  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.042\n",
      "iter 11201/1000000  loss         0.183242  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.042\n",
      "iter 11300/1000000  loss         0.183240  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.043\n",
      "iter 11301/1000000  loss         0.183240  avg_L1_norm_grad         0.000010  w[0]   -0.039 bias    1.043\n",
      "iter 11400/1000000  loss         0.183238  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.043\n",
      "iter 11401/1000000  loss         0.183238  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.043\n",
      "iter 11500/1000000  loss         0.183236  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.044\n",
      "iter 11501/1000000  loss         0.183236  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.044\n",
      "iter 11600/1000000  loss         0.183234  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.045\n",
      "iter 11601/1000000  loss         0.183234  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.045\n",
      "iter 11700/1000000  loss         0.183233  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.045\n",
      "iter 11701/1000000  loss         0.183233  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.045\n",
      "iter 11800/1000000  loss         0.183231  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.046\n",
      "iter 11801/1000000  loss         0.183231  avg_L1_norm_grad         0.000009  w[0]   -0.039 bias    1.046\n",
      "iter 11900/1000000  loss         0.183229  avg_L1_norm_grad         0.000008  w[0]   -0.039 bias    1.047\n",
      "iter 11901/1000000  loss         0.183229  avg_L1_norm_grad         0.000008  w[0]   -0.039 bias    1.047\n",
      "iter 12000/1000000  loss         0.183228  avg_L1_norm_grad         0.000008  w[0]   -0.039 bias    1.047\n",
      "iter 12001/1000000  loss         0.183228  avg_L1_norm_grad         0.000008  w[0]   -0.039 bias    1.047\n",
      "iter 12100/1000000  loss         0.183226  avg_L1_norm_grad         0.000008  w[0]   -0.040 bias    1.048\n",
      "iter 12101/1000000  loss         0.183226  avg_L1_norm_grad         0.000008  w[0]   -0.040 bias    1.048\n",
      "iter 12200/1000000  loss         0.183225  avg_L1_norm_grad         0.000008  w[0]   -0.040 bias    1.049\n",
      "iter 12201/1000000  loss         0.183225  avg_L1_norm_grad         0.000008  w[0]   -0.040 bias    1.049\n",
      "iter 12300/1000000  loss         0.183223  avg_L1_norm_grad         0.000008  w[0]   -0.040 bias    1.049\n",
      "iter 12301/1000000  loss         0.183223  avg_L1_norm_grad         0.000008  w[0]   -0.040 bias    1.049\n",
      "iter 12400/1000000  loss         0.183222  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.050\n",
      "iter 12401/1000000  loss         0.183222  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.050\n",
      "iter 12500/1000000  loss         0.183221  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.050\n",
      "iter 12501/1000000  loss         0.183221  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.050\n",
      "iter 12600/1000000  loss         0.183220  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.051\n",
      "iter 12601/1000000  loss         0.183220  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.051\n",
      "iter 12700/1000000  loss         0.183219  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.051\n",
      "iter 12701/1000000  loss         0.183219  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.051\n",
      "iter 12800/1000000  loss         0.183218  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.052\n",
      "iter 12801/1000000  loss         0.183218  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.052\n",
      "iter 12900/1000000  loss         0.183217  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.052\n",
      "iter 12901/1000000  loss         0.183217  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.052\n",
      "iter 13000/1000000  loss         0.183216  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.053\n",
      "iter 13001/1000000  loss         0.183216  avg_L1_norm_grad         0.000007  w[0]   -0.040 bias    1.053\n",
      "iter 13100/1000000  loss         0.183215  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.053\n",
      "iter 13101/1000000  loss         0.183215  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.053\n",
      "iter 13200/1000000  loss         0.183214  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.054\n",
      "iter 13201/1000000  loss         0.183214  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.054\n",
      "iter 13300/1000000  loss         0.183213  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.054\n",
      "iter 13301/1000000  loss         0.183213  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.054\n",
      "iter 13400/1000000  loss         0.183212  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.055\n",
      "iter 13401/1000000  loss         0.183212  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.055\n",
      "iter 13500/1000000  loss         0.183211  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.055\n",
      "iter 13501/1000000  loss         0.183211  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.055\n",
      "iter 13600/1000000  loss         0.183211  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.056\n",
      "iter 13601/1000000  loss         0.183211  avg_L1_norm_grad         0.000006  w[0]   -0.040 bias    1.056\n",
      "Done. Converged after 13636 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr2 = LRGDF(alpha=100.0, step_size=0.1)\n",
    "new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax Transform\n",
      "TurnOn Loaded 0.0\n",
      "(1, 84000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Shape of Transformed Data (84000, 790)\n",
      "Initializing w_G with 790 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.032054  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.895895  avg_L1_norm_grad         0.025763  w[0]   -0.000 bias   -0.000\n",
      "iter    2/1000000  loss         0.824415  avg_L1_norm_grad         0.018880  w[0]    0.000 bias    0.021\n",
      "iter    3/1000000  loss         0.776940  avg_L1_norm_grad         0.015590  w[0]    0.000 bias    0.026\n",
      "iter    4/1000000  loss         0.741764  avg_L1_norm_grad         0.012616  w[0]    0.000 bias    0.041\n",
      "iter    5/1000000  loss         0.714632  avg_L1_norm_grad         0.011298  w[0]    0.000 bias    0.048\n",
      "iter    6/1000000  loss         0.692295  avg_L1_norm_grad         0.010019  w[0]    0.000 bias    0.060\n",
      "iter    7/1000000  loss         0.673138  avg_L1_norm_grad         0.009311  w[0]    0.000 bias    0.068\n",
      "iter    8/1000000  loss         0.656207  avg_L1_norm_grad         0.008657  w[0]    0.000 bias    0.078\n",
      "iter    9/1000000  loss         0.640944  avg_L1_norm_grad         0.008175  w[0]    0.000 bias    0.087\n",
      "iter   10/1000000  loss         0.626992  avg_L1_norm_grad         0.007751  w[0]    0.000 bias    0.096\n",
      "iter   11/1000000  loss         0.614108  avg_L1_norm_grad         0.007397  w[0]    0.000 bias    0.104\n",
      "iter   12/1000000  loss         0.602121  avg_L1_norm_grad         0.007088  w[0]    0.000 bias    0.113\n",
      "iter   13/1000000  loss         0.590903  avg_L1_norm_grad         0.006817  w[0]    0.000 bias    0.121\n",
      "iter   14/1000000  loss         0.580358  avg_L1_norm_grad         0.006577  w[0]    0.000 bias    0.129\n",
      "iter   15/1000000  loss         0.570409  avg_L1_norm_grad         0.006361  w[0]    0.000 bias    0.137\n",
      "iter   16/1000000  loss         0.560995  avg_L1_norm_grad         0.006164  w[0]    0.000 bias    0.144\n",
      "iter   17/1000000  loss         0.552066  avg_L1_norm_grad         0.005983  w[0]    0.001 bias    0.152\n",
      "iter   18/1000000  loss         0.543581  avg_L1_norm_grad         0.005813  w[0]    0.001 bias    0.159\n",
      "iter   19/1000000  loss         0.535502  avg_L1_norm_grad         0.005656  w[0]    0.001 bias    0.166\n",
      "iter  100/1000000  loss         0.311935  avg_L1_norm_grad         0.001967  w[0]    0.001 bias    0.502\n",
      "iter  101/1000000  loss         0.310935  avg_L1_norm_grad         0.001953  w[0]    0.001 bias    0.504\n",
      "iter  200/1000000  loss         0.253393  avg_L1_norm_grad         0.001180  w[0]    0.001 bias    0.686\n",
      "iter  201/1000000  loss         0.253048  avg_L1_norm_grad         0.001176  w[0]    0.001 bias    0.687\n",
      "iter  300/1000000  loss         0.228383  avg_L1_norm_grad         0.000877  w[0]    0.000 bias    0.790\n",
      "iter  301/1000000  loss         0.228201  avg_L1_norm_grad         0.000875  w[0]    0.000 bias    0.791\n",
      "iter  400/1000000  loss         0.213873  avg_L1_norm_grad         0.000712  w[0]   -0.001 bias    0.857\n",
      "iter  401/1000000  loss         0.213757  avg_L1_norm_grad         0.000711  w[0]   -0.001 bias    0.858\n",
      "iter  500/1000000  loss         0.204106  avg_L1_norm_grad         0.000610  w[0]   -0.002 bias    0.904\n",
      "iter  501/1000000  loss         0.204023  avg_L1_norm_grad         0.000609  w[0]   -0.002 bias    0.904\n",
      "iter  600/1000000  loss         0.196921  avg_L1_norm_grad         0.000538  w[0]   -0.003 bias    0.937\n",
      "iter  601/1000000  loss         0.196859  avg_L1_norm_grad         0.000537  w[0]   -0.003 bias    0.937\n",
      "iter  700/1000000  loss         0.191319  avg_L1_norm_grad         0.000485  w[0]   -0.004 bias    0.961\n",
      "iter  701/1000000  loss         0.191269  avg_L1_norm_grad         0.000484  w[0]   -0.004 bias    0.961\n",
      "iter  800/1000000  loss         0.186768  avg_L1_norm_grad         0.000443  w[0]   -0.005 bias    0.979\n",
      "iter  801/1000000  loss         0.186727  avg_L1_norm_grad         0.000443  w[0]   -0.005 bias    0.979\n",
      "iter  900/1000000  loss         0.182958  avg_L1_norm_grad         0.000409  w[0]   -0.006 bias    0.992\n",
      "iter  901/1000000  loss         0.182923  avg_L1_norm_grad         0.000409  w[0]   -0.006 bias    0.993\n",
      "iter 1000/1000000  loss         0.179696  avg_L1_norm_grad         0.000382  w[0]   -0.007 bias    1.003\n",
      "iter 1001/1000000  loss         0.179666  avg_L1_norm_grad         0.000381  w[0]   -0.007 bias    1.003\n",
      "iter 1100/1000000  loss         0.176853  avg_L1_norm_grad         0.000358  w[0]   -0.008 bias    1.010\n",
      "iter 1101/1000000  loss         0.176826  avg_L1_norm_grad         0.000358  w[0]   -0.008 bias    1.011\n",
      "iter 1200/1000000  loss         0.174340  avg_L1_norm_grad         0.000338  w[0]   -0.009 bias    1.016\n",
      "iter 1201/1000000  loss         0.174316  avg_L1_norm_grad         0.000338  w[0]   -0.009 bias    1.016\n",
      "iter 1300/1000000  loss         0.172094  avg_L1_norm_grad         0.000321  w[0]   -0.010 bias    1.021\n",
      "iter 1301/1000000  loss         0.172072  avg_L1_norm_grad         0.000321  w[0]   -0.010 bias    1.021\n",
      "iter 1400/1000000  loss         0.170067  avg_L1_norm_grad         0.000306  w[0]   -0.011 bias    1.024\n",
      "iter 1401/1000000  loss         0.170048  avg_L1_norm_grad         0.000306  w[0]   -0.011 bias    1.024\n",
      "iter 1500/1000000  loss         0.168224  avg_L1_norm_grad         0.000292  w[0]   -0.012 bias    1.027\n",
      "iter 1501/1000000  loss         0.168206  avg_L1_norm_grad         0.000292  w[0]   -0.012 bias    1.027\n",
      "iter 1600/1000000  loss         0.166537  avg_L1_norm_grad         0.000280  w[0]   -0.013 bias    1.029\n",
      "iter 1601/1000000  loss         0.166521  avg_L1_norm_grad         0.000280  w[0]   -0.013 bias    1.029\n",
      "iter 1700/1000000  loss         0.164984  avg_L1_norm_grad         0.000269  w[0]   -0.014 bias    1.030\n",
      "iter 1701/1000000  loss         0.164969  avg_L1_norm_grad         0.000269  w[0]   -0.014 bias    1.031\n",
      "iter 1800/1000000  loss         0.163547  avg_L1_norm_grad         0.000259  w[0]   -0.015 bias    1.032\n",
      "iter 1801/1000000  loss         0.163533  avg_L1_norm_grad         0.000259  w[0]   -0.015 bias    1.032\n",
      "iter 1900/1000000  loss         0.162212  avg_L1_norm_grad         0.000250  w[0]   -0.016 bias    1.033\n",
      "iter 1901/1000000  loss         0.162200  avg_L1_norm_grad         0.000250  w[0]   -0.016 bias    1.033\n",
      "iter 2000/1000000  loss         0.160968  avg_L1_norm_grad         0.000242  w[0]   -0.017 bias    1.033\n",
      "iter 2001/1000000  loss         0.160956  avg_L1_norm_grad         0.000242  w[0]   -0.017 bias    1.033\n",
      "iter 2100/1000000  loss         0.159803  avg_L1_norm_grad         0.000235  w[0]   -0.018 bias    1.034\n",
      "iter 2101/1000000  loss         0.159792  avg_L1_norm_grad         0.000235  w[0]   -0.018 bias    1.034\n",
      "iter 2200/1000000  loss         0.158710  avg_L1_norm_grad         0.000228  w[0]   -0.019 bias    1.035\n",
      "iter 2201/1000000  loss         0.158699  avg_L1_norm_grad         0.000228  w[0]   -0.019 bias    1.035\n",
      "iter 2300/1000000  loss         0.157681  avg_L1_norm_grad         0.000221  w[0]   -0.020 bias    1.035\n",
      "iter 2301/1000000  loss         0.157671  avg_L1_norm_grad         0.000221  w[0]   -0.020 bias    1.035\n",
      "iter 2400/1000000  loss         0.156711  avg_L1_norm_grad         0.000215  w[0]   -0.020 bias    1.036\n",
      "iter 2401/1000000  loss         0.156701  avg_L1_norm_grad         0.000215  w[0]   -0.020 bias    1.036\n",
      "iter 2500/1000000  loss         0.155793  avg_L1_norm_grad         0.000209  w[0]   -0.021 bias    1.037\n",
      "iter 2501/1000000  loss         0.155784  avg_L1_norm_grad         0.000209  w[0]   -0.021 bias    1.037\n",
      "iter 2600/1000000  loss         0.154923  avg_L1_norm_grad         0.000204  w[0]   -0.022 bias    1.037\n",
      "iter 2601/1000000  loss         0.154915  avg_L1_norm_grad         0.000204  w[0]   -0.022 bias    1.037\n",
      "iter 2700/1000000  loss         0.154097  avg_L1_norm_grad         0.000199  w[0]   -0.023 bias    1.038\n",
      "iter 2701/1000000  loss         0.154089  avg_L1_norm_grad         0.000199  w[0]   -0.023 bias    1.038\n",
      "iter 2800/1000000  loss         0.153312  avg_L1_norm_grad         0.000194  w[0]   -0.024 bias    1.039\n",
      "iter 2801/1000000  loss         0.153304  avg_L1_norm_grad         0.000194  w[0]   -0.024 bias    1.039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.152564  avg_L1_norm_grad         0.000190  w[0]   -0.025 bias    1.040\n",
      "iter 2901/1000000  loss         0.152557  avg_L1_norm_grad         0.000190  w[0]   -0.025 bias    1.040\n",
      "iter 3000/1000000  loss         0.151850  avg_L1_norm_grad         0.000185  w[0]   -0.026 bias    1.041\n",
      "iter 3001/1000000  loss         0.151843  avg_L1_norm_grad         0.000185  w[0]   -0.026 bias    1.041\n",
      "iter 3100/1000000  loss         0.151168  avg_L1_norm_grad         0.000181  w[0]   -0.027 bias    1.042\n",
      "iter 3101/1000000  loss         0.151161  avg_L1_norm_grad         0.000181  w[0]   -0.027 bias    1.042\n",
      "iter 3200/1000000  loss         0.150515  avg_L1_norm_grad         0.000178  w[0]   -0.027 bias    1.043\n",
      "iter 3201/1000000  loss         0.150509  avg_L1_norm_grad         0.000178  w[0]   -0.027 bias    1.043\n",
      "iter 3300/1000000  loss         0.149890  avg_L1_norm_grad         0.000174  w[0]   -0.028 bias    1.044\n",
      "iter 3301/1000000  loss         0.149884  avg_L1_norm_grad         0.000174  w[0]   -0.028 bias    1.044\n",
      "iter 3400/1000000  loss         0.149290  avg_L1_norm_grad         0.000170  w[0]   -0.029 bias    1.045\n",
      "iter 3401/1000000  loss         0.149284  avg_L1_norm_grad         0.000170  w[0]   -0.029 bias    1.045\n",
      "iter 3500/1000000  loss         0.148714  avg_L1_norm_grad         0.000167  w[0]   -0.030 bias    1.047\n",
      "iter 3501/1000000  loss         0.148709  avg_L1_norm_grad         0.000167  w[0]   -0.030 bias    1.047\n",
      "iter 3600/1000000  loss         0.148161  avg_L1_norm_grad         0.000164  w[0]   -0.031 bias    1.048\n",
      "iter 3601/1000000  loss         0.148155  avg_L1_norm_grad         0.000164  w[0]   -0.031 bias    1.048\n",
      "iter 3700/1000000  loss         0.147628  avg_L1_norm_grad         0.000161  w[0]   -0.031 bias    1.050\n",
      "iter 3701/1000000  loss         0.147622  avg_L1_norm_grad         0.000161  w[0]   -0.031 bias    1.050\n",
      "iter 3800/1000000  loss         0.147114  avg_L1_norm_grad         0.000158  w[0]   -0.032 bias    1.051\n",
      "iter 3801/1000000  loss         0.147109  avg_L1_norm_grad         0.000158  w[0]   -0.032 bias    1.052\n",
      "iter 3900/1000000  loss         0.146620  avg_L1_norm_grad         0.000155  w[0]   -0.033 bias    1.053\n",
      "iter 3901/1000000  loss         0.146615  avg_L1_norm_grad         0.000155  w[0]   -0.033 bias    1.053\n",
      "iter 4000/1000000  loss         0.146142  avg_L1_norm_grad         0.000153  w[0]   -0.034 bias    1.055\n",
      "iter 4001/1000000  loss         0.146138  avg_L1_norm_grad         0.000153  w[0]   -0.034 bias    1.055\n",
      "iter 4100/1000000  loss         0.145681  avg_L1_norm_grad         0.000150  w[0]   -0.035 bias    1.057\n",
      "iter 4101/1000000  loss         0.145677  avg_L1_norm_grad         0.000150  w[0]   -0.035 bias    1.057\n",
      "iter 4200/1000000  loss         0.145235  avg_L1_norm_grad         0.000148  w[0]   -0.035 bias    1.059\n",
      "iter 4201/1000000  loss         0.145231  avg_L1_norm_grad         0.000148  w[0]   -0.035 bias    1.059\n",
      "iter 4300/1000000  loss         0.144804  avg_L1_norm_grad         0.000145  w[0]   -0.036 bias    1.061\n",
      "iter 4301/1000000  loss         0.144800  avg_L1_norm_grad         0.000145  w[0]   -0.036 bias    1.061\n",
      "iter 4400/1000000  loss         0.144387  avg_L1_norm_grad         0.000143  w[0]   -0.037 bias    1.063\n",
      "iter 4401/1000000  loss         0.144383  avg_L1_norm_grad         0.000143  w[0]   -0.037 bias    1.063\n",
      "iter 4500/1000000  loss         0.143983  avg_L1_norm_grad         0.000141  w[0]   -0.038 bias    1.066\n",
      "iter 4501/1000000  loss         0.143979  avg_L1_norm_grad         0.000141  w[0]   -0.038 bias    1.066\n",
      "iter 4600/1000000  loss         0.143591  avg_L1_norm_grad         0.000139  w[0]   -0.038 bias    1.068\n",
      "iter 4601/1000000  loss         0.143587  avg_L1_norm_grad         0.000139  w[0]   -0.038 bias    1.068\n",
      "iter 4700/1000000  loss         0.143211  avg_L1_norm_grad         0.000137  w[0]   -0.039 bias    1.070\n",
      "iter 4701/1000000  loss         0.143208  avg_L1_norm_grad         0.000137  w[0]   -0.039 bias    1.070\n",
      "iter 4800/1000000  loss         0.142843  avg_L1_norm_grad         0.000135  w[0]   -0.040 bias    1.073\n",
      "iter 4801/1000000  loss         0.142839  avg_L1_norm_grad         0.000135  w[0]   -0.040 bias    1.073\n",
      "iter 4900/1000000  loss         0.142484  avg_L1_norm_grad         0.000133  w[0]   -0.041 bias    1.075\n",
      "iter 4901/1000000  loss         0.142481  avg_L1_norm_grad         0.000133  w[0]   -0.041 bias    1.075\n",
      "iter 5000/1000000  loss         0.142137  avg_L1_norm_grad         0.000131  w[0]   -0.041 bias    1.078\n",
      "iter 5001/1000000  loss         0.142133  avg_L1_norm_grad         0.000131  w[0]   -0.041 bias    1.078\n",
      "iter 5100/1000000  loss         0.141799  avg_L1_norm_grad         0.000129  w[0]   -0.042 bias    1.081\n",
      "iter 5101/1000000  loss         0.141795  avg_L1_norm_grad         0.000129  w[0]   -0.042 bias    1.081\n",
      "iter 5200/1000000  loss         0.141470  avg_L1_norm_grad         0.000127  w[0]   -0.043 bias    1.083\n",
      "iter 5201/1000000  loss         0.141466  avg_L1_norm_grad         0.000127  w[0]   -0.043 bias    1.083\n",
      "iter 5300/1000000  loss         0.141150  avg_L1_norm_grad         0.000126  w[0]   -0.044 bias    1.086\n",
      "iter 5301/1000000  loss         0.141146  avg_L1_norm_grad         0.000126  w[0]   -0.044 bias    1.086\n",
      "iter 5400/1000000  loss         0.140838  avg_L1_norm_grad         0.000124  w[0]   -0.044 bias    1.089\n",
      "iter 5401/1000000  loss         0.140835  avg_L1_norm_grad         0.000124  w[0]   -0.044 bias    1.089\n",
      "iter 5500/1000000  loss         0.140535  avg_L1_norm_grad         0.000122  w[0]   -0.045 bias    1.092\n",
      "iter 5501/1000000  loss         0.140532  avg_L1_norm_grad         0.000122  w[0]   -0.045 bias    1.092\n",
      "iter 5600/1000000  loss         0.140239  avg_L1_norm_grad         0.000121  w[0]   -0.046 bias    1.094\n",
      "iter 5601/1000000  loss         0.140236  avg_L1_norm_grad         0.000121  w[0]   -0.046 bias    1.094\n",
      "iter 5700/1000000  loss         0.139951  avg_L1_norm_grad         0.000119  w[0]   -0.046 bias    1.097\n",
      "iter 5701/1000000  loss         0.139948  avg_L1_norm_grad         0.000119  w[0]   -0.046 bias    1.097\n",
      "iter 5800/1000000  loss         0.139670  avg_L1_norm_grad         0.000118  w[0]   -0.047 bias    1.100\n",
      "iter 5801/1000000  loss         0.139667  avg_L1_norm_grad         0.000118  w[0]   -0.047 bias    1.100\n",
      "iter 5900/1000000  loss         0.139395  avg_L1_norm_grad         0.000116  w[0]   -0.048 bias    1.103\n",
      "iter 5901/1000000  loss         0.139393  avg_L1_norm_grad         0.000116  w[0]   -0.048 bias    1.103\n",
      "iter 6000/1000000  loss         0.139128  avg_L1_norm_grad         0.000115  w[0]   -0.048 bias    1.106\n",
      "iter 6001/1000000  loss         0.139125  avg_L1_norm_grad         0.000115  w[0]   -0.048 bias    1.106\n",
      "iter 6100/1000000  loss         0.138867  avg_L1_norm_grad         0.000114  w[0]   -0.049 bias    1.109\n",
      "iter 6101/1000000  loss         0.138864  avg_L1_norm_grad         0.000114  w[0]   -0.049 bias    1.109\n",
      "iter 6200/1000000  loss         0.138611  avg_L1_norm_grad         0.000112  w[0]   -0.050 bias    1.113\n",
      "iter 6201/1000000  loss         0.138609  avg_L1_norm_grad         0.000112  w[0]   -0.050 bias    1.113\n",
      "iter 6300/1000000  loss         0.138362  avg_L1_norm_grad         0.000111  w[0]   -0.050 bias    1.116\n",
      "iter 6301/1000000  loss         0.138360  avg_L1_norm_grad         0.000111  w[0]   -0.050 bias    1.116\n",
      "iter 6400/1000000  loss         0.138118  avg_L1_norm_grad         0.000110  w[0]   -0.051 bias    1.119\n",
      "iter 6401/1000000  loss         0.138116  avg_L1_norm_grad         0.000110  w[0]   -0.051 bias    1.119\n",
      "iter 6500/1000000  loss         0.137880  avg_L1_norm_grad         0.000109  w[0]   -0.052 bias    1.122\n",
      "iter 6501/1000000  loss         0.137878  avg_L1_norm_grad         0.000109  w[0]   -0.052 bias    1.122\n",
      "iter 6600/1000000  loss         0.137647  avg_L1_norm_grad         0.000107  w[0]   -0.052 bias    1.125\n",
      "iter 6601/1000000  loss         0.137645  avg_L1_norm_grad         0.000107  w[0]   -0.052 bias    1.125\n",
      "iter 6700/1000000  loss         0.137419  avg_L1_norm_grad         0.000106  w[0]   -0.053 bias    1.129\n",
      "iter 6701/1000000  loss         0.137417  avg_L1_norm_grad         0.000106  w[0]   -0.053 bias    1.129\n",
      "iter 6800/1000000  loss         0.137196  avg_L1_norm_grad         0.000105  w[0]   -0.054 bias    1.132\n",
      "iter 6801/1000000  loss         0.137194  avg_L1_norm_grad         0.000105  w[0]   -0.054 bias    1.132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.136978  avg_L1_norm_grad         0.000104  w[0]   -0.054 bias    1.135\n",
      "iter 6901/1000000  loss         0.136976  avg_L1_norm_grad         0.000104  w[0]   -0.054 bias    1.135\n",
      "iter 7000/1000000  loss         0.136764  avg_L1_norm_grad         0.000103  w[0]   -0.055 bias    1.138\n",
      "iter 7001/1000000  loss         0.136762  avg_L1_norm_grad         0.000103  w[0]   -0.055 bias    1.139\n",
      "iter 7100/1000000  loss         0.136555  avg_L1_norm_grad         0.000102  w[0]   -0.056 bias    1.142\n",
      "iter 7101/1000000  loss         0.136553  avg_L1_norm_grad         0.000102  w[0]   -0.056 bias    1.142\n",
      "iter 7200/1000000  loss         0.136350  avg_L1_norm_grad         0.000101  w[0]   -0.056 bias    1.145\n",
      "iter 7201/1000000  loss         0.136348  avg_L1_norm_grad         0.000101  w[0]   -0.056 bias    1.145\n",
      "iter 7300/1000000  loss         0.136149  avg_L1_norm_grad         0.000100  w[0]   -0.057 bias    1.149\n",
      "iter 7301/1000000  loss         0.136147  avg_L1_norm_grad         0.000100  w[0]   -0.057 bias    1.149\n",
      "iter 7400/1000000  loss         0.135952  avg_L1_norm_grad         0.000099  w[0]   -0.057 bias    1.152\n",
      "iter 7401/1000000  loss         0.135950  avg_L1_norm_grad         0.000099  w[0]   -0.057 bias    1.152\n",
      "iter 7500/1000000  loss         0.135759  avg_L1_norm_grad         0.000098  w[0]   -0.058 bias    1.155\n",
      "iter 7501/1000000  loss         0.135757  avg_L1_norm_grad         0.000098  w[0]   -0.058 bias    1.155\n",
      "iter 7600/1000000  loss         0.135570  avg_L1_norm_grad         0.000097  w[0]   -0.059 bias    1.159\n",
      "iter 7601/1000000  loss         0.135568  avg_L1_norm_grad         0.000097  w[0]   -0.059 bias    1.159\n",
      "iter 7700/1000000  loss         0.135384  avg_L1_norm_grad         0.000096  w[0]   -0.059 bias    1.162\n",
      "iter 7701/1000000  loss         0.135382  avg_L1_norm_grad         0.000096  w[0]   -0.059 bias    1.162\n",
      "iter 7800/1000000  loss         0.135202  avg_L1_norm_grad         0.000095  w[0]   -0.060 bias    1.166\n",
      "iter 7801/1000000  loss         0.135200  avg_L1_norm_grad         0.000095  w[0]   -0.060 bias    1.166\n",
      "iter 7900/1000000  loss         0.135023  avg_L1_norm_grad         0.000094  w[0]   -0.060 bias    1.169\n",
      "iter 7901/1000000  loss         0.135022  avg_L1_norm_grad         0.000094  w[0]   -0.060 bias    1.169\n",
      "iter 8000/1000000  loss         0.134848  avg_L1_norm_grad         0.000093  w[0]   -0.061 bias    1.173\n",
      "iter 8001/1000000  loss         0.134846  avg_L1_norm_grad         0.000093  w[0]   -0.061 bias    1.173\n",
      "iter 8100/1000000  loss         0.134676  avg_L1_norm_grad         0.000092  w[0]   -0.062 bias    1.176\n",
      "iter 8101/1000000  loss         0.134674  avg_L1_norm_grad         0.000092  w[0]   -0.062 bias    1.176\n",
      "iter 8200/1000000  loss         0.134507  avg_L1_norm_grad         0.000091  w[0]   -0.062 bias    1.180\n",
      "iter 8201/1000000  loss         0.134505  avg_L1_norm_grad         0.000091  w[0]   -0.062 bias    1.180\n",
      "iter 8300/1000000  loss         0.134341  avg_L1_norm_grad         0.000091  w[0]   -0.063 bias    1.183\n",
      "iter 8301/1000000  loss         0.134340  avg_L1_norm_grad         0.000091  w[0]   -0.063 bias    1.183\n",
      "iter 8400/1000000  loss         0.134178  avg_L1_norm_grad         0.000090  w[0]   -0.063 bias    1.187\n",
      "iter 8401/1000000  loss         0.134177  avg_L1_norm_grad         0.000090  w[0]   -0.063 bias    1.187\n",
      "iter 8500/1000000  loss         0.134018  avg_L1_norm_grad         0.000089  w[0]   -0.064 bias    1.190\n",
      "iter 8501/1000000  loss         0.134017  avg_L1_norm_grad         0.000089  w[0]   -0.064 bias    1.190\n",
      "iter 8600/1000000  loss         0.133861  avg_L1_norm_grad         0.000088  w[0]   -0.065 bias    1.194\n",
      "iter 8601/1000000  loss         0.133860  avg_L1_norm_grad         0.000088  w[0]   -0.065 bias    1.194\n",
      "iter 8700/1000000  loss         0.133707  avg_L1_norm_grad         0.000087  w[0]   -0.065 bias    1.197\n",
      "iter 8701/1000000  loss         0.133705  avg_L1_norm_grad         0.000087  w[0]   -0.065 bias    1.198\n",
      "iter 8800/1000000  loss         0.133555  avg_L1_norm_grad         0.000087  w[0]   -0.066 bias    1.201\n",
      "iter 8801/1000000  loss         0.133553  avg_L1_norm_grad         0.000087  w[0]   -0.066 bias    1.201\n",
      "iter 8900/1000000  loss         0.133406  avg_L1_norm_grad         0.000086  w[0]   -0.066 bias    1.205\n",
      "iter 8901/1000000  loss         0.133404  avg_L1_norm_grad         0.000086  w[0]   -0.066 bias    1.205\n",
      "iter 9000/1000000  loss         0.133259  avg_L1_norm_grad         0.000085  w[0]   -0.067 bias    1.208\n",
      "iter 9001/1000000  loss         0.133258  avg_L1_norm_grad         0.000085  w[0]   -0.067 bias    1.208\n",
      "iter 9100/1000000  loss         0.133115  avg_L1_norm_grad         0.000084  w[0]   -0.067 bias    1.212\n",
      "iter 9101/1000000  loss         0.133113  avg_L1_norm_grad         0.000084  w[0]   -0.067 bias    1.212\n",
      "iter 9200/1000000  loss         0.132973  avg_L1_norm_grad         0.000084  w[0]   -0.068 bias    1.215\n",
      "iter 9201/1000000  loss         0.132972  avg_L1_norm_grad         0.000084  w[0]   -0.068 bias    1.215\n",
      "iter 9300/1000000  loss         0.132833  avg_L1_norm_grad         0.000083  w[0]   -0.068 bias    1.219\n",
      "iter 9301/1000000  loss         0.132832  avg_L1_norm_grad         0.000083  w[0]   -0.068 bias    1.219\n",
      "iter 9400/1000000  loss         0.132696  avg_L1_norm_grad         0.000082  w[0]   -0.069 bias    1.223\n",
      "iter 9401/1000000  loss         0.132695  avg_L1_norm_grad         0.000082  w[0]   -0.069 bias    1.223\n",
      "iter 9500/1000000  loss         0.132561  avg_L1_norm_grad         0.000081  w[0]   -0.070 bias    1.226\n",
      "iter 9501/1000000  loss         0.132560  avg_L1_norm_grad         0.000081  w[0]   -0.070 bias    1.226\n",
      "iter 9600/1000000  loss         0.132428  avg_L1_norm_grad         0.000081  w[0]   -0.070 bias    1.230\n",
      "iter 9601/1000000  loss         0.132427  avg_L1_norm_grad         0.000081  w[0]   -0.070 bias    1.230\n",
      "iter 9700/1000000  loss         0.132298  avg_L1_norm_grad         0.000080  w[0]   -0.071 bias    1.233\n",
      "iter 9701/1000000  loss         0.132296  avg_L1_norm_grad         0.000080  w[0]   -0.071 bias    1.233\n",
      "iter 9800/1000000  loss         0.132169  avg_L1_norm_grad         0.000079  w[0]   -0.071 bias    1.237\n",
      "iter 9801/1000000  loss         0.132168  avg_L1_norm_grad         0.000079  w[0]   -0.071 bias    1.237\n",
      "iter 9900/1000000  loss         0.132043  avg_L1_norm_grad         0.000079  w[0]   -0.072 bias    1.241\n",
      "iter 9901/1000000  loss         0.132041  avg_L1_norm_grad         0.000079  w[0]   -0.072 bias    1.241\n",
      "iter 10000/1000000  loss         0.131918  avg_L1_norm_grad         0.000078  w[0]   -0.072 bias    1.244\n",
      "iter 10001/1000000  loss         0.131917  avg_L1_norm_grad         0.000078  w[0]   -0.072 bias    1.244\n",
      "iter 10100/1000000  loss         0.131795  avg_L1_norm_grad         0.000078  w[0]   -0.073 bias    1.248\n",
      "iter 10101/1000000  loss         0.131794  avg_L1_norm_grad         0.000078  w[0]   -0.073 bias    1.248\n",
      "iter 10200/1000000  loss         0.131674  avg_L1_norm_grad         0.000077  w[0]   -0.073 bias    1.251\n",
      "iter 10201/1000000  loss         0.131673  avg_L1_norm_grad         0.000077  w[0]   -0.073 bias    1.252\n",
      "iter 10300/1000000  loss         0.131555  avg_L1_norm_grad         0.000076  w[0]   -0.074 bias    1.255\n",
      "iter 10301/1000000  loss         0.131554  avg_L1_norm_grad         0.000076  w[0]   -0.074 bias    1.255\n",
      "iter 10400/1000000  loss         0.131438  avg_L1_norm_grad         0.000076  w[0]   -0.074 bias    1.259\n",
      "iter 10401/1000000  loss         0.131437  avg_L1_norm_grad         0.000076  w[0]   -0.074 bias    1.259\n",
      "iter 10500/1000000  loss         0.131323  avg_L1_norm_grad         0.000075  w[0]   -0.075 bias    1.262\n",
      "iter 10501/1000000  loss         0.131322  avg_L1_norm_grad         0.000075  w[0]   -0.075 bias    1.262\n",
      "iter 10600/1000000  loss         0.131209  avg_L1_norm_grad         0.000075  w[0]   -0.075 bias    1.266\n",
      "iter 10601/1000000  loss         0.131208  avg_L1_norm_grad         0.000075  w[0]   -0.075 bias    1.266\n",
      "iter 10700/1000000  loss         0.131097  avg_L1_norm_grad         0.000074  w[0]   -0.076 bias    1.270\n",
      "iter 10701/1000000  loss         0.131096  avg_L1_norm_grad         0.000074  w[0]   -0.076 bias    1.270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.130987  avg_L1_norm_grad         0.000073  w[0]   -0.076 bias    1.273\n",
      "iter 10801/1000000  loss         0.130986  avg_L1_norm_grad         0.000073  w[0]   -0.076 bias    1.273\n",
      "iter 10900/1000000  loss         0.130878  avg_L1_norm_grad         0.000073  w[0]   -0.077 bias    1.277\n",
      "iter 10901/1000000  loss         0.130877  avg_L1_norm_grad         0.000073  w[0]   -0.077 bias    1.277\n",
      "iter 11000/1000000  loss         0.130771  avg_L1_norm_grad         0.000072  w[0]   -0.077 bias    1.280\n",
      "iter 11001/1000000  loss         0.130770  avg_L1_norm_grad         0.000072  w[0]   -0.077 bias    1.280\n",
      "iter 11100/1000000  loss         0.130665  avg_L1_norm_grad         0.000072  w[0]   -0.078 bias    1.284\n",
      "iter 11101/1000000  loss         0.130664  avg_L1_norm_grad         0.000072  w[0]   -0.078 bias    1.284\n",
      "iter 11200/1000000  loss         0.130561  avg_L1_norm_grad         0.000071  w[0]   -0.078 bias    1.288\n",
      "iter 11201/1000000  loss         0.130560  avg_L1_norm_grad         0.000071  w[0]   -0.078 bias    1.288\n",
      "iter 11300/1000000  loss         0.130458  avg_L1_norm_grad         0.000071  w[0]   -0.079 bias    1.291\n",
      "iter 11301/1000000  loss         0.130457  avg_L1_norm_grad         0.000071  w[0]   -0.079 bias    1.291\n",
      "iter 11400/1000000  loss         0.130357  avg_L1_norm_grad         0.000070  w[0]   -0.079 bias    1.295\n",
      "iter 11401/1000000  loss         0.130356  avg_L1_norm_grad         0.000070  w[0]   -0.079 bias    1.295\n",
      "iter 11500/1000000  loss         0.130257  avg_L1_norm_grad         0.000070  w[0]   -0.080 bias    1.298\n",
      "iter 11501/1000000  loss         0.130256  avg_L1_norm_grad         0.000070  w[0]   -0.080 bias    1.299\n",
      "iter 11600/1000000  loss         0.130159  avg_L1_norm_grad         0.000069  w[0]   -0.080 bias    1.302\n",
      "iter 11601/1000000  loss         0.130158  avg_L1_norm_grad         0.000069  w[0]   -0.080 bias    1.302\n",
      "iter 11700/1000000  loss         0.130062  avg_L1_norm_grad         0.000069  w[0]   -0.081 bias    1.306\n",
      "iter 11701/1000000  loss         0.130061  avg_L1_norm_grad         0.000069  w[0]   -0.081 bias    1.306\n",
      "iter 11800/1000000  loss         0.129966  avg_L1_norm_grad         0.000068  w[0]   -0.081 bias    1.309\n",
      "iter 11801/1000000  loss         0.129965  avg_L1_norm_grad         0.000068  w[0]   -0.081 bias    1.309\n",
      "iter 11900/1000000  loss         0.129872  avg_L1_norm_grad         0.000068  w[0]   -0.081 bias    1.313\n",
      "iter 11901/1000000  loss         0.129871  avg_L1_norm_grad         0.000068  w[0]   -0.081 bias    1.313\n",
      "iter 12000/1000000  loss         0.129779  avg_L1_norm_grad         0.000067  w[0]   -0.082 bias    1.316\n",
      "iter 12001/1000000  loss         0.129778  avg_L1_norm_grad         0.000067  w[0]   -0.082 bias    1.317\n",
      "iter 12100/1000000  loss         0.129687  avg_L1_norm_grad         0.000067  w[0]   -0.082 bias    1.320\n",
      "iter 12101/1000000  loss         0.129686  avg_L1_norm_grad         0.000067  w[0]   -0.082 bias    1.320\n",
      "iter 12200/1000000  loss         0.129596  avg_L1_norm_grad         0.000066  w[0]   -0.083 bias    1.324\n",
      "iter 12201/1000000  loss         0.129595  avg_L1_norm_grad         0.000066  w[0]   -0.083 bias    1.324\n",
      "iter 12300/1000000  loss         0.129507  avg_L1_norm_grad         0.000066  w[0]   -0.083 bias    1.327\n",
      "iter 12301/1000000  loss         0.129506  avg_L1_norm_grad         0.000066  w[0]   -0.083 bias    1.327\n",
      "iter 12400/1000000  loss         0.129419  avg_L1_norm_grad         0.000065  w[0]   -0.084 bias    1.331\n",
      "iter 12401/1000000  loss         0.129418  avg_L1_norm_grad         0.000065  w[0]   -0.084 bias    1.331\n",
      "iter 12500/1000000  loss         0.129331  avg_L1_norm_grad         0.000065  w[0]   -0.084 bias    1.334\n",
      "iter 12501/1000000  loss         0.129331  avg_L1_norm_grad         0.000065  w[0]   -0.084 bias    1.334\n",
      "iter 12600/1000000  loss         0.129246  avg_L1_norm_grad         0.000064  w[0]   -0.085 bias    1.338\n",
      "iter 12601/1000000  loss         0.129245  avg_L1_norm_grad         0.000064  w[0]   -0.085 bias    1.338\n",
      "iter 12700/1000000  loss         0.129161  avg_L1_norm_grad         0.000064  w[0]   -0.085 bias    1.342\n",
      "iter 12701/1000000  loss         0.129160  avg_L1_norm_grad         0.000064  w[0]   -0.085 bias    1.342\n",
      "iter 12800/1000000  loss         0.129077  avg_L1_norm_grad         0.000064  w[0]   -0.086 bias    1.345\n",
      "iter 12801/1000000  loss         0.129076  avg_L1_norm_grad         0.000064  w[0]   -0.086 bias    1.345\n",
      "iter 12900/1000000  loss         0.128995  avg_L1_norm_grad         0.000063  w[0]   -0.086 bias    1.349\n",
      "iter 12901/1000000  loss         0.128994  avg_L1_norm_grad         0.000063  w[0]   -0.086 bias    1.349\n",
      "iter 13000/1000000  loss         0.128913  avg_L1_norm_grad         0.000063  w[0]   -0.086 bias    1.352\n",
      "iter 13001/1000000  loss         0.128912  avg_L1_norm_grad         0.000063  w[0]   -0.086 bias    1.352\n",
      "iter 13100/1000000  loss         0.128833  avg_L1_norm_grad         0.000062  w[0]   -0.087 bias    1.356\n",
      "iter 13101/1000000  loss         0.128832  avg_L1_norm_grad         0.000062  w[0]   -0.087 bias    1.356\n",
      "iter 13200/1000000  loss         0.128753  avg_L1_norm_grad         0.000062  w[0]   -0.087 bias    1.359\n",
      "iter 13201/1000000  loss         0.128752  avg_L1_norm_grad         0.000062  w[0]   -0.087 bias    1.359\n",
      "iter 13300/1000000  loss         0.128675  avg_L1_norm_grad         0.000062  w[0]   -0.088 bias    1.363\n",
      "iter 13301/1000000  loss         0.128674  avg_L1_norm_grad         0.000062  w[0]   -0.088 bias    1.363\n",
      "iter 13400/1000000  loss         0.128597  avg_L1_norm_grad         0.000061  w[0]   -0.088 bias    1.366\n",
      "iter 13401/1000000  loss         0.128596  avg_L1_norm_grad         0.000061  w[0]   -0.088 bias    1.366\n",
      "iter 13500/1000000  loss         0.128521  avg_L1_norm_grad         0.000061  w[0]   -0.089 bias    1.370\n",
      "iter 13501/1000000  loss         0.128520  avg_L1_norm_grad         0.000061  w[0]   -0.089 bias    1.370\n",
      "iter 13600/1000000  loss         0.128445  avg_L1_norm_grad         0.000060  w[0]   -0.089 bias    1.373\n",
      "iter 13601/1000000  loss         0.128444  avg_L1_norm_grad         0.000060  w[0]   -0.089 bias    1.373\n",
      "iter 13700/1000000  loss         0.128370  avg_L1_norm_grad         0.000060  w[0]   -0.089 bias    1.377\n",
      "iter 13701/1000000  loss         0.128370  avg_L1_norm_grad         0.000060  w[0]   -0.089 bias    1.377\n",
      "iter 13800/1000000  loss         0.128297  avg_L1_norm_grad         0.000060  w[0]   -0.090 bias    1.380\n",
      "iter 13801/1000000  loss         0.128296  avg_L1_norm_grad         0.000060  w[0]   -0.090 bias    1.380\n",
      "iter 13900/1000000  loss         0.128224  avg_L1_norm_grad         0.000059  w[0]   -0.090 bias    1.384\n",
      "iter 13901/1000000  loss         0.128223  avg_L1_norm_grad         0.000059  w[0]   -0.090 bias    1.384\n",
      "iter 14000/1000000  loss         0.128152  avg_L1_norm_grad         0.000059  w[0]   -0.091 bias    1.387\n",
      "iter 14001/1000000  loss         0.128151  avg_L1_norm_grad         0.000059  w[0]   -0.091 bias    1.387\n",
      "iter 14100/1000000  loss         0.128081  avg_L1_norm_grad         0.000058  w[0]   -0.091 bias    1.391\n",
      "iter 14101/1000000  loss         0.128080  avg_L1_norm_grad         0.000058  w[0]   -0.091 bias    1.391\n",
      "iter 14200/1000000  loss         0.128011  avg_L1_norm_grad         0.000058  w[0]   -0.091 bias    1.394\n",
      "iter 14201/1000000  loss         0.128010  avg_L1_norm_grad         0.000058  w[0]   -0.091 bias    1.394\n",
      "iter 14300/1000000  loss         0.127942  avg_L1_norm_grad         0.000058  w[0]   -0.092 bias    1.398\n",
      "iter 14301/1000000  loss         0.127941  avg_L1_norm_grad         0.000058  w[0]   -0.092 bias    1.398\n",
      "iter 14400/1000000  loss         0.127873  avg_L1_norm_grad         0.000057  w[0]   -0.092 bias    1.401\n",
      "iter 14401/1000000  loss         0.127873  avg_L1_norm_grad         0.000057  w[0]   -0.092 bias    1.401\n",
      "iter 14500/1000000  loss         0.127806  avg_L1_norm_grad         0.000057  w[0]   -0.093 bias    1.405\n",
      "iter 14501/1000000  loss         0.127805  avg_L1_norm_grad         0.000057  w[0]   -0.093 bias    1.405\n",
      "iter 14600/1000000  loss         0.127739  avg_L1_norm_grad         0.000057  w[0]   -0.093 bias    1.408\n",
      "iter 14601/1000000  loss         0.127738  avg_L1_norm_grad         0.000057  w[0]   -0.093 bias    1.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14700/1000000  loss         0.127673  avg_L1_norm_grad         0.000056  w[0]   -0.093 bias    1.412\n",
      "iter 14701/1000000  loss         0.127672  avg_L1_norm_grad         0.000056  w[0]   -0.093 bias    1.412\n",
      "iter 14800/1000000  loss         0.127607  avg_L1_norm_grad         0.000056  w[0]   -0.094 bias    1.415\n",
      "iter 14801/1000000  loss         0.127607  avg_L1_norm_grad         0.000056  w[0]   -0.094 bias    1.415\n",
      "iter 14900/1000000  loss         0.127543  avg_L1_norm_grad         0.000056  w[0]   -0.094 bias    1.419\n",
      "iter 14901/1000000  loss         0.127542  avg_L1_norm_grad         0.000056  w[0]   -0.094 bias    1.419\n",
      "iter 15000/1000000  loss         0.127479  avg_L1_norm_grad         0.000055  w[0]   -0.095 bias    1.422\n",
      "iter 15001/1000000  loss         0.127479  avg_L1_norm_grad         0.000055  w[0]   -0.095 bias    1.422\n",
      "iter 15100/1000000  loss         0.127416  avg_L1_norm_grad         0.000055  w[0]   -0.095 bias    1.426\n",
      "iter 15101/1000000  loss         0.127416  avg_L1_norm_grad         0.000055  w[0]   -0.095 bias    1.426\n",
      "iter 15200/1000000  loss         0.127354  avg_L1_norm_grad         0.000055  w[0]   -0.095 bias    1.429\n",
      "iter 15201/1000000  loss         0.127353  avg_L1_norm_grad         0.000055  w[0]   -0.095 bias    1.429\n",
      "iter 15300/1000000  loss         0.127292  avg_L1_norm_grad         0.000054  w[0]   -0.096 bias    1.432\n",
      "iter 15301/1000000  loss         0.127292  avg_L1_norm_grad         0.000054  w[0]   -0.096 bias    1.433\n",
      "iter 15400/1000000  loss         0.127231  avg_L1_norm_grad         0.000054  w[0]   -0.096 bias    1.436\n",
      "iter 15401/1000000  loss         0.127231  avg_L1_norm_grad         0.000054  w[0]   -0.096 bias    1.436\n",
      "iter 15500/1000000  loss         0.127171  avg_L1_norm_grad         0.000054  w[0]   -0.096 bias    1.439\n",
      "iter 15501/1000000  loss         0.127171  avg_L1_norm_grad         0.000054  w[0]   -0.096 bias    1.439\n",
      "iter 15600/1000000  loss         0.127112  avg_L1_norm_grad         0.000053  w[0]   -0.097 bias    1.443\n",
      "iter 15601/1000000  loss         0.127111  avg_L1_norm_grad         0.000053  w[0]   -0.097 bias    1.443\n",
      "iter 15700/1000000  loss         0.127053  avg_L1_norm_grad         0.000053  w[0]   -0.097 bias    1.446\n",
      "iter 15701/1000000  loss         0.127052  avg_L1_norm_grad         0.000053  w[0]   -0.097 bias    1.446\n",
      "iter 15800/1000000  loss         0.126995  avg_L1_norm_grad         0.000053  w[0]   -0.098 bias    1.450\n",
      "iter 15801/1000000  loss         0.126994  avg_L1_norm_grad         0.000053  w[0]   -0.098 bias    1.450\n",
      "iter 15900/1000000  loss         0.126937  avg_L1_norm_grad         0.000052  w[0]   -0.098 bias    1.453\n",
      "iter 15901/1000000  loss         0.126937  avg_L1_norm_grad         0.000052  w[0]   -0.098 bias    1.453\n",
      "iter 16000/1000000  loss         0.126881  avg_L1_norm_grad         0.000052  w[0]   -0.098 bias    1.456\n",
      "iter 16001/1000000  loss         0.126880  avg_L1_norm_grad         0.000052  w[0]   -0.098 bias    1.456\n",
      "iter 16100/1000000  loss         0.126824  avg_L1_norm_grad         0.000052  w[0]   -0.099 bias    1.460\n",
      "iter 16101/1000000  loss         0.126824  avg_L1_norm_grad         0.000052  w[0]   -0.099 bias    1.460\n",
      "iter 16200/1000000  loss         0.126769  avg_L1_norm_grad         0.000051  w[0]   -0.099 bias    1.463\n",
      "iter 16201/1000000  loss         0.126768  avg_L1_norm_grad         0.000051  w[0]   -0.099 bias    1.463\n",
      "iter 16300/1000000  loss         0.126714  avg_L1_norm_grad         0.000051  w[0]   -0.099 bias    1.466\n",
      "iter 16301/1000000  loss         0.126713  avg_L1_norm_grad         0.000051  w[0]   -0.099 bias    1.466\n",
      "iter 16400/1000000  loss         0.126660  avg_L1_norm_grad         0.000051  w[0]   -0.100 bias    1.470\n",
      "iter 16401/1000000  loss         0.126659  avg_L1_norm_grad         0.000051  w[0]   -0.100 bias    1.470\n",
      "iter 16500/1000000  loss         0.126606  avg_L1_norm_grad         0.000051  w[0]   -0.100 bias    1.473\n",
      "iter 16501/1000000  loss         0.126605  avg_L1_norm_grad         0.000051  w[0]   -0.100 bias    1.473\n",
      "iter 16600/1000000  loss         0.126553  avg_L1_norm_grad         0.000050  w[0]   -0.100 bias    1.476\n",
      "iter 16601/1000000  loss         0.126552  avg_L1_norm_grad         0.000050  w[0]   -0.100 bias    1.477\n",
      "iter 16700/1000000  loss         0.126500  avg_L1_norm_grad         0.000050  w[0]   -0.101 bias    1.480\n",
      "iter 16701/1000000  loss         0.126500  avg_L1_norm_grad         0.000050  w[0]   -0.101 bias    1.480\n",
      "iter 16800/1000000  loss         0.126448  avg_L1_norm_grad         0.000050  w[0]   -0.101 bias    1.483\n",
      "iter 16801/1000000  loss         0.126448  avg_L1_norm_grad         0.000050  w[0]   -0.101 bias    1.483\n",
      "iter 16900/1000000  loss         0.126397  avg_L1_norm_grad         0.000049  w[0]   -0.101 bias    1.486\n",
      "iter 16901/1000000  loss         0.126396  avg_L1_norm_grad         0.000049  w[0]   -0.101 bias    1.486\n",
      "iter 17000/1000000  loss         0.126346  avg_L1_norm_grad         0.000049  w[0]   -0.102 bias    1.490\n",
      "iter 17001/1000000  loss         0.126346  avg_L1_norm_grad         0.000049  w[0]   -0.102 bias    1.490\n",
      "iter 17100/1000000  loss         0.126296  avg_L1_norm_grad         0.000049  w[0]   -0.102 bias    1.493\n",
      "iter 17101/1000000  loss         0.126295  avg_L1_norm_grad         0.000049  w[0]   -0.102 bias    1.493\n",
      "iter 17200/1000000  loss         0.126246  avg_L1_norm_grad         0.000049  w[0]   -0.102 bias    1.496\n",
      "iter 17201/1000000  loss         0.126245  avg_L1_norm_grad         0.000049  w[0]   -0.102 bias    1.496\n",
      "iter 17300/1000000  loss         0.126197  avg_L1_norm_grad         0.000048  w[0]   -0.103 bias    1.500\n",
      "iter 17301/1000000  loss         0.126196  avg_L1_norm_grad         0.000048  w[0]   -0.103 bias    1.500\n",
      "iter 17400/1000000  loss         0.126148  avg_L1_norm_grad         0.000048  w[0]   -0.103 bias    1.503\n",
      "iter 17401/1000000  loss         0.126148  avg_L1_norm_grad         0.000048  w[0]   -0.103 bias    1.503\n",
      "iter 17500/1000000  loss         0.126100  avg_L1_norm_grad         0.000048  w[0]   -0.103 bias    1.506\n",
      "iter 17501/1000000  loss         0.126099  avg_L1_norm_grad         0.000048  w[0]   -0.103 bias    1.506\n",
      "iter 17600/1000000  loss         0.126052  avg_L1_norm_grad         0.000048  w[0]   -0.104 bias    1.510\n",
      "iter 17601/1000000  loss         0.126052  avg_L1_norm_grad         0.000048  w[0]   -0.104 bias    1.510\n",
      "iter 17700/1000000  loss         0.126005  avg_L1_norm_grad         0.000047  w[0]   -0.104 bias    1.513\n",
      "iter 17701/1000000  loss         0.126005  avg_L1_norm_grad         0.000047  w[0]   -0.104 bias    1.513\n",
      "iter 17800/1000000  loss         0.125958  avg_L1_norm_grad         0.000047  w[0]   -0.104 bias    1.516\n",
      "iter 17801/1000000  loss         0.125958  avg_L1_norm_grad         0.000047  w[0]   -0.104 bias    1.516\n",
      "iter 17900/1000000  loss         0.125912  avg_L1_norm_grad         0.000047  w[0]   -0.105 bias    1.519\n",
      "iter 17901/1000000  loss         0.125912  avg_L1_norm_grad         0.000047  w[0]   -0.105 bias    1.519\n",
      "iter 18000/1000000  loss         0.125866  avg_L1_norm_grad         0.000047  w[0]   -0.105 bias    1.523\n",
      "iter 18001/1000000  loss         0.125866  avg_L1_norm_grad         0.000047  w[0]   -0.105 bias    1.523\n",
      "iter 18100/1000000  loss         0.125821  avg_L1_norm_grad         0.000046  w[0]   -0.105 bias    1.526\n",
      "iter 18101/1000000  loss         0.125821  avg_L1_norm_grad         0.000046  w[0]   -0.105 bias    1.526\n",
      "iter 18200/1000000  loss         0.125776  avg_L1_norm_grad         0.000046  w[0]   -0.106 bias    1.529\n",
      "iter 18201/1000000  loss         0.125776  avg_L1_norm_grad         0.000046  w[0]   -0.106 bias    1.529\n",
      "iter 18300/1000000  loss         0.125732  avg_L1_norm_grad         0.000046  w[0]   -0.106 bias    1.532\n",
      "iter 18301/1000000  loss         0.125732  avg_L1_norm_grad         0.000046  w[0]   -0.106 bias    1.532\n",
      "iter 18400/1000000  loss         0.125688  avg_L1_norm_grad         0.000046  w[0]   -0.106 bias    1.536\n",
      "iter 18401/1000000  loss         0.125688  avg_L1_norm_grad         0.000046  w[0]   -0.106 bias    1.536\n",
      "iter 18500/1000000  loss         0.125645  avg_L1_norm_grad         0.000045  w[0]   -0.107 bias    1.539\n",
      "iter 18501/1000000  loss         0.125645  avg_L1_norm_grad         0.000045  w[0]   -0.107 bias    1.539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18600/1000000  loss         0.125602  avg_L1_norm_grad         0.000045  w[0]   -0.107 bias    1.542\n",
      "iter 18601/1000000  loss         0.125602  avg_L1_norm_grad         0.000045  w[0]   -0.107 bias    1.542\n",
      "iter 18700/1000000  loss         0.125560  avg_L1_norm_grad         0.000045  w[0]   -0.107 bias    1.545\n",
      "iter 18701/1000000  loss         0.125559  avg_L1_norm_grad         0.000045  w[0]   -0.107 bias    1.545\n",
      "iter 18800/1000000  loss         0.125518  avg_L1_norm_grad         0.000045  w[0]   -0.108 bias    1.548\n",
      "iter 18801/1000000  loss         0.125517  avg_L1_norm_grad         0.000045  w[0]   -0.108 bias    1.548\n",
      "iter 18900/1000000  loss         0.125476  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias    1.552\n",
      "iter 18901/1000000  loss         0.125476  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias    1.552\n",
      "iter 19000/1000000  loss         0.125435  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias    1.555\n",
      "iter 19001/1000000  loss         0.125434  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias    1.555\n",
      "iter 19100/1000000  loss         0.125394  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias    1.558\n",
      "iter 19101/1000000  loss         0.125394  avg_L1_norm_grad         0.000044  w[0]   -0.108 bias    1.558\n",
      "iter 19200/1000000  loss         0.125354  avg_L1_norm_grad         0.000044  w[0]   -0.109 bias    1.561\n",
      "iter 19201/1000000  loss         0.125353  avg_L1_norm_grad         0.000044  w[0]   -0.109 bias    1.561\n",
      "iter 19300/1000000  loss         0.125314  avg_L1_norm_grad         0.000044  w[0]   -0.109 bias    1.564\n",
      "iter 19301/1000000  loss         0.125313  avg_L1_norm_grad         0.000043  w[0]   -0.109 bias    1.564\n",
      "iter 19400/1000000  loss         0.125274  avg_L1_norm_grad         0.000043  w[0]   -0.109 bias    1.567\n",
      "iter 19401/1000000  loss         0.125274  avg_L1_norm_grad         0.000043  w[0]   -0.109 bias    1.567\n",
      "iter 19500/1000000  loss         0.125235  avg_L1_norm_grad         0.000043  w[0]   -0.110 bias    1.571\n",
      "iter 19501/1000000  loss         0.125235  avg_L1_norm_grad         0.000043  w[0]   -0.110 bias    1.571\n",
      "iter 19600/1000000  loss         0.125196  avg_L1_norm_grad         0.000043  w[0]   -0.110 bias    1.574\n",
      "iter 19601/1000000  loss         0.125196  avg_L1_norm_grad         0.000043  w[0]   -0.110 bias    1.574\n",
      "iter 19700/1000000  loss         0.125158  avg_L1_norm_grad         0.000043  w[0]   -0.110 bias    1.577\n",
      "iter 19701/1000000  loss         0.125157  avg_L1_norm_grad         0.000043  w[0]   -0.110 bias    1.577\n",
      "iter 19800/1000000  loss         0.125120  avg_L1_norm_grad         0.000042  w[0]   -0.110 bias    1.580\n",
      "iter 19801/1000000  loss         0.125119  avg_L1_norm_grad         0.000042  w[0]   -0.110 bias    1.580\n",
      "iter 19900/1000000  loss         0.125082  avg_L1_norm_grad         0.000042  w[0]   -0.111 bias    1.583\n",
      "iter 19901/1000000  loss         0.125082  avg_L1_norm_grad         0.000042  w[0]   -0.111 bias    1.583\n",
      "iter 20000/1000000  loss         0.125045  avg_L1_norm_grad         0.000042  w[0]   -0.111 bias    1.586\n",
      "iter 20001/1000000  loss         0.125045  avg_L1_norm_grad         0.000042  w[0]   -0.111 bias    1.586\n",
      "iter 20100/1000000  loss         0.125008  avg_L1_norm_grad         0.000042  w[0]   -0.111 bias    1.589\n",
      "iter 20101/1000000  loss         0.125008  avg_L1_norm_grad         0.000042  w[0]   -0.111 bias    1.589\n",
      "iter 20200/1000000  loss         0.124972  avg_L1_norm_grad         0.000042  w[0]   -0.112 bias    1.592\n",
      "iter 20201/1000000  loss         0.124971  avg_L1_norm_grad         0.000042  w[0]   -0.112 bias    1.592\n",
      "iter 20300/1000000  loss         0.124935  avg_L1_norm_grad         0.000041  w[0]   -0.112 bias    1.595\n",
      "iter 20301/1000000  loss         0.124935  avg_L1_norm_grad         0.000041  w[0]   -0.112 bias    1.595\n",
      "iter 20400/1000000  loss         0.124900  avg_L1_norm_grad         0.000041  w[0]   -0.112 bias    1.599\n",
      "iter 20401/1000000  loss         0.124899  avg_L1_norm_grad         0.000041  w[0]   -0.112 bias    1.599\n",
      "iter 20500/1000000  loss         0.124864  avg_L1_norm_grad         0.000041  w[0]   -0.112 bias    1.602\n",
      "iter 20501/1000000  loss         0.124864  avg_L1_norm_grad         0.000041  w[0]   -0.112 bias    1.602\n",
      "iter 20600/1000000  loss         0.124829  avg_L1_norm_grad         0.000041  w[0]   -0.113 bias    1.605\n",
      "iter 20601/1000000  loss         0.124829  avg_L1_norm_grad         0.000041  w[0]   -0.113 bias    1.605\n",
      "iter 20700/1000000  loss         0.124794  avg_L1_norm_grad         0.000041  w[0]   -0.113 bias    1.608\n",
      "iter 20701/1000000  loss         0.124794  avg_L1_norm_grad         0.000041  w[0]   -0.113 bias    1.608\n",
      "iter 20800/1000000  loss         0.124760  avg_L1_norm_grad         0.000040  w[0]   -0.113 bias    1.611\n",
      "iter 20801/1000000  loss         0.124760  avg_L1_norm_grad         0.000040  w[0]   -0.113 bias    1.611\n",
      "iter 20900/1000000  loss         0.124726  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.614\n",
      "iter 20901/1000000  loss         0.124725  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.614\n",
      "iter 21000/1000000  loss         0.124692  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.617\n",
      "iter 21001/1000000  loss         0.124692  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.617\n",
      "iter 21100/1000000  loss         0.124659  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.620\n",
      "iter 21101/1000000  loss         0.124658  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.620\n",
      "iter 21200/1000000  loss         0.124625  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.623\n",
      "iter 21201/1000000  loss         0.124625  avg_L1_norm_grad         0.000040  w[0]   -0.114 bias    1.623\n",
      "iter 21300/1000000  loss         0.124593  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.626\n",
      "iter 21301/1000000  loss         0.124592  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.626\n",
      "iter 21400/1000000  loss         0.124560  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.629\n",
      "iter 21401/1000000  loss         0.124560  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.629\n",
      "iter 21500/1000000  loss         0.124528  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.632\n",
      "iter 21501/1000000  loss         0.124528  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.632\n",
      "iter 21600/1000000  loss         0.124496  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.635\n",
      "iter 21601/1000000  loss         0.124496  avg_L1_norm_grad         0.000039  w[0]   -0.115 bias    1.635\n",
      "iter 21700/1000000  loss         0.124464  avg_L1_norm_grad         0.000039  w[0]   -0.116 bias    1.638\n",
      "iter 21701/1000000  loss         0.124464  avg_L1_norm_grad         0.000039  w[0]   -0.116 bias    1.638\n",
      "iter 21800/1000000  loss         0.124433  avg_L1_norm_grad         0.000038  w[0]   -0.116 bias    1.641\n",
      "iter 21801/1000000  loss         0.124433  avg_L1_norm_grad         0.000038  w[0]   -0.116 bias    1.641\n",
      "iter 21900/1000000  loss         0.124402  avg_L1_norm_grad         0.000038  w[0]   -0.116 bias    1.644\n",
      "iter 21901/1000000  loss         0.124402  avg_L1_norm_grad         0.000038  w[0]   -0.116 bias    1.644\n",
      "iter 22000/1000000  loss         0.124372  avg_L1_norm_grad         0.000038  w[0]   -0.116 bias    1.647\n",
      "iter 22001/1000000  loss         0.124371  avg_L1_norm_grad         0.000038  w[0]   -0.116 bias    1.647\n",
      "iter 22100/1000000  loss         0.124341  avg_L1_norm_grad         0.000038  w[0]   -0.117 bias    1.650\n",
      "iter 22101/1000000  loss         0.124341  avg_L1_norm_grad         0.000038  w[0]   -0.117 bias    1.650\n",
      "iter 22200/1000000  loss         0.124311  avg_L1_norm_grad         0.000038  w[0]   -0.117 bias    1.653\n",
      "iter 22201/1000000  loss         0.124311  avg_L1_norm_grad         0.000038  w[0]   -0.117 bias    1.653\n",
      "iter 22300/1000000  loss         0.124281  avg_L1_norm_grad         0.000037  w[0]   -0.117 bias    1.656\n",
      "iter 22301/1000000  loss         0.124281  avg_L1_norm_grad         0.000037  w[0]   -0.117 bias    1.656\n",
      "iter 22400/1000000  loss         0.124252  avg_L1_norm_grad         0.000037  w[0]   -0.117 bias    1.659\n",
      "iter 22401/1000000  loss         0.124251  avg_L1_norm_grad         0.000037  w[0]   -0.117 bias    1.659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22500/1000000  loss         0.124222  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.662\n",
      "iter 22501/1000000  loss         0.124222  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.662\n",
      "iter 22600/1000000  loss         0.124193  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.665\n",
      "iter 22601/1000000  loss         0.124193  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.665\n",
      "iter 22700/1000000  loss         0.124165  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.668\n",
      "iter 22701/1000000  loss         0.124164  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.668\n",
      "iter 22800/1000000  loss         0.124136  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.670\n",
      "iter 22801/1000000  loss         0.124136  avg_L1_norm_grad         0.000037  w[0]   -0.118 bias    1.670\n",
      "iter 22900/1000000  loss         0.124108  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.673\n",
      "iter 22901/1000000  loss         0.124108  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.673\n",
      "iter 23000/1000000  loss         0.124080  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.676\n",
      "iter 23001/1000000  loss         0.124080  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.676\n",
      "iter 23100/1000000  loss         0.124052  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.679\n",
      "iter 23101/1000000  loss         0.124052  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.679\n",
      "iter 23200/1000000  loss         0.124025  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.682\n",
      "iter 23201/1000000  loss         0.124025  avg_L1_norm_grad         0.000036  w[0]   -0.119 bias    1.682\n",
      "iter 23300/1000000  loss         0.123998  avg_L1_norm_grad         0.000036  w[0]   -0.120 bias    1.685\n",
      "iter 23301/1000000  loss         0.123997  avg_L1_norm_grad         0.000036  w[0]   -0.120 bias    1.685\n",
      "iter 23400/1000000  loss         0.123971  avg_L1_norm_grad         0.000036  w[0]   -0.120 bias    1.688\n",
      "iter 23401/1000000  loss         0.123971  avg_L1_norm_grad         0.000036  w[0]   -0.120 bias    1.688\n",
      "iter 23500/1000000  loss         0.123944  avg_L1_norm_grad         0.000035  w[0]   -0.120 bias    1.691\n",
      "iter 23501/1000000  loss         0.123944  avg_L1_norm_grad         0.000035  w[0]   -0.120 bias    1.691\n",
      "iter 23600/1000000  loss         0.123918  avg_L1_norm_grad         0.000035  w[0]   -0.120 bias    1.694\n",
      "iter 23601/1000000  loss         0.123917  avg_L1_norm_grad         0.000035  w[0]   -0.120 bias    1.694\n",
      "iter 23700/1000000  loss         0.123891  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.696\n",
      "iter 23701/1000000  loss         0.123891  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.696\n",
      "iter 23800/1000000  loss         0.123865  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.699\n",
      "iter 23801/1000000  loss         0.123865  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.699\n",
      "iter 23900/1000000  loss         0.123840  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.702\n",
      "iter 23901/1000000  loss         0.123839  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.702\n",
      "iter 24000/1000000  loss         0.123814  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.705\n",
      "iter 24001/1000000  loss         0.123814  avg_L1_norm_grad         0.000035  w[0]   -0.121 bias    1.705\n",
      "iter 24100/1000000  loss         0.123789  avg_L1_norm_grad         0.000034  w[0]   -0.121 bias    1.708\n",
      "iter 24101/1000000  loss         0.123789  avg_L1_norm_grad         0.000034  w[0]   -0.121 bias    1.708\n",
      "iter 24200/1000000  loss         0.123764  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.711\n",
      "iter 24201/1000000  loss         0.123764  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.711\n",
      "iter 24300/1000000  loss         0.123739  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.713\n",
      "iter 24301/1000000  loss         0.123739  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.713\n",
      "iter 24400/1000000  loss         0.123714  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.716\n",
      "iter 24401/1000000  loss         0.123714  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.716\n",
      "iter 24500/1000000  loss         0.123690  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.719\n",
      "iter 24501/1000000  loss         0.123690  avg_L1_norm_grad         0.000034  w[0]   -0.122 bias    1.719\n",
      "iter 24600/1000000  loss         0.123666  avg_L1_norm_grad         0.000034  w[0]   -0.123 bias    1.722\n",
      "iter 24601/1000000  loss         0.123666  avg_L1_norm_grad         0.000034  w[0]   -0.123 bias    1.722\n",
      "iter 24700/1000000  loss         0.123642  avg_L1_norm_grad         0.000034  w[0]   -0.123 bias    1.725\n",
      "iter 24701/1000000  loss         0.123642  avg_L1_norm_grad         0.000034  w[0]   -0.123 bias    1.725\n",
      "iter 24800/1000000  loss         0.123618  avg_L1_norm_grad         0.000033  w[0]   -0.123 bias    1.727\n",
      "iter 24801/1000000  loss         0.123618  avg_L1_norm_grad         0.000033  w[0]   -0.123 bias    1.727\n",
      "iter 24900/1000000  loss         0.123595  avg_L1_norm_grad         0.000033  w[0]   -0.123 bias    1.730\n",
      "iter 24901/1000000  loss         0.123595  avg_L1_norm_grad         0.000033  w[0]   -0.123 bias    1.730\n",
      "iter 25000/1000000  loss         0.123571  avg_L1_norm_grad         0.000033  w[0]   -0.123 bias    1.733\n",
      "iter 25001/1000000  loss         0.123571  avg_L1_norm_grad         0.000033  w[0]   -0.123 bias    1.733\n",
      "iter 25100/1000000  loss         0.123548  avg_L1_norm_grad         0.000033  w[0]   -0.124 bias    1.736\n",
      "iter 25101/1000000  loss         0.123548  avg_L1_norm_grad         0.000033  w[0]   -0.124 bias    1.736\n",
      "iter 25200/1000000  loss         0.123525  avg_L1_norm_grad         0.000033  w[0]   -0.124 bias    1.738\n",
      "iter 25201/1000000  loss         0.123525  avg_L1_norm_grad         0.000033  w[0]   -0.124 bias    1.738\n",
      "iter 25300/1000000  loss         0.123503  avg_L1_norm_grad         0.000033  w[0]   -0.124 bias    1.741\n",
      "iter 25301/1000000  loss         0.123503  avg_L1_norm_grad         0.000033  w[0]   -0.124 bias    1.741\n",
      "iter 25400/1000000  loss         0.123480  avg_L1_norm_grad         0.000032  w[0]   -0.124 bias    1.744\n",
      "iter 25401/1000000  loss         0.123480  avg_L1_norm_grad         0.000032  w[0]   -0.124 bias    1.744\n",
      "iter 25500/1000000  loss         0.123458  avg_L1_norm_grad         0.000032  w[0]   -0.124 bias    1.747\n",
      "iter 25501/1000000  loss         0.123458  avg_L1_norm_grad         0.000032  w[0]   -0.124 bias    1.747\n",
      "iter 25600/1000000  loss         0.123436  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.749\n",
      "iter 25601/1000000  loss         0.123436  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.749\n",
      "iter 25700/1000000  loss         0.123414  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.752\n",
      "iter 25701/1000000  loss         0.123414  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.752\n",
      "iter 25800/1000000  loss         0.123392  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.755\n",
      "iter 25801/1000000  loss         0.123392  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.755\n",
      "iter 25900/1000000  loss         0.123371  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.757\n",
      "iter 25901/1000000  loss         0.123370  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.757\n",
      "iter 26000/1000000  loss         0.123349  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.760\n",
      "iter 26001/1000000  loss         0.123349  avg_L1_norm_grad         0.000032  w[0]   -0.125 bias    1.760\n",
      "iter 26100/1000000  loss         0.123328  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.763\n",
      "iter 26101/1000000  loss         0.123328  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.763\n",
      "iter 26200/1000000  loss         0.123307  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.765\n",
      "iter 26201/1000000  loss         0.123307  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.765\n",
      "iter 26300/1000000  loss         0.123286  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.768\n",
      "iter 26301/1000000  loss         0.123286  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26400/1000000  loss         0.123266  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.771\n",
      "iter 26401/1000000  loss         0.123266  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.771\n",
      "iter 26500/1000000  loss         0.123245  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.773\n",
      "iter 26501/1000000  loss         0.123245  avg_L1_norm_grad         0.000031  w[0]   -0.126 bias    1.773\n",
      "iter 26600/1000000  loss         0.123225  avg_L1_norm_grad         0.000031  w[0]   -0.127 bias    1.776\n",
      "iter 26601/1000000  loss         0.123225  avg_L1_norm_grad         0.000031  w[0]   -0.127 bias    1.776\n",
      "iter 26700/1000000  loss         0.123205  avg_L1_norm_grad         0.000031  w[0]   -0.127 bias    1.779\n",
      "iter 26701/1000000  loss         0.123205  avg_L1_norm_grad         0.000031  w[0]   -0.127 bias    1.779\n",
      "iter 26800/1000000  loss         0.123185  avg_L1_norm_grad         0.000031  w[0]   -0.127 bias    1.781\n",
      "iter 26801/1000000  loss         0.123185  avg_L1_norm_grad         0.000031  w[0]   -0.127 bias    1.781\n",
      "iter 26900/1000000  loss         0.123165  avg_L1_norm_grad         0.000030  w[0]   -0.127 bias    1.784\n",
      "iter 26901/1000000  loss         0.123165  avg_L1_norm_grad         0.000030  w[0]   -0.127 bias    1.784\n",
      "iter 27000/1000000  loss         0.123146  avg_L1_norm_grad         0.000030  w[0]   -0.127 bias    1.787\n",
      "iter 27001/1000000  loss         0.123145  avg_L1_norm_grad         0.000030  w[0]   -0.127 bias    1.787\n",
      "iter 27100/1000000  loss         0.123126  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.789\n",
      "iter 27101/1000000  loss         0.123126  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.789\n",
      "iter 27200/1000000  loss         0.123107  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.792\n",
      "iter 27201/1000000  loss         0.123107  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.792\n",
      "iter 27300/1000000  loss         0.123088  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.794\n",
      "iter 27301/1000000  loss         0.123088  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.794\n",
      "iter 27400/1000000  loss         0.123069  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.797\n",
      "iter 27401/1000000  loss         0.123069  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.797\n",
      "iter 27500/1000000  loss         0.123050  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.800\n",
      "iter 27501/1000000  loss         0.123050  avg_L1_norm_grad         0.000030  w[0]   -0.128 bias    1.800\n",
      "iter 27600/1000000  loss         0.123032  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.802\n",
      "iter 27601/1000000  loss         0.123031  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.802\n",
      "iter 27700/1000000  loss         0.123013  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.805\n",
      "iter 27701/1000000  loss         0.123013  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.805\n",
      "iter 27800/1000000  loss         0.122995  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.807\n",
      "iter 27801/1000000  loss         0.122995  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.807\n",
      "iter 27900/1000000  loss         0.122977  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.810\n",
      "iter 27901/1000000  loss         0.122977  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.810\n",
      "iter 28000/1000000  loss         0.122959  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.812\n",
      "iter 28001/1000000  loss         0.122959  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.812\n",
      "iter 28100/1000000  loss         0.122941  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.815\n",
      "iter 28101/1000000  loss         0.122941  avg_L1_norm_grad         0.000029  w[0]   -0.129 bias    1.815\n",
      "iter 28200/1000000  loss         0.122923  avg_L1_norm_grad         0.000029  w[0]   -0.130 bias    1.818\n",
      "iter 28201/1000000  loss         0.122923  avg_L1_norm_grad         0.000029  w[0]   -0.130 bias    1.818\n",
      "iter 28300/1000000  loss         0.122906  avg_L1_norm_grad         0.000029  w[0]   -0.130 bias    1.820\n",
      "iter 28301/1000000  loss         0.122906  avg_L1_norm_grad         0.000029  w[0]   -0.130 bias    1.820\n",
      "iter 28400/1000000  loss         0.122888  avg_L1_norm_grad         0.000028  w[0]   -0.130 bias    1.823\n",
      "iter 28401/1000000  loss         0.122888  avg_L1_norm_grad         0.000028  w[0]   -0.130 bias    1.823\n",
      "iter 28500/1000000  loss         0.122871  avg_L1_norm_grad         0.000028  w[0]   -0.130 bias    1.825\n",
      "iter 28501/1000000  loss         0.122871  avg_L1_norm_grad         0.000028  w[0]   -0.130 bias    1.825\n",
      "iter 28600/1000000  loss         0.122854  avg_L1_norm_grad         0.000028  w[0]   -0.130 bias    1.828\n",
      "iter 28601/1000000  loss         0.122854  avg_L1_norm_grad         0.000028  w[0]   -0.130 bias    1.828\n",
      "iter 28700/1000000  loss         0.122837  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.830\n",
      "iter 28701/1000000  loss         0.122837  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.830\n",
      "iter 28800/1000000  loss         0.122820  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.833\n",
      "iter 28801/1000000  loss         0.122820  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.833\n",
      "iter 28900/1000000  loss         0.122803  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.835\n",
      "iter 28901/1000000  loss         0.122803  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.835\n",
      "iter 29000/1000000  loss         0.122787  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.838\n",
      "iter 29001/1000000  loss         0.122787  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.838\n",
      "iter 29100/1000000  loss         0.122771  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.840\n",
      "iter 29101/1000000  loss         0.122770  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.840\n",
      "iter 29200/1000000  loss         0.122754  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.843\n",
      "iter 29201/1000000  loss         0.122754  avg_L1_norm_grad         0.000028  w[0]   -0.131 bias    1.843\n",
      "iter 29300/1000000  loss         0.122738  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.845\n",
      "iter 29301/1000000  loss         0.122738  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.845\n",
      "iter 29400/1000000  loss         0.122722  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.848\n",
      "iter 29401/1000000  loss         0.122722  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.848\n",
      "iter 29500/1000000  loss         0.122706  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.850\n",
      "iter 29501/1000000  loss         0.122706  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.850\n",
      "iter 29600/1000000  loss         0.122691  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.852\n",
      "iter 29601/1000000  loss         0.122690  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.853\n",
      "iter 29700/1000000  loss         0.122675  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.855\n",
      "iter 29701/1000000  loss         0.122675  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.855\n",
      "iter 29800/1000000  loss         0.122659  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.857\n",
      "iter 29801/1000000  loss         0.122659  avg_L1_norm_grad         0.000027  w[0]   -0.132 bias    1.857\n",
      "iter 29900/1000000  loss         0.122644  avg_L1_norm_grad         0.000027  w[0]   -0.133 bias    1.860\n",
      "iter 29901/1000000  loss         0.122644  avg_L1_norm_grad         0.000027  w[0]   -0.133 bias    1.860\n",
      "iter 30000/1000000  loss         0.122629  avg_L1_norm_grad         0.000027  w[0]   -0.133 bias    1.862\n",
      "iter 30001/1000000  loss         0.122629  avg_L1_norm_grad         0.000027  w[0]   -0.133 bias    1.862\n",
      "iter 30100/1000000  loss         0.122614  avg_L1_norm_grad         0.000027  w[0]   -0.133 bias    1.865\n",
      "iter 30101/1000000  loss         0.122614  avg_L1_norm_grad         0.000027  w[0]   -0.133 bias    1.865\n",
      "iter 30200/1000000  loss         0.122599  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.867\n",
      "iter 30201/1000000  loss         0.122599  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30300/1000000  loss         0.122584  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.869\n",
      "iter 30301/1000000  loss         0.122584  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.869\n",
      "iter 30400/1000000  loss         0.122569  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.872\n",
      "iter 30401/1000000  loss         0.122569  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.872\n",
      "iter 30500/1000000  loss         0.122554  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.874\n",
      "iter 30501/1000000  loss         0.122554  avg_L1_norm_grad         0.000026  w[0]   -0.133 bias    1.874\n",
      "iter 30600/1000000  loss         0.122540  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.877\n",
      "iter 30601/1000000  loss         0.122540  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.877\n",
      "iter 30700/1000000  loss         0.122526  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.879\n",
      "iter 30701/1000000  loss         0.122525  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.879\n",
      "iter 30800/1000000  loss         0.122511  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.881\n",
      "iter 30801/1000000  loss         0.122511  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.881\n",
      "iter 30900/1000000  loss         0.122497  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.884\n",
      "iter 30901/1000000  loss         0.122497  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.884\n",
      "iter 31000/1000000  loss         0.122483  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.886\n",
      "iter 31001/1000000  loss         0.122483  avg_L1_norm_grad         0.000026  w[0]   -0.134 bias    1.886\n",
      "iter 31100/1000000  loss         0.122469  avg_L1_norm_grad         0.000025  w[0]   -0.134 bias    1.888\n",
      "iter 31101/1000000  loss         0.122469  avg_L1_norm_grad         0.000025  w[0]   -0.134 bias    1.889\n",
      "iter 31200/1000000  loss         0.122455  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.891\n",
      "iter 31201/1000000  loss         0.122455  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.891\n",
      "iter 31300/1000000  loss         0.122442  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.893\n",
      "iter 31301/1000000  loss         0.122441  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.893\n",
      "iter 31400/1000000  loss         0.122428  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.896\n",
      "iter 31401/1000000  loss         0.122428  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.896\n",
      "iter 31500/1000000  loss         0.122415  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.898\n",
      "iter 31501/1000000  loss         0.122414  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.898\n",
      "iter 31600/1000000  loss         0.122401  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.900\n",
      "iter 31601/1000000  loss         0.122401  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.900\n",
      "iter 31700/1000000  loss         0.122388  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.902\n",
      "iter 31701/1000000  loss         0.122388  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.903\n",
      "iter 31800/1000000  loss         0.122375  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.905\n",
      "iter 31801/1000000  loss         0.122375  avg_L1_norm_grad         0.000025  w[0]   -0.135 bias    1.905\n",
      "iter 31900/1000000  loss         0.122362  avg_L1_norm_grad         0.000025  w[0]   -0.136 bias    1.907\n",
      "iter 31901/1000000  loss         0.122361  avg_L1_norm_grad         0.000025  w[0]   -0.136 bias    1.907\n",
      "iter 32000/1000000  loss         0.122349  avg_L1_norm_grad         0.000025  w[0]   -0.136 bias    1.909\n",
      "iter 32001/1000000  loss         0.122348  avg_L1_norm_grad         0.000025  w[0]   -0.136 bias    1.909\n",
      "iter 32100/1000000  loss         0.122336  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.912\n",
      "iter 32101/1000000  loss         0.122336  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.912\n",
      "iter 32200/1000000  loss         0.122323  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.914\n",
      "iter 32201/1000000  loss         0.122323  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.914\n",
      "iter 32300/1000000  loss         0.122310  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.916\n",
      "iter 32301/1000000  loss         0.122310  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.916\n",
      "iter 32400/1000000  loss         0.122298  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.919\n",
      "iter 32401/1000000  loss         0.122298  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.919\n",
      "iter 32500/1000000  loss         0.122285  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.921\n",
      "iter 32501/1000000  loss         0.122285  avg_L1_norm_grad         0.000024  w[0]   -0.136 bias    1.921\n",
      "iter 32600/1000000  loss         0.122273  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.923\n",
      "iter 32601/1000000  loss         0.122273  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.923\n",
      "iter 32700/1000000  loss         0.122261  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.925\n",
      "iter 32701/1000000  loss         0.122261  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.925\n",
      "iter 32800/1000000  loss         0.122248  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.928\n",
      "iter 32801/1000000  loss         0.122248  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.928\n",
      "iter 32900/1000000  loss         0.122236  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.930\n",
      "iter 32901/1000000  loss         0.122236  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.930\n",
      "iter 33000/1000000  loss         0.122224  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.932\n",
      "iter 33001/1000000  loss         0.122224  avg_L1_norm_grad         0.000024  w[0]   -0.137 bias    1.932\n",
      "iter 33100/1000000  loss         0.122213  avg_L1_norm_grad         0.000023  w[0]   -0.137 bias    1.934\n",
      "iter 33101/1000000  loss         0.122212  avg_L1_norm_grad         0.000023  w[0]   -0.137 bias    1.934\n",
      "iter 33200/1000000  loss         0.122201  avg_L1_norm_grad         0.000023  w[0]   -0.137 bias    1.936\n",
      "iter 33201/1000000  loss         0.122201  avg_L1_norm_grad         0.000023  w[0]   -0.137 bias    1.937\n",
      "iter 33300/1000000  loss         0.122189  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.939\n",
      "iter 33301/1000000  loss         0.122189  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.939\n",
      "iter 33400/1000000  loss         0.122177  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.941\n",
      "iter 33401/1000000  loss         0.122177  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.941\n",
      "iter 33500/1000000  loss         0.122166  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.943\n",
      "iter 33501/1000000  loss         0.122166  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.943\n",
      "iter 33600/1000000  loss         0.122154  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.945\n",
      "iter 33601/1000000  loss         0.122154  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.945\n",
      "iter 33700/1000000  loss         0.122143  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.948\n",
      "iter 33701/1000000  loss         0.122143  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.948\n",
      "iter 33800/1000000  loss         0.122132  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.950\n",
      "iter 33801/1000000  loss         0.122132  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.950\n",
      "iter 33900/1000000  loss         0.122121  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.952\n",
      "iter 33901/1000000  loss         0.122121  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.952\n",
      "iter 34000/1000000  loss         0.122110  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.954\n",
      "iter 34001/1000000  loss         0.122109  avg_L1_norm_grad         0.000023  w[0]   -0.138 bias    1.954\n",
      "iter 34100/1000000  loss         0.122099  avg_L1_norm_grad         0.000023  w[0]   -0.139 bias    1.956\n",
      "iter 34101/1000000  loss         0.122098  avg_L1_norm_grad         0.000023  w[0]   -0.139 bias    1.956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34200/1000000  loss         0.122088  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.958\n",
      "iter 34201/1000000  loss         0.122088  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.958\n",
      "iter 34300/1000000  loss         0.122077  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.961\n",
      "iter 34301/1000000  loss         0.122077  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.961\n",
      "iter 34400/1000000  loss         0.122066  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.963\n",
      "iter 34401/1000000  loss         0.122066  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.963\n",
      "iter 34500/1000000  loss         0.122055  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.965\n",
      "iter 34501/1000000  loss         0.122055  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.965\n",
      "iter 34600/1000000  loss         0.122045  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.967\n",
      "iter 34601/1000000  loss         0.122045  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.967\n",
      "iter 34700/1000000  loss         0.122034  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.969\n",
      "iter 34701/1000000  loss         0.122034  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.969\n",
      "iter 34800/1000000  loss         0.122024  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.971\n",
      "iter 34801/1000000  loss         0.122024  avg_L1_norm_grad         0.000022  w[0]   -0.139 bias    1.971\n",
      "iter 34900/1000000  loss         0.122014  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.973\n",
      "iter 34901/1000000  loss         0.122013  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.973\n",
      "iter 35000/1000000  loss         0.122003  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.975\n",
      "iter 35001/1000000  loss         0.122003  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.975\n",
      "iter 35100/1000000  loss         0.121993  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.978\n",
      "iter 35101/1000000  loss         0.121993  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.978\n",
      "iter 35200/1000000  loss         0.121983  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.980\n",
      "iter 35201/1000000  loss         0.121983  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.980\n",
      "iter 35300/1000000  loss         0.121973  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.982\n",
      "iter 35301/1000000  loss         0.121973  avg_L1_norm_grad         0.000022  w[0]   -0.140 bias    1.982\n",
      "iter 35400/1000000  loss         0.121963  avg_L1_norm_grad         0.000021  w[0]   -0.140 bias    1.984\n",
      "iter 35401/1000000  loss         0.121963  avg_L1_norm_grad         0.000021  w[0]   -0.140 bias    1.984\n",
      "iter 35500/1000000  loss         0.121953  avg_L1_norm_grad         0.000021  w[0]   -0.140 bias    1.986\n",
      "iter 35501/1000000  loss         0.121953  avg_L1_norm_grad         0.000021  w[0]   -0.140 bias    1.986\n",
      "iter 35600/1000000  loss         0.121943  avg_L1_norm_grad         0.000021  w[0]   -0.140 bias    1.988\n",
      "iter 35601/1000000  loss         0.121943  avg_L1_norm_grad         0.000021  w[0]   -0.140 bias    1.988\n",
      "iter 35700/1000000  loss         0.121934  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.990\n",
      "iter 35701/1000000  loss         0.121933  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.990\n",
      "iter 35800/1000000  loss         0.121924  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.992\n",
      "iter 35801/1000000  loss         0.121924  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.992\n",
      "iter 35900/1000000  loss         0.121914  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.994\n",
      "iter 35901/1000000  loss         0.121914  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.994\n",
      "iter 36000/1000000  loss         0.121905  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.996\n",
      "iter 36001/1000000  loss         0.121905  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.996\n",
      "iter 36100/1000000  loss         0.121895  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.998\n",
      "iter 36101/1000000  loss         0.121895  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    1.998\n",
      "iter 36200/1000000  loss         0.121886  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.000\n",
      "iter 36201/1000000  loss         0.121886  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.000\n",
      "iter 36300/1000000  loss         0.121877  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.002\n",
      "iter 36301/1000000  loss         0.121877  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.002\n",
      "iter 36400/1000000  loss         0.121867  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.004\n",
      "iter 36401/1000000  loss         0.121867  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.004\n",
      "iter 36500/1000000  loss         0.121858  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.006\n",
      "iter 36501/1000000  loss         0.121858  avg_L1_norm_grad         0.000021  w[0]   -0.141 bias    2.007\n",
      "iter 36600/1000000  loss         0.121849  avg_L1_norm_grad         0.000021  w[0]   -0.142 bias    2.009\n",
      "iter 36601/1000000  loss         0.121849  avg_L1_norm_grad         0.000021  w[0]   -0.142 bias    2.009\n",
      "iter 36700/1000000  loss         0.121840  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.011\n",
      "iter 36701/1000000  loss         0.121840  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.011\n",
      "iter 36800/1000000  loss         0.121831  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.013\n",
      "iter 36801/1000000  loss         0.121831  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.013\n",
      "iter 36900/1000000  loss         0.121822  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.015\n",
      "iter 36901/1000000  loss         0.121822  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.015\n",
      "iter 37000/1000000  loss         0.121814  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.017\n",
      "iter 37001/1000000  loss         0.121813  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.017\n",
      "iter 37100/1000000  loss         0.121805  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.019\n",
      "iter 37101/1000000  loss         0.121805  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.019\n",
      "iter 37200/1000000  loss         0.121796  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.021\n",
      "iter 37201/1000000  loss         0.121796  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.021\n",
      "iter 37300/1000000  loss         0.121787  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.023\n",
      "iter 37301/1000000  loss         0.121787  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.023\n",
      "iter 37400/1000000  loss         0.121779  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.024\n",
      "iter 37401/1000000  loss         0.121779  avg_L1_norm_grad         0.000020  w[0]   -0.142 bias    2.025\n",
      "iter 37500/1000000  loss         0.121770  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.026\n",
      "iter 37501/1000000  loss         0.121770  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.026\n",
      "iter 37600/1000000  loss         0.121762  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.028\n",
      "iter 37601/1000000  loss         0.121762  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.028\n",
      "iter 37700/1000000  loss         0.121754  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.030\n",
      "iter 37701/1000000  loss         0.121753  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.030\n",
      "iter 37800/1000000  loss         0.121745  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.032\n",
      "iter 37801/1000000  loss         0.121745  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.032\n",
      "iter 37900/1000000  loss         0.121737  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.034\n",
      "iter 37901/1000000  loss         0.121737  avg_L1_norm_grad         0.000020  w[0]   -0.143 bias    2.034\n",
      "iter 38000/1000000  loss         0.121729  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.036\n",
      "iter 38001/1000000  loss         0.121729  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38100/1000000  loss         0.121721  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.038\n",
      "iter 38101/1000000  loss         0.121721  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.038\n",
      "iter 38200/1000000  loss         0.121713  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.040\n",
      "iter 38201/1000000  loss         0.121712  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.040\n",
      "iter 38300/1000000  loss         0.121705  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.042\n",
      "iter 38301/1000000  loss         0.121704  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.042\n",
      "iter 38400/1000000  loss         0.121697  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.044\n",
      "iter 38401/1000000  loss         0.121696  avg_L1_norm_grad         0.000019  w[0]   -0.143 bias    2.044\n",
      "iter 38500/1000000  loss         0.121689  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.046\n",
      "iter 38501/1000000  loss         0.121689  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.046\n",
      "iter 38600/1000000  loss         0.121681  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.048\n",
      "iter 38601/1000000  loss         0.121681  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.048\n",
      "iter 38700/1000000  loss         0.121673  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.050\n",
      "iter 38701/1000000  loss         0.121673  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.050\n",
      "iter 38800/1000000  loss         0.121665  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.052\n",
      "iter 38801/1000000  loss         0.121665  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.052\n",
      "iter 38900/1000000  loss         0.121658  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.053\n",
      "iter 38901/1000000  loss         0.121658  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.053\n",
      "iter 39000/1000000  loss         0.121650  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.055\n",
      "iter 39001/1000000  loss         0.121650  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.055\n",
      "iter 39100/1000000  loss         0.121642  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.057\n",
      "iter 39101/1000000  loss         0.121642  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.057\n",
      "iter 39200/1000000  loss         0.121635  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.059\n",
      "iter 39201/1000000  loss         0.121635  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.059\n",
      "iter 39300/1000000  loss         0.121628  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.061\n",
      "iter 39301/1000000  loss         0.121627  avg_L1_norm_grad         0.000019  w[0]   -0.144 bias    2.061\n",
      "iter 39400/1000000  loss         0.121620  avg_L1_norm_grad         0.000018  w[0]   -0.144 bias    2.063\n",
      "iter 39401/1000000  loss         0.121620  avg_L1_norm_grad         0.000018  w[0]   -0.144 bias    2.063\n",
      "iter 39500/1000000  loss         0.121613  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.065\n",
      "iter 39501/1000000  loss         0.121613  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.065\n",
      "iter 39600/1000000  loss         0.121605  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.067\n",
      "iter 39601/1000000  loss         0.121605  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.067\n",
      "iter 39700/1000000  loss         0.121598  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.068\n",
      "iter 39701/1000000  loss         0.121598  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.068\n",
      "iter 39800/1000000  loss         0.121591  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.070\n",
      "iter 39801/1000000  loss         0.121591  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.070\n",
      "iter 39900/1000000  loss         0.121584  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.072\n",
      "iter 39901/1000000  loss         0.121584  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.072\n",
      "iter 40000/1000000  loss         0.121577  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.074\n",
      "iter 40001/1000000  loss         0.121577  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.074\n",
      "iter 40100/1000000  loss         0.121570  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.076\n",
      "iter 40101/1000000  loss         0.121570  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.076\n",
      "iter 40200/1000000  loss         0.121563  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.078\n",
      "iter 40201/1000000  loss         0.121563  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.078\n",
      "iter 40300/1000000  loss         0.121556  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.079\n",
      "iter 40301/1000000  loss         0.121556  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.079\n",
      "iter 40400/1000000  loss         0.121549  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.081\n",
      "iter 40401/1000000  loss         0.121549  avg_L1_norm_grad         0.000018  w[0]   -0.145 bias    2.081\n",
      "iter 40500/1000000  loss         0.121542  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.083\n",
      "iter 40501/1000000  loss         0.121542  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.083\n",
      "iter 40600/1000000  loss         0.121535  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.085\n",
      "iter 40601/1000000  loss         0.121535  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.085\n",
      "iter 40700/1000000  loss         0.121529  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.087\n",
      "iter 40701/1000000  loss         0.121529  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.087\n",
      "iter 40800/1000000  loss         0.121522  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.088\n",
      "iter 40801/1000000  loss         0.121522  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.088\n",
      "iter 40900/1000000  loss         0.121515  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.090\n",
      "iter 40901/1000000  loss         0.121515  avg_L1_norm_grad         0.000018  w[0]   -0.146 bias    2.090\n",
      "iter 41000/1000000  loss         0.121509  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.092\n",
      "iter 41001/1000000  loss         0.121509  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.092\n",
      "iter 41100/1000000  loss         0.121502  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.094\n",
      "iter 41101/1000000  loss         0.121502  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.094\n",
      "iter 41200/1000000  loss         0.121496  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.096\n",
      "iter 41201/1000000  loss         0.121496  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.096\n",
      "iter 41300/1000000  loss         0.121489  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.097\n",
      "iter 41301/1000000  loss         0.121489  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.097\n",
      "iter 41400/1000000  loss         0.121483  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.099\n",
      "iter 41401/1000000  loss         0.121483  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.099\n",
      "iter 41500/1000000  loss         0.121477  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.101\n",
      "iter 41501/1000000  loss         0.121476  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.101\n",
      "iter 41600/1000000  loss         0.121470  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.103\n",
      "iter 41601/1000000  loss         0.121470  avg_L1_norm_grad         0.000017  w[0]   -0.146 bias    2.103\n",
      "iter 41700/1000000  loss         0.121464  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.104\n",
      "iter 41701/1000000  loss         0.121464  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.104\n",
      "iter 41800/1000000  loss         0.121458  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.106\n",
      "iter 41801/1000000  loss         0.121458  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.106\n",
      "iter 41900/1000000  loss         0.121452  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.108\n",
      "iter 41901/1000000  loss         0.121451  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42000/1000000  loss         0.121445  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.110\n",
      "iter 42001/1000000  loss         0.121445  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.110\n",
      "iter 42100/1000000  loss         0.121439  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.111\n",
      "iter 42101/1000000  loss         0.121439  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.111\n",
      "iter 42200/1000000  loss         0.121433  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.113\n",
      "iter 42201/1000000  loss         0.121433  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.113\n",
      "iter 42300/1000000  loss         0.121427  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.115\n",
      "iter 42301/1000000  loss         0.121427  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.115\n",
      "iter 42400/1000000  loss         0.121421  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.116\n",
      "iter 42401/1000000  loss         0.121421  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.116\n",
      "iter 42500/1000000  loss         0.121415  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.118\n",
      "iter 42501/1000000  loss         0.121415  avg_L1_norm_grad         0.000017  w[0]   -0.147 bias    2.118\n",
      "iter 42600/1000000  loss         0.121409  avg_L1_norm_grad         0.000016  w[0]   -0.147 bias    2.120\n",
      "iter 42601/1000000  loss         0.121409  avg_L1_norm_grad         0.000016  w[0]   -0.147 bias    2.120\n",
      "iter 42700/1000000  loss         0.121404  avg_L1_norm_grad         0.000016  w[0]   -0.147 bias    2.122\n",
      "iter 42701/1000000  loss         0.121404  avg_L1_norm_grad         0.000016  w[0]   -0.147 bias    2.122\n",
      "iter 42800/1000000  loss         0.121398  avg_L1_norm_grad         0.000016  w[0]   -0.147 bias    2.123\n",
      "iter 42801/1000000  loss         0.121398  avg_L1_norm_grad         0.000016  w[0]   -0.147 bias    2.123\n",
      "iter 42900/1000000  loss         0.121392  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.125\n",
      "iter 42901/1000000  loss         0.121392  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.125\n",
      "iter 43000/1000000  loss         0.121386  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.127\n",
      "iter 43001/1000000  loss         0.121386  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.127\n",
      "iter 43100/1000000  loss         0.121381  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.128\n",
      "iter 43101/1000000  loss         0.121381  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.128\n",
      "iter 43200/1000000  loss         0.121375  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.130\n",
      "iter 43201/1000000  loss         0.121375  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.130\n",
      "iter 43300/1000000  loss         0.121369  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.132\n",
      "iter 43301/1000000  loss         0.121369  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.132\n",
      "iter 43400/1000000  loss         0.121364  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.133\n",
      "iter 43401/1000000  loss         0.121364  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.133\n",
      "iter 43500/1000000  loss         0.121358  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.135\n",
      "iter 43501/1000000  loss         0.121358  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.135\n",
      "iter 43600/1000000  loss         0.121353  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.137\n",
      "iter 43601/1000000  loss         0.121353  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.137\n",
      "iter 43700/1000000  loss         0.121347  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.138\n",
      "iter 43701/1000000  loss         0.121347  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.138\n",
      "iter 43800/1000000  loss         0.121342  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.140\n",
      "iter 43801/1000000  loss         0.121342  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.140\n",
      "iter 43900/1000000  loss         0.121337  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.142\n",
      "iter 43901/1000000  loss         0.121337  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.142\n",
      "iter 44000/1000000  loss         0.121331  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.143\n",
      "iter 44001/1000000  loss         0.121331  avg_L1_norm_grad         0.000016  w[0]   -0.148 bias    2.143\n",
      "iter 44100/1000000  loss         0.121326  avg_L1_norm_grad         0.000016  w[0]   -0.149 bias    2.145\n",
      "iter 44101/1000000  loss         0.121326  avg_L1_norm_grad         0.000016  w[0]   -0.149 bias    2.145\n",
      "iter 44200/1000000  loss         0.121321  avg_L1_norm_grad         0.000016  w[0]   -0.149 bias    2.146\n",
      "iter 44201/1000000  loss         0.121321  avg_L1_norm_grad         0.000016  w[0]   -0.149 bias    2.146\n",
      "iter 44300/1000000  loss         0.121316  avg_L1_norm_grad         0.000016  w[0]   -0.149 bias    2.148\n",
      "iter 44301/1000000  loss         0.121316  avg_L1_norm_grad         0.000016  w[0]   -0.149 bias    2.148\n",
      "iter 44400/1000000  loss         0.121310  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.150\n",
      "iter 44401/1000000  loss         0.121310  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.150\n",
      "iter 44500/1000000  loss         0.121305  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.151\n",
      "iter 44501/1000000  loss         0.121305  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.151\n",
      "iter 44600/1000000  loss         0.121300  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.153\n",
      "iter 44601/1000000  loss         0.121300  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.153\n",
      "iter 44700/1000000  loss         0.121295  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.155\n",
      "iter 44701/1000000  loss         0.121295  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.155\n",
      "iter 44800/1000000  loss         0.121290  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.156\n",
      "iter 44801/1000000  loss         0.121290  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.156\n",
      "iter 44900/1000000  loss         0.121285  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.158\n",
      "iter 44901/1000000  loss         0.121285  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.158\n",
      "iter 45000/1000000  loss         0.121280  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.159\n",
      "iter 45001/1000000  loss         0.121280  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.159\n",
      "iter 45100/1000000  loss         0.121275  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.161\n",
      "iter 45101/1000000  loss         0.121275  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.161\n",
      "iter 45200/1000000  loss         0.121270  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.162\n",
      "iter 45201/1000000  loss         0.121270  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.162\n",
      "iter 45300/1000000  loss         0.121265  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.164\n",
      "iter 45301/1000000  loss         0.121265  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.164\n",
      "iter 45400/1000000  loss         0.121260  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.166\n",
      "iter 45401/1000000  loss         0.121260  avg_L1_norm_grad         0.000015  w[0]   -0.149 bias    2.166\n",
      "iter 45500/1000000  loss         0.121256  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.167\n",
      "iter 45501/1000000  loss         0.121256  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.167\n",
      "iter 45600/1000000  loss         0.121251  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.169\n",
      "iter 45601/1000000  loss         0.121251  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.169\n",
      "iter 45700/1000000  loss         0.121246  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.170\n",
      "iter 45701/1000000  loss         0.121246  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.170\n",
      "iter 45800/1000000  loss         0.121241  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.172\n",
      "iter 45801/1000000  loss         0.121241  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 45900/1000000  loss         0.121237  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.173\n",
      "iter 45901/1000000  loss         0.121237  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.173\n",
      "iter 46000/1000000  loss         0.121232  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.175\n",
      "iter 46001/1000000  loss         0.121232  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.175\n",
      "iter 46100/1000000  loss         0.121227  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.176\n",
      "iter 46101/1000000  loss         0.121227  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.176\n",
      "iter 46200/1000000  loss         0.121223  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.178\n",
      "iter 46201/1000000  loss         0.121223  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.178\n",
      "iter 46300/1000000  loss         0.121218  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.180\n",
      "iter 46301/1000000  loss         0.121218  avg_L1_norm_grad         0.000015  w[0]   -0.150 bias    2.180\n",
      "iter 46400/1000000  loss         0.121214  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.181\n",
      "iter 46401/1000000  loss         0.121214  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.181\n",
      "iter 46500/1000000  loss         0.121209  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.183\n",
      "iter 46501/1000000  loss         0.121209  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.183\n",
      "iter 46600/1000000  loss         0.121205  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.184\n",
      "iter 46601/1000000  loss         0.121205  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.184\n",
      "iter 46700/1000000  loss         0.121200  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.186\n",
      "iter 46701/1000000  loss         0.121200  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.186\n",
      "iter 46800/1000000  loss         0.121196  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.187\n",
      "iter 46801/1000000  loss         0.121196  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.187\n",
      "iter 46900/1000000  loss         0.121192  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.189\n",
      "iter 46901/1000000  loss         0.121192  avg_L1_norm_grad         0.000014  w[0]   -0.150 bias    2.189\n",
      "iter 47000/1000000  loss         0.121187  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.190\n",
      "iter 47001/1000000  loss         0.121187  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.190\n",
      "iter 47100/1000000  loss         0.121183  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.192\n",
      "iter 47101/1000000  loss         0.121183  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.192\n",
      "iter 47200/1000000  loss         0.121179  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.193\n",
      "iter 47201/1000000  loss         0.121179  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.193\n",
      "iter 47300/1000000  loss         0.121175  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.195\n",
      "iter 47301/1000000  loss         0.121174  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.195\n",
      "iter 47400/1000000  loss         0.121170  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.196\n",
      "iter 47401/1000000  loss         0.121170  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.196\n",
      "iter 47500/1000000  loss         0.121166  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.197\n",
      "iter 47501/1000000  loss         0.121166  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.198\n",
      "iter 47600/1000000  loss         0.121162  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.199\n",
      "iter 47601/1000000  loss         0.121162  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.199\n",
      "iter 47700/1000000  loss         0.121158  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.200\n",
      "iter 47701/1000000  loss         0.121158  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.200\n",
      "iter 47800/1000000  loss         0.121154  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.202\n",
      "iter 47801/1000000  loss         0.121154  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.202\n",
      "iter 47900/1000000  loss         0.121150  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.203\n",
      "iter 47901/1000000  loss         0.121150  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.203\n",
      "iter 48000/1000000  loss         0.121146  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.205\n",
      "iter 48001/1000000  loss         0.121146  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.205\n",
      "iter 48100/1000000  loss         0.121142  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.206\n",
      "iter 48101/1000000  loss         0.121141  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.206\n",
      "iter 48200/1000000  loss         0.121138  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.208\n",
      "iter 48201/1000000  loss         0.121137  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.208\n",
      "iter 48300/1000000  loss         0.121134  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.209\n",
      "iter 48301/1000000  loss         0.121134  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.209\n",
      "iter 48400/1000000  loss         0.121130  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.211\n",
      "iter 48401/1000000  loss         0.121130  avg_L1_norm_grad         0.000014  w[0]   -0.151 bias    2.211\n",
      "iter 48500/1000000  loss         0.121126  avg_L1_norm_grad         0.000013  w[0]   -0.151 bias    2.212\n",
      "iter 48501/1000000  loss         0.121126  avg_L1_norm_grad         0.000013  w[0]   -0.151 bias    2.212\n",
      "iter 48600/1000000  loss         0.121122  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.213\n",
      "iter 48601/1000000  loss         0.121122  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.213\n",
      "iter 48700/1000000  loss         0.121118  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.215\n",
      "iter 48701/1000000  loss         0.121118  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.215\n",
      "iter 48800/1000000  loss         0.121114  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.216\n",
      "iter 48801/1000000  loss         0.121114  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.216\n",
      "iter 48900/1000000  loss         0.121110  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.218\n",
      "iter 48901/1000000  loss         0.121110  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.218\n",
      "iter 49000/1000000  loss         0.121106  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.219\n",
      "iter 49001/1000000  loss         0.121106  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.219\n",
      "iter 49100/1000000  loss         0.121103  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.221\n",
      "iter 49101/1000000  loss         0.121103  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.221\n",
      "iter 49200/1000000  loss         0.121099  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.222\n",
      "iter 49201/1000000  loss         0.121099  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.222\n",
      "iter 49300/1000000  loss         0.121095  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.223\n",
      "iter 49301/1000000  loss         0.121095  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.223\n",
      "iter 49400/1000000  loss         0.121092  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.225\n",
      "iter 49401/1000000  loss         0.121092  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.225\n",
      "iter 49500/1000000  loss         0.121088  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.226\n",
      "iter 49501/1000000  loss         0.121088  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.226\n",
      "iter 49600/1000000  loss         0.121084  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.227\n",
      "iter 49601/1000000  loss         0.121084  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.228\n",
      "iter 49700/1000000  loss         0.121081  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.229\n",
      "iter 49701/1000000  loss         0.121081  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 49800/1000000  loss         0.121077  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.230\n",
      "iter 49801/1000000  loss         0.121077  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.230\n",
      "iter 49900/1000000  loss         0.121073  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.232\n",
      "iter 49901/1000000  loss         0.121073  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.232\n",
      "iter 50000/1000000  loss         0.121070  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.233\n",
      "iter 50001/1000000  loss         0.121070  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.233\n",
      "iter 50100/1000000  loss         0.121066  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.234\n",
      "iter 50101/1000000  loss         0.121066  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.234\n",
      "iter 50200/1000000  loss         0.121063  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.236\n",
      "iter 50201/1000000  loss         0.121063  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.236\n",
      "iter 50300/1000000  loss         0.121059  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.237\n",
      "iter 50301/1000000  loss         0.121059  avg_L1_norm_grad         0.000013  w[0]   -0.152 bias    2.237\n",
      "iter 50400/1000000  loss         0.121056  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.238\n",
      "iter 50401/1000000  loss         0.121056  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.238\n",
      "iter 50500/1000000  loss         0.121053  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.240\n",
      "iter 50501/1000000  loss         0.121053  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.240\n",
      "iter 50600/1000000  loss         0.121049  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.241\n",
      "iter 50601/1000000  loss         0.121049  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.241\n",
      "iter 50700/1000000  loss         0.121046  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.242\n",
      "iter 50701/1000000  loss         0.121046  avg_L1_norm_grad         0.000013  w[0]   -0.153 bias    2.242\n",
      "iter 50800/1000000  loss         0.121042  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.244\n",
      "iter 50801/1000000  loss         0.121042  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.244\n",
      "iter 50900/1000000  loss         0.121039  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.245\n",
      "iter 50901/1000000  loss         0.121039  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.245\n",
      "iter 51000/1000000  loss         0.121036  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.246\n",
      "iter 51001/1000000  loss         0.121036  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.246\n",
      "iter 51100/1000000  loss         0.121032  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.248\n",
      "iter 51101/1000000  loss         0.121032  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.248\n",
      "iter 51200/1000000  loss         0.121029  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.249\n",
      "iter 51201/1000000  loss         0.121029  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.249\n",
      "iter 51300/1000000  loss         0.121026  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.250\n",
      "iter 51301/1000000  loss         0.121026  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.250\n",
      "iter 51400/1000000  loss         0.121023  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.252\n",
      "iter 51401/1000000  loss         0.121023  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.252\n",
      "iter 51500/1000000  loss         0.121019  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.253\n",
      "iter 51501/1000000  loss         0.121019  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.253\n",
      "iter 51600/1000000  loss         0.121016  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.254\n",
      "iter 51601/1000000  loss         0.121016  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.254\n",
      "iter 51700/1000000  loss         0.121013  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.256\n",
      "iter 51701/1000000  loss         0.121013  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.256\n",
      "iter 51800/1000000  loss         0.121010  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.257\n",
      "iter 51801/1000000  loss         0.121010  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.257\n",
      "iter 51900/1000000  loss         0.121007  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.258\n",
      "iter 51901/1000000  loss         0.121007  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.258\n",
      "iter 52000/1000000  loss         0.121004  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.260\n",
      "iter 52001/1000000  loss         0.121004  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.260\n",
      "iter 52100/1000000  loss         0.121001  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.261\n",
      "iter 52101/1000000  loss         0.121001  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.261\n",
      "iter 52200/1000000  loss         0.120998  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.262\n",
      "iter 52201/1000000  loss         0.120998  avg_L1_norm_grad         0.000012  w[0]   -0.153 bias    2.262\n",
      "iter 52300/1000000  loss         0.120995  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.263\n",
      "iter 52301/1000000  loss         0.120994  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.263\n",
      "iter 52400/1000000  loss         0.120991  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.265\n",
      "iter 52401/1000000  loss         0.120991  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.265\n",
      "iter 52500/1000000  loss         0.120988  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.266\n",
      "iter 52501/1000000  loss         0.120988  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.266\n",
      "iter 52600/1000000  loss         0.120985  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.267\n",
      "iter 52601/1000000  loss         0.120985  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.267\n",
      "iter 52700/1000000  loss         0.120983  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.268\n",
      "iter 52701/1000000  loss         0.120982  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.268\n",
      "iter 52800/1000000  loss         0.120980  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.270\n",
      "iter 52801/1000000  loss         0.120980  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.270\n",
      "iter 52900/1000000  loss         0.120977  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.271\n",
      "iter 52901/1000000  loss         0.120977  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.271\n",
      "iter 53000/1000000  loss         0.120974  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.272\n",
      "iter 53001/1000000  loss         0.120974  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.272\n",
      "iter 53100/1000000  loss         0.120971  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.273\n",
      "iter 53101/1000000  loss         0.120971  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.274\n",
      "iter 53200/1000000  loss         0.120968  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.275\n",
      "iter 53201/1000000  loss         0.120968  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.275\n",
      "iter 53300/1000000  loss         0.120965  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.276\n",
      "iter 53301/1000000  loss         0.120965  avg_L1_norm_grad         0.000012  w[0]   -0.154 bias    2.276\n",
      "iter 53400/1000000  loss         0.120962  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.277\n",
      "iter 53401/1000000  loss         0.120962  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.277\n",
      "iter 53500/1000000  loss         0.120959  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.278\n",
      "iter 53501/1000000  loss         0.120959  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.278\n",
      "iter 53600/1000000  loss         0.120957  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.280\n",
      "iter 53601/1000000  loss         0.120957  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 53700/1000000  loss         0.120954  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.281\n",
      "iter 53701/1000000  loss         0.120954  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.281\n",
      "iter 53800/1000000  loss         0.120951  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.282\n",
      "iter 53801/1000000  loss         0.120951  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.282\n",
      "iter 53900/1000000  loss         0.120948  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.283\n",
      "iter 53901/1000000  loss         0.120948  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.283\n",
      "iter 54000/1000000  loss         0.120946  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.285\n",
      "iter 54001/1000000  loss         0.120946  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.285\n",
      "iter 54100/1000000  loss         0.120943  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.286\n",
      "iter 54101/1000000  loss         0.120943  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.286\n",
      "iter 54200/1000000  loss         0.120940  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.287\n",
      "iter 54201/1000000  loss         0.120940  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.287\n",
      "iter 54300/1000000  loss         0.120938  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.288\n",
      "iter 54301/1000000  loss         0.120938  avg_L1_norm_grad         0.000011  w[0]   -0.154 bias    2.288\n",
      "iter 54400/1000000  loss         0.120935  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.289\n",
      "iter 54401/1000000  loss         0.120935  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.289\n",
      "iter 54500/1000000  loss         0.120932  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.291\n",
      "iter 54501/1000000  loss         0.120932  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.291\n",
      "iter 54600/1000000  loss         0.120930  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.292\n",
      "iter 54601/1000000  loss         0.120930  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.292\n",
      "iter 54700/1000000  loss         0.120927  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.293\n",
      "iter 54701/1000000  loss         0.120927  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.293\n",
      "iter 54800/1000000  loss         0.120924  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.294\n",
      "iter 54801/1000000  loss         0.120924  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.294\n",
      "iter 54900/1000000  loss         0.120922  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.295\n",
      "iter 54901/1000000  loss         0.120922  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.295\n",
      "iter 55000/1000000  loss         0.120919  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.297\n",
      "iter 55001/1000000  loss         0.120919  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.297\n",
      "iter 55100/1000000  loss         0.120917  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.298\n",
      "iter 55101/1000000  loss         0.120917  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.298\n",
      "iter 55200/1000000  loss         0.120914  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.299\n",
      "iter 55201/1000000  loss         0.120914  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.299\n",
      "iter 55300/1000000  loss         0.120912  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.300\n",
      "iter 55301/1000000  loss         0.120912  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.300\n",
      "iter 55400/1000000  loss         0.120909  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.301\n",
      "iter 55401/1000000  loss         0.120909  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.301\n",
      "iter 55500/1000000  loss         0.120907  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.302\n",
      "iter 55501/1000000  loss         0.120907  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.302\n",
      "iter 55600/1000000  loss         0.120904  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.304\n",
      "iter 55601/1000000  loss         0.120904  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.304\n",
      "iter 55700/1000000  loss         0.120902  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.305\n",
      "iter 55701/1000000  loss         0.120902  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.305\n",
      "iter 55800/1000000  loss         0.120899  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.306\n",
      "iter 55801/1000000  loss         0.120899  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.306\n",
      "iter 55900/1000000  loss         0.120897  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.307\n",
      "iter 55901/1000000  loss         0.120897  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.307\n",
      "iter 56000/1000000  loss         0.120895  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.308\n",
      "iter 56001/1000000  loss         0.120895  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.308\n",
      "iter 56100/1000000  loss         0.120892  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.309\n",
      "iter 56101/1000000  loss         0.120892  avg_L1_norm_grad         0.000011  w[0]   -0.155 bias    2.309\n",
      "iter 56200/1000000  loss         0.120890  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.310\n",
      "iter 56201/1000000  loss         0.120890  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.310\n",
      "iter 56300/1000000  loss         0.120887  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.312\n",
      "iter 56301/1000000  loss         0.120887  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.312\n",
      "iter 56400/1000000  loss         0.120885  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.313\n",
      "iter 56401/1000000  loss         0.120885  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.313\n",
      "iter 56500/1000000  loss         0.120883  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.314\n",
      "iter 56501/1000000  loss         0.120883  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.314\n",
      "iter 56600/1000000  loss         0.120880  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.315\n",
      "iter 56601/1000000  loss         0.120880  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.315\n",
      "iter 56700/1000000  loss         0.120878  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.316\n",
      "iter 56701/1000000  loss         0.120878  avg_L1_norm_grad         0.000010  w[0]   -0.155 bias    2.316\n",
      "iter 56800/1000000  loss         0.120876  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.317\n",
      "iter 56801/1000000  loss         0.120876  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.317\n",
      "iter 56900/1000000  loss         0.120874  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.318\n",
      "iter 56901/1000000  loss         0.120874  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.318\n",
      "iter 57000/1000000  loss         0.120871  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.319\n",
      "iter 57001/1000000  loss         0.120871  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.319\n",
      "iter 57100/1000000  loss         0.120869  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.321\n",
      "iter 57101/1000000  loss         0.120869  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.321\n",
      "iter 57200/1000000  loss         0.120867  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.322\n",
      "iter 57201/1000000  loss         0.120867  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.322\n",
      "iter 57300/1000000  loss         0.120865  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.323\n",
      "iter 57301/1000000  loss         0.120865  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.323\n",
      "iter 57400/1000000  loss         0.120863  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.324\n",
      "iter 57401/1000000  loss         0.120863  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.324\n",
      "iter 57500/1000000  loss         0.120860  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.325\n",
      "iter 57501/1000000  loss         0.120860  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 57600/1000000  loss         0.120858  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.326\n",
      "iter 57601/1000000  loss         0.120858  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.326\n",
      "iter 57700/1000000  loss         0.120856  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.327\n",
      "iter 57701/1000000  loss         0.120856  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.327\n",
      "iter 57800/1000000  loss         0.120854  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.328\n",
      "iter 57801/1000000  loss         0.120854  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.328\n",
      "iter 57900/1000000  loss         0.120852  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.329\n",
      "iter 57901/1000000  loss         0.120852  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.329\n",
      "iter 58000/1000000  loss         0.120850  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.330\n",
      "iter 58001/1000000  loss         0.120850  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.330\n",
      "iter 58100/1000000  loss         0.120848  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.331\n",
      "iter 58101/1000000  loss         0.120848  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.331\n",
      "iter 58200/1000000  loss         0.120845  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.332\n",
      "iter 58201/1000000  loss         0.120845  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.333\n",
      "iter 58300/1000000  loss         0.120843  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.334\n",
      "iter 58301/1000000  loss         0.120843  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.334\n",
      "iter 58400/1000000  loss         0.120841  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.335\n",
      "iter 58401/1000000  loss         0.120841  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.335\n",
      "iter 58500/1000000  loss         0.120839  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.336\n",
      "iter 58501/1000000  loss         0.120839  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.336\n",
      "iter 58600/1000000  loss         0.120837  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.337\n",
      "iter 58601/1000000  loss         0.120837  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.337\n",
      "iter 58700/1000000  loss         0.120835  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.338\n",
      "iter 58701/1000000  loss         0.120835  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.338\n",
      "iter 58800/1000000  loss         0.120833  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.339\n",
      "iter 58801/1000000  loss         0.120833  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.339\n",
      "iter 58900/1000000  loss         0.120831  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.340\n",
      "iter 58901/1000000  loss         0.120831  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.340\n",
      "iter 59000/1000000  loss         0.120829  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.341\n",
      "iter 59001/1000000  loss         0.120829  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.341\n",
      "iter 59100/1000000  loss         0.120827  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.342\n",
      "iter 59101/1000000  loss         0.120827  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.342\n",
      "iter 59200/1000000  loss         0.120825  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.343\n",
      "iter 59201/1000000  loss         0.120825  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.343\n",
      "iter 59300/1000000  loss         0.120823  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.344\n",
      "iter 59301/1000000  loss         0.120823  avg_L1_norm_grad         0.000010  w[0]   -0.156 bias    2.344\n",
      "iter 59400/1000000  loss         0.120821  avg_L1_norm_grad         0.000009  w[0]   -0.156 bias    2.345\n",
      "iter 59401/1000000  loss         0.120821  avg_L1_norm_grad         0.000009  w[0]   -0.156 bias    2.345\n",
      "iter 59500/1000000  loss         0.120819  avg_L1_norm_grad         0.000009  w[0]   -0.156 bias    2.346\n",
      "iter 59501/1000000  loss         0.120819  avg_L1_norm_grad         0.000009  w[0]   -0.156 bias    2.346\n",
      "iter 59600/1000000  loss         0.120818  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.347\n",
      "iter 59601/1000000  loss         0.120818  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.347\n",
      "iter 59700/1000000  loss         0.120816  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.348\n",
      "iter 59701/1000000  loss         0.120816  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.348\n",
      "iter 59800/1000000  loss         0.120814  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.349\n",
      "iter 59801/1000000  loss         0.120814  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.349\n",
      "iter 59900/1000000  loss         0.120812  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.350\n",
      "iter 59901/1000000  loss         0.120812  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.350\n",
      "iter 60000/1000000  loss         0.120810  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.351\n",
      "iter 60001/1000000  loss         0.120810  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.351\n",
      "iter 60100/1000000  loss         0.120808  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.352\n",
      "iter 60101/1000000  loss         0.120808  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.352\n",
      "iter 60200/1000000  loss         0.120806  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.353\n",
      "iter 60201/1000000  loss         0.120806  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.353\n",
      "iter 60300/1000000  loss         0.120805  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.354\n",
      "iter 60301/1000000  loss         0.120804  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.354\n",
      "iter 60400/1000000  loss         0.120803  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.355\n",
      "iter 60401/1000000  loss         0.120803  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.355\n",
      "iter 60500/1000000  loss         0.120801  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.356\n",
      "iter 60501/1000000  loss         0.120801  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.356\n",
      "iter 60600/1000000  loss         0.120799  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.357\n",
      "iter 60601/1000000  loss         0.120799  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.357\n",
      "iter 60700/1000000  loss         0.120797  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.358\n",
      "iter 60701/1000000  loss         0.120797  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.358\n",
      "iter 60800/1000000  loss         0.120796  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.359\n",
      "iter 60801/1000000  loss         0.120796  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.359\n",
      "iter 60900/1000000  loss         0.120794  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.360\n",
      "iter 60901/1000000  loss         0.120794  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.360\n",
      "iter 61000/1000000  loss         0.120792  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.361\n",
      "iter 61001/1000000  loss         0.120792  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.361\n",
      "iter 61100/1000000  loss         0.120790  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.362\n",
      "iter 61101/1000000  loss         0.120790  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.362\n",
      "iter 61200/1000000  loss         0.120789  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.363\n",
      "iter 61201/1000000  loss         0.120789  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.363\n",
      "iter 61300/1000000  loss         0.120787  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.364\n",
      "iter 61301/1000000  loss         0.120787  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.364\n",
      "iter 61400/1000000  loss         0.120785  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.365\n",
      "iter 61401/1000000  loss         0.120785  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 61500/1000000  loss         0.120783  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.366\n",
      "iter 61501/1000000  loss         0.120783  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.366\n",
      "iter 61600/1000000  loss         0.120782  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.367\n",
      "iter 61601/1000000  loss         0.120782  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.367\n",
      "iter 61700/1000000  loss         0.120780  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.368\n",
      "iter 61701/1000000  loss         0.120780  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.368\n",
      "iter 61800/1000000  loss         0.120778  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.369\n",
      "iter 61801/1000000  loss         0.120778  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.369\n",
      "iter 61900/1000000  loss         0.120777  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.370\n",
      "iter 61901/1000000  loss         0.120777  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.370\n",
      "iter 62000/1000000  loss         0.120775  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.371\n",
      "iter 62001/1000000  loss         0.120775  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.371\n",
      "iter 62100/1000000  loss         0.120773  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.372\n",
      "iter 62101/1000000  loss         0.120773  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.372\n",
      "iter 62200/1000000  loss         0.120772  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.373\n",
      "iter 62201/1000000  loss         0.120772  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.373\n",
      "iter 62300/1000000  loss         0.120770  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.374\n",
      "iter 62301/1000000  loss         0.120770  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.374\n",
      "iter 62400/1000000  loss         0.120769  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.375\n",
      "iter 62401/1000000  loss         0.120769  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.375\n",
      "iter 62500/1000000  loss         0.120767  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.376\n",
      "iter 62501/1000000  loss         0.120767  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.376\n",
      "iter 62600/1000000  loss         0.120765  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.377\n",
      "iter 62601/1000000  loss         0.120765  avg_L1_norm_grad         0.000009  w[0]   -0.157 bias    2.377\n",
      "iter 62700/1000000  loss         0.120764  avg_L1_norm_grad         0.000009  w[0]   -0.158 bias    2.378\n",
      "iter 62701/1000000  loss         0.120764  avg_L1_norm_grad         0.000009  w[0]   -0.158 bias    2.378\n",
      "iter 62800/1000000  loss         0.120762  avg_L1_norm_grad         0.000009  w[0]   -0.158 bias    2.379\n",
      "iter 62801/1000000  loss         0.120762  avg_L1_norm_grad         0.000009  w[0]   -0.158 bias    2.379\n",
      "iter 62900/1000000  loss         0.120761  avg_L1_norm_grad         0.000009  w[0]   -0.158 bias    2.379\n",
      "iter 62901/1000000  loss         0.120761  avg_L1_norm_grad         0.000009  w[0]   -0.158 bias    2.379\n",
      "iter 63000/1000000  loss         0.120759  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.380\n",
      "iter 63001/1000000  loss         0.120759  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.380\n",
      "iter 63100/1000000  loss         0.120758  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.381\n",
      "iter 63101/1000000  loss         0.120758  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.381\n",
      "iter 63200/1000000  loss         0.120756  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.382\n",
      "iter 63201/1000000  loss         0.120756  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.382\n",
      "iter 63300/1000000  loss         0.120755  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.383\n",
      "iter 63301/1000000  loss         0.120755  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.383\n",
      "iter 63400/1000000  loss         0.120753  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.384\n",
      "iter 63401/1000000  loss         0.120753  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.384\n",
      "iter 63500/1000000  loss         0.120752  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.385\n",
      "iter 63501/1000000  loss         0.120752  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.385\n",
      "iter 63600/1000000  loss         0.120750  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.386\n",
      "iter 63601/1000000  loss         0.120750  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.386\n",
      "iter 63700/1000000  loss         0.120749  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.387\n",
      "iter 63701/1000000  loss         0.120749  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.387\n",
      "iter 63800/1000000  loss         0.120747  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.388\n",
      "iter 63801/1000000  loss         0.120747  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.388\n",
      "iter 63900/1000000  loss         0.120746  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.389\n",
      "iter 63901/1000000  loss         0.120746  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.389\n",
      "iter 64000/1000000  loss         0.120744  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.389\n",
      "iter 64001/1000000  loss         0.120744  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.390\n",
      "iter 64100/1000000  loss         0.120743  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.390\n",
      "iter 64101/1000000  loss         0.120743  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.390\n",
      "iter 64200/1000000  loss         0.120741  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.391\n",
      "iter 64201/1000000  loss         0.120741  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.391\n",
      "iter 64300/1000000  loss         0.120740  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.392\n",
      "iter 64301/1000000  loss         0.120740  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.392\n",
      "iter 64400/1000000  loss         0.120738  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.393\n",
      "iter 64401/1000000  loss         0.120738  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.393\n",
      "iter 64500/1000000  loss         0.120737  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.394\n",
      "iter 64501/1000000  loss         0.120737  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.394\n",
      "iter 64600/1000000  loss         0.120736  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.395\n",
      "iter 64601/1000000  loss         0.120736  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.395\n",
      "iter 64700/1000000  loss         0.120734  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.396\n",
      "iter 64701/1000000  loss         0.120734  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.396\n",
      "iter 64800/1000000  loss         0.120733  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.397\n",
      "iter 64801/1000000  loss         0.120733  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.397\n",
      "iter 64900/1000000  loss         0.120731  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.397\n",
      "iter 64901/1000000  loss         0.120731  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.397\n",
      "iter 65000/1000000  loss         0.120730  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.398\n",
      "iter 65001/1000000  loss         0.120730  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.398\n",
      "iter 65100/1000000  loss         0.120729  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.399\n",
      "iter 65101/1000000  loss         0.120729  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.399\n",
      "iter 65200/1000000  loss         0.120727  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.400\n",
      "iter 65201/1000000  loss         0.120727  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.400\n",
      "iter 65300/1000000  loss         0.120726  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.401\n",
      "iter 65301/1000000  loss         0.120726  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 65400/1000000  loss         0.120725  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.402\n",
      "iter 65401/1000000  loss         0.120725  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.402\n",
      "iter 65500/1000000  loss         0.120723  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.403\n",
      "iter 65501/1000000  loss         0.120723  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.403\n",
      "iter 65600/1000000  loss         0.120722  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.404\n",
      "iter 65601/1000000  loss         0.120722  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.404\n",
      "iter 65700/1000000  loss         0.120721  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.404\n",
      "iter 65701/1000000  loss         0.120721  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.404\n",
      "iter 65800/1000000  loss         0.120719  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.405\n",
      "iter 65801/1000000  loss         0.120719  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.405\n",
      "iter 65900/1000000  loss         0.120718  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.406\n",
      "iter 65901/1000000  loss         0.120718  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.406\n",
      "iter 66000/1000000  loss         0.120717  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.407\n",
      "iter 66001/1000000  loss         0.120717  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.407\n",
      "iter 66100/1000000  loss         0.120716  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.408\n",
      "iter 66101/1000000  loss         0.120716  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.408\n",
      "iter 66200/1000000  loss         0.120714  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.409\n",
      "iter 66201/1000000  loss         0.120714  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.409\n",
      "iter 66300/1000000  loss         0.120713  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.409\n",
      "iter 66301/1000000  loss         0.120713  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.409\n",
      "iter 66400/1000000  loss         0.120712  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.410\n",
      "iter 66401/1000000  loss         0.120712  avg_L1_norm_grad         0.000008  w[0]   -0.158 bias    2.410\n",
      "iter 66500/1000000  loss         0.120710  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.411\n",
      "iter 66501/1000000  loss         0.120710  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.411\n",
      "iter 66600/1000000  loss         0.120709  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.412\n",
      "iter 66601/1000000  loss         0.120709  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.412\n",
      "iter 66700/1000000  loss         0.120708  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.413\n",
      "iter 66701/1000000  loss         0.120708  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.413\n",
      "iter 66800/1000000  loss         0.120707  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.414\n",
      "iter 66801/1000000  loss         0.120707  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.414\n",
      "iter 66900/1000000  loss         0.120706  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.414\n",
      "iter 66901/1000000  loss         0.120706  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.414\n",
      "iter 67000/1000000  loss         0.120704  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.415\n",
      "iter 67001/1000000  loss         0.120704  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.415\n",
      "iter 67100/1000000  loss         0.120703  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.416\n",
      "iter 67101/1000000  loss         0.120703  avg_L1_norm_grad         0.000008  w[0]   -0.159 bias    2.416\n",
      "iter 67200/1000000  loss         0.120702  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.417\n",
      "iter 67201/1000000  loss         0.120702  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.417\n",
      "iter 67300/1000000  loss         0.120701  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.418\n",
      "iter 67301/1000000  loss         0.120701  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.418\n",
      "iter 67400/1000000  loss         0.120700  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.419\n",
      "iter 67401/1000000  loss         0.120700  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.419\n",
      "iter 67500/1000000  loss         0.120698  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.419\n",
      "iter 67501/1000000  loss         0.120698  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.419\n",
      "iter 67600/1000000  loss         0.120697  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.420\n",
      "iter 67601/1000000  loss         0.120697  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.420\n",
      "iter 67700/1000000  loss         0.120696  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.421\n",
      "iter 67701/1000000  loss         0.120696  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.421\n",
      "iter 67800/1000000  loss         0.120695  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.422\n",
      "iter 67801/1000000  loss         0.120695  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.422\n",
      "iter 67900/1000000  loss         0.120694  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.423\n",
      "iter 67901/1000000  loss         0.120694  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.423\n",
      "iter 68000/1000000  loss         0.120693  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.423\n",
      "iter 68001/1000000  loss         0.120693  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.423\n",
      "iter 68100/1000000  loss         0.120691  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.424\n",
      "iter 68101/1000000  loss         0.120691  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.424\n",
      "iter 68200/1000000  loss         0.120690  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.425\n",
      "iter 68201/1000000  loss         0.120690  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.425\n",
      "iter 68300/1000000  loss         0.120689  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.426\n",
      "iter 68301/1000000  loss         0.120689  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.426\n",
      "iter 68400/1000000  loss         0.120688  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.427\n",
      "iter 68401/1000000  loss         0.120688  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.427\n",
      "iter 68500/1000000  loss         0.120687  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.427\n",
      "iter 68501/1000000  loss         0.120687  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.427\n",
      "iter 68600/1000000  loss         0.120686  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.428\n",
      "iter 68601/1000000  loss         0.120686  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.428\n",
      "iter 68700/1000000  loss         0.120685  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.429\n",
      "iter 68701/1000000  loss         0.120685  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.429\n",
      "iter 68800/1000000  loss         0.120684  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.430\n",
      "iter 68801/1000000  loss         0.120684  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.430\n",
      "iter 68900/1000000  loss         0.120683  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.430\n",
      "iter 68901/1000000  loss         0.120683  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.430\n",
      "iter 69000/1000000  loss         0.120681  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.431\n",
      "iter 69001/1000000  loss         0.120681  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.431\n",
      "iter 69100/1000000  loss         0.120680  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.432\n",
      "iter 69101/1000000  loss         0.120680  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.432\n",
      "iter 69200/1000000  loss         0.120679  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.433\n",
      "iter 69201/1000000  loss         0.120679  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69300/1000000  loss         0.120678  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.434\n",
      "iter 69301/1000000  loss         0.120678  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.434\n",
      "iter 69400/1000000  loss         0.120677  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.434\n",
      "iter 69401/1000000  loss         0.120677  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.434\n",
      "iter 69500/1000000  loss         0.120676  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.435\n",
      "iter 69501/1000000  loss         0.120676  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.435\n",
      "iter 69600/1000000  loss         0.120675  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.436\n",
      "iter 69601/1000000  loss         0.120675  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.436\n",
      "iter 69700/1000000  loss         0.120674  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.437\n",
      "iter 69701/1000000  loss         0.120674  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.437\n",
      "iter 69800/1000000  loss         0.120673  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.437\n",
      "iter 69801/1000000  loss         0.120673  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.437\n",
      "iter 69900/1000000  loss         0.120672  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.438\n",
      "iter 69901/1000000  loss         0.120672  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.438\n",
      "iter 70000/1000000  loss         0.120671  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.439\n",
      "iter 70001/1000000  loss         0.120671  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.439\n",
      "iter 70100/1000000  loss         0.120670  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.440\n",
      "iter 70101/1000000  loss         0.120670  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.440\n",
      "iter 70200/1000000  loss         0.120669  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.440\n",
      "iter 70201/1000000  loss         0.120669  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.440\n",
      "iter 70300/1000000  loss         0.120668  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.441\n",
      "iter 70301/1000000  loss         0.120668  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.441\n",
      "iter 70400/1000000  loss         0.120667  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.442\n",
      "iter 70401/1000000  loss         0.120667  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.442\n",
      "iter 70500/1000000  loss         0.120666  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.443\n",
      "iter 70501/1000000  loss         0.120666  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.443\n",
      "iter 70600/1000000  loss         0.120665  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.443\n",
      "iter 70601/1000000  loss         0.120665  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.443\n",
      "iter 70700/1000000  loss         0.120664  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.444\n",
      "iter 70701/1000000  loss         0.120664  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.444\n",
      "iter 70800/1000000  loss         0.120663  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.445\n",
      "iter 70801/1000000  loss         0.120663  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.445\n",
      "iter 70900/1000000  loss         0.120662  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.446\n",
      "iter 70901/1000000  loss         0.120662  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.446\n",
      "iter 71000/1000000  loss         0.120661  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.446\n",
      "iter 71001/1000000  loss         0.120661  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.446\n",
      "iter 71100/1000000  loss         0.120660  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.447\n",
      "iter 71101/1000000  loss         0.120660  avg_L1_norm_grad         0.000007  w[0]   -0.159 bias    2.447\n",
      "iter 71200/1000000  loss         0.120659  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.448\n",
      "iter 71201/1000000  loss         0.120659  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.448\n",
      "iter 71300/1000000  loss         0.120658  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.448\n",
      "iter 71301/1000000  loss         0.120658  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.448\n",
      "iter 71400/1000000  loss         0.120657  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.449\n",
      "iter 71401/1000000  loss         0.120657  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.449\n",
      "iter 71500/1000000  loss         0.120657  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.450\n",
      "iter 71501/1000000  loss         0.120657  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.450\n",
      "iter 71600/1000000  loss         0.120656  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.451\n",
      "iter 71601/1000000  loss         0.120656  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.451\n",
      "iter 71700/1000000  loss         0.120655  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.451\n",
      "iter 71701/1000000  loss         0.120655  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.451\n",
      "iter 71800/1000000  loss         0.120654  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.452\n",
      "iter 71801/1000000  loss         0.120654  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.452\n",
      "iter 71900/1000000  loss         0.120653  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.453\n",
      "iter 71901/1000000  loss         0.120653  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.453\n",
      "iter 72000/1000000  loss         0.120652  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.453\n",
      "iter 72001/1000000  loss         0.120652  avg_L1_norm_grad         0.000007  w[0]   -0.160 bias    2.453\n",
      "iter 72100/1000000  loss         0.120651  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.454\n",
      "iter 72101/1000000  loss         0.120651  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.454\n",
      "iter 72200/1000000  loss         0.120650  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.455\n",
      "iter 72201/1000000  loss         0.120650  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.455\n",
      "iter 72300/1000000  loss         0.120649  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.456\n",
      "iter 72301/1000000  loss         0.120649  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.456\n",
      "iter 72400/1000000  loss         0.120648  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.456\n",
      "iter 72401/1000000  loss         0.120648  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.456\n",
      "iter 72500/1000000  loss         0.120648  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.457\n",
      "iter 72501/1000000  loss         0.120648  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.457\n",
      "iter 72600/1000000  loss         0.120647  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.458\n",
      "iter 72601/1000000  loss         0.120647  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.458\n",
      "iter 72700/1000000  loss         0.120646  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.458\n",
      "iter 72701/1000000  loss         0.120646  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.458\n",
      "iter 72800/1000000  loss         0.120645  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.459\n",
      "iter 72801/1000000  loss         0.120645  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.459\n",
      "iter 72900/1000000  loss         0.120644  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.460\n",
      "iter 72901/1000000  loss         0.120644  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.460\n",
      "iter 73000/1000000  loss         0.120643  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.460\n",
      "iter 73001/1000000  loss         0.120643  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.460\n",
      "iter 73100/1000000  loss         0.120642  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.461\n",
      "iter 73101/1000000  loss         0.120642  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 73200/1000000  loss         0.120642  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.462\n",
      "iter 73201/1000000  loss         0.120642  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.462\n",
      "iter 73300/1000000  loss         0.120641  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.462\n",
      "iter 73301/1000000  loss         0.120641  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.462\n",
      "iter 73400/1000000  loss         0.120640  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.463\n",
      "iter 73401/1000000  loss         0.120640  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.463\n",
      "iter 73500/1000000  loss         0.120639  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.464\n",
      "iter 73501/1000000  loss         0.120639  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.464\n",
      "iter 73600/1000000  loss         0.120638  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.464\n",
      "iter 73601/1000000  loss         0.120638  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.464\n",
      "iter 73700/1000000  loss         0.120637  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.465\n",
      "iter 73701/1000000  loss         0.120637  avg_L1_norm_grad         0.000006  w[0]   -0.160 bias    2.465\n",
      "Done. Converged after 73709 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "new_lr1.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 36000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "New Accuracy 0.9657777777777751\n"
     ]
    }
   ],
   "source": [
    "y_hat_New=np.asarray(new_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2a, axes_arr = plt.subplots(nrows=1, ncols=2,figsize=(13,5))\n",
    "ax1=axes_arr[0]\n",
    "ax1.set_title('ROC'); ax1.set_xlabel(\"FPR\"); ax1.set_ylabel(\"TPR\");\n",
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "ax1.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "ax1.plot(fpr3te,tpr3te, label=\"Adding All turn_on of X & Y with Noise\")\n",
    "\n",
    "#fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "#ax1.plot(fpr1Tte,tpr1Tte, label=\"No added Feature, No Noise\")\n",
    "\n",
    "#fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "#ax1.plot(fprTte,tprTte, label=\"Adding All turn_on of X & Y, No Noise\")\n",
    "\n",
    "ax1.set_xlim([-0.0, 1.0]);\n",
    "ax1.set_ylim([-0.0, 1.0]);\n",
    "ax1.legend();\n",
    "\n",
    "ax2=axes_arr[1]\n",
    "ax2.set_title('Part of ROC'); ax2.set_xlabel(\"FPR\"); ax2.set_ylabel(\"TPR\");\n",
    "ax2.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "ax2.plot(fpr3te,tpr3te, label=\"Adding All turn_on of X & Y with Noise\")\n",
    "\n",
    "#ax2.plot(fpr1Tte,tpr1Tte, label=\"No added Feature, No Noise\")\n",
    "\n",
    "#ax2.plot(fprTte,tprTte, label=\"Adding All turn_on of X & Y, No Noise\")\n",
    "\n",
    "ax2.set_xlim([0.0, 0.2]);\n",
    "ax2.set_ylim([0.8, 1.0]);\n",
    "ax2.legend();\n",
    "\n",
    "fig3b, axes_arr = plt.subplots(nrows=1, ncols=2)\n",
    "w1=orig_lr22.w_G[:-1]\n",
    "ax1=axes_arr[0]; ax1.set_title('No New Feature');\n",
    "ax1.imshow(w1.reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "\n",
    "w2=new_lr1.w_G[:784]\n",
    "ax2=axes_arr[1]; ax2.set_title('With Noise');\n",
    "ax2.imshow(w1.reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 2000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr2.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 2000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = orig_lr1.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba11_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.415 0.993713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 2000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = orig_lr1.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba11_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0385 0.993808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 2000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n"
     ]
    }
   ],
   "source": [
    "#x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr1.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba17N_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0360 0.994828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W11=new_lr1.w_G\n",
    "np.savetxt('trained_weights/Noise_and_5TurnON_P3_Copy7.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
