{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x.copy()\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(0,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on original features!\n",
    "#orig_lr2 = LRGD(alpha=10.0, step_size=0.1)\n",
    "#orig_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_Origin=np.asarray(orig_lr2.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "#tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "#acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "#print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run LR on transformed features!\n",
    "#new_lr2 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "#new_lr2.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## self.feature_transform\n",
    " self.feature_transform_pipeline = sklearn.pipeline.Pipeline(\n",
    "                    [('rescaler', sklearn.preprocessing.MinMaxScaler()),  \n",
    "                    ('feature_transform', sklearn.pipeline.FeatureUnion(transformer_list=[  \n",
    "                    ('original_x', sklearn.preprocessing.PolynomialFeatures(degree=1, include_bias=False)),  \n",
    "                    ('TurnOn_x', TurnOnFeatureExtractor()),  \n",
    "                    ('TurnOn_x2', TurnOnOnceFeatureExtractor()),  \n",
    "                    ('TurnOn_all', TurnOnAllFeatureExtractor()),  \n",
    "                    ('TurnOn_y', TurnOnFeatureExtractorY()),  \n",
    "                    ('TurnOn_y2', TurnOnOnceFeatureExtractorY()),  \n",
    "                    ('squared_x', SquaredFeatureExtractor()),  \n",
    "                    ]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax Transform\n",
      "TurnOn Loaded 0.0\n",
      "(1, 8400)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Square Loaded!\n",
      "Shape of Transformed Data (8400, 1574)\n",
      "Initializing w_G with 1574 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.027764  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.914183  avg_L1_norm_grad         0.033593  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.880440  avg_L1_norm_grad         0.035470  w[0]    0.000 bias    0.031\n",
      "iter    3/1000000  loss         0.913287  avg_L1_norm_grad         0.042291  w[0]    0.000 bias    0.018\n",
      "iter    4/1000000  loss         0.807783  avg_L1_norm_grad         0.034863  w[0]    0.000 bias    0.056\n",
      "iter    5/1000000  loss         0.815084  avg_L1_norm_grad         0.035766  w[0]    0.000 bias    0.041\n",
      "iter    6/1000000  loss         0.711749  avg_L1_norm_grad         0.026426  w[0]    0.000 bias    0.075\n",
      "iter    7/1000000  loss         0.697250  avg_L1_norm_grad         0.025151  w[0]    0.000 bias    0.066\n",
      "iter    8/1000000  loss         0.642592  avg_L1_norm_grad         0.019015  w[0]    0.000 bias    0.092\n",
      "iter    9/1000000  loss         0.624201  avg_L1_norm_grad         0.017075  w[0]    0.000 bias    0.088\n",
      "iter   10/1000000  loss         0.594333  avg_L1_norm_grad         0.013345  w[0]    0.000 bias    0.108\n",
      "iter   11/1000000  loss         0.577979  avg_L1_norm_grad         0.011373  w[0]    0.000 bias    0.107\n",
      "iter   12/1000000  loss         0.559641  avg_L1_norm_grad         0.009256  w[0]    0.000 bias    0.123\n",
      "iter   13/1000000  loss         0.546361  avg_L1_norm_grad         0.007608  w[0]    0.000 bias    0.125\n",
      "iter   14/1000000  loss         0.533608  avg_L1_norm_grad         0.006502  w[0]    0.000 bias    0.137\n",
      "iter   15/1000000  loss         0.522896  avg_L1_norm_grad         0.005362  w[0]    0.000 bias    0.141\n",
      "iter   16/1000000  loss         0.513006  avg_L1_norm_grad         0.004944  w[0]    0.000 bias    0.150\n",
      "iter   17/1000000  loss         0.504086  avg_L1_norm_grad         0.004283  w[0]    0.000 bias    0.155\n",
      "iter   18/1000000  loss         0.495802  avg_L1_norm_grad         0.004172  w[0]    0.000 bias    0.163\n",
      "iter   19/1000000  loss         0.488092  avg_L1_norm_grad         0.003823  w[0]    0.000 bias    0.169\n",
      "iter  100/1000000  loss         0.296911  avg_L1_norm_grad         0.001259  w[0]    0.000 bias    0.448\n",
      "iter  101/1000000  loss         0.296098  avg_L1_norm_grad         0.001250  w[0]    0.000 bias    0.450\n",
      "iter  200/1000000  loss         0.248998  avg_L1_norm_grad         0.000763  w[0]    0.000 bias    0.594\n",
      "iter  201/1000000  loss         0.248712  avg_L1_norm_grad         0.000760  w[0]    0.000 bias    0.595\n",
      "iter  300/1000000  loss         0.228106  avg_L1_norm_grad         0.000567  w[0]    0.000 bias    0.681\n",
      "iter  301/1000000  loss         0.227954  avg_L1_norm_grad         0.000566  w[0]    0.000 bias    0.682\n",
      "iter  400/1000000  loss         0.215902  avg_L1_norm_grad         0.000459  w[0]    0.000 bias    0.744\n",
      "iter  401/1000000  loss         0.215805  avg_L1_norm_grad         0.000458  w[0]    0.000 bias    0.745\n",
      "iter  500/1000000  loss         0.207744  avg_L1_norm_grad         0.000388  w[0]    0.000 bias    0.794\n",
      "iter  501/1000000  loss         0.207676  avg_L1_norm_grad         0.000387  w[0]    0.000 bias    0.794\n",
      "iter  600/1000000  loss         0.201847  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    0.835\n",
      "iter  601/1000000  loss         0.201796  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    0.835\n",
      "iter  700/1000000  loss         0.197362  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    0.870\n",
      "iter  701/1000000  loss         0.197323  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    0.871\n",
      "iter  800/1000000  loss         0.193827  avg_L1_norm_grad         0.000271  w[0]    0.000 bias    0.901\n",
      "iter  801/1000000  loss         0.193795  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    0.902\n",
      "iter  900/1000000  loss         0.190964  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    0.929\n",
      "iter  901/1000000  loss         0.190938  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    0.929\n",
      "iter 1000/1000000  loss         0.188598  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    0.954\n",
      "iter 1001/1000000  loss         0.188576  avg_L1_norm_grad         0.000226  w[0]    0.000 bias    0.954\n",
      "iter 1100/1000000  loss         0.186610  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    0.977\n",
      "iter 1101/1000000  loss         0.186591  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    0.977\n",
      "iter 1200/1000000  loss         0.184917  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    0.998\n",
      "iter 1201/1000000  loss         0.184901  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    0.998\n",
      "iter 1300/1000000  loss         0.183458  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    1.017\n",
      "iter 1301/1000000  loss         0.183445  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    1.017\n",
      "iter 1400/1000000  loss         0.182190  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    1.035\n",
      "iter 1401/1000000  loss         0.182179  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    1.035\n",
      "iter 1500/1000000  loss         0.181079  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    1.051\n",
      "iter 1501/1000000  loss         0.181068  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    1.051\n",
      "iter 1600/1000000  loss         0.180097  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    1.067\n",
      "iter 1601/1000000  loss         0.180088  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    1.067\n",
      "iter 1700/1000000  loss         0.179225  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    1.081\n",
      "iter 1701/1000000  loss         0.179217  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    1.081\n",
      "iter 1800/1000000  loss         0.178447  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    1.095\n",
      "iter 1801/1000000  loss         0.178439  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    1.095\n",
      "iter 1900/1000000  loss         0.177747  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    1.108\n",
      "iter 1901/1000000  loss         0.177741  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    1.108\n",
      "iter 2000/1000000  loss         0.177117  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    1.120\n",
      "iter 2001/1000000  loss         0.177111  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    1.120\n",
      "iter 2100/1000000  loss         0.176546  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    1.131\n",
      "iter 2101/1000000  loss         0.176541  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    1.131\n",
      "iter 2200/1000000  loss         0.176028  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    1.142\n",
      "iter 2201/1000000  loss         0.176023  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    1.142\n",
      "iter 2300/1000000  loss         0.175555  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    1.152\n",
      "iter 2301/1000000  loss         0.175551  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    1.152\n",
      "iter 2400/1000000  loss         0.175123  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    1.162\n",
      "iter 2401/1000000  loss         0.175119  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    1.162\n",
      "iter 2500/1000000  loss         0.174728  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    1.171\n",
      "iter 2501/1000000  loss         0.174724  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    1.171\n",
      "iter 2600/1000000  loss         0.174364  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    1.180\n",
      "iter 2601/1000000  loss         0.174361  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    1.180\n",
      "iter 2700/1000000  loss         0.174030  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    1.188\n",
      "iter 2701/1000000  loss         0.174027  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    1.188\n",
      "iter 2800/1000000  loss         0.173722  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    1.196\n",
      "iter 2801/1000000  loss         0.173719  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    1.196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.173437  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    1.203\n",
      "iter 2901/1000000  loss         0.173434  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    1.203\n",
      "iter 3000/1000000  loss         0.173173  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    1.211\n",
      "iter 3001/1000000  loss         0.173170  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    1.211\n",
      "iter 3100/1000000  loss         0.172928  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    1.217\n",
      "iter 3101/1000000  loss         0.172926  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    1.218\n",
      "iter 3200/1000000  loss         0.172701  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    1.224\n",
      "iter 3201/1000000  loss         0.172699  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    1.224\n",
      "iter 3300/1000000  loss         0.172490  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    1.230\n",
      "iter 3301/1000000  loss         0.172488  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    1.230\n",
      "iter 3400/1000000  loss         0.172294  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    1.236\n",
      "iter 3401/1000000  loss         0.172292  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    1.236\n",
      "iter 3500/1000000  loss         0.172110  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    1.242\n",
      "iter 3501/1000000  loss         0.172109  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    1.242\n",
      "iter 3600/1000000  loss         0.171940  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    1.248\n",
      "iter 3601/1000000  loss         0.171938  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    1.248\n",
      "iter 3700/1000000  loss         0.171780  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    1.253\n",
      "iter 3701/1000000  loss         0.171778  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    1.253\n",
      "iter 3800/1000000  loss         0.171631  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    1.258\n",
      "iter 3801/1000000  loss         0.171629  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    1.258\n",
      "iter 3900/1000000  loss         0.171491  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    1.263\n",
      "iter 3901/1000000  loss         0.171490  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    1.263\n",
      "iter 4000/1000000  loss         0.171361  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    1.268\n",
      "iter 4001/1000000  loss         0.171359  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    1.268\n",
      "iter 4100/1000000  loss         0.171238  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    1.272\n",
      "iter 4101/1000000  loss         0.171237  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    1.272\n",
      "iter 4200/1000000  loss         0.171123  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    1.276\n",
      "iter 4201/1000000  loss         0.171122  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    1.276\n",
      "iter 4300/1000000  loss         0.171015  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    1.281\n",
      "iter 4301/1000000  loss         0.171014  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    1.281\n",
      "iter 4400/1000000  loss         0.170914  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    1.285\n",
      "iter 4401/1000000  loss         0.170913  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    1.285\n",
      "iter 4500/1000000  loss         0.170819  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    1.288\n",
      "iter 4501/1000000  loss         0.170818  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    1.288\n",
      "iter 4600/1000000  loss         0.170729  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    1.292\n",
      "iter 4601/1000000  loss         0.170728  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    1.292\n",
      "iter 4700/1000000  loss         0.170645  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    1.296\n",
      "iter 4701/1000000  loss         0.170644  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    1.296\n",
      "iter 4800/1000000  loss         0.170566  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    1.299\n",
      "iter 4801/1000000  loss         0.170565  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    1.299\n",
      "iter 4900/1000000  loss         0.170491  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    1.303\n",
      "iter 4901/1000000  loss         0.170490  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    1.303\n",
      "iter 5000/1000000  loss         0.170421  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    1.306\n",
      "iter 5001/1000000  loss         0.170420  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    1.306\n",
      "iter 5100/1000000  loss         0.170354  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    1.309\n",
      "iter 5101/1000000  loss         0.170354  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    1.309\n",
      "iter 5200/1000000  loss         0.170292  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    1.312\n",
      "iter 5201/1000000  loss         0.170291  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    1.312\n",
      "iter 5300/1000000  loss         0.170233  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    1.315\n",
      "iter 5301/1000000  loss         0.170232  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    1.315\n",
      "iter 5400/1000000  loss         0.170177  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    1.318\n",
      "iter 5401/1000000  loss         0.170176  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    1.318\n",
      "iter 5500/1000000  loss         0.170124  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    1.320\n",
      "iter 5501/1000000  loss         0.170123  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    1.320\n",
      "iter 5600/1000000  loss         0.170074  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    1.323\n",
      "iter 5601/1000000  loss         0.170074  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    1.323\n",
      "iter 5700/1000000  loss         0.170027  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    1.326\n",
      "iter 5701/1000000  loss         0.170027  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    1.326\n",
      "iter 5800/1000000  loss         0.169982  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    1.328\n",
      "iter 5801/1000000  loss         0.169982  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    1.328\n",
      "iter 5900/1000000  loss         0.169940  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    1.330\n",
      "iter 5901/1000000  loss         0.169940  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    1.330\n",
      "iter 6000/1000000  loss         0.169900  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    1.333\n",
      "iter 6001/1000000  loss         0.169900  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    1.333\n",
      "iter 6100/1000000  loss         0.169863  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.335\n",
      "iter 6101/1000000  loss         0.169862  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.335\n",
      "iter 6200/1000000  loss         0.169827  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    1.337\n",
      "iter 6201/1000000  loss         0.169827  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    1.337\n",
      "iter 6300/1000000  loss         0.169793  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.339\n",
      "iter 6301/1000000  loss         0.169793  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.339\n",
      "iter 6400/1000000  loss         0.169761  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.341\n",
      "iter 6401/1000000  loss         0.169761  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.341\n",
      "iter 6500/1000000  loss         0.169731  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    1.343\n",
      "iter 6501/1000000  loss         0.169730  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    1.343\n",
      "iter 6600/1000000  loss         0.169702  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.345\n",
      "iter 6601/1000000  loss         0.169701  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.345\n",
      "iter 6700/1000000  loss         0.169674  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.347\n",
      "iter 6701/1000000  loss         0.169674  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.347\n",
      "iter 6800/1000000  loss         0.169648  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.349\n",
      "iter 6801/1000000  loss         0.169648  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6900/1000000  loss         0.169624  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.351\n",
      "iter 6901/1000000  loss         0.169624  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.351\n",
      "iter 7000/1000000  loss         0.169600  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.352\n",
      "iter 7001/1000000  loss         0.169600  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.352\n",
      "iter 7100/1000000  loss         0.169578  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.354\n",
      "iter 7101/1000000  loss         0.169578  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.354\n",
      "iter 7200/1000000  loss         0.169557  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.356\n",
      "iter 7201/1000000  loss         0.169557  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.356\n",
      "iter 7300/1000000  loss         0.169537  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.357\n",
      "iter 7301/1000000  loss         0.169537  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.357\n",
      "iter 7400/1000000  loss         0.169518  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.359\n",
      "iter 7401/1000000  loss         0.169518  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.359\n",
      "iter 7500/1000000  loss         0.169500  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.360\n",
      "iter 7501/1000000  loss         0.169500  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.360\n",
      "iter 7600/1000000  loss         0.169483  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.362\n",
      "iter 7601/1000000  loss         0.169483  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.362\n",
      "iter 7700/1000000  loss         0.169467  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.363\n",
      "iter 7701/1000000  loss         0.169467  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.363\n",
      "iter 7800/1000000  loss         0.169451  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.364\n",
      "iter 7801/1000000  loss         0.169451  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.364\n",
      "iter 7900/1000000  loss         0.169436  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.366\n",
      "iter 7901/1000000  loss         0.169436  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.366\n",
      "iter 8000/1000000  loss         0.169422  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.367\n",
      "iter 8001/1000000  loss         0.169422  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.367\n",
      "iter 8100/1000000  loss         0.169409  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.368\n",
      "iter 8101/1000000  loss         0.169409  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.368\n",
      "iter 8200/1000000  loss         0.169396  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.369\n",
      "iter 8201/1000000  loss         0.169396  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.369\n",
      "iter 8300/1000000  loss         0.169384  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.370\n",
      "iter 8301/1000000  loss         0.169384  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.370\n",
      "iter 8400/1000000  loss         0.169373  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.372\n",
      "iter 8401/1000000  loss         0.169373  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.372\n",
      "iter 8500/1000000  loss         0.169362  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.373\n",
      "iter 8501/1000000  loss         0.169362  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.373\n",
      "iter 8600/1000000  loss         0.169351  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.374\n",
      "iter 8601/1000000  loss         0.169351  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.374\n",
      "iter 8700/1000000  loss         0.169341  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.375\n",
      "iter 8701/1000000  loss         0.169341  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.375\n",
      "iter 8800/1000000  loss         0.169332  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.376\n",
      "iter 8801/1000000  loss         0.169332  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.376\n",
      "iter 8900/1000000  loss         0.169323  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.377\n",
      "iter 8901/1000000  loss         0.169323  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.377\n",
      "iter 9000/1000000  loss         0.169314  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.378\n",
      "iter 9001/1000000  loss         0.169314  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.378\n",
      "iter 9100/1000000  loss         0.169306  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.379\n",
      "iter 9101/1000000  loss         0.169306  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.379\n",
      "iter 9200/1000000  loss         0.169298  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.380\n",
      "iter 9201/1000000  loss         0.169298  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.380\n",
      "iter 9300/1000000  loss         0.169291  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.381\n",
      "iter 9301/1000000  loss         0.169291  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.381\n",
      "iter 9400/1000000  loss         0.169284  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.381\n",
      "iter 9401/1000000  loss         0.169284  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.381\n",
      "iter 9500/1000000  loss         0.169277  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.382\n",
      "iter 9501/1000000  loss         0.169277  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.382\n",
      "iter 9600/1000000  loss         0.169271  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.383\n",
      "iter 9601/1000000  loss         0.169271  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.383\n",
      "iter 9700/1000000  loss         0.169265  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.384\n",
      "iter 9701/1000000  loss         0.169265  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.384\n",
      "iter 9800/1000000  loss         0.169259  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.385\n",
      "iter 9801/1000000  loss         0.169259  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.385\n",
      "iter 9900/1000000  loss         0.169253  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.385\n",
      "iter 9901/1000000  loss         0.169253  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.385\n",
      "iter 10000/1000000  loss         0.169248  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.386\n",
      "iter 10001/1000000  loss         0.169248  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.386\n",
      "iter 10100/1000000  loss         0.169243  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.387\n",
      "iter 10101/1000000  loss         0.169243  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.387\n",
      "iter 10200/1000000  loss         0.169238  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.388\n",
      "iter 10201/1000000  loss         0.169238  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.388\n",
      "iter 10300/1000000  loss         0.169233  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.388\n",
      "iter 10301/1000000  loss         0.169233  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.388\n",
      "iter 10400/1000000  loss         0.169229  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.389\n",
      "iter 10401/1000000  loss         0.169229  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.389\n",
      "iter 10500/1000000  loss         0.169225  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.390\n",
      "iter 10501/1000000  loss         0.169224  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.390\n",
      "iter 10600/1000000  loss         0.169220  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.390\n",
      "iter 10601/1000000  loss         0.169220  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.390\n",
      "iter 10700/1000000  loss         0.169217  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.391\n",
      "iter 10701/1000000  loss         0.169217  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10800/1000000  loss         0.169213  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.392\n",
      "iter 10801/1000000  loss         0.169213  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.392\n",
      "iter 10900/1000000  loss         0.169209  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.392\n",
      "iter 10901/1000000  loss         0.169209  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    1.392\n",
      "iter 11000/1000000  loss         0.169206  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.393\n",
      "iter 11001/1000000  loss         0.169206  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.393\n",
      "iter 11100/1000000  loss         0.169203  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.393\n",
      "iter 11101/1000000  loss         0.169203  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.393\n",
      "iter 11200/1000000  loss         0.169200  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.394\n",
      "iter 11201/1000000  loss         0.169200  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.394\n",
      "iter 11300/1000000  loss         0.169197  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.394\n",
      "iter 11301/1000000  loss         0.169197  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.394\n",
      "iter 11400/1000000  loss         0.169194  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.395\n",
      "iter 11401/1000000  loss         0.169194  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    1.395\n",
      "iter 11500/1000000  loss         0.169191  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.395\n",
      "iter 11501/1000000  loss         0.169191  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.395\n",
      "iter 11600/1000000  loss         0.169189  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.396\n",
      "iter 11601/1000000  loss         0.169189  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.396\n",
      "iter 11700/1000000  loss         0.169186  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.396\n",
      "iter 11701/1000000  loss         0.169186  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.396\n",
      "iter 11800/1000000  loss         0.169184  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.397\n",
      "iter 11801/1000000  loss         0.169184  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.397\n",
      "iter 11900/1000000  loss         0.169182  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.397\n",
      "iter 11901/1000000  loss         0.169182  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    1.397\n",
      "iter 12000/1000000  loss         0.169180  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.398\n",
      "iter 12001/1000000  loss         0.169180  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.398\n",
      "iter 12100/1000000  loss         0.169178  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.398\n",
      "iter 12101/1000000  loss         0.169178  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.398\n",
      "iter 12200/1000000  loss         0.169176  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.399\n",
      "iter 12201/1000000  loss         0.169176  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    1.399\n",
      "Done. Converged after 12261 iterations.\n",
      "Shape of Transformed Data (8400, 785)\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.031696  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.906516  avg_L1_norm_grad         0.026254  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.841723  avg_L1_norm_grad         0.018679  w[0]    0.000 bias    0.022\n",
      "iter    3/1000000  loss         0.799066  avg_L1_norm_grad         0.015978  w[0]    0.000 bias    0.028\n",
      "iter    4/1000000  loss         0.767438  avg_L1_norm_grad         0.012957  w[0]    0.000 bias    0.044\n",
      "iter    5/1000000  loss         0.742949  avg_L1_norm_grad         0.011840  w[0]    0.000 bias    0.053\n",
      "iter    6/1000000  loss         0.722724  avg_L1_norm_grad         0.010529  w[0]    0.000 bias    0.065\n",
      "iter    7/1000000  loss         0.705326  avg_L1_norm_grad         0.009883  w[0]    0.000 bias    0.075\n",
      "iter    8/1000000  loss         0.689909  avg_L1_norm_grad         0.009207  w[0]    0.000 bias    0.086\n",
      "iter    9/1000000  loss         0.675973  avg_L1_norm_grad         0.008730  w[0]    0.000 bias    0.096\n",
      "iter   10/1000000  loss         0.663194  avg_L1_norm_grad         0.008304  w[0]    0.000 bias    0.107\n",
      "iter   11/1000000  loss         0.651355  avg_L1_norm_grad         0.007948  w[0]    0.000 bias    0.117\n",
      "iter   12/1000000  loss         0.640302  avg_L1_norm_grad         0.007636  w[0]    0.000 bias    0.126\n",
      "iter   13/1000000  loss         0.629922  avg_L1_norm_grad         0.007362  w[0]    0.000 bias    0.136\n",
      "iter   14/1000000  loss         0.620128  avg_L1_norm_grad         0.007119  w[0]    0.000 bias    0.145\n",
      "iter   15/1000000  loss         0.610855  avg_L1_norm_grad         0.006899  w[0]    0.000 bias    0.155\n",
      "iter   16/1000000  loss         0.602049  avg_L1_norm_grad         0.006696  w[0]    0.000 bias    0.164\n",
      "iter   17/1000000  loss         0.593667  avg_L1_norm_grad         0.006510  w[0]    0.000 bias    0.173\n",
      "iter   18/1000000  loss         0.585674  avg_L1_norm_grad         0.006337  w[0]    0.000 bias    0.182\n",
      "iter   19/1000000  loss         0.578038  avg_L1_norm_grad         0.006178  w[0]    0.000 bias    0.191\n",
      "iter  100/1000000  loss         0.354226  avg_L1_norm_grad         0.002238  w[0]    0.000 bias    0.660\n",
      "iter  101/1000000  loss         0.353157  avg_L1_norm_grad         0.002221  w[0]    0.000 bias    0.664\n",
      "iter  200/1000000  loss         0.290401  avg_L1_norm_grad         0.001318  w[0]    0.000 bias    0.983\n",
      "iter  201/1000000  loss         0.290019  avg_L1_norm_grad         0.001313  w[0]    0.000 bias    0.985\n",
      "iter  300/1000000  loss         0.262677  avg_L1_norm_grad         0.000950  w[0]    0.000 bias    1.201\n",
      "iter  301/1000000  loss         0.262477  avg_L1_norm_grad         0.000947  w[0]    0.000 bias    1.202\n",
      "iter  400/1000000  loss         0.246901  avg_L1_norm_grad         0.000749  w[0]    0.000 bias    1.363\n",
      "iter  401/1000000  loss         0.246778  avg_L1_norm_grad         0.000748  w[0]    0.000 bias    1.364\n",
      "iter  500/1000000  loss         0.236682  avg_L1_norm_grad         0.000622  w[0]    0.000 bias    1.490\n",
      "iter  501/1000000  loss         0.236598  avg_L1_norm_grad         0.000621  w[0]    0.000 bias    1.491\n",
      "iter  600/1000000  loss         0.229525  avg_L1_norm_grad         0.000534  w[0]    0.000 bias    1.593\n",
      "iter  601/1000000  loss         0.229464  avg_L1_norm_grad         0.000533  w[0]    0.000 bias    1.593\n",
      "iter  700/1000000  loss         0.224242  avg_L1_norm_grad         0.000468  w[0]    0.000 bias    1.677\n",
      "iter  701/1000000  loss         0.224196  avg_L1_norm_grad         0.000467  w[0]    0.000 bias    1.678\n",
      "iter  800/1000000  loss         0.220191  avg_L1_norm_grad         0.000416  w[0]    0.000 bias    1.748\n",
      "iter  801/1000000  loss         0.220156  avg_L1_norm_grad         0.000416  w[0]    0.000 bias    1.749\n",
      "iter  900/1000000  loss         0.216996  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.808\n",
      "iter  901/1000000  loss         0.216967  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.809\n",
      "iter 1000/1000000  loss         0.214417  avg_L1_norm_grad         0.000340  w[0]    0.000 bias    1.859\n",
      "iter 1001/1000000  loss         0.214394  avg_L1_norm_grad         0.000339  w[0]    0.000 bias    1.860\n",
      "iter 1100/1000000  loss         0.212299  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    1.903\n",
      "iter 1101/1000000  loss         0.212280  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    1.904\n",
      "iter 1200/1000000  loss         0.210533  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    1.942\n",
      "iter 1201/1000000  loss         0.210517  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    1.942\n",
      "iter 1300/1000000  loss         0.209043  avg_L1_norm_grad         0.000264  w[0]    0.000 bias    1.975\n",
      "iter 1301/1000000  loss         0.209029  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    1.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1400/1000000  loss         0.207772  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.004\n",
      "iter 1401/1000000  loss         0.207760  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.004\n",
      "iter 1500/1000000  loss         0.206677  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.029\n",
      "iter 1501/1000000  loss         0.206667  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.030\n",
      "iter 1600/1000000  loss         0.205728  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.052\n",
      "iter 1601/1000000  loss         0.205719  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.052\n",
      "iter 1700/1000000  loss         0.204899  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.071\n",
      "iter 1701/1000000  loss         0.204892  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.071\n",
      "iter 1800/1000000  loss         0.204171  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.089\n",
      "iter 1801/1000000  loss         0.204164  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.089\n",
      "iter 1900/1000000  loss         0.203527  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.104\n",
      "iter 1901/1000000  loss         0.203521  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.104\n",
      "iter 2000/1000000  loss         0.202956  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.117\n",
      "iter 2001/1000000  loss         0.202951  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    2.117\n",
      "iter 2100/1000000  loss         0.202447  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.129\n",
      "iter 2101/1000000  loss         0.202442  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.129\n",
      "iter 2200/1000000  loss         0.201991  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.140\n",
      "iter 2201/1000000  loss         0.201987  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.140\n",
      "iter 2300/1000000  loss         0.201581  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.149\n",
      "iter 2301/1000000  loss         0.201577  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.149\n",
      "iter 2400/1000000  loss         0.201212  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.157\n",
      "iter 2401/1000000  loss         0.201208  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    2.158\n",
      "iter 2500/1000000  loss         0.200878  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.165\n",
      "iter 2501/1000000  loss         0.200875  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.165\n",
      "iter 2600/1000000  loss         0.200575  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.171\n",
      "iter 2601/1000000  loss         0.200572  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.171\n",
      "iter 2700/1000000  loss         0.200300  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.177\n",
      "iter 2701/1000000  loss         0.200297  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.177\n",
      "iter 2800/1000000  loss         0.200049  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    2.182\n",
      "iter 2801/1000000  loss         0.200046  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    2.182\n",
      "iter 2900/1000000  loss         0.199820  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.186\n",
      "iter 2901/1000000  loss         0.199817  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.187\n",
      "iter 3000/1000000  loss         0.199610  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.190\n",
      "iter 3001/1000000  loss         0.199608  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.190\n",
      "iter 3100/1000000  loss         0.199417  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.194\n",
      "iter 3101/1000000  loss         0.199416  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.194\n",
      "iter 3200/1000000  loss         0.199241  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.197\n",
      "iter 3201/1000000  loss         0.199239  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.197\n",
      "iter 3300/1000000  loss         0.199078  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.200\n",
      "iter 3301/1000000  loss         0.199076  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.200\n",
      "iter 3400/1000000  loss         0.198928  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.202\n",
      "iter 3401/1000000  loss         0.198927  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.202\n",
      "iter 3500/1000000  loss         0.198790  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.204\n",
      "iter 3501/1000000  loss         0.198788  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.204\n",
      "iter 3600/1000000  loss         0.198662  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.206\n",
      "iter 3601/1000000  loss         0.198660  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.206\n",
      "iter 3700/1000000  loss         0.198543  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.207\n",
      "iter 3701/1000000  loss         0.198542  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.207\n",
      "iter 3800/1000000  loss         0.198433  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.208\n",
      "iter 3801/1000000  loss         0.198432  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.208\n",
      "iter 3900/1000000  loss         0.198331  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.210\n",
      "iter 3901/1000000  loss         0.198330  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.210\n",
      "iter 4000/1000000  loss         0.198237  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.210\n",
      "iter 4001/1000000  loss         0.198236  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.210\n",
      "iter 4100/1000000  loss         0.198148  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.211\n",
      "iter 4101/1000000  loss         0.198148  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.211\n",
      "iter 4200/1000000  loss         0.198066  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.212\n",
      "iter 4201/1000000  loss         0.198066  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.212\n",
      "iter 4300/1000000  loss         0.197990  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.212\n",
      "iter 4301/1000000  loss         0.197989  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.212\n",
      "iter 4400/1000000  loss         0.197919  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.213\n",
      "iter 4401/1000000  loss         0.197918  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.213\n",
      "iter 4500/1000000  loss         0.197852  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.213\n",
      "iter 4501/1000000  loss         0.197851  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.213\n",
      "iter 4600/1000000  loss         0.197790  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.213\n",
      "iter 4601/1000000  loss         0.197789  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.213\n",
      "iter 4700/1000000  loss         0.197731  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.213\n",
      "iter 4701/1000000  loss         0.197731  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.213\n",
      "iter 4800/1000000  loss         0.197677  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.213\n",
      "iter 4801/1000000  loss         0.197676  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.213\n",
      "iter 4900/1000000  loss         0.197626  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.213\n",
      "iter 4901/1000000  loss         0.197625  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.213\n",
      "iter 5000/1000000  loss         0.197578  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.213\n",
      "iter 5001/1000000  loss         0.197578  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.213\n",
      "iter 5100/1000000  loss         0.197533  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.213\n",
      "iter 5101/1000000  loss         0.197533  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.213\n",
      "iter 5200/1000000  loss         0.197491  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.213\n",
      "iter 5201/1000000  loss         0.197491  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.213\n",
      "iter 5300/1000000  loss         0.197452  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.213\n",
      "iter 5301/1000000  loss         0.197451  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5400/1000000  loss         0.197415  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.213\n",
      "iter 5401/1000000  loss         0.197414  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.213\n",
      "iter 5500/1000000  loss         0.197380  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.212\n",
      "iter 5501/1000000  loss         0.197379  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.212\n",
      "iter 5600/1000000  loss         0.197347  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.212\n",
      "iter 5601/1000000  loss         0.197346  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.212\n",
      "iter 5700/1000000  loss         0.197316  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.212\n",
      "iter 5701/1000000  loss         0.197316  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.212\n",
      "iter 5800/1000000  loss         0.197287  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.212\n",
      "iter 5801/1000000  loss         0.197287  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.212\n",
      "iter 5900/1000000  loss         0.197259  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.211\n",
      "iter 5901/1000000  loss         0.197259  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.211\n",
      "iter 6000/1000000  loss         0.197234  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.211\n",
      "iter 6001/1000000  loss         0.197233  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.211\n",
      "iter 6100/1000000  loss         0.197209  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.211\n",
      "iter 6101/1000000  loss         0.197209  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.211\n",
      "iter 6200/1000000  loss         0.197186  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.210\n",
      "iter 6201/1000000  loss         0.197186  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.210\n",
      "iter 6300/1000000  loss         0.197165  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.210\n",
      "iter 6301/1000000  loss         0.197164  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.210\n",
      "iter 6400/1000000  loss         0.197144  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.210\n",
      "iter 6401/1000000  loss         0.197144  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.210\n",
      "iter 6500/1000000  loss         0.197125  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.209\n",
      "iter 6501/1000000  loss         0.197125  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.209\n",
      "iter 6600/1000000  loss         0.197107  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.209\n",
      "iter 6601/1000000  loss         0.197106  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.209\n",
      "iter 6700/1000000  loss         0.197089  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.208\n",
      "iter 6701/1000000  loss         0.197089  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.208\n",
      "iter 6800/1000000  loss         0.197073  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.208\n",
      "iter 6801/1000000  loss         0.197073  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.208\n",
      "iter 6900/1000000  loss         0.197058  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.208\n",
      "iter 6901/1000000  loss         0.197057  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.208\n",
      "iter 7000/1000000  loss         0.197043  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7001/1000000  loss         0.197043  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7100/1000000  loss         0.197029  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7101/1000000  loss         0.197029  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.207\n",
      "iter 7200/1000000  loss         0.197016  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.207\n",
      "iter 7201/1000000  loss         0.197016  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.207\n",
      "iter 7300/1000000  loss         0.197004  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7301/1000000  loss         0.197004  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7400/1000000  loss         0.196992  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7401/1000000  loss         0.196992  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.206\n",
      "iter 7500/1000000  loss         0.196981  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.206\n",
      "iter 7501/1000000  loss         0.196981  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.206\n",
      "iter 7600/1000000  loss         0.196971  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7601/1000000  loss         0.196970  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7700/1000000  loss         0.196961  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7701/1000000  loss         0.196960  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.205\n",
      "iter 7800/1000000  loss         0.196951  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.205\n",
      "iter 7801/1000000  loss         0.196951  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.205\n",
      "iter 7900/1000000  loss         0.196942  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 7901/1000000  loss         0.196942  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 8000/1000000  loss         0.196934  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 8001/1000000  loss         0.196934  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.204\n",
      "iter 8100/1000000  loss         0.196926  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.204\n",
      "iter 8101/1000000  loss         0.196926  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.204\n",
      "iter 8200/1000000  loss         0.196918  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.203\n",
      "iter 8201/1000000  loss         0.196918  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.203\n",
      "iter 8300/1000000  loss         0.196911  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8301/1000000  loss         0.196911  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8400/1000000  loss         0.196904  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8401/1000000  loss         0.196904  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.203\n",
      "iter 8500/1000000  loss         0.196897  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.203\n",
      "iter 8501/1000000  loss         0.196897  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.203\n",
      "iter 8600/1000000  loss         0.196891  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.202\n",
      "iter 8601/1000000  loss         0.196891  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.202\n",
      "iter 8700/1000000  loss         0.196885  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8701/1000000  loss         0.196885  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8800/1000000  loss         0.196880  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8801/1000000  loss         0.196880  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.202\n",
      "iter 8900/1000000  loss         0.196874  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.202\n",
      "iter 8901/1000000  loss         0.196874  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.202\n",
      "iter 9000/1000000  loss         0.196869  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9001/1000000  loss         0.196869  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9100/1000000  loss         0.196864  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9101/1000000  loss         0.196864  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.201\n",
      "iter 9200/1000000  loss         0.196860  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9201/1000000  loss         0.196860  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9300/1000000  loss         0.196855  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n",
      "iter 9301/1000000  loss         0.196855  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9400/1000000  loss         0.196851  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9401/1000000  loss         0.196851  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9500/1000000  loss         0.196847  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9501/1000000  loss         0.196847  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9600/1000000  loss         0.196844  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9601/1000000  loss         0.196844  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.200\n",
      "iter 9700/1000000  loss         0.196840  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.200\n",
      "iter 9701/1000000  loss         0.196840  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.200\n",
      "Done. Converged after 9785 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Noise Ori 0.9577777777777512\n",
      "TurnOn Loaded 0.0\n",
      "(1, 3600)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Square Loaded!\n",
      "No Noise New 0.9633333333333065\n"
     ]
    }
   ],
   "source": [
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformed Data (84000, 785)\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.031734  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.905841  avg_L1_norm_grad         0.026133  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.840862  avg_L1_norm_grad         0.018532  w[0]    0.000 bias    0.022\n",
      "iter    3/1000000  loss         0.798216  avg_L1_norm_grad         0.015840  w[0]    0.000 bias    0.028\n",
      "iter    4/1000000  loss         0.766661  avg_L1_norm_grad         0.012919  w[0]    0.000 bias    0.044\n",
      "iter    5/1000000  loss         0.742190  avg_L1_norm_grad         0.011807  w[0]    0.000 bias    0.053\n",
      "iter    6/1000000  loss         0.721968  avg_L1_norm_grad         0.010532  w[0]    0.000 bias    0.065\n",
      "iter    7/1000000  loss         0.704564  avg_L1_norm_grad         0.009869  w[0]    0.000 bias    0.075\n",
      "iter    8/1000000  loss         0.689144  avg_L1_norm_grad         0.009206  w[0]    0.000 bias    0.086\n",
      "iter    9/1000000  loss         0.675206  avg_L1_norm_grad         0.008726  w[0]    0.000 bias    0.097\n",
      "iter   10/1000000  loss         0.662428  avg_L1_norm_grad         0.008298  w[0]    0.000 bias    0.107\n",
      "iter   11/1000000  loss         0.650590  avg_L1_norm_grad         0.007941  w[0]    0.000 bias    0.117\n",
      "iter   12/1000000  loss         0.639536  avg_L1_norm_grad         0.007629  w[0]    0.000 bias    0.127\n",
      "iter   13/1000000  loss         0.629152  avg_L1_norm_grad         0.007359  w[0]    0.000 bias    0.136\n",
      "iter   14/1000000  loss         0.619353  avg_L1_norm_grad         0.007117  w[0]    0.000 bias    0.146\n",
      "iter   15/1000000  loss         0.610071  avg_L1_norm_grad         0.006897  w[0]    0.000 bias    0.155\n",
      "iter   16/1000000  loss         0.601254  avg_L1_norm_grad         0.006695  w[0]    0.000 bias    0.164\n",
      "iter   17/1000000  loss         0.592858  avg_L1_norm_grad         0.006512  w[0]    0.000 bias    0.174\n",
      "iter   18/1000000  loss         0.584847  avg_L1_norm_grad         0.006341  w[0]    0.000 bias    0.182\n",
      "iter   19/1000000  loss         0.577191  avg_L1_norm_grad         0.006182  w[0]    0.000 bias    0.191\n",
      "iter  100/1000000  loss         0.349552  avg_L1_norm_grad         0.002279  w[0]    0.000 bias    0.669\n",
      "iter  101/1000000  loss         0.348437  avg_L1_norm_grad         0.002263  w[0]    0.000 bias    0.674\n",
      "iter  200/1000000  loss         0.281938  avg_L1_norm_grad         0.001367  w[0]    0.000 bias    1.007\n",
      "iter  201/1000000  loss         0.281524  avg_L1_norm_grad         0.001361  w[0]    0.000 bias    1.010\n",
      "iter  300/1000000  loss         0.251387  avg_L1_norm_grad         0.001002  w[0]    0.000 bias    1.241\n",
      "iter  301/1000000  loss         0.251161  avg_L1_norm_grad         0.000999  w[0]    0.000 bias    1.243\n",
      "iter  400/1000000  loss         0.233295  avg_L1_norm_grad         0.000803  w[0]    0.000 bias    1.420\n",
      "iter  401/1000000  loss         0.233150  avg_L1_norm_grad         0.000802  w[0]    0.000 bias    1.421\n",
      "iter  500/1000000  loss         0.221083  avg_L1_norm_grad         0.000681  w[0]    0.000 bias    1.564\n",
      "iter  501/1000000  loss         0.220980  avg_L1_norm_grad         0.000680  w[0]    0.000 bias    1.565\n",
      "iter  600/1000000  loss         0.212161  avg_L1_norm_grad         0.000595  w[0]    0.000 bias    1.684\n",
      "iter  601/1000000  loss         0.212084  avg_L1_norm_grad         0.000594  w[0]    0.000 bias    1.685\n",
      "iter  700/1000000  loss         0.205289  avg_L1_norm_grad         0.000531  w[0]    0.000 bias    1.786\n",
      "iter  701/1000000  loss         0.205228  avg_L1_norm_grad         0.000530  w[0]    0.000 bias    1.787\n",
      "iter  800/1000000  loss         0.199789  avg_L1_norm_grad         0.000481  w[0]    0.000 bias    1.874\n",
      "iter  801/1000000  loss         0.199739  avg_L1_norm_grad         0.000481  w[0]    0.000 bias    1.875\n",
      "iter  900/1000000  loss         0.195259  avg_L1_norm_grad         0.000442  w[0]    0.000 bias    1.951\n",
      "iter  901/1000000  loss         0.195218  avg_L1_norm_grad         0.000441  w[0]    0.000 bias    1.952\n",
      "iter 1000/1000000  loss         0.191445  avg_L1_norm_grad         0.000409  w[0]    0.000 bias    2.020\n",
      "iter 1001/1000000  loss         0.191410  avg_L1_norm_grad         0.000409  w[0]    0.000 bias    2.020\n",
      "iter 1100/1000000  loss         0.188176  avg_L1_norm_grad         0.000381  w[0]    0.000 bias    2.080\n",
      "iter 1101/1000000  loss         0.188146  avg_L1_norm_grad         0.000381  w[0]    0.000 bias    2.081\n",
      "iter 1200/1000000  loss         0.185334  avg_L1_norm_grad         0.000358  w[0]    0.000 bias    2.135\n",
      "iter 1201/1000000  loss         0.185307  avg_L1_norm_grad         0.000358  w[0]    0.000 bias    2.136\n",
      "iter 1300/1000000  loss         0.182832  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    2.184\n",
      "iter 1301/1000000  loss         0.182809  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    2.185\n",
      "iter 1400/1000000  loss         0.180609  avg_L1_norm_grad         0.000320  w[0]    0.000 bias    2.229\n",
      "iter 1401/1000000  loss         0.180588  avg_L1_norm_grad         0.000320  w[0]    0.000 bias    2.229\n",
      "iter 1500/1000000  loss         0.178616  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    2.270\n",
      "iter 1501/1000000  loss         0.178597  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    2.270\n",
      "iter 1600/1000000  loss         0.176816  avg_L1_norm_grad         0.000291  w[0]    0.000 bias    2.307\n",
      "iter 1601/1000000  loss         0.176799  avg_L1_norm_grad         0.000291  w[0]    0.000 bias    2.307\n",
      "iter 1700/1000000  loss         0.175179  avg_L1_norm_grad         0.000278  w[0]    0.000 bias    2.341\n",
      "iter 1701/1000000  loss         0.175164  avg_L1_norm_grad         0.000278  w[0]    0.000 bias    2.341\n",
      "iter 1800/1000000  loss         0.173684  avg_L1_norm_grad         0.000267  w[0]    0.000 bias    2.372\n",
      "iter 1801/1000000  loss         0.173669  avg_L1_norm_grad         0.000267  w[0]    0.000 bias    2.373\n",
      "iter 1900/1000000  loss         0.172309  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    2.401\n",
      "iter 1901/1000000  loss         0.172296  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    2.401\n",
      "iter 2000/1000000  loss         0.171041  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    2.428\n",
      "iter 2001/1000000  loss         0.171029  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    2.428\n",
      "iter 2100/1000000  loss         0.169866  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    2.453\n",
      "iter 2101/1000000  loss         0.169854  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    2.453\n",
      "iter 2200/1000000  loss         0.168773  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    2.476\n",
      "iter 2201/1000000  loss         0.168762  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    2.476\n",
      "iter 2300/1000000  loss         0.167753  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    2.497\n",
      "iter 2301/1000000  loss         0.167743  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    2.497\n",
      "iter 2400/1000000  loss         0.166799  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    2.517\n",
      "iter 2401/1000000  loss         0.166790  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    2.517\n",
      "iter 2500/1000000  loss         0.165904  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.535\n",
      "iter 2501/1000000  loss         0.165895  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.535\n",
      "iter 2600/1000000  loss         0.165062  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.553\n",
      "iter 2601/1000000  loss         0.165054  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.553\n",
      "iter 2700/1000000  loss         0.164268  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.569\n",
      "iter 2701/1000000  loss         0.164260  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.569\n",
      "iter 2800/1000000  loss         0.163518  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.584\n",
      "iter 2801/1000000  loss         0.163510  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.584\n",
      "iter 2900/1000000  loss         0.162807  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.598\n",
      "iter 2901/1000000  loss         0.162800  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.162134  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.611\n",
      "iter 3001/1000000  loss         0.162127  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.612\n",
      "iter 3100/1000000  loss         0.161493  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.624\n",
      "iter 3101/1000000  loss         0.161487  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.624\n",
      "iter 3200/1000000  loss         0.160884  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.636\n",
      "iter 3201/1000000  loss         0.160878  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.636\n",
      "iter 3300/1000000  loss         0.160303  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    2.647\n",
      "iter 3301/1000000  loss         0.160298  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    2.647\n",
      "iter 3400/1000000  loss         0.159749  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.657\n",
      "iter 3401/1000000  loss         0.159744  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.657\n",
      "iter 3500/1000000  loss         0.159219  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.667\n",
      "iter 3501/1000000  loss         0.159214  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.667\n",
      "iter 3600/1000000  loss         0.158713  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.676\n",
      "iter 3601/1000000  loss         0.158708  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.677\n",
      "iter 3700/1000000  loss         0.158227  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    2.685\n",
      "iter 3701/1000000  loss         0.158222  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    2.685\n",
      "iter 3800/1000000  loss         0.157761  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.694\n",
      "iter 3801/1000000  loss         0.157756  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.694\n",
      "iter 3900/1000000  loss         0.157314  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.701\n",
      "iter 3901/1000000  loss         0.157309  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.701\n",
      "iter 4000/1000000  loss         0.156884  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    2.709\n",
      "iter 4001/1000000  loss         0.156880  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    2.709\n",
      "iter 4100/1000000  loss         0.156470  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.716\n",
      "iter 4101/1000000  loss         0.156466  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.716\n",
      "iter 4200/1000000  loss         0.156072  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.723\n",
      "iter 4201/1000000  loss         0.156068  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.723\n",
      "iter 4300/1000000  loss         0.155688  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.729\n",
      "iter 4301/1000000  loss         0.155684  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.729\n",
      "iter 4400/1000000  loss         0.155318  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.735\n",
      "iter 4401/1000000  loss         0.155314  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.735\n",
      "iter 4500/1000000  loss         0.154961  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.741\n",
      "iter 4501/1000000  loss         0.154957  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.741\n",
      "iter 4600/1000000  loss         0.154615  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.746\n",
      "iter 4601/1000000  loss         0.154612  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.746\n",
      "iter 4700/1000000  loss         0.154282  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.751\n",
      "iter 4701/1000000  loss         0.154278  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.751\n",
      "iter 4800/1000000  loss         0.153959  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.756\n",
      "iter 4801/1000000  loss         0.153955  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.756\n",
      "iter 4900/1000000  loss         0.153646  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.761\n",
      "iter 4901/1000000  loss         0.153643  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.761\n",
      "iter 5000/1000000  loss         0.153343  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.765\n",
      "iter 5001/1000000  loss         0.153340  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.765\n",
      "iter 5100/1000000  loss         0.153050  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.769\n",
      "iter 5101/1000000  loss         0.153047  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.769\n",
      "iter 5200/1000000  loss         0.152765  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.773\n",
      "iter 5201/1000000  loss         0.152763  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.773\n",
      "iter 5300/1000000  loss         0.152489  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.777\n",
      "iter 5301/1000000  loss         0.152487  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.777\n",
      "iter 5400/1000000  loss         0.152221  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.781\n",
      "iter 5401/1000000  loss         0.152219  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.781\n",
      "iter 5500/1000000  loss         0.151961  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.784\n",
      "iter 5501/1000000  loss         0.151958  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.784\n",
      "iter 5600/1000000  loss         0.151708  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    2.787\n",
      "iter 5601/1000000  loss         0.151705  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    2.787\n",
      "iter 5700/1000000  loss         0.151462  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    2.791\n",
      "iter 5701/1000000  loss         0.151459  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    2.791\n",
      "iter 5800/1000000  loss         0.151223  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    2.794\n",
      "iter 5801/1000000  loss         0.151220  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    2.794\n",
      "iter 5900/1000000  loss         0.150990  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    2.797\n",
      "iter 5901/1000000  loss         0.150987  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    2.797\n",
      "iter 6000/1000000  loss         0.150763  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    2.799\n",
      "iter 6001/1000000  loss         0.150761  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    2.799\n",
      "iter 6100/1000000  loss         0.150542  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    2.802\n",
      "iter 6101/1000000  loss         0.150540  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    2.802\n",
      "iter 6200/1000000  loss         0.150327  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.804\n",
      "iter 6201/1000000  loss         0.150325  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.804\n",
      "iter 6300/1000000  loss         0.150118  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    2.807\n",
      "iter 6301/1000000  loss         0.150115  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    2.807\n",
      "iter 6400/1000000  loss         0.149913  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    2.809\n",
      "iter 6401/1000000  loss         0.149911  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    2.809\n",
      "iter 6500/1000000  loss         0.149714  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    2.811\n",
      "iter 6501/1000000  loss         0.149712  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    2.811\n",
      "iter 6600/1000000  loss         0.149519  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.813\n",
      "iter 6601/1000000  loss         0.149517  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.814\n",
      "iter 6700/1000000  loss         0.149329  avg_L1_norm_grad         0.000103  w[0]   -0.000 bias    2.816\n",
      "iter 6701/1000000  loss         0.149327  avg_L1_norm_grad         0.000103  w[0]   -0.000 bias    2.816\n",
      "iter 6800/1000000  loss         0.149144  avg_L1_norm_grad         0.000102  w[0]   -0.000 bias    2.817\n",
      "iter 6801/1000000  loss         0.149142  avg_L1_norm_grad         0.000102  w[0]   -0.000 bias    2.817\n",
      "iter 6900/1000000  loss         0.148963  avg_L1_norm_grad         0.000101  w[0]   -0.000 bias    2.819\n",
      "iter 6901/1000000  loss         0.148961  avg_L1_norm_grad         0.000101  w[0]   -0.000 bias    2.819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.148786  avg_L1_norm_grad         0.000100  w[0]   -0.000 bias    2.821\n",
      "iter 7001/1000000  loss         0.148784  avg_L1_norm_grad         0.000100  w[0]   -0.000 bias    2.821\n",
      "iter 7100/1000000  loss         0.148613  avg_L1_norm_grad         0.000099  w[0]   -0.000 bias    2.823\n",
      "iter 7101/1000000  loss         0.148611  avg_L1_norm_grad         0.000099  w[0]   -0.000 bias    2.823\n",
      "iter 7200/1000000  loss         0.148444  avg_L1_norm_grad         0.000098  w[0]   -0.000 bias    2.824\n",
      "iter 7201/1000000  loss         0.148442  avg_L1_norm_grad         0.000098  w[0]   -0.000 bias    2.824\n",
      "iter 7300/1000000  loss         0.148278  avg_L1_norm_grad         0.000097  w[0]   -0.000 bias    2.826\n",
      "iter 7301/1000000  loss         0.148277  avg_L1_norm_grad         0.000097  w[0]   -0.000 bias    2.826\n",
      "iter 7400/1000000  loss         0.148116  avg_L1_norm_grad         0.000096  w[0]   -0.000 bias    2.828\n",
      "iter 7401/1000000  loss         0.148115  avg_L1_norm_grad         0.000096  w[0]   -0.000 bias    2.828\n",
      "iter 7500/1000000  loss         0.147958  avg_L1_norm_grad         0.000095  w[0]   -0.000 bias    2.829\n",
      "iter 7501/1000000  loss         0.147957  avg_L1_norm_grad         0.000095  w[0]   -0.000 bias    2.829\n",
      "iter 7600/1000000  loss         0.147803  avg_L1_norm_grad         0.000094  w[0]   -0.000 bias    2.830\n",
      "iter 7601/1000000  loss         0.147802  avg_L1_norm_grad         0.000094  w[0]   -0.000 bias    2.830\n",
      "iter 7700/1000000  loss         0.147652  avg_L1_norm_grad         0.000093  w[0]   -0.000 bias    2.832\n",
      "iter 7701/1000000  loss         0.147650  avg_L1_norm_grad         0.000093  w[0]   -0.000 bias    2.832\n",
      "iter 7800/1000000  loss         0.147503  avg_L1_norm_grad         0.000092  w[0]   -0.000 bias    2.833\n",
      "iter 7801/1000000  loss         0.147502  avg_L1_norm_grad         0.000092  w[0]   -0.000 bias    2.833\n",
      "iter 7900/1000000  loss         0.147358  avg_L1_norm_grad         0.000091  w[0]   -0.000 bias    2.834\n",
      "iter 7901/1000000  loss         0.147357  avg_L1_norm_grad         0.000091  w[0]   -0.000 bias    2.834\n",
      "iter 8000/1000000  loss         0.147216  avg_L1_norm_grad         0.000090  w[0]   -0.000 bias    2.835\n",
      "iter 8001/1000000  loss         0.147214  avg_L1_norm_grad         0.000090  w[0]   -0.000 bias    2.835\n",
      "iter 8100/1000000  loss         0.147076  avg_L1_norm_grad         0.000089  w[0]   -0.000 bias    2.837\n",
      "iter 8101/1000000  loss         0.147075  avg_L1_norm_grad         0.000089  w[0]   -0.000 bias    2.837\n",
      "iter 8200/1000000  loss         0.146940  avg_L1_norm_grad         0.000089  w[0]   -0.000 bias    2.838\n",
      "iter 8201/1000000  loss         0.146938  avg_L1_norm_grad         0.000089  w[0]   -0.000 bias    2.838\n",
      "iter 8300/1000000  loss         0.146806  avg_L1_norm_grad         0.000088  w[0]   -0.000 bias    2.839\n",
      "iter 8301/1000000  loss         0.146804  avg_L1_norm_grad         0.000088  w[0]   -0.000 bias    2.839\n",
      "iter 8400/1000000  loss         0.146674  avg_L1_norm_grad         0.000087  w[0]   -0.000 bias    2.840\n",
      "iter 8401/1000000  loss         0.146673  avg_L1_norm_grad         0.000087  w[0]   -0.000 bias    2.840\n",
      "iter 8500/1000000  loss         0.146545  avg_L1_norm_grad         0.000086  w[0]   -0.000 bias    2.841\n",
      "iter 8501/1000000  loss         0.146544  avg_L1_norm_grad         0.000086  w[0]   -0.000 bias    2.841\n",
      "iter 8600/1000000  loss         0.146419  avg_L1_norm_grad         0.000085  w[0]   -0.000 bias    2.842\n",
      "iter 8601/1000000  loss         0.146418  avg_L1_norm_grad         0.000085  w[0]   -0.000 bias    2.842\n",
      "iter 8700/1000000  loss         0.146295  avg_L1_norm_grad         0.000085  w[0]   -0.000 bias    2.843\n",
      "iter 8701/1000000  loss         0.146294  avg_L1_norm_grad         0.000085  w[0]   -0.000 bias    2.843\n",
      "iter 8800/1000000  loss         0.146174  avg_L1_norm_grad         0.000084  w[0]   -0.000 bias    2.844\n",
      "iter 8801/1000000  loss         0.146172  avg_L1_norm_grad         0.000084  w[0]   -0.000 bias    2.844\n",
      "iter 8900/1000000  loss         0.146054  avg_L1_norm_grad         0.000083  w[0]   -0.000 bias    2.844\n",
      "iter 8901/1000000  loss         0.146053  avg_L1_norm_grad         0.000083  w[0]   -0.000 bias    2.844\n",
      "iter 9000/1000000  loss         0.145937  avg_L1_norm_grad         0.000082  w[0]   -0.000 bias    2.845\n",
      "iter 9001/1000000  loss         0.145936  avg_L1_norm_grad         0.000082  w[0]   -0.000 bias    2.845\n",
      "iter 9100/1000000  loss         0.145822  avg_L1_norm_grad         0.000082  w[0]   -0.000 bias    2.846\n",
      "iter 9101/1000000  loss         0.145821  avg_L1_norm_grad         0.000082  w[0]   -0.000 bias    2.846\n",
      "iter 9200/1000000  loss         0.145709  avg_L1_norm_grad         0.000081  w[0]   -0.000 bias    2.847\n",
      "iter 9201/1000000  loss         0.145708  avg_L1_norm_grad         0.000081  w[0]   -0.000 bias    2.847\n",
      "iter 9300/1000000  loss         0.145598  avg_L1_norm_grad         0.000080  w[0]   -0.000 bias    2.848\n",
      "iter 9301/1000000  loss         0.145597  avg_L1_norm_grad         0.000080  w[0]   -0.000 bias    2.848\n",
      "iter 9400/1000000  loss         0.145489  avg_L1_norm_grad         0.000080  w[0]   -0.000 bias    2.848\n",
      "iter 9401/1000000  loss         0.145488  avg_L1_norm_grad         0.000080  w[0]   -0.000 bias    2.848\n",
      "iter 9500/1000000  loss         0.145383  avg_L1_norm_grad         0.000079  w[0]   -0.000 bias    2.849\n",
      "iter 9501/1000000  loss         0.145381  avg_L1_norm_grad         0.000079  w[0]   -0.000 bias    2.849\n",
      "iter 9600/1000000  loss         0.145277  avg_L1_norm_grad         0.000078  w[0]   -0.000 bias    2.850\n",
      "iter 9601/1000000  loss         0.145276  avg_L1_norm_grad         0.000078  w[0]   -0.000 bias    2.850\n",
      "iter 9700/1000000  loss         0.145174  avg_L1_norm_grad         0.000078  w[0]   -0.000 bias    2.850\n",
      "iter 9701/1000000  loss         0.145173  avg_L1_norm_grad         0.000078  w[0]   -0.000 bias    2.850\n",
      "iter 9800/1000000  loss         0.145073  avg_L1_norm_grad         0.000077  w[0]   -0.000 bias    2.851\n",
      "iter 9801/1000000  loss         0.145072  avg_L1_norm_grad         0.000077  w[0]   -0.000 bias    2.851\n",
      "iter 9900/1000000  loss         0.144973  avg_L1_norm_grad         0.000076  w[0]   -0.000 bias    2.852\n",
      "iter 9901/1000000  loss         0.144972  avg_L1_norm_grad         0.000076  w[0]   -0.000 bias    2.852\n",
      "iter 10000/1000000  loss         0.144875  avg_L1_norm_grad         0.000076  w[0]   -0.000 bias    2.852\n",
      "iter 10001/1000000  loss         0.144874  avg_L1_norm_grad         0.000076  w[0]   -0.000 bias    2.852\n",
      "iter 10100/1000000  loss         0.144779  avg_L1_norm_grad         0.000075  w[0]   -0.000 bias    2.853\n",
      "iter 10101/1000000  loss         0.144778  avg_L1_norm_grad         0.000075  w[0]   -0.000 bias    2.853\n",
      "iter 10200/1000000  loss         0.144684  avg_L1_norm_grad         0.000074  w[0]   -0.000 bias    2.853\n",
      "iter 10201/1000000  loss         0.144683  avg_L1_norm_grad         0.000074  w[0]   -0.000 bias    2.853\n",
      "iter 10300/1000000  loss         0.144590  avg_L1_norm_grad         0.000074  w[0]   -0.000 bias    2.854\n",
      "iter 10301/1000000  loss         0.144590  avg_L1_norm_grad         0.000074  w[0]   -0.000 bias    2.854\n",
      "iter 10400/1000000  loss         0.144499  avg_L1_norm_grad         0.000073  w[0]   -0.000 bias    2.854\n",
      "iter 10401/1000000  loss         0.144498  avg_L1_norm_grad         0.000073  w[0]   -0.000 bias    2.854\n",
      "iter 10500/1000000  loss         0.144409  avg_L1_norm_grad         0.000073  w[0]   -0.000 bias    2.855\n",
      "iter 10501/1000000  loss         0.144408  avg_L1_norm_grad         0.000073  w[0]   -0.000 bias    2.855\n",
      "iter 10600/1000000  loss         0.144320  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    2.855\n",
      "iter 10601/1000000  loss         0.144319  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    2.855\n",
      "iter 10700/1000000  loss         0.144233  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    2.856\n",
      "iter 10701/1000000  loss         0.144232  avg_L1_norm_grad         0.000072  w[0]   -0.000 bias    2.856\n",
      "iter 10800/1000000  loss         0.144147  avg_L1_norm_grad         0.000071  w[0]   -0.000 bias    2.856\n",
      "iter 10801/1000000  loss         0.144146  avg_L1_norm_grad         0.000071  w[0]   -0.000 bias    2.856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.144062  avg_L1_norm_grad         0.000071  w[0]   -0.000 bias    2.857\n",
      "iter 10901/1000000  loss         0.144061  avg_L1_norm_grad         0.000071  w[0]   -0.000 bias    2.857\n",
      "iter 11000/1000000  loss         0.143979  avg_L1_norm_grad         0.000070  w[0]   -0.000 bias    2.857\n",
      "iter 11001/1000000  loss         0.143978  avg_L1_norm_grad         0.000070  w[0]   -0.000 bias    2.857\n",
      "iter 11100/1000000  loss         0.143897  avg_L1_norm_grad         0.000069  w[0]   -0.000 bias    2.858\n",
      "iter 11101/1000000  loss         0.143896  avg_L1_norm_grad         0.000069  w[0]   -0.000 bias    2.858\n",
      "iter 11200/1000000  loss         0.143816  avg_L1_norm_grad         0.000069  w[0]   -0.000 bias    2.858\n",
      "iter 11201/1000000  loss         0.143815  avg_L1_norm_grad         0.000069  w[0]   -0.000 bias    2.858\n",
      "iter 11300/1000000  loss         0.143737  avg_L1_norm_grad         0.000068  w[0]   -0.000 bias    2.858\n",
      "iter 11301/1000000  loss         0.143736  avg_L1_norm_grad         0.000068  w[0]   -0.000 bias    2.858\n",
      "iter 11400/1000000  loss         0.143659  avg_L1_norm_grad         0.000068  w[0]   -0.000 bias    2.859\n",
      "iter 11401/1000000  loss         0.143658  avg_L1_norm_grad         0.000068  w[0]   -0.000 bias    2.859\n",
      "iter 11500/1000000  loss         0.143582  avg_L1_norm_grad         0.000067  w[0]   -0.000 bias    2.859\n",
      "iter 11501/1000000  loss         0.143581  avg_L1_norm_grad         0.000067  w[0]   -0.000 bias    2.859\n",
      "iter 11600/1000000  loss         0.143506  avg_L1_norm_grad         0.000067  w[0]   -0.000 bias    2.859\n",
      "iter 11601/1000000  loss         0.143505  avg_L1_norm_grad         0.000067  w[0]   -0.000 bias    2.859\n",
      "iter 11700/1000000  loss         0.143431  avg_L1_norm_grad         0.000066  w[0]   -0.000 bias    2.860\n",
      "iter 11701/1000000  loss         0.143430  avg_L1_norm_grad         0.000066  w[0]   -0.000 bias    2.860\n",
      "iter 11800/1000000  loss         0.143358  avg_L1_norm_grad         0.000066  w[0]   -0.000 bias    2.860\n",
      "iter 11801/1000000  loss         0.143357  avg_L1_norm_grad         0.000066  w[0]   -0.000 bias    2.860\n",
      "iter 11900/1000000  loss         0.143285  avg_L1_norm_grad         0.000066  w[0]   -0.000 bias    2.861\n",
      "iter 11901/1000000  loss         0.143284  avg_L1_norm_grad         0.000066  w[0]   -0.000 bias    2.861\n",
      "iter 12000/1000000  loss         0.143214  avg_L1_norm_grad         0.000065  w[0]   -0.000 bias    2.861\n",
      "iter 12001/1000000  loss         0.143213  avg_L1_norm_grad         0.000065  w[0]   -0.000 bias    2.861\n",
      "iter 12100/1000000  loss         0.143143  avg_L1_norm_grad         0.000065  w[0]   -0.000 bias    2.861\n",
      "iter 12101/1000000  loss         0.143142  avg_L1_norm_grad         0.000065  w[0]   -0.000 bias    2.861\n",
      "iter 12200/1000000  loss         0.143074  avg_L1_norm_grad         0.000064  w[0]   -0.000 bias    2.861\n",
      "iter 12201/1000000  loss         0.143073  avg_L1_norm_grad         0.000064  w[0]   -0.000 bias    2.862\n",
      "iter 12300/1000000  loss         0.143005  avg_L1_norm_grad         0.000064  w[0]   -0.000 bias    2.862\n",
      "iter 12301/1000000  loss         0.143005  avg_L1_norm_grad         0.000064  w[0]   -0.000 bias    2.862\n",
      "iter 12400/1000000  loss         0.142938  avg_L1_norm_grad         0.000063  w[0]   -0.000 bias    2.862\n",
      "iter 12401/1000000  loss         0.142937  avg_L1_norm_grad         0.000063  w[0]   -0.000 bias    2.862\n",
      "iter 12500/1000000  loss         0.142872  avg_L1_norm_grad         0.000063  w[0]   -0.000 bias    2.862\n",
      "iter 12501/1000000  loss         0.142871  avg_L1_norm_grad         0.000063  w[0]   -0.000 bias    2.862\n",
      "iter 12600/1000000  loss         0.142806  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.863\n",
      "iter 12601/1000000  loss         0.142806  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.863\n",
      "iter 12700/1000000  loss         0.142742  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.863\n",
      "iter 12701/1000000  loss         0.142741  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.863\n",
      "iter 12800/1000000  loss         0.142678  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.863\n",
      "iter 12801/1000000  loss         0.142677  avg_L1_norm_grad         0.000062  w[0]   -0.000 bias    2.863\n",
      "iter 12900/1000000  loss         0.142615  avg_L1_norm_grad         0.000061  w[0]   -0.000 bias    2.864\n",
      "iter 12901/1000000  loss         0.142615  avg_L1_norm_grad         0.000061  w[0]   -0.000 bias    2.864\n",
      "iter 13000/1000000  loss         0.142553  avg_L1_norm_grad         0.000061  w[0]   -0.000 bias    2.864\n",
      "iter 13001/1000000  loss         0.142553  avg_L1_norm_grad         0.000061  w[0]   -0.000 bias    2.864\n",
      "iter 13100/1000000  loss         0.142492  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.864\n",
      "iter 13101/1000000  loss         0.142492  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.864\n",
      "iter 13200/1000000  loss         0.142432  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.864\n",
      "iter 13201/1000000  loss         0.142431  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.864\n",
      "iter 13300/1000000  loss         0.142373  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.865\n",
      "iter 13301/1000000  loss         0.142372  avg_L1_norm_grad         0.000060  w[0]   -0.000 bias    2.865\n",
      "iter 13400/1000000  loss         0.142314  avg_L1_norm_grad         0.000059  w[0]   -0.000 bias    2.865\n",
      "iter 13401/1000000  loss         0.142314  avg_L1_norm_grad         0.000059  w[0]   -0.000 bias    2.865\n",
      "iter 13500/1000000  loss         0.142256  avg_L1_norm_grad         0.000059  w[0]   -0.000 bias    2.865\n",
      "iter 13501/1000000  loss         0.142256  avg_L1_norm_grad         0.000059  w[0]   -0.000 bias    2.865\n",
      "iter 13600/1000000  loss         0.142199  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.865\n",
      "iter 13601/1000000  loss         0.142199  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.865\n",
      "iter 13700/1000000  loss         0.142143  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.865\n",
      "iter 13701/1000000  loss         0.142143  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.865\n",
      "iter 13800/1000000  loss         0.142088  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.866\n",
      "iter 13801/1000000  loss         0.142087  avg_L1_norm_grad         0.000058  w[0]   -0.000 bias    2.866\n",
      "iter 13900/1000000  loss         0.142033  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.866\n",
      "iter 13901/1000000  loss         0.142032  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.866\n",
      "iter 14000/1000000  loss         0.141979  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.866\n",
      "iter 14001/1000000  loss         0.141978  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.866\n",
      "iter 14100/1000000  loss         0.141925  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.866\n",
      "iter 14101/1000000  loss         0.141925  avg_L1_norm_grad         0.000057  w[0]   -0.000 bias    2.866\n",
      "iter 14200/1000000  loss         0.141873  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    2.867\n",
      "iter 14201/1000000  loss         0.141872  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    2.867\n",
      "iter 14300/1000000  loss         0.141821  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    2.867\n",
      "iter 14301/1000000  loss         0.141820  avg_L1_norm_grad         0.000056  w[0]   -0.000 bias    2.867\n",
      "iter 14400/1000000  loss         0.141770  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.867\n",
      "iter 14401/1000000  loss         0.141769  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.867\n",
      "iter 14500/1000000  loss         0.141719  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.867\n",
      "iter 14501/1000000  loss         0.141718  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.867\n",
      "iter 14600/1000000  loss         0.141669  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.867\n",
      "iter 14601/1000000  loss         0.141668  avg_L1_norm_grad         0.000055  w[0]   -0.000 bias    2.867\n",
      "iter 14700/1000000  loss         0.141620  avg_L1_norm_grad         0.000054  w[0]   -0.000 bias    2.868\n",
      "iter 14701/1000000  loss         0.141619  avg_L1_norm_grad         0.000054  w[0]   -0.000 bias    2.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14800/1000000  loss         0.141571  avg_L1_norm_grad         0.000054  w[0]   -0.000 bias    2.868\n",
      "iter 14801/1000000  loss         0.141570  avg_L1_norm_grad         0.000054  w[0]   -0.000 bias    2.868\n",
      "iter 14900/1000000  loss         0.141523  avg_L1_norm_grad         0.000054  w[0]   -0.000 bias    2.868\n",
      "iter 14901/1000000  loss         0.141522  avg_L1_norm_grad         0.000054  w[0]   -0.000 bias    2.868\n",
      "iter 15000/1000000  loss         0.141475  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.868\n",
      "iter 15001/1000000  loss         0.141475  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.868\n",
      "iter 15100/1000000  loss         0.141428  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.868\n",
      "iter 15101/1000000  loss         0.141428  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.868\n",
      "iter 15200/1000000  loss         0.141382  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.868\n",
      "iter 15201/1000000  loss         0.141382  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.868\n",
      "iter 15300/1000000  loss         0.141336  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.869\n",
      "iter 15301/1000000  loss         0.141336  avg_L1_norm_grad         0.000053  w[0]   -0.000 bias    2.869\n",
      "iter 15400/1000000  loss         0.141291  avg_L1_norm_grad         0.000052  w[0]   -0.000 bias    2.869\n",
      "iter 15401/1000000  loss         0.141291  avg_L1_norm_grad         0.000052  w[0]   -0.000 bias    2.869\n",
      "iter 15500/1000000  loss         0.141246  avg_L1_norm_grad         0.000052  w[0]   -0.000 bias    2.869\n",
      "iter 15501/1000000  loss         0.141246  avg_L1_norm_grad         0.000052  w[0]   -0.000 bias    2.869\n",
      "iter 15600/1000000  loss         0.141202  avg_L1_norm_grad         0.000052  w[0]   -0.000 bias    2.869\n",
      "iter 15601/1000000  loss         0.141202  avg_L1_norm_grad         0.000052  w[0]   -0.000 bias    2.869\n",
      "iter 15700/1000000  loss         0.141159  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.869\n",
      "iter 15701/1000000  loss         0.141158  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.869\n",
      "iter 15800/1000000  loss         0.141116  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.869\n",
      "iter 15801/1000000  loss         0.141115  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.869\n",
      "iter 15900/1000000  loss         0.141073  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.870\n",
      "iter 15901/1000000  loss         0.141073  avg_L1_norm_grad         0.000051  w[0]   -0.000 bias    2.870\n",
      "iter 16000/1000000  loss         0.141031  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16001/1000000  loss         0.141031  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16100/1000000  loss         0.140990  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16101/1000000  loss         0.140989  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16200/1000000  loss         0.140949  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16201/1000000  loss         0.140948  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16300/1000000  loss         0.140908  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16301/1000000  loss         0.140908  avg_L1_norm_grad         0.000050  w[0]   -0.000 bias    2.870\n",
      "iter 16400/1000000  loss         0.140868  avg_L1_norm_grad         0.000049  w[0]   -0.000 bias    2.870\n",
      "iter 16401/1000000  loss         0.140868  avg_L1_norm_grad         0.000049  w[0]   -0.000 bias    2.870\n",
      "iter 16500/1000000  loss         0.140829  avg_L1_norm_grad         0.000049  w[0]   -0.000 bias    2.871\n",
      "iter 16501/1000000  loss         0.140828  avg_L1_norm_grad         0.000049  w[0]   -0.000 bias    2.871\n",
      "iter 16600/1000000  loss         0.140790  avg_L1_norm_grad         0.000049  w[0]   -0.000 bias    2.871\n",
      "iter 16601/1000000  loss         0.140789  avg_L1_norm_grad         0.000049  w[0]   -0.000 bias    2.871\n",
      "iter 16700/1000000  loss         0.140751  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 16701/1000000  loss         0.140751  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 16800/1000000  loss         0.140713  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 16801/1000000  loss         0.140712  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 16900/1000000  loss         0.140675  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 16901/1000000  loss         0.140675  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 17000/1000000  loss         0.140638  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 17001/1000000  loss         0.140637  avg_L1_norm_grad         0.000048  w[0]   -0.000 bias    2.871\n",
      "iter 17100/1000000  loss         0.140601  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.871\n",
      "iter 17101/1000000  loss         0.140600  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.871\n",
      "iter 17200/1000000  loss         0.140564  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.872\n",
      "iter 17201/1000000  loss         0.140564  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.872\n",
      "iter 17300/1000000  loss         0.140528  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.872\n",
      "iter 17301/1000000  loss         0.140528  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.872\n",
      "iter 17400/1000000  loss         0.140493  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.872\n",
      "iter 17401/1000000  loss         0.140492  avg_L1_norm_grad         0.000047  w[0]   -0.000 bias    2.872\n",
      "iter 17500/1000000  loss         0.140457  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17501/1000000  loss         0.140457  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17600/1000000  loss         0.140423  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17601/1000000  loss         0.140422  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17700/1000000  loss         0.140388  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17701/1000000  loss         0.140388  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17800/1000000  loss         0.140354  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17801/1000000  loss         0.140354  avg_L1_norm_grad         0.000046  w[0]   -0.000 bias    2.872\n",
      "iter 17900/1000000  loss         0.140320  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 17901/1000000  loss         0.140320  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18000/1000000  loss         0.140287  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18001/1000000  loss         0.140287  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18100/1000000  loss         0.140254  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18101/1000000  loss         0.140254  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18200/1000000  loss         0.140222  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18201/1000000  loss         0.140221  avg_L1_norm_grad         0.000045  w[0]   -0.000 bias    2.873\n",
      "iter 18300/1000000  loss         0.140189  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18301/1000000  loss         0.140189  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18400/1000000  loss         0.140158  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18401/1000000  loss         0.140157  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18500/1000000  loss         0.140126  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18501/1000000  loss         0.140126  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18600/1000000  loss         0.140095  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n",
      "iter 18601/1000000  loss         0.140095  avg_L1_norm_grad         0.000044  w[0]   -0.000 bias    2.873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18700/1000000  loss         0.140064  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 18701/1000000  loss         0.140064  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 18800/1000000  loss         0.140034  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 18801/1000000  loss         0.140033  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 18900/1000000  loss         0.140003  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 18901/1000000  loss         0.140003  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 19000/1000000  loss         0.139974  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 19001/1000000  loss         0.139973  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 19100/1000000  loss         0.139944  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 19101/1000000  loss         0.139944  avg_L1_norm_grad         0.000043  w[0]   -0.000 bias    2.874\n",
      "iter 19200/1000000  loss         0.139915  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.874\n",
      "iter 19201/1000000  loss         0.139915  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.874\n",
      "iter 19300/1000000  loss         0.139886  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.874\n",
      "iter 19301/1000000  loss         0.139886  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.874\n",
      "iter 19400/1000000  loss         0.139857  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.874\n",
      "iter 19401/1000000  loss         0.139857  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.874\n",
      "iter 19500/1000000  loss         0.139829  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.875\n",
      "iter 19501/1000000  loss         0.139829  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.875\n",
      "iter 19600/1000000  loss         0.139801  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.875\n",
      "iter 19601/1000000  loss         0.139801  avg_L1_norm_grad         0.000042  w[0]   -0.001 bias    2.875\n",
      "iter 19700/1000000  loss         0.139773  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 19701/1000000  loss         0.139773  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 19800/1000000  loss         0.139746  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 19801/1000000  loss         0.139746  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 19900/1000000  loss         0.139719  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 19901/1000000  loss         0.139719  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 20000/1000000  loss         0.139692  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 20001/1000000  loss         0.139692  avg_L1_norm_grad         0.000041  w[0]   -0.001 bias    2.875\n",
      "iter 20100/1000000  loss         0.139666  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.875\n",
      "iter 20101/1000000  loss         0.139665  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.875\n",
      "iter 20200/1000000  loss         0.139639  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.875\n",
      "iter 20201/1000000  loss         0.139639  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.875\n",
      "iter 20300/1000000  loss         0.139613  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.875\n",
      "iter 20301/1000000  loss         0.139613  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.875\n",
      "iter 20400/1000000  loss         0.139588  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.876\n",
      "iter 20401/1000000  loss         0.139587  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.876\n",
      "iter 20500/1000000  loss         0.139562  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.876\n",
      "iter 20501/1000000  loss         0.139562  avg_L1_norm_grad         0.000040  w[0]   -0.001 bias    2.876\n",
      "iter 20600/1000000  loss         0.139537  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20601/1000000  loss         0.139537  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20700/1000000  loss         0.139512  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20701/1000000  loss         0.139512  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20800/1000000  loss         0.139487  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20801/1000000  loss         0.139487  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20900/1000000  loss         0.139463  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 20901/1000000  loss         0.139463  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 21000/1000000  loss         0.139439  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 21001/1000000  loss         0.139438  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 21100/1000000  loss         0.139415  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 21101/1000000  loss         0.139415  avg_L1_norm_grad         0.000039  w[0]   -0.001 bias    2.876\n",
      "iter 21200/1000000  loss         0.139391  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21201/1000000  loss         0.139391  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21300/1000000  loss         0.139368  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21301/1000000  loss         0.139367  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21400/1000000  loss         0.139344  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21401/1000000  loss         0.139344  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21500/1000000  loss         0.139321  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21501/1000000  loss         0.139321  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21600/1000000  loss         0.139299  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21601/1000000  loss         0.139298  avg_L1_norm_grad         0.000038  w[0]   -0.001 bias    2.877\n",
      "iter 21700/1000000  loss         0.139276  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 21701/1000000  loss         0.139276  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 21800/1000000  loss         0.139254  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 21801/1000000  loss         0.139253  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 21900/1000000  loss         0.139232  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 21901/1000000  loss         0.139231  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 22000/1000000  loss         0.139210  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 22001/1000000  loss         0.139210  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 22100/1000000  loss         0.139188  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 22101/1000000  loss         0.139188  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.877\n",
      "iter 22200/1000000  loss         0.139167  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.878\n",
      "iter 22201/1000000  loss         0.139166  avg_L1_norm_grad         0.000037  w[0]   -0.001 bias    2.878\n",
      "iter 22300/1000000  loss         0.139145  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22301/1000000  loss         0.139145  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22400/1000000  loss         0.139124  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22401/1000000  loss         0.139124  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22500/1000000  loss         0.139103  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22501/1000000  loss         0.139103  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22600/1000000  loss         0.139083  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22601/1000000  loss         0.139083  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22700/1000000  loss         0.139062  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22701/1000000  loss         0.139062  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22800/1000000  loss         0.139042  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22801/1000000  loss         0.139042  avg_L1_norm_grad         0.000036  w[0]   -0.001 bias    2.878\n",
      "iter 22900/1000000  loss         0.139022  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.878\n",
      "iter 22901/1000000  loss         0.139022  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.878\n",
      "iter 23000/1000000  loss         0.139002  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.878\n",
      "iter 23001/1000000  loss         0.139002  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.878\n",
      "iter 23100/1000000  loss         0.138983  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23101/1000000  loss         0.138982  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23200/1000000  loss         0.138963  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23201/1000000  loss         0.138963  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23300/1000000  loss         0.138944  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23301/1000000  loss         0.138944  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23400/1000000  loss         0.138925  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23401/1000000  loss         0.138925  avg_L1_norm_grad         0.000035  w[0]   -0.001 bias    2.879\n",
      "iter 23500/1000000  loss         0.138906  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23501/1000000  loss         0.138906  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23600/1000000  loss         0.138887  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23601/1000000  loss         0.138887  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23700/1000000  loss         0.138869  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23701/1000000  loss         0.138868  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23800/1000000  loss         0.138850  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23801/1000000  loss         0.138850  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23900/1000000  loss         0.138832  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 23901/1000000  loss         0.138832  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 24000/1000000  loss         0.138814  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 24001/1000000  loss         0.138814  avg_L1_norm_grad         0.000034  w[0]   -0.001 bias    2.879\n",
      "iter 24100/1000000  loss         0.138796  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24101/1000000  loss         0.138796  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24200/1000000  loss         0.138778  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24201/1000000  loss         0.138778  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24300/1000000  loss         0.138761  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24301/1000000  loss         0.138761  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24400/1000000  loss         0.138744  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24401/1000000  loss         0.138743  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24500/1000000  loss         0.138726  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24501/1000000  loss         0.138726  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24600/1000000  loss         0.138709  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24601/1000000  loss         0.138709  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24700/1000000  loss         0.138692  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24701/1000000  loss         0.138692  avg_L1_norm_grad         0.000033  w[0]   -0.001 bias    2.880\n",
      "iter 24800/1000000  loss         0.138676  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.880\n",
      "iter 24801/1000000  loss         0.138675  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.880\n",
      "iter 24900/1000000  loss         0.138659  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.880\n",
      "iter 24901/1000000  loss         0.138659  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.880\n",
      "iter 25000/1000000  loss         0.138643  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.880\n",
      "iter 25001/1000000  loss         0.138642  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.880\n",
      "iter 25100/1000000  loss         0.138626  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25101/1000000  loss         0.138626  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25200/1000000  loss         0.138610  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25201/1000000  loss         0.138610  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25300/1000000  loss         0.138594  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25301/1000000  loss         0.138594  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25400/1000000  loss         0.138578  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25401/1000000  loss         0.138578  avg_L1_norm_grad         0.000032  w[0]   -0.001 bias    2.881\n",
      "iter 25500/1000000  loss         0.138563  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25501/1000000  loss         0.138563  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25600/1000000  loss         0.138547  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25601/1000000  loss         0.138547  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25700/1000000  loss         0.138532  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25701/1000000  loss         0.138532  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25800/1000000  loss         0.138516  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25801/1000000  loss         0.138516  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25900/1000000  loss         0.138501  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 25901/1000000  loss         0.138501  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 26000/1000000  loss         0.138486  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 26001/1000000  loss         0.138486  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 26100/1000000  loss         0.138471  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 26101/1000000  loss         0.138471  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.881\n",
      "iter 26200/1000000  loss         0.138457  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.882\n",
      "iter 26201/1000000  loss         0.138457  avg_L1_norm_grad         0.000031  w[0]   -0.001 bias    2.882\n",
      "iter 26300/1000000  loss         0.138442  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26301/1000000  loss         0.138442  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26400/1000000  loss         0.138428  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26401/1000000  loss         0.138428  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26500/1000000  loss         0.138413  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26501/1000000  loss         0.138413  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26600/1000000  loss         0.138399  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26601/1000000  loss         0.138399  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26700/1000000  loss         0.138385  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26701/1000000  loss         0.138385  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26800/1000000  loss         0.138371  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26801/1000000  loss         0.138371  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26900/1000000  loss         0.138357  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 26901/1000000  loss         0.138357  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 27000/1000000  loss         0.138344  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 27001/1000000  loss         0.138343  avg_L1_norm_grad         0.000030  w[0]   -0.001 bias    2.882\n",
      "iter 27100/1000000  loss         0.138330  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.882\n",
      "iter 27101/1000000  loss         0.138330  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.882\n",
      "iter 27200/1000000  loss         0.138316  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.882\n",
      "iter 27201/1000000  loss         0.138316  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.882\n",
      "iter 27300/1000000  loss         0.138303  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27301/1000000  loss         0.138303  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27400/1000000  loss         0.138290  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27401/1000000  loss         0.138290  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27500/1000000  loss         0.138277  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27501/1000000  loss         0.138277  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27600/1000000  loss         0.138264  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27601/1000000  loss         0.138264  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27700/1000000  loss         0.138251  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27701/1000000  loss         0.138251  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27800/1000000  loss         0.138238  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27801/1000000  loss         0.138238  avg_L1_norm_grad         0.000029  w[0]   -0.001 bias    2.883\n",
      "iter 27900/1000000  loss         0.138225  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 27901/1000000  loss         0.138225  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28000/1000000  loss         0.138213  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28001/1000000  loss         0.138213  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28100/1000000  loss         0.138200  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28101/1000000  loss         0.138200  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28200/1000000  loss         0.138188  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28201/1000000  loss         0.138188  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28300/1000000  loss         0.138176  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28301/1000000  loss         0.138176  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.883\n",
      "iter 28400/1000000  loss         0.138164  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28401/1000000  loss         0.138163  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28500/1000000  loss         0.138152  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28501/1000000  loss         0.138151  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28600/1000000  loss         0.138140  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28601/1000000  loss         0.138140  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28700/1000000  loss         0.138128  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28701/1000000  loss         0.138128  avg_L1_norm_grad         0.000028  w[0]   -0.001 bias    2.884\n",
      "iter 28800/1000000  loss         0.138116  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 28801/1000000  loss         0.138116  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 28900/1000000  loss         0.138104  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 28901/1000000  loss         0.138104  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29000/1000000  loss         0.138093  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29001/1000000  loss         0.138093  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29100/1000000  loss         0.138081  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29101/1000000  loss         0.138081  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29200/1000000  loss         0.138070  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29201/1000000  loss         0.138070  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29300/1000000  loss         0.138059  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29301/1000000  loss         0.138059  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29400/1000000  loss         0.138048  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29401/1000000  loss         0.138048  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29500/1000000  loss         0.138037  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29501/1000000  loss         0.138037  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.884\n",
      "iter 29600/1000000  loss         0.138026  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.885\n",
      "iter 29601/1000000  loss         0.138026  avg_L1_norm_grad         0.000027  w[0]   -0.001 bias    2.885\n",
      "iter 29700/1000000  loss         0.138015  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 29701/1000000  loss         0.138015  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 29800/1000000  loss         0.138004  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 29801/1000000  loss         0.138004  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 29900/1000000  loss         0.137993  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 29901/1000000  loss         0.137993  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30000/1000000  loss         0.137983  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30001/1000000  loss         0.137983  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30100/1000000  loss         0.137972  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30101/1000000  loss         0.137972  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30200/1000000  loss         0.137962  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30201/1000000  loss         0.137962  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30300/1000000  loss         0.137952  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30301/1000000  loss         0.137951  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30400/1000000  loss         0.137941  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30401/1000000  loss         0.137941  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30500/1000000  loss         0.137931  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30501/1000000  loss         0.137931  avg_L1_norm_grad         0.000026  w[0]   -0.001 bias    2.885\n",
      "iter 30600/1000000  loss         0.137921  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.885\n",
      "iter 30601/1000000  loss         0.137921  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.885\n",
      "iter 30700/1000000  loss         0.137911  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.885\n",
      "iter 30701/1000000  loss         0.137911  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.885\n",
      "iter 30800/1000000  loss         0.137901  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 30801/1000000  loss         0.137901  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 30900/1000000  loss         0.137891  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 30901/1000000  loss         0.137891  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31000/1000000  loss         0.137882  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31001/1000000  loss         0.137881  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31100/1000000  loss         0.137872  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31101/1000000  loss         0.137872  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31200/1000000  loss         0.137862  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31201/1000000  loss         0.137862  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31300/1000000  loss         0.137853  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31301/1000000  loss         0.137853  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31400/1000000  loss         0.137843  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31401/1000000  loss         0.137843  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31500/1000000  loss         0.137834  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31501/1000000  loss         0.137834  avg_L1_norm_grad         0.000025  w[0]   -0.001 bias    2.886\n",
      "iter 31600/1000000  loss         0.137825  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31601/1000000  loss         0.137825  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31700/1000000  loss         0.137815  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31701/1000000  loss         0.137815  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31800/1000000  loss         0.137806  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31801/1000000  loss         0.137806  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31900/1000000  loss         0.137797  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 31901/1000000  loss         0.137797  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 32000/1000000  loss         0.137788  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 32001/1000000  loss         0.137788  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.886\n",
      "iter 32100/1000000  loss         0.137779  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32101/1000000  loss         0.137779  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32200/1000000  loss         0.137771  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32201/1000000  loss         0.137770  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32300/1000000  loss         0.137762  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32301/1000000  loss         0.137762  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32400/1000000  loss         0.137753  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32401/1000000  loss         0.137753  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32500/1000000  loss         0.137744  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32501/1000000  loss         0.137744  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32600/1000000  loss         0.137736  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32601/1000000  loss         0.137736  avg_L1_norm_grad         0.000024  w[0]   -0.001 bias    2.887\n",
      "iter 32700/1000000  loss         0.137727  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 32701/1000000  loss         0.137727  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 32800/1000000  loss         0.137719  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 32801/1000000  loss         0.137719  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 32900/1000000  loss         0.137711  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 32901/1000000  loss         0.137711  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33000/1000000  loss         0.137702  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33001/1000000  loss         0.137702  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33100/1000000  loss         0.137694  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33101/1000000  loss         0.137694  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33200/1000000  loss         0.137686  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33201/1000000  loss         0.137686  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33300/1000000  loss         0.137678  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33301/1000000  loss         0.137678  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.887\n",
      "iter 33400/1000000  loss         0.137670  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33401/1000000  loss         0.137670  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33500/1000000  loss         0.137662  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33501/1000000  loss         0.137662  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33600/1000000  loss         0.137654  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33601/1000000  loss         0.137654  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33700/1000000  loss         0.137646  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33701/1000000  loss         0.137646  avg_L1_norm_grad         0.000023  w[0]   -0.001 bias    2.888\n",
      "iter 33800/1000000  loss         0.137638  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 33801/1000000  loss         0.137638  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 33900/1000000  loss         0.137631  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 33901/1000000  loss         0.137630  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34000/1000000  loss         0.137623  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34001/1000000  loss         0.137623  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34100/1000000  loss         0.137615  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34101/1000000  loss         0.137615  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34200/1000000  loss         0.137608  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34201/1000000  loss         0.137608  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34300/1000000  loss         0.137600  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34301/1000000  loss         0.137600  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34400/1000000  loss         0.137593  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34401/1000000  loss         0.137593  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34500/1000000  loss         0.137585  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34501/1000000  loss         0.137585  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34600/1000000  loss         0.137578  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34601/1000000  loss         0.137578  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34700/1000000  loss         0.137571  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34701/1000000  loss         0.137571  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.888\n",
      "iter 34800/1000000  loss         0.137564  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.889\n",
      "iter 34801/1000000  loss         0.137564  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.889\n",
      "iter 34900/1000000  loss         0.137557  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.889\n",
      "iter 34901/1000000  loss         0.137557  avg_L1_norm_grad         0.000022  w[0]   -0.001 bias    2.889\n",
      "iter 35000/1000000  loss         0.137549  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35001/1000000  loss         0.137549  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35100/1000000  loss         0.137542  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35101/1000000  loss         0.137542  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35200/1000000  loss         0.137535  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35201/1000000  loss         0.137535  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35300/1000000  loss         0.137529  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35301/1000000  loss         0.137528  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35400/1000000  loss         0.137522  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35401/1000000  loss         0.137522  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35500/1000000  loss         0.137515  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35501/1000000  loss         0.137515  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35600/1000000  loss         0.137508  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35601/1000000  loss         0.137508  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35700/1000000  loss         0.137501  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35701/1000000  loss         0.137501  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35800/1000000  loss         0.137495  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35801/1000000  loss         0.137495  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35900/1000000  loss         0.137488  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 35901/1000000  loss         0.137488  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36000/1000000  loss         0.137482  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36001/1000000  loss         0.137481  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36100/1000000  loss         0.137475  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36101/1000000  loss         0.137475  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36200/1000000  loss         0.137469  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36201/1000000  loss         0.137469  avg_L1_norm_grad         0.000021  w[0]   -0.001 bias    2.889\n",
      "iter 36300/1000000  loss         0.137462  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36301/1000000  loss         0.137462  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36400/1000000  loss         0.137456  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36401/1000000  loss         0.137456  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36500/1000000  loss         0.137449  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36501/1000000  loss         0.137449  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36600/1000000  loss         0.137443  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36601/1000000  loss         0.137443  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36700/1000000  loss         0.137437  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36701/1000000  loss         0.137437  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36800/1000000  loss         0.137431  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36801/1000000  loss         0.137431  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36900/1000000  loss         0.137425  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 36901/1000000  loss         0.137425  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37000/1000000  loss         0.137419  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37001/1000000  loss         0.137419  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37100/1000000  loss         0.137413  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37101/1000000  loss         0.137413  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37200/1000000  loss         0.137407  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37201/1000000  loss         0.137407  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37300/1000000  loss         0.137401  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37301/1000000  loss         0.137401  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37400/1000000  loss         0.137395  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37401/1000000  loss         0.137395  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37500/1000000  loss         0.137389  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37501/1000000  loss         0.137389  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37600/1000000  loss         0.137383  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37601/1000000  loss         0.137383  avg_L1_norm_grad         0.000020  w[0]   -0.001 bias    2.890\n",
      "iter 37700/1000000  loss         0.137377  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.890\n",
      "iter 37701/1000000  loss         0.137377  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.890\n",
      "iter 37800/1000000  loss         0.137372  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.890\n",
      "iter 37801/1000000  loss         0.137372  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.890\n",
      "iter 37900/1000000  loss         0.137366  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 37901/1000000  loss         0.137366  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38000/1000000  loss         0.137360  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38001/1000000  loss         0.137360  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38100/1000000  loss         0.137355  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38101/1000000  loss         0.137355  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38200/1000000  loss         0.137349  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38201/1000000  loss         0.137349  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38300/1000000  loss         0.137344  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38301/1000000  loss         0.137344  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38400/1000000  loss         0.137338  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38401/1000000  loss         0.137338  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38500/1000000  loss         0.137333  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38501/1000000  loss         0.137333  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38600/1000000  loss         0.137327  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38601/1000000  loss         0.137327  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38700/1000000  loss         0.137322  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38701/1000000  loss         0.137322  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38800/1000000  loss         0.137317  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38801/1000000  loss         0.137317  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38900/1000000  loss         0.137311  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 38901/1000000  loss         0.137311  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 39000/1000000  loss         0.137306  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 39001/1000000  loss         0.137306  avg_L1_norm_grad         0.000019  w[0]   -0.001 bias    2.891\n",
      "iter 39100/1000000  loss         0.137301  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39101/1000000  loss         0.137301  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39200/1000000  loss         0.137296  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39201/1000000  loss         0.137296  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39300/1000000  loss         0.137291  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39301/1000000  loss         0.137291  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39400/1000000  loss         0.137285  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39401/1000000  loss         0.137285  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39500/1000000  loss         0.137280  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39501/1000000  loss         0.137280  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.891\n",
      "iter 39600/1000000  loss         0.137275  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39601/1000000  loss         0.137275  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39700/1000000  loss         0.137270  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39701/1000000  loss         0.137270  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39800/1000000  loss         0.137266  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39801/1000000  loss         0.137265  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39900/1000000  loss         0.137261  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 39901/1000000  loss         0.137261  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40000/1000000  loss         0.137256  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40001/1000000  loss         0.137256  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40100/1000000  loss         0.137251  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40101/1000000  loss         0.137251  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40200/1000000  loss         0.137246  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40201/1000000  loss         0.137246  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40300/1000000  loss         0.137241  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40301/1000000  loss         0.137241  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40400/1000000  loss         0.137237  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40401/1000000  loss         0.137237  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40500/1000000  loss         0.137232  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40501/1000000  loss         0.137232  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40600/1000000  loss         0.137227  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40601/1000000  loss         0.137227  avg_L1_norm_grad         0.000018  w[0]   -0.001 bias    2.892\n",
      "iter 40700/1000000  loss         0.137223  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 40701/1000000  loss         0.137223  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 40800/1000000  loss         0.137218  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 40801/1000000  loss         0.137218  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 40900/1000000  loss         0.137213  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 40901/1000000  loss         0.137213  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41000/1000000  loss         0.137209  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41001/1000000  loss         0.137209  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41100/1000000  loss         0.137204  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41101/1000000  loss         0.137204  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41200/1000000  loss         0.137200  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41201/1000000  loss         0.137200  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41300/1000000  loss         0.137196  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41301/1000000  loss         0.137195  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.892\n",
      "iter 41400/1000000  loss         0.137191  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41401/1000000  loss         0.137191  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41500/1000000  loss         0.137187  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41501/1000000  loss         0.137187  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41600/1000000  loss         0.137182  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41601/1000000  loss         0.137182  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41700/1000000  loss         0.137178  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41701/1000000  loss         0.137178  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41800/1000000  loss         0.137174  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41801/1000000  loss         0.137174  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41900/1000000  loss         0.137170  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 41901/1000000  loss         0.137170  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 42000/1000000  loss         0.137165  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 42001/1000000  loss         0.137165  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42100/1000000  loss         0.137161  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 42101/1000000  loss         0.137161  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 42200/1000000  loss         0.137157  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 42201/1000000  loss         0.137157  avg_L1_norm_grad         0.000017  w[0]   -0.001 bias    2.893\n",
      "iter 42300/1000000  loss         0.137153  avg_L1_norm_grad         0.000016  w[0]   -0.001 bias    2.893\n",
      "iter 42301/1000000  loss         0.137153  avg_L1_norm_grad         0.000016  w[0]   -0.001 bias    2.893\n",
      "Done. Converged after 42352 iterations.\n",
      "Origin Accuracy 0.9687499999999972\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr22 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr22.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Accuracy 0.965277777777775\n"
     ]
    }
   ],
   "source": [
    "y_hat_Origin=np.asarray(orig_lr22.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With MinMax Transform\n",
      "TurnOn Loaded 0.0\n",
      "(1, 84000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Square Loaded!\n",
      "Shape of Transformed Data (84000, 1574)\n",
      "Initializing w_G with 1574 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.027314  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.912844  avg_L1_norm_grad         0.033884  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.880506  avg_L1_norm_grad         0.035253  w[0]    0.000 bias    0.031\n",
      "iter    3/1000000  loss         0.914255  avg_L1_norm_grad         0.043552  w[0]    0.000 bias    0.017\n",
      "iter    4/1000000  loss         0.812239  avg_L1_norm_grad         0.035536  w[0]    0.000 bias    0.055\n",
      "iter    5/1000000  loss         0.820943  avg_L1_norm_grad         0.037429  w[0]    0.000 bias    0.040\n",
      "iter    6/1000000  loss         0.714078  avg_L1_norm_grad         0.026945  w[0]    0.000 bias    0.074\n",
      "iter    7/1000000  loss         0.699388  avg_L1_norm_grad         0.026298  w[0]    0.000 bias    0.064\n",
      "iter    8/1000000  loss         0.642835  avg_L1_norm_grad         0.019189  w[0]    0.000 bias    0.091\n",
      "iter    9/1000000  loss         0.624030  avg_L1_norm_grad         0.017686  w[0]    0.000 bias    0.086\n",
      "iter   10/1000000  loss         0.593678  avg_L1_norm_grad         0.013310  w[0]    0.000 bias    0.106\n",
      "iter   11/1000000  loss         0.577134  avg_L1_norm_grad         0.011645  w[0]    0.000 bias    0.105\n",
      "iter   12/1000000  loss         0.558779  avg_L1_norm_grad         0.009148  w[0]    0.001 bias    0.120\n",
      "iter   13/1000000  loss         0.545478  avg_L1_norm_grad         0.007717  w[0]    0.001 bias    0.122\n",
      "iter   14/1000000  loss         0.532790  avg_L1_norm_grad         0.006420  w[0]    0.001 bias    0.134\n",
      "iter   15/1000000  loss         0.522108  avg_L1_norm_grad         0.005417  w[0]    0.001 bias    0.138\n",
      "iter   16/1000000  loss         0.512261  avg_L1_norm_grad         0.004924  w[0]    0.001 bias    0.147\n",
      "iter   17/1000000  loss         0.503361  avg_L1_norm_grad         0.004344  w[0]    0.001 bias    0.152\n",
      "iter   18/1000000  loss         0.495091  avg_L1_norm_grad         0.004203  w[0]    0.001 bias    0.160\n",
      "iter   19/1000000  loss         0.487381  avg_L1_norm_grad         0.003895  w[0]    0.001 bias    0.165\n",
      "iter  100/1000000  loss         0.293866  avg_L1_norm_grad         0.001277  w[0]    0.002 bias    0.441\n",
      "iter  101/1000000  loss         0.293027  avg_L1_norm_grad         0.001268  w[0]    0.002 bias    0.443\n",
      "iter  200/1000000  loss         0.243734  avg_L1_norm_grad         0.000784  w[0]    0.003 bias    0.588\n",
      "iter  201/1000000  loss         0.243429  avg_L1_norm_grad         0.000781  w[0]    0.003 bias    0.589\n",
      "iter  300/1000000  loss         0.221132  avg_L1_norm_grad         0.000591  w[0]    0.003 bias    0.680\n",
      "iter  301/1000000  loss         0.220964  avg_L1_norm_grad         0.000590  w[0]    0.003 bias    0.680\n",
      "iter  400/1000000  loss         0.207461  avg_L1_norm_grad         0.000489  w[0]    0.004 bias    0.747\n",
      "iter  401/1000000  loss         0.207350  avg_L1_norm_grad         0.000488  w[0]    0.004 bias    0.748\n",
      "iter  500/1000000  loss         0.197985  avg_L1_norm_grad         0.000423  w[0]    0.005 bias    0.802\n",
      "iter  501/1000000  loss         0.197904  avg_L1_norm_grad         0.000423  w[0]    0.005 bias    0.803\n",
      "iter  600/1000000  loss         0.190878  avg_L1_norm_grad         0.000377  w[0]    0.005 bias    0.849\n",
      "iter  601/1000000  loss         0.190816  avg_L1_norm_grad         0.000376  w[0]    0.005 bias    0.850\n",
      "iter  700/1000000  loss         0.185268  avg_L1_norm_grad         0.000342  w[0]    0.006 bias    0.891\n",
      "iter  701/1000000  loss         0.185217  avg_L1_norm_grad         0.000342  w[0]    0.006 bias    0.892\n",
      "iter  800/1000000  loss         0.180677  avg_L1_norm_grad         0.000315  w[0]    0.006 bias    0.929\n",
      "iter  801/1000000  loss         0.180636  avg_L1_norm_grad         0.000315  w[0]    0.006 bias    0.929\n",
      "iter  900/1000000  loss         0.176821  avg_L1_norm_grad         0.000293  w[0]    0.007 bias    0.963\n",
      "iter  901/1000000  loss         0.176786  avg_L1_norm_grad         0.000293  w[0]    0.007 bias    0.964\n",
      "iter 1000/1000000  loss         0.173515  avg_L1_norm_grad         0.000274  w[0]    0.007 bias    0.996\n",
      "iter 1001/1000000  loss         0.173484  avg_L1_norm_grad         0.000274  w[0]    0.007 bias    0.996\n",
      "iter 1100/1000000  loss         0.170635  avg_L1_norm_grad         0.000259  w[0]    0.008 bias    1.026\n",
      "iter 1101/1000000  loss         0.170608  avg_L1_norm_grad         0.000259  w[0]    0.008 bias    1.026\n",
      "iter 1200/1000000  loss         0.168092  avg_L1_norm_grad         0.000245  w[0]    0.008 bias    1.054\n",
      "iter 1201/1000000  loss         0.168068  avg_L1_norm_grad         0.000245  w[0]    0.008 bias    1.055\n",
      "iter 1300/1000000  loss         0.165824  avg_L1_norm_grad         0.000234  w[0]    0.009 bias    1.081\n",
      "iter 1301/1000000  loss         0.165802  avg_L1_norm_grad         0.000234  w[0]    0.009 bias    1.082\n",
      "iter 1400/1000000  loss         0.163781  avg_L1_norm_grad         0.000223  w[0]    0.009 bias    1.107\n",
      "iter 1401/1000000  loss         0.163762  avg_L1_norm_grad         0.000223  w[0]    0.009 bias    1.107\n",
      "iter 1500/1000000  loss         0.161928  avg_L1_norm_grad         0.000214  w[0]    0.010 bias    1.132\n",
      "iter 1501/1000000  loss         0.161910  avg_L1_norm_grad         0.000214  w[0]    0.010 bias    1.132\n",
      "iter 1600/1000000  loss         0.160234  avg_L1_norm_grad         0.000206  w[0]    0.010 bias    1.155\n",
      "iter 1601/1000000  loss         0.160218  avg_L1_norm_grad         0.000206  w[0]    0.010 bias    1.156\n",
      "iter 1700/1000000  loss         0.158678  avg_L1_norm_grad         0.000199  w[0]    0.011 bias    1.178\n",
      "iter 1701/1000000  loss         0.158663  avg_L1_norm_grad         0.000199  w[0]    0.011 bias    1.178\n",
      "iter 1800/1000000  loss         0.157240  avg_L1_norm_grad         0.000192  w[0]    0.011 bias    1.200\n",
      "iter 1801/1000000  loss         0.157226  avg_L1_norm_grad         0.000192  w[0]    0.011 bias    1.200\n",
      "iter 1900/1000000  loss         0.155906  avg_L1_norm_grad         0.000186  w[0]    0.011 bias    1.221\n",
      "iter 1901/1000000  loss         0.155893  avg_L1_norm_grad         0.000186  w[0]    0.012 bias    1.221\n",
      "iter 2000/1000000  loss         0.154662  avg_L1_norm_grad         0.000180  w[0]    0.012 bias    1.241\n",
      "iter 2001/1000000  loss         0.154650  avg_L1_norm_grad         0.000180  w[0]    0.012 bias    1.242\n",
      "iter 2100/1000000  loss         0.153498  avg_L1_norm_grad         0.000175  w[0]    0.012 bias    1.261\n",
      "iter 2101/1000000  loss         0.153487  avg_L1_norm_grad         0.000175  w[0]    0.012 bias    1.261\n",
      "iter 2200/1000000  loss         0.152406  avg_L1_norm_grad         0.000170  w[0]    0.013 bias    1.280\n",
      "iter 2201/1000000  loss         0.152396  avg_L1_norm_grad         0.000170  w[0]    0.013 bias    1.280\n",
      "iter 2300/1000000  loss         0.151378  avg_L1_norm_grad         0.000165  w[0]    0.013 bias    1.299\n",
      "iter 2301/1000000  loss         0.151368  avg_L1_norm_grad         0.000165  w[0]    0.013 bias    1.299\n",
      "iter 2400/1000000  loss         0.150408  avg_L1_norm_grad         0.000161  w[0]    0.014 bias    1.317\n",
      "iter 2401/1000000  loss         0.150398  avg_L1_norm_grad         0.000161  w[0]    0.014 bias    1.317\n",
      "iter 2500/1000000  loss         0.149489  avg_L1_norm_grad         0.000157  w[0]    0.014 bias    1.335\n",
      "iter 2501/1000000  loss         0.149480  avg_L1_norm_grad         0.000157  w[0]    0.014 bias    1.335\n",
      "iter 2600/1000000  loss         0.148618  avg_L1_norm_grad         0.000153  w[0]    0.014 bias    1.352\n",
      "iter 2601/1000000  loss         0.148610  avg_L1_norm_grad         0.000153  w[0]    0.014 bias    1.352\n",
      "iter 2700/1000000  loss         0.147790  avg_L1_norm_grad         0.000150  w[0]    0.015 bias    1.368\n",
      "iter 2701/1000000  loss         0.147782  avg_L1_norm_grad         0.000150  w[0]    0.015 bias    1.369\n",
      "iter 2800/1000000  loss         0.147001  avg_L1_norm_grad         0.000146  w[0]    0.015 bias    1.385\n",
      "iter 2801/1000000  loss         0.146993  avg_L1_norm_grad         0.000146  w[0]    0.015 bias    1.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2900/1000000  loss         0.146248  avg_L1_norm_grad         0.000143  w[0]    0.015 bias    1.401\n",
      "iter 2901/1000000  loss         0.146241  avg_L1_norm_grad         0.000143  w[0]    0.015 bias    1.401\n",
      "iter 3000/1000000  loss         0.145529  avg_L1_norm_grad         0.000140  w[0]    0.016 bias    1.416\n",
      "iter 3001/1000000  loss         0.145522  avg_L1_norm_grad         0.000140  w[0]    0.016 bias    1.416\n",
      "iter 3100/1000000  loss         0.144840  avg_L1_norm_grad         0.000137  w[0]    0.016 bias    1.431\n",
      "iter 3101/1000000  loss         0.144833  avg_L1_norm_grad         0.000137  w[0]    0.016 bias    1.432\n",
      "iter 3200/1000000  loss         0.144180  avg_L1_norm_grad         0.000135  w[0]    0.016 bias    1.446\n",
      "iter 3201/1000000  loss         0.144173  avg_L1_norm_grad         0.000135  w[0]    0.016 bias    1.447\n",
      "iter 3300/1000000  loss         0.143546  avg_L1_norm_grad         0.000132  w[0]    0.017 bias    1.461\n",
      "iter 3301/1000000  loss         0.143540  avg_L1_norm_grad         0.000132  w[0]    0.017 bias    1.461\n",
      "iter 3400/1000000  loss         0.142937  avg_L1_norm_grad         0.000130  w[0]    0.017 bias    1.475\n",
      "iter 3401/1000000  loss         0.142931  avg_L1_norm_grad         0.000130  w[0]    0.017 bias    1.475\n",
      "iter 3500/1000000  loss         0.142351  avg_L1_norm_grad         0.000127  w[0]    0.017 bias    1.489\n",
      "iter 3501/1000000  loss         0.142345  avg_L1_norm_grad         0.000127  w[0]    0.017 bias    1.490\n",
      "iter 3600/1000000  loss         0.141786  avg_L1_norm_grad         0.000125  w[0]    0.018 bias    1.503\n",
      "iter 3601/1000000  loss         0.141781  avg_L1_norm_grad         0.000125  w[0]    0.018 bias    1.503\n",
      "iter 3700/1000000  loss         0.141242  avg_L1_norm_grad         0.000123  w[0]    0.018 bias    1.517\n",
      "iter 3701/1000000  loss         0.141236  avg_L1_norm_grad         0.000123  w[0]    0.018 bias    1.517\n",
      "iter 3800/1000000  loss         0.140716  avg_L1_norm_grad         0.000121  w[0]    0.018 bias    1.530\n",
      "iter 3801/1000000  loss         0.140711  avg_L1_norm_grad         0.000121  w[0]    0.018 bias    1.530\n",
      "iter 3900/1000000  loss         0.140209  avg_L1_norm_grad         0.000119  w[0]    0.019 bias    1.543\n",
      "iter 3901/1000000  loss         0.140204  avg_L1_norm_grad         0.000119  w[0]    0.019 bias    1.543\n",
      "iter 4000/1000000  loss         0.139718  avg_L1_norm_grad         0.000117  w[0]    0.019 bias    1.556\n",
      "iter 4001/1000000  loss         0.139713  avg_L1_norm_grad         0.000117  w[0]    0.019 bias    1.556\n",
      "iter 4100/1000000  loss         0.139243  avg_L1_norm_grad         0.000115  w[0]    0.019 bias    1.568\n",
      "iter 4101/1000000  loss         0.139238  avg_L1_norm_grad         0.000115  w[0]    0.019 bias    1.569\n",
      "iter 4200/1000000  loss         0.138783  avg_L1_norm_grad         0.000113  w[0]    0.020 bias    1.581\n",
      "iter 4201/1000000  loss         0.138778  avg_L1_norm_grad         0.000113  w[0]    0.020 bias    1.581\n",
      "iter 4300/1000000  loss         0.138337  avg_L1_norm_grad         0.000111  w[0]    0.020 bias    1.593\n",
      "iter 4301/1000000  loss         0.138333  avg_L1_norm_grad         0.000111  w[0]    0.020 bias    1.593\n",
      "iter 4400/1000000  loss         0.137905  avg_L1_norm_grad         0.000110  w[0]    0.020 bias    1.605\n",
      "iter 4401/1000000  loss         0.137900  avg_L1_norm_grad         0.000110  w[0]    0.020 bias    1.605\n",
      "iter 4500/1000000  loss         0.137485  avg_L1_norm_grad         0.000108  w[0]    0.020 bias    1.617\n",
      "iter 4501/1000000  loss         0.137481  avg_L1_norm_grad         0.000108  w[0]    0.020 bias    1.617\n",
      "iter 4600/1000000  loss         0.137078  avg_L1_norm_grad         0.000107  w[0]    0.021 bias    1.629\n",
      "iter 4601/1000000  loss         0.137074  avg_L1_norm_grad         0.000107  w[0]    0.021 bias    1.629\n",
      "iter 4700/1000000  loss         0.136682  avg_L1_norm_grad         0.000105  w[0]    0.021 bias    1.640\n",
      "iter 4701/1000000  loss         0.136678  avg_L1_norm_grad         0.000105  w[0]    0.021 bias    1.640\n",
      "iter 4800/1000000  loss         0.136297  avg_L1_norm_grad         0.000104  w[0]    0.021 bias    1.651\n",
      "iter 4801/1000000  loss         0.136293  avg_L1_norm_grad         0.000104  w[0]    0.021 bias    1.651\n",
      "iter 4900/1000000  loss         0.135923  avg_L1_norm_grad         0.000102  w[0]    0.022 bias    1.663\n",
      "iter 4901/1000000  loss         0.135919  avg_L1_norm_grad         0.000102  w[0]    0.022 bias    1.663\n",
      "iter 5000/1000000  loss         0.135559  avg_L1_norm_grad         0.000101  w[0]    0.022 bias    1.674\n",
      "iter 5001/1000000  loss         0.135555  avg_L1_norm_grad         0.000101  w[0]    0.022 bias    1.674\n",
      "iter 5100/1000000  loss         0.135204  avg_L1_norm_grad         0.000100  w[0]    0.022 bias    1.684\n",
      "iter 5101/1000000  loss         0.135201  avg_L1_norm_grad         0.000099  w[0]    0.022 bias    1.685\n",
      "iter 5200/1000000  loss         0.134859  avg_L1_norm_grad         0.000098  w[0]    0.022 bias    1.695\n",
      "iter 5201/1000000  loss         0.134855  avg_L1_norm_grad         0.000098  w[0]    0.022 bias    1.695\n",
      "iter 5300/1000000  loss         0.134522  avg_L1_norm_grad         0.000097  w[0]    0.023 bias    1.706\n",
      "iter 5301/1000000  loss         0.134519  avg_L1_norm_grad         0.000097  w[0]    0.023 bias    1.706\n",
      "iter 5400/1000000  loss         0.134194  avg_L1_norm_grad         0.000096  w[0]    0.023 bias    1.716\n",
      "iter 5401/1000000  loss         0.134191  avg_L1_norm_grad         0.000096  w[0]    0.023 bias    1.716\n",
      "iter 5500/1000000  loss         0.133874  avg_L1_norm_grad         0.000095  w[0]    0.023 bias    1.726\n",
      "iter 5501/1000000  loss         0.133871  avg_L1_norm_grad         0.000095  w[0]    0.023 bias    1.727\n",
      "iter 5600/1000000  loss         0.133562  avg_L1_norm_grad         0.000093  w[0]    0.023 bias    1.737\n",
      "iter 5601/1000000  loss         0.133559  avg_L1_norm_grad         0.000093  w[0]    0.023 bias    1.737\n",
      "iter 5700/1000000  loss         0.133257  avg_L1_norm_grad         0.000092  w[0]    0.024 bias    1.747\n",
      "iter 5701/1000000  loss         0.133254  avg_L1_norm_grad         0.000092  w[0]    0.024 bias    1.747\n",
      "iter 5800/1000000  loss         0.132959  avg_L1_norm_grad         0.000091  w[0]    0.024 bias    1.757\n",
      "iter 5801/1000000  loss         0.132956  avg_L1_norm_grad         0.000091  w[0]    0.024 bias    1.757\n",
      "iter 5900/1000000  loss         0.132669  avg_L1_norm_grad         0.000090  w[0]    0.024 bias    1.766\n",
      "iter 5901/1000000  loss         0.132666  avg_L1_norm_grad         0.000090  w[0]    0.024 bias    1.767\n",
      "iter 6000/1000000  loss         0.132385  avg_L1_norm_grad         0.000089  w[0]    0.024 bias    1.776\n",
      "iter 6001/1000000  loss         0.132382  avg_L1_norm_grad         0.000089  w[0]    0.024 bias    1.776\n",
      "iter 6100/1000000  loss         0.132107  avg_L1_norm_grad         0.000088  w[0]    0.024 bias    1.786\n",
      "iter 6101/1000000  loss         0.132104  avg_L1_norm_grad         0.000088  w[0]    0.024 bias    1.786\n",
      "iter 6200/1000000  loss         0.131836  avg_L1_norm_grad         0.000087  w[0]    0.025 bias    1.795\n",
      "iter 6201/1000000  loss         0.131833  avg_L1_norm_grad         0.000087  w[0]    0.025 bias    1.795\n",
      "iter 6300/1000000  loss         0.131570  avg_L1_norm_grad         0.000086  w[0]    0.025 bias    1.805\n",
      "iter 6301/1000000  loss         0.131568  avg_L1_norm_grad         0.000086  w[0]    0.025 bias    1.805\n",
      "iter 6400/1000000  loss         0.131310  avg_L1_norm_grad         0.000085  w[0]    0.025 bias    1.814\n",
      "iter 6401/1000000  loss         0.131308  avg_L1_norm_grad         0.000085  w[0]    0.025 bias    1.814\n",
      "iter 6500/1000000  loss         0.131056  avg_L1_norm_grad         0.000084  w[0]    0.025 bias    1.823\n",
      "iter 6501/1000000  loss         0.131054  avg_L1_norm_grad         0.000084  w[0]    0.025 bias    1.823\n",
      "iter 6600/1000000  loss         0.130808  avg_L1_norm_grad         0.000083  w[0]    0.025 bias    1.832\n",
      "iter 6601/1000000  loss         0.130805  avg_L1_norm_grad         0.000083  w[0]    0.025 bias    1.832\n",
      "iter 6700/1000000  loss         0.130564  avg_L1_norm_grad         0.000083  w[0]    0.026 bias    1.841\n",
      "iter 6701/1000000  loss         0.130562  avg_L1_norm_grad         0.000083  w[0]    0.026 bias    1.841\n",
      "iter 6800/1000000  loss         0.130325  avg_L1_norm_grad         0.000082  w[0]    0.026 bias    1.850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6801/1000000  loss         0.130323  avg_L1_norm_grad         0.000082  w[0]    0.026 bias    1.850\n",
      "iter 6900/1000000  loss         0.130092  avg_L1_norm_grad         0.000081  w[0]    0.026 bias    1.859\n",
      "iter 6901/1000000  loss         0.130089  avg_L1_norm_grad         0.000081  w[0]    0.026 bias    1.859\n",
      "iter 7000/1000000  loss         0.129863  avg_L1_norm_grad         0.000080  w[0]    0.026 bias    1.868\n",
      "iter 7001/1000000  loss         0.129860  avg_L1_norm_grad         0.000080  w[0]    0.026 bias    1.868\n",
      "iter 7100/1000000  loss         0.129638  avg_L1_norm_grad         0.000079  w[0]    0.026 bias    1.876\n",
      "iter 7101/1000000  loss         0.129636  avg_L1_norm_grad         0.000079  w[0]    0.026 bias    1.877\n",
      "iter 7200/1000000  loss         0.129418  avg_L1_norm_grad         0.000078  w[0]    0.027 bias    1.885\n",
      "iter 7201/1000000  loss         0.129416  avg_L1_norm_grad         0.000078  w[0]    0.027 bias    1.885\n",
      "iter 7300/1000000  loss         0.129202  avg_L1_norm_grad         0.000078  w[0]    0.027 bias    1.894\n",
      "iter 7301/1000000  loss         0.129200  avg_L1_norm_grad         0.000078  w[0]    0.027 bias    1.894\n",
      "iter 7400/1000000  loss         0.128991  avg_L1_norm_grad         0.000077  w[0]    0.027 bias    1.902\n",
      "iter 7401/1000000  loss         0.128989  avg_L1_norm_grad         0.000077  w[0]    0.027 bias    1.902\n",
      "iter 7500/1000000  loss         0.128783  avg_L1_norm_grad         0.000076  w[0]    0.027 bias    1.910\n",
      "iter 7501/1000000  loss         0.128781  avg_L1_norm_grad         0.000076  w[0]    0.027 bias    1.910\n",
      "iter 7600/1000000  loss         0.128580  avg_L1_norm_grad         0.000075  w[0]    0.027 bias    1.919\n",
      "iter 7601/1000000  loss         0.128578  avg_L1_norm_grad         0.000075  w[0]    0.027 bias    1.919\n",
      "iter 7700/1000000  loss         0.128380  avg_L1_norm_grad         0.000075  w[0]    0.027 bias    1.927\n",
      "iter 7701/1000000  loss         0.128378  avg_L1_norm_grad         0.000075  w[0]    0.027 bias    1.927\n",
      "iter 7800/1000000  loss         0.128184  avg_L1_norm_grad         0.000074  w[0]    0.028 bias    1.935\n",
      "iter 7801/1000000  loss         0.128182  avg_L1_norm_grad         0.000074  w[0]    0.028 bias    1.935\n",
      "iter 7900/1000000  loss         0.127991  avg_L1_norm_grad         0.000073  w[0]    0.028 bias    1.943\n",
      "iter 7901/1000000  loss         0.127989  avg_L1_norm_grad         0.000073  w[0]    0.028 bias    1.943\n",
      "iter 8000/1000000  loss         0.127802  avg_L1_norm_grad         0.000073  w[0]    0.028 bias    1.951\n",
      "iter 8001/1000000  loss         0.127800  avg_L1_norm_grad         0.000073  w[0]    0.028 bias    1.951\n",
      "iter 8100/1000000  loss         0.127616  avg_L1_norm_grad         0.000072  w[0]    0.028 bias    1.959\n",
      "iter 8101/1000000  loss         0.127615  avg_L1_norm_grad         0.000072  w[0]    0.028 bias    1.959\n",
      "iter 8200/1000000  loss         0.127434  avg_L1_norm_grad         0.000071  w[0]    0.028 bias    1.967\n",
      "iter 8201/1000000  loss         0.127432  avg_L1_norm_grad         0.000071  w[0]    0.028 bias    1.967\n",
      "iter 8300/1000000  loss         0.127255  avg_L1_norm_grad         0.000071  w[0]    0.028 bias    1.974\n",
      "iter 8301/1000000  loss         0.127253  avg_L1_norm_grad         0.000071  w[0]    0.028 bias    1.974\n",
      "iter 8400/1000000  loss         0.127079  avg_L1_norm_grad         0.000070  w[0]    0.029 bias    1.982\n",
      "iter 8401/1000000  loss         0.127077  avg_L1_norm_grad         0.000070  w[0]    0.029 bias    1.982\n",
      "iter 8500/1000000  loss         0.126906  avg_L1_norm_grad         0.000070  w[0]    0.029 bias    1.990\n",
      "iter 8501/1000000  loss         0.126904  avg_L1_norm_grad         0.000070  w[0]    0.029 bias    1.990\n",
      "iter 8600/1000000  loss         0.126736  avg_L1_norm_grad         0.000069  w[0]    0.029 bias    1.997\n",
      "iter 8601/1000000  loss         0.126734  avg_L1_norm_grad         0.000069  w[0]    0.029 bias    1.997\n",
      "iter 8700/1000000  loss         0.126568  avg_L1_norm_grad         0.000068  w[0]    0.029 bias    2.005\n",
      "iter 8701/1000000  loss         0.126567  avg_L1_norm_grad         0.000068  w[0]    0.029 bias    2.005\n",
      "iter 8800/1000000  loss         0.126404  avg_L1_norm_grad         0.000068  w[0]    0.029 bias    2.012\n",
      "iter 8801/1000000  loss         0.126402  avg_L1_norm_grad         0.000068  w[0]    0.029 bias    2.012\n",
      "iter 8900/1000000  loss         0.126242  avg_L1_norm_grad         0.000067  w[0]    0.029 bias    2.019\n",
      "iter 8901/1000000  loss         0.126241  avg_L1_norm_grad         0.000067  w[0]    0.029 bias    2.020\n",
      "iter 9000/1000000  loss         0.126083  avg_L1_norm_grad         0.000067  w[0]    0.030 bias    2.027\n",
      "iter 9001/1000000  loss         0.126082  avg_L1_norm_grad         0.000067  w[0]    0.030 bias    2.027\n",
      "iter 9100/1000000  loss         0.125927  avg_L1_norm_grad         0.000066  w[0]    0.030 bias    2.034\n",
      "iter 9101/1000000  loss         0.125926  avg_L1_norm_grad         0.000066  w[0]    0.030 bias    2.034\n",
      "iter 9200/1000000  loss         0.125773  avg_L1_norm_grad         0.000065  w[0]    0.030 bias    2.041\n",
      "iter 9201/1000000  loss         0.125772  avg_L1_norm_grad         0.000065  w[0]    0.030 bias    2.041\n",
      "iter 9300/1000000  loss         0.125622  avg_L1_norm_grad         0.000065  w[0]    0.030 bias    2.048\n",
      "iter 9301/1000000  loss         0.125620  avg_L1_norm_grad         0.000065  w[0]    0.030 bias    2.049\n",
      "iter 9400/1000000  loss         0.125473  avg_L1_norm_grad         0.000064  w[0]    0.030 bias    2.056\n",
      "iter 9401/1000000  loss         0.125472  avg_L1_norm_grad         0.000064  w[0]    0.030 bias    2.056\n",
      "iter 9500/1000000  loss         0.125326  avg_L1_norm_grad         0.000064  w[0]    0.030 bias    2.063\n",
      "iter 9501/1000000  loss         0.125325  avg_L1_norm_grad         0.000064  w[0]    0.030 bias    2.063\n",
      "iter 9600/1000000  loss         0.125182  avg_L1_norm_grad         0.000063  w[0]    0.030 bias    2.070\n",
      "iter 9601/1000000  loss         0.125181  avg_L1_norm_grad         0.000063  w[0]    0.030 bias    2.070\n",
      "iter 9700/1000000  loss         0.125040  avg_L1_norm_grad         0.000063  w[0]    0.030 bias    2.077\n",
      "iter 9701/1000000  loss         0.125039  avg_L1_norm_grad         0.000063  w[0]    0.030 bias    2.077\n",
      "iter 9800/1000000  loss         0.124900  avg_L1_norm_grad         0.000062  w[0]    0.031 bias    2.083\n",
      "iter 9801/1000000  loss         0.124899  avg_L1_norm_grad         0.000062  w[0]    0.031 bias    2.084\n",
      "iter 9900/1000000  loss         0.124763  avg_L1_norm_grad         0.000062  w[0]    0.031 bias    2.090\n",
      "iter 9901/1000000  loss         0.124761  avg_L1_norm_grad         0.000062  w[0]    0.031 bias    2.090\n",
      "iter 10000/1000000  loss         0.124627  avg_L1_norm_grad         0.000061  w[0]    0.031 bias    2.097\n",
      "iter 10001/1000000  loss         0.124626  avg_L1_norm_grad         0.000061  w[0]    0.031 bias    2.097\n",
      "iter 10100/1000000  loss         0.124494  avg_L1_norm_grad         0.000061  w[0]    0.031 bias    2.104\n",
      "iter 10101/1000000  loss         0.124492  avg_L1_norm_grad         0.000061  w[0]    0.031 bias    2.104\n",
      "iter 10200/1000000  loss         0.124362  avg_L1_norm_grad         0.000060  w[0]    0.031 bias    2.111\n",
      "iter 10201/1000000  loss         0.124361  avg_L1_norm_grad         0.000060  w[0]    0.031 bias    2.111\n",
      "iter 10300/1000000  loss         0.124232  avg_L1_norm_grad         0.000060  w[0]    0.031 bias    2.117\n",
      "iter 10301/1000000  loss         0.124231  avg_L1_norm_grad         0.000060  w[0]    0.031 bias    2.117\n",
      "iter 10400/1000000  loss         0.124105  avg_L1_norm_grad         0.000060  w[0]    0.031 bias    2.124\n",
      "iter 10401/1000000  loss         0.124104  avg_L1_norm_grad         0.000060  w[0]    0.031 bias    2.124\n",
      "iter 10500/1000000  loss         0.123979  avg_L1_norm_grad         0.000059  w[0]    0.031 bias    2.130\n",
      "iter 10501/1000000  loss         0.123978  avg_L1_norm_grad         0.000059  w[0]    0.031 bias    2.130\n",
      "iter 10600/1000000  loss         0.123855  avg_L1_norm_grad         0.000059  w[0]    0.032 bias    2.137\n",
      "iter 10601/1000000  loss         0.123854  avg_L1_norm_grad         0.000059  w[0]    0.032 bias    2.137\n",
      "iter 10700/1000000  loss         0.123733  avg_L1_norm_grad         0.000058  w[0]    0.032 bias    2.143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10701/1000000  loss         0.123732  avg_L1_norm_grad         0.000058  w[0]    0.032 bias    2.143\n",
      "iter 10800/1000000  loss         0.123613  avg_L1_norm_grad         0.000058  w[0]    0.032 bias    2.150\n",
      "iter 10801/1000000  loss         0.123611  avg_L1_norm_grad         0.000058  w[0]    0.032 bias    2.150\n",
      "iter 10900/1000000  loss         0.123494  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    2.156\n",
      "iter 10901/1000000  loss         0.123493  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    2.156\n",
      "iter 11000/1000000  loss         0.123377  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    2.162\n",
      "iter 11001/1000000  loss         0.123376  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    2.162\n",
      "iter 11100/1000000  loss         0.123262  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    2.169\n",
      "iter 11101/1000000  loss         0.123260  avg_L1_norm_grad         0.000057  w[0]    0.032 bias    2.169\n",
      "iter 11200/1000000  loss         0.123148  avg_L1_norm_grad         0.000056  w[0]    0.032 bias    2.175\n",
      "iter 11201/1000000  loss         0.123147  avg_L1_norm_grad         0.000056  w[0]    0.032 bias    2.175\n",
      "iter 11300/1000000  loss         0.123036  avg_L1_norm_grad         0.000056  w[0]    0.032 bias    2.181\n",
      "iter 11301/1000000  loss         0.123035  avg_L1_norm_grad         0.000056  w[0]    0.032 bias    2.181\n",
      "iter 11400/1000000  loss         0.122925  avg_L1_norm_grad         0.000055  w[0]    0.032 bias    2.187\n",
      "iter 11401/1000000  loss         0.122924  avg_L1_norm_grad         0.000055  w[0]    0.032 bias    2.187\n",
      "iter 11500/1000000  loss         0.122816  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    2.193\n",
      "iter 11501/1000000  loss         0.122815  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    2.193\n",
      "iter 11600/1000000  loss         0.122709  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    2.199\n",
      "iter 11601/1000000  loss         0.122708  avg_L1_norm_grad         0.000055  w[0]    0.033 bias    2.200\n",
      "iter 11700/1000000  loss         0.122602  avg_L1_norm_grad         0.000054  w[0]    0.033 bias    2.206\n",
      "iter 11701/1000000  loss         0.122601  avg_L1_norm_grad         0.000054  w[0]    0.033 bias    2.206\n",
      "iter 11800/1000000  loss         0.122498  avg_L1_norm_grad         0.000054  w[0]    0.033 bias    2.212\n",
      "iter 11801/1000000  loss         0.122497  avg_L1_norm_grad         0.000054  w[0]    0.033 bias    2.212\n",
      "iter 11900/1000000  loss         0.122394  avg_L1_norm_grad         0.000053  w[0]    0.033 bias    2.218\n",
      "iter 11901/1000000  loss         0.122393  avg_L1_norm_grad         0.000053  w[0]    0.033 bias    2.218\n",
      "iter 12000/1000000  loss         0.122293  avg_L1_norm_grad         0.000053  w[0]    0.033 bias    2.223\n",
      "iter 12001/1000000  loss         0.122292  avg_L1_norm_grad         0.000053  w[0]    0.033 bias    2.223\n",
      "iter 12100/1000000  loss         0.122192  avg_L1_norm_grad         0.000053  w[0]    0.033 bias    2.229\n",
      "iter 12101/1000000  loss         0.122191  avg_L1_norm_grad         0.000053  w[0]    0.033 bias    2.229\n",
      "iter 12200/1000000  loss         0.122093  avg_L1_norm_grad         0.000052  w[0]    0.033 bias    2.235\n",
      "iter 12201/1000000  loss         0.122092  avg_L1_norm_grad         0.000052  w[0]    0.033 bias    2.235\n",
      "iter 12300/1000000  loss         0.121995  avg_L1_norm_grad         0.000052  w[0]    0.033 bias    2.241\n",
      "iter 12301/1000000  loss         0.121994  avg_L1_norm_grad         0.000052  w[0]    0.033 bias    2.241\n",
      "iter 12400/1000000  loss         0.121898  avg_L1_norm_grad         0.000052  w[0]    0.033 bias    2.247\n",
      "iter 12401/1000000  loss         0.121897  avg_L1_norm_grad         0.000052  w[0]    0.033 bias    2.247\n",
      "iter 12500/1000000  loss         0.121803  avg_L1_norm_grad         0.000051  w[0]    0.033 bias    2.253\n",
      "iter 12501/1000000  loss         0.121802  avg_L1_norm_grad         0.000051  w[0]    0.033 bias    2.253\n",
      "iter 12600/1000000  loss         0.121709  avg_L1_norm_grad         0.000051  w[0]    0.034 bias    2.258\n",
      "iter 12601/1000000  loss         0.121708  avg_L1_norm_grad         0.000051  w[0]    0.034 bias    2.258\n",
      "iter 12700/1000000  loss         0.121616  avg_L1_norm_grad         0.000051  w[0]    0.034 bias    2.264\n",
      "iter 12701/1000000  loss         0.121615  avg_L1_norm_grad         0.000051  w[0]    0.034 bias    2.264\n",
      "iter 12800/1000000  loss         0.121524  avg_L1_norm_grad         0.000050  w[0]    0.034 bias    2.270\n",
      "iter 12801/1000000  loss         0.121523  avg_L1_norm_grad         0.000050  w[0]    0.034 bias    2.270\n",
      "iter 12900/1000000  loss         0.121433  avg_L1_norm_grad         0.000050  w[0]    0.034 bias    2.275\n",
      "iter 12901/1000000  loss         0.121432  avg_L1_norm_grad         0.000050  w[0]    0.034 bias    2.275\n",
      "iter 13000/1000000  loss         0.121344  avg_L1_norm_grad         0.000050  w[0]    0.034 bias    2.281\n",
      "iter 13001/1000000  loss         0.121343  avg_L1_norm_grad         0.000050  w[0]    0.034 bias    2.281\n",
      "iter 13100/1000000  loss         0.121255  avg_L1_norm_grad         0.000049  w[0]    0.034 bias    2.286\n",
      "iter 13101/1000000  loss         0.121254  avg_L1_norm_grad         0.000049  w[0]    0.034 bias    2.286\n",
      "iter 13200/1000000  loss         0.121168  avg_L1_norm_grad         0.000049  w[0]    0.034 bias    2.292\n",
      "iter 13201/1000000  loss         0.121167  avg_L1_norm_grad         0.000049  w[0]    0.034 bias    2.292\n",
      "iter 13300/1000000  loss         0.121082  avg_L1_norm_grad         0.000049  w[0]    0.034 bias    2.297\n",
      "iter 13301/1000000  loss         0.121081  avg_L1_norm_grad         0.000049  w[0]    0.034 bias    2.297\n",
      "iter 13400/1000000  loss         0.120996  avg_L1_norm_grad         0.000048  w[0]    0.034 bias    2.303\n",
      "iter 13401/1000000  loss         0.120996  avg_L1_norm_grad         0.000048  w[0]    0.034 bias    2.303\n",
      "iter 13500/1000000  loss         0.120912  avg_L1_norm_grad         0.000048  w[0]    0.034 bias    2.308\n",
      "iter 13501/1000000  loss         0.120912  avg_L1_norm_grad         0.000048  w[0]    0.034 bias    2.308\n",
      "iter 13600/1000000  loss         0.120829  avg_L1_norm_grad         0.000048  w[0]    0.034 bias    2.313\n",
      "iter 13601/1000000  loss         0.120828  avg_L1_norm_grad         0.000048  w[0]    0.034 bias    2.313\n",
      "iter 13700/1000000  loss         0.120747  avg_L1_norm_grad         0.000047  w[0]    0.034 bias    2.319\n",
      "iter 13701/1000000  loss         0.120746  avg_L1_norm_grad         0.000047  w[0]    0.034 bias    2.319\n",
      "iter 13800/1000000  loss         0.120666  avg_L1_norm_grad         0.000047  w[0]    0.034 bias    2.324\n",
      "iter 13801/1000000  loss         0.120665  avg_L1_norm_grad         0.000047  w[0]    0.034 bias    2.324\n",
      "iter 13900/1000000  loss         0.120586  avg_L1_norm_grad         0.000047  w[0]    0.035 bias    2.329\n",
      "iter 13901/1000000  loss         0.120585  avg_L1_norm_grad         0.000047  w[0]    0.035 bias    2.329\n",
      "iter 14000/1000000  loss         0.120507  avg_L1_norm_grad         0.000047  w[0]    0.035 bias    2.335\n",
      "iter 14001/1000000  loss         0.120506  avg_L1_norm_grad         0.000047  w[0]    0.035 bias    2.335\n",
      "iter 14100/1000000  loss         0.120429  avg_L1_norm_grad         0.000046  w[0]    0.035 bias    2.340\n",
      "iter 14101/1000000  loss         0.120428  avg_L1_norm_grad         0.000046  w[0]    0.035 bias    2.340\n",
      "iter 14200/1000000  loss         0.120351  avg_L1_norm_grad         0.000046  w[0]    0.035 bias    2.345\n",
      "iter 14201/1000000  loss         0.120350  avg_L1_norm_grad         0.000046  w[0]    0.035 bias    2.345\n",
      "iter 14300/1000000  loss         0.120275  avg_L1_norm_grad         0.000046  w[0]    0.035 bias    2.350\n",
      "iter 14301/1000000  loss         0.120274  avg_L1_norm_grad         0.000046  w[0]    0.035 bias    2.350\n",
      "iter 14400/1000000  loss         0.120199  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.355\n",
      "iter 14401/1000000  loss         0.120198  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.355\n",
      "iter 14500/1000000  loss         0.120125  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.360\n",
      "iter 14501/1000000  loss         0.120124  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.360\n",
      "iter 14600/1000000  loss         0.120051  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14601/1000000  loss         0.120050  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.366\n",
      "iter 14700/1000000  loss         0.119978  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.371\n",
      "iter 14701/1000000  loss         0.119977  avg_L1_norm_grad         0.000045  w[0]    0.035 bias    2.371\n",
      "iter 14800/1000000  loss         0.119906  avg_L1_norm_grad         0.000044  w[0]    0.035 bias    2.376\n",
      "iter 14801/1000000  loss         0.119905  avg_L1_norm_grad         0.000044  w[0]    0.035 bias    2.376\n",
      "iter 14900/1000000  loss         0.119834  avg_L1_norm_grad         0.000044  w[0]    0.035 bias    2.381\n",
      "iter 14901/1000000  loss         0.119834  avg_L1_norm_grad         0.000044  w[0]    0.035 bias    2.381\n",
      "iter 15000/1000000  loss         0.119764  avg_L1_norm_grad         0.000044  w[0]    0.035 bias    2.386\n",
      "iter 15001/1000000  loss         0.119763  avg_L1_norm_grad         0.000044  w[0]    0.035 bias    2.386\n",
      "iter 15100/1000000  loss         0.119694  avg_L1_norm_grad         0.000043  w[0]    0.035 bias    2.390\n",
      "iter 15101/1000000  loss         0.119694  avg_L1_norm_grad         0.000043  w[0]    0.035 bias    2.390\n",
      "iter 15200/1000000  loss         0.119625  avg_L1_norm_grad         0.000043  w[0]    0.035 bias    2.395\n",
      "iter 15201/1000000  loss         0.119625  avg_L1_norm_grad         0.000043  w[0]    0.035 bias    2.395\n",
      "iter 15300/1000000  loss         0.119557  avg_L1_norm_grad         0.000043  w[0]    0.035 bias    2.400\n",
      "iter 15301/1000000  loss         0.119557  avg_L1_norm_grad         0.000043  w[0]    0.035 bias    2.400\n",
      "iter 15400/1000000  loss         0.119490  avg_L1_norm_grad         0.000043  w[0]    0.036 bias    2.405\n",
      "iter 15401/1000000  loss         0.119489  avg_L1_norm_grad         0.000043  w[0]    0.036 bias    2.405\n",
      "iter 15500/1000000  loss         0.119423  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.410\n",
      "iter 15501/1000000  loss         0.119423  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.410\n",
      "iter 15600/1000000  loss         0.119357  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.415\n",
      "iter 15601/1000000  loss         0.119357  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.415\n",
      "iter 15700/1000000  loss         0.119292  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.419\n",
      "iter 15701/1000000  loss         0.119292  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.420\n",
      "iter 15800/1000000  loss         0.119228  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.424\n",
      "iter 15801/1000000  loss         0.119227  avg_L1_norm_grad         0.000042  w[0]    0.036 bias    2.424\n",
      "iter 15900/1000000  loss         0.119164  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.429\n",
      "iter 15901/1000000  loss         0.119163  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.429\n",
      "iter 16000/1000000  loss         0.119101  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.434\n",
      "iter 16001/1000000  loss         0.119100  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.434\n",
      "iter 16100/1000000  loss         0.119039  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.438\n",
      "iter 16101/1000000  loss         0.119038  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.438\n",
      "iter 16200/1000000  loss         0.118977  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.443\n",
      "iter 16201/1000000  loss         0.118976  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.443\n",
      "iter 16300/1000000  loss         0.118916  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.448\n",
      "iter 16301/1000000  loss         0.118915  avg_L1_norm_grad         0.000041  w[0]    0.036 bias    2.448\n",
      "iter 16400/1000000  loss         0.118855  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.452\n",
      "iter 16401/1000000  loss         0.118855  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.452\n",
      "iter 16500/1000000  loss         0.118796  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.457\n",
      "iter 16501/1000000  loss         0.118795  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.457\n",
      "iter 16600/1000000  loss         0.118737  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.461\n",
      "iter 16601/1000000  loss         0.118736  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.461\n",
      "iter 16700/1000000  loss         0.118678  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.466\n",
      "iter 16701/1000000  loss         0.118677  avg_L1_norm_grad         0.000040  w[0]    0.036 bias    2.466\n",
      "iter 16800/1000000  loss         0.118620  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.470\n",
      "iter 16801/1000000  loss         0.118620  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.470\n",
      "iter 16900/1000000  loss         0.118563  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.475\n",
      "iter 16901/1000000  loss         0.118562  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.475\n",
      "iter 17000/1000000  loss         0.118506  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.479\n",
      "iter 17001/1000000  loss         0.118506  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.479\n",
      "iter 17100/1000000  loss         0.118450  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.484\n",
      "iter 17101/1000000  loss         0.118449  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.484\n",
      "iter 17200/1000000  loss         0.118394  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.488\n",
      "iter 17201/1000000  loss         0.118394  avg_L1_norm_grad         0.000039  w[0]    0.036 bias    2.488\n",
      "iter 17300/1000000  loss         0.118339  avg_L1_norm_grad         0.000038  w[0]    0.036 bias    2.493\n",
      "iter 17301/1000000  loss         0.118339  avg_L1_norm_grad         0.000038  w[0]    0.036 bias    2.493\n",
      "iter 17400/1000000  loss         0.118285  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.497\n",
      "iter 17401/1000000  loss         0.118284  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.497\n",
      "iter 17500/1000000  loss         0.118231  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.501\n",
      "iter 17501/1000000  loss         0.118231  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.501\n",
      "iter 17600/1000000  loss         0.118178  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.506\n",
      "iter 17601/1000000  loss         0.118177  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.506\n",
      "iter 17700/1000000  loss         0.118125  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.510\n",
      "iter 17701/1000000  loss         0.118125  avg_L1_norm_grad         0.000038  w[0]    0.037 bias    2.510\n",
      "iter 17800/1000000  loss         0.118073  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.514\n",
      "iter 17801/1000000  loss         0.118072  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.514\n",
      "iter 17900/1000000  loss         0.118021  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.518\n",
      "iter 17901/1000000  loss         0.118021  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.518\n",
      "iter 18000/1000000  loss         0.117970  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.523\n",
      "iter 18001/1000000  loss         0.117969  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.523\n",
      "iter 18100/1000000  loss         0.117919  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.527\n",
      "iter 18101/1000000  loss         0.117919  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.527\n",
      "iter 18200/1000000  loss         0.117869  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.531\n",
      "iter 18201/1000000  loss         0.117869  avg_L1_norm_grad         0.000037  w[0]    0.037 bias    2.531\n",
      "iter 18300/1000000  loss         0.117819  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.535\n",
      "iter 18301/1000000  loss         0.117819  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.535\n",
      "iter 18400/1000000  loss         0.117770  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.539\n",
      "iter 18401/1000000  loss         0.117770  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.539\n",
      "iter 18500/1000000  loss         0.117721  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18501/1000000  loss         0.117721  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.544\n",
      "iter 18600/1000000  loss         0.117673  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.548\n",
      "iter 18601/1000000  loss         0.117673  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.548\n",
      "iter 18700/1000000  loss         0.117625  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.552\n",
      "iter 18701/1000000  loss         0.117625  avg_L1_norm_grad         0.000036  w[0]    0.037 bias    2.552\n",
      "iter 18800/1000000  loss         0.117578  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.556\n",
      "iter 18801/1000000  loss         0.117578  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.556\n",
      "iter 18900/1000000  loss         0.117531  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.560\n",
      "iter 18901/1000000  loss         0.117531  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.560\n",
      "iter 19000/1000000  loss         0.117485  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.564\n",
      "iter 19001/1000000  loss         0.117484  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.564\n",
      "iter 19100/1000000  loss         0.117439  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.568\n",
      "iter 19101/1000000  loss         0.117438  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.568\n",
      "iter 19200/1000000  loss         0.117393  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.572\n",
      "iter 19201/1000000  loss         0.117393  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.572\n",
      "iter 19300/1000000  loss         0.117348  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.576\n",
      "iter 19301/1000000  loss         0.117348  avg_L1_norm_grad         0.000035  w[0]    0.037 bias    2.576\n",
      "iter 19400/1000000  loss         0.117303  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.580\n",
      "iter 19401/1000000  loss         0.117303  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.580\n",
      "iter 19500/1000000  loss         0.117259  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.584\n",
      "iter 19501/1000000  loss         0.117259  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.584\n",
      "iter 19600/1000000  loss         0.117215  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.588\n",
      "iter 19601/1000000  loss         0.117215  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.588\n",
      "iter 19700/1000000  loss         0.117172  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.592\n",
      "iter 19701/1000000  loss         0.117172  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.592\n",
      "iter 19800/1000000  loss         0.117129  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.595\n",
      "iter 19801/1000000  loss         0.117129  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.595\n",
      "iter 19900/1000000  loss         0.117086  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.599\n",
      "iter 19901/1000000  loss         0.117086  avg_L1_norm_grad         0.000034  w[0]    0.037 bias    2.599\n",
      "iter 20000/1000000  loss         0.117044  avg_L1_norm_grad         0.000033  w[0]    0.037 bias    2.603\n",
      "iter 20001/1000000  loss         0.117044  avg_L1_norm_grad         0.000033  w[0]    0.037 bias    2.603\n",
      "iter 20100/1000000  loss         0.117002  avg_L1_norm_grad         0.000033  w[0]    0.037 bias    2.607\n",
      "iter 20101/1000000  loss         0.117002  avg_L1_norm_grad         0.000033  w[0]    0.037 bias    2.607\n",
      "iter 20200/1000000  loss         0.116961  avg_L1_norm_grad         0.000033  w[0]    0.037 bias    2.611\n",
      "iter 20201/1000000  loss         0.116960  avg_L1_norm_grad         0.000033  w[0]    0.037 bias    2.611\n",
      "iter 20300/1000000  loss         0.116920  avg_L1_norm_grad         0.000033  w[0]    0.038 bias    2.615\n",
      "iter 20301/1000000  loss         0.116919  avg_L1_norm_grad         0.000033  w[0]    0.038 bias    2.615\n",
      "iter 20400/1000000  loss         0.116879  avg_L1_norm_grad         0.000033  w[0]    0.038 bias    2.618\n",
      "iter 20401/1000000  loss         0.116879  avg_L1_norm_grad         0.000033  w[0]    0.038 bias    2.618\n",
      "iter 20500/1000000  loss         0.116839  avg_L1_norm_grad         0.000033  w[0]    0.038 bias    2.622\n",
      "iter 20501/1000000  loss         0.116838  avg_L1_norm_grad         0.000033  w[0]    0.038 bias    2.622\n",
      "iter 20600/1000000  loss         0.116799  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.626\n",
      "iter 20601/1000000  loss         0.116798  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.626\n",
      "iter 20700/1000000  loss         0.116759  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.630\n",
      "iter 20701/1000000  loss         0.116759  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.630\n",
      "iter 20800/1000000  loss         0.116720  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.633\n",
      "iter 20801/1000000  loss         0.116720  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.633\n",
      "iter 20900/1000000  loss         0.116681  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.637\n",
      "iter 20901/1000000  loss         0.116681  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.637\n",
      "iter 21000/1000000  loss         0.116643  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.641\n",
      "iter 21001/1000000  loss         0.116642  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.641\n",
      "iter 21100/1000000  loss         0.116605  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.644\n",
      "iter 21101/1000000  loss         0.116604  avg_L1_norm_grad         0.000032  w[0]    0.038 bias    2.644\n",
      "iter 21200/1000000  loss         0.116567  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.648\n",
      "iter 21201/1000000  loss         0.116566  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.648\n",
      "iter 21300/1000000  loss         0.116529  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.651\n",
      "iter 21301/1000000  loss         0.116529  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.651\n",
      "iter 21400/1000000  loss         0.116492  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.655\n",
      "iter 21401/1000000  loss         0.116492  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.655\n",
      "iter 21500/1000000  loss         0.116455  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.659\n",
      "iter 21501/1000000  loss         0.116455  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.659\n",
      "iter 21600/1000000  loss         0.116419  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.662\n",
      "iter 21601/1000000  loss         0.116418  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.662\n",
      "iter 21700/1000000  loss         0.116382  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.666\n",
      "iter 21701/1000000  loss         0.116382  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.666\n",
      "iter 21800/1000000  loss         0.116347  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.669\n",
      "iter 21801/1000000  loss         0.116346  avg_L1_norm_grad         0.000031  w[0]    0.038 bias    2.669\n",
      "iter 21900/1000000  loss         0.116311  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.673\n",
      "iter 21901/1000000  loss         0.116311  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.673\n",
      "iter 22000/1000000  loss         0.116276  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.676\n",
      "iter 22001/1000000  loss         0.116275  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.676\n",
      "iter 22100/1000000  loss         0.116241  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.680\n",
      "iter 22101/1000000  loss         0.116241  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.680\n",
      "iter 22200/1000000  loss         0.116206  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.683\n",
      "iter 22201/1000000  loss         0.116206  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.683\n",
      "iter 22300/1000000  loss         0.116172  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.687\n",
      "iter 22301/1000000  loss         0.116172  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.687\n",
      "iter 22400/1000000  loss         0.116138  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22401/1000000  loss         0.116138  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.690\n",
      "iter 22500/1000000  loss         0.116104  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.694\n",
      "iter 22501/1000000  loss         0.116104  avg_L1_norm_grad         0.000030  w[0]    0.038 bias    2.694\n",
      "iter 22600/1000000  loss         0.116071  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.697\n",
      "iter 22601/1000000  loss         0.116070  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.697\n",
      "iter 22700/1000000  loss         0.116037  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.700\n",
      "iter 22701/1000000  loss         0.116037  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.700\n",
      "iter 22800/1000000  loss         0.116005  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.704\n",
      "iter 22801/1000000  loss         0.116004  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.704\n",
      "iter 22900/1000000  loss         0.115972  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.707\n",
      "iter 22901/1000000  loss         0.115972  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.707\n",
      "iter 23000/1000000  loss         0.115940  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.710\n",
      "iter 23001/1000000  loss         0.115939  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.710\n",
      "iter 23100/1000000  loss         0.115908  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.714\n",
      "iter 23101/1000000  loss         0.115907  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.714\n",
      "iter 23200/1000000  loss         0.115876  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.717\n",
      "iter 23201/1000000  loss         0.115875  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.717\n",
      "iter 23300/1000000  loss         0.115844  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.720\n",
      "iter 23301/1000000  loss         0.115844  avg_L1_norm_grad         0.000029  w[0]    0.038 bias    2.720\n",
      "iter 23400/1000000  loss         0.115813  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.724\n",
      "iter 23401/1000000  loss         0.115813  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.724\n",
      "iter 23500/1000000  loss         0.115782  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.727\n",
      "iter 23501/1000000  loss         0.115782  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.727\n",
      "iter 23600/1000000  loss         0.115751  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.730\n",
      "iter 23601/1000000  loss         0.115751  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.730\n",
      "iter 23700/1000000  loss         0.115721  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.733\n",
      "iter 23701/1000000  loss         0.115720  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.733\n",
      "iter 23800/1000000  loss         0.115691  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.737\n",
      "iter 23801/1000000  loss         0.115690  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.737\n",
      "iter 23900/1000000  loss         0.115661  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.740\n",
      "iter 23901/1000000  loss         0.115660  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.740\n",
      "iter 24000/1000000  loss         0.115631  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.743\n",
      "iter 24001/1000000  loss         0.115631  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.743\n",
      "iter 24100/1000000  loss         0.115601  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.746\n",
      "iter 24101/1000000  loss         0.115601  avg_L1_norm_grad         0.000028  w[0]    0.038 bias    2.746\n",
      "iter 24200/1000000  loss         0.115572  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.749\n",
      "iter 24201/1000000  loss         0.115572  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.749\n",
      "iter 24300/1000000  loss         0.115543  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.753\n",
      "iter 24301/1000000  loss         0.115543  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.753\n",
      "iter 24400/1000000  loss         0.115514  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.756\n",
      "iter 24401/1000000  loss         0.115514  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.756\n",
      "iter 24500/1000000  loss         0.115486  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.759\n",
      "iter 24501/1000000  loss         0.115486  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.759\n",
      "iter 24600/1000000  loss         0.115458  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.762\n",
      "iter 24601/1000000  loss         0.115457  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.762\n",
      "iter 24700/1000000  loss         0.115429  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.765\n",
      "iter 24701/1000000  loss         0.115429  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.765\n",
      "iter 24800/1000000  loss         0.115402  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.768\n",
      "iter 24801/1000000  loss         0.115401  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.768\n",
      "iter 24900/1000000  loss         0.115374  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.771\n",
      "iter 24901/1000000  loss         0.115374  avg_L1_norm_grad         0.000027  w[0]    0.038 bias    2.771\n",
      "iter 25000/1000000  loss         0.115347  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.774\n",
      "iter 25001/1000000  loss         0.115346  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.774\n",
      "iter 25100/1000000  loss         0.115319  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.777\n",
      "iter 25101/1000000  loss         0.115319  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.777\n",
      "iter 25200/1000000  loss         0.115292  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.780\n",
      "iter 25201/1000000  loss         0.115292  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.780\n",
      "iter 25300/1000000  loss         0.115266  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.783\n",
      "iter 25301/1000000  loss         0.115265  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.783\n",
      "iter 25400/1000000  loss         0.115239  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.786\n",
      "iter 25401/1000000  loss         0.115239  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.786\n",
      "iter 25500/1000000  loss         0.115213  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.789\n",
      "iter 25501/1000000  loss         0.115213  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.789\n",
      "iter 25600/1000000  loss         0.115187  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.792\n",
      "iter 25601/1000000  loss         0.115186  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.792\n",
      "iter 25700/1000000  loss         0.115161  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.795\n",
      "iter 25701/1000000  loss         0.115161  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.795\n",
      "iter 25800/1000000  loss         0.115135  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.798\n",
      "iter 25801/1000000  loss         0.115135  avg_L1_norm_grad         0.000026  w[0]    0.038 bias    2.798\n",
      "iter 25900/1000000  loss         0.115110  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.801\n",
      "iter 25901/1000000  loss         0.115109  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.801\n",
      "iter 26000/1000000  loss         0.115084  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.804\n",
      "iter 26001/1000000  loss         0.115084  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.804\n",
      "iter 26100/1000000  loss         0.115059  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.807\n",
      "iter 26101/1000000  loss         0.115059  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.807\n",
      "iter 26200/1000000  loss         0.115034  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.810\n",
      "iter 26201/1000000  loss         0.115034  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.810\n",
      "iter 26300/1000000  loss         0.115010  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26301/1000000  loss         0.115009  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.813\n",
      "iter 26400/1000000  loss         0.114985  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.816\n",
      "iter 26401/1000000  loss         0.114985  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.816\n",
      "iter 26500/1000000  loss         0.114961  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.819\n",
      "iter 26501/1000000  loss         0.114961  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.819\n",
      "iter 26600/1000000  loss         0.114937  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.822\n",
      "iter 26601/1000000  loss         0.114936  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.822\n",
      "iter 26700/1000000  loss         0.114913  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.824\n",
      "iter 26701/1000000  loss         0.114912  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.824\n",
      "iter 26800/1000000  loss         0.114889  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.827\n",
      "iter 26801/1000000  loss         0.114889  avg_L1_norm_grad         0.000025  w[0]    0.039 bias    2.827\n",
      "iter 26900/1000000  loss         0.114865  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.830\n",
      "iter 26901/1000000  loss         0.114865  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.830\n",
      "iter 27000/1000000  loss         0.114842  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.833\n",
      "iter 27001/1000000  loss         0.114842  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.833\n",
      "iter 27100/1000000  loss         0.114819  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.836\n",
      "iter 27101/1000000  loss         0.114819  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.836\n",
      "iter 27200/1000000  loss         0.114796  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.839\n",
      "iter 27201/1000000  loss         0.114795  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.839\n",
      "iter 27300/1000000  loss         0.114773  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.841\n",
      "iter 27301/1000000  loss         0.114773  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.841\n",
      "iter 27400/1000000  loss         0.114750  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.844\n",
      "iter 27401/1000000  loss         0.114750  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.844\n",
      "iter 27500/1000000  loss         0.114728  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.847\n",
      "iter 27501/1000000  loss         0.114727  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.847\n",
      "iter 27600/1000000  loss         0.114705  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.850\n",
      "iter 27601/1000000  loss         0.114705  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.850\n",
      "iter 27700/1000000  loss         0.114683  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.852\n",
      "iter 27701/1000000  loss         0.114683  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.852\n",
      "iter 27800/1000000  loss         0.114661  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.855\n",
      "iter 27801/1000000  loss         0.114661  avg_L1_norm_grad         0.000024  w[0]    0.039 bias    2.855\n",
      "iter 27900/1000000  loss         0.114639  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.858\n",
      "iter 27901/1000000  loss         0.114639  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.858\n",
      "iter 28000/1000000  loss         0.114618  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.860\n",
      "iter 28001/1000000  loss         0.114617  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.860\n",
      "iter 28100/1000000  loss         0.114596  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.863\n",
      "iter 28101/1000000  loss         0.114596  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.863\n",
      "iter 28200/1000000  loss         0.114575  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.866\n",
      "iter 28201/1000000  loss         0.114574  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.866\n",
      "iter 28300/1000000  loss         0.114553  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.869\n",
      "iter 28301/1000000  loss         0.114553  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.869\n",
      "iter 28400/1000000  loss         0.114532  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.871\n",
      "iter 28401/1000000  loss         0.114532  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.871\n",
      "iter 28500/1000000  loss         0.114512  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.874\n",
      "iter 28501/1000000  loss         0.114511  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.874\n",
      "iter 28600/1000000  loss         0.114491  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.876\n",
      "iter 28601/1000000  loss         0.114491  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.876\n",
      "iter 28700/1000000  loss         0.114470  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.879\n",
      "iter 28701/1000000  loss         0.114470  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.879\n",
      "iter 28800/1000000  loss         0.114450  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.882\n",
      "iter 28801/1000000  loss         0.114450  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.882\n",
      "iter 28900/1000000  loss         0.114430  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.884\n",
      "iter 28901/1000000  loss         0.114429  avg_L1_norm_grad         0.000023  w[0]    0.039 bias    2.884\n",
      "iter 29000/1000000  loss         0.114409  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.887\n",
      "iter 29001/1000000  loss         0.114409  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.887\n",
      "iter 29100/1000000  loss         0.114389  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.889\n",
      "iter 29101/1000000  loss         0.114389  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.889\n",
      "iter 29200/1000000  loss         0.114370  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.892\n",
      "iter 29201/1000000  loss         0.114369  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.892\n",
      "iter 29300/1000000  loss         0.114350  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.895\n",
      "iter 29301/1000000  loss         0.114350  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.895\n",
      "iter 29400/1000000  loss         0.114330  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.897\n",
      "iter 29401/1000000  loss         0.114330  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.897\n",
      "iter 29500/1000000  loss         0.114311  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.900\n",
      "iter 29501/1000000  loss         0.114311  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.900\n",
      "iter 29600/1000000  loss         0.114292  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.902\n",
      "iter 29601/1000000  loss         0.114292  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.902\n",
      "iter 29700/1000000  loss         0.114273  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.905\n",
      "iter 29701/1000000  loss         0.114273  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.905\n",
      "iter 29800/1000000  loss         0.114254  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.907\n",
      "iter 29801/1000000  loss         0.114254  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.907\n",
      "iter 29900/1000000  loss         0.114235  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.910\n",
      "iter 29901/1000000  loss         0.114235  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.910\n",
      "iter 30000/1000000  loss         0.114216  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.912\n",
      "iter 30001/1000000  loss         0.114216  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.912\n",
      "iter 30100/1000000  loss         0.114198  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.915\n",
      "iter 30101/1000000  loss         0.114197  avg_L1_norm_grad         0.000022  w[0]    0.039 bias    2.915\n",
      "iter 30200/1000000  loss         0.114179  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30201/1000000  loss         0.114179  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.917\n",
      "iter 30300/1000000  loss         0.114161  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.920\n",
      "iter 30301/1000000  loss         0.114161  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.920\n",
      "iter 30400/1000000  loss         0.114143  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.922\n",
      "iter 30401/1000000  loss         0.114143  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.922\n",
      "iter 30500/1000000  loss         0.114125  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.925\n",
      "iter 30501/1000000  loss         0.114125  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.925\n",
      "iter 30600/1000000  loss         0.114107  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.927\n",
      "iter 30601/1000000  loss         0.114107  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.927\n",
      "iter 30700/1000000  loss         0.114089  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.929\n",
      "iter 30701/1000000  loss         0.114089  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.929\n",
      "iter 30800/1000000  loss         0.114071  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.932\n",
      "iter 30801/1000000  loss         0.114071  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.932\n",
      "iter 30900/1000000  loss         0.114054  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.934\n",
      "iter 30901/1000000  loss         0.114054  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.934\n",
      "iter 31000/1000000  loss         0.114036  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.937\n",
      "iter 31001/1000000  loss         0.114036  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.937\n",
      "iter 31100/1000000  loss         0.114019  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.939\n",
      "iter 31101/1000000  loss         0.114019  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.939\n",
      "iter 31200/1000000  loss         0.114002  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.941\n",
      "iter 31201/1000000  loss         0.114002  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.941\n",
      "iter 31300/1000000  loss         0.113985  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.944\n",
      "iter 31301/1000000  loss         0.113985  avg_L1_norm_grad         0.000021  w[0]    0.039 bias    2.944\n",
      "iter 31400/1000000  loss         0.113968  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.946\n",
      "iter 31401/1000000  loss         0.113968  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.946\n",
      "iter 31500/1000000  loss         0.113951  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.948\n",
      "iter 31501/1000000  loss         0.113951  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.948\n",
      "iter 31600/1000000  loss         0.113935  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.951\n",
      "iter 31601/1000000  loss         0.113934  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.951\n",
      "iter 31700/1000000  loss         0.113918  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.953\n",
      "iter 31701/1000000  loss         0.113918  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.953\n",
      "iter 31800/1000000  loss         0.113902  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.955\n",
      "iter 31801/1000000  loss         0.113901  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.955\n",
      "iter 31900/1000000  loss         0.113885  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.958\n",
      "iter 31901/1000000  loss         0.113885  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.958\n",
      "iter 32000/1000000  loss         0.113869  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.960\n",
      "iter 32001/1000000  loss         0.113869  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.960\n",
      "iter 32100/1000000  loss         0.113853  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.962\n",
      "iter 32101/1000000  loss         0.113853  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.962\n",
      "iter 32200/1000000  loss         0.113837  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.965\n",
      "iter 32201/1000000  loss         0.113837  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.965\n",
      "iter 32300/1000000  loss         0.113821  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.967\n",
      "iter 32301/1000000  loss         0.113821  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.967\n",
      "iter 32400/1000000  loss         0.113805  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.969\n",
      "iter 32401/1000000  loss         0.113805  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.969\n",
      "iter 32500/1000000  loss         0.113790  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.971\n",
      "iter 32501/1000000  loss         0.113790  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.971\n",
      "iter 32600/1000000  loss         0.113774  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.974\n",
      "iter 32601/1000000  loss         0.113774  avg_L1_norm_grad         0.000020  w[0]    0.039 bias    2.974\n",
      "iter 32700/1000000  loss         0.113759  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.976\n",
      "iter 32701/1000000  loss         0.113759  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.976\n",
      "iter 32800/1000000  loss         0.113743  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.978\n",
      "iter 32801/1000000  loss         0.113743  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.978\n",
      "iter 32900/1000000  loss         0.113728  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.980\n",
      "iter 32901/1000000  loss         0.113728  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.980\n",
      "iter 33000/1000000  loss         0.113713  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.983\n",
      "iter 33001/1000000  loss         0.113713  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.983\n",
      "iter 33100/1000000  loss         0.113698  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.985\n",
      "iter 33101/1000000  loss         0.113698  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.985\n",
      "iter 33200/1000000  loss         0.113683  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.987\n",
      "iter 33201/1000000  loss         0.113683  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.987\n",
      "iter 33300/1000000  loss         0.113668  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.989\n",
      "iter 33301/1000000  loss         0.113668  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.989\n",
      "iter 33400/1000000  loss         0.113654  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.991\n",
      "iter 33401/1000000  loss         0.113653  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.991\n",
      "iter 33500/1000000  loss         0.113639  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.994\n",
      "iter 33501/1000000  loss         0.113639  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.994\n",
      "iter 33600/1000000  loss         0.113625  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.996\n",
      "iter 33601/1000000  loss         0.113624  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.996\n",
      "iter 33700/1000000  loss         0.113610  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.998\n",
      "iter 33701/1000000  loss         0.113610  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    2.998\n",
      "iter 33800/1000000  loss         0.113596  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.000\n",
      "iter 33801/1000000  loss         0.113596  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.000\n",
      "iter 33900/1000000  loss         0.113582  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.002\n",
      "iter 33901/1000000  loss         0.113581  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.002\n",
      "iter 34000/1000000  loss         0.113567  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.004\n",
      "iter 34001/1000000  loss         0.113567  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.004\n",
      "iter 34100/1000000  loss         0.113553  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34101/1000000  loss         0.113553  avg_L1_norm_grad         0.000019  w[0]    0.039 bias    3.006\n",
      "iter 34200/1000000  loss         0.113540  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.009\n",
      "iter 34201/1000000  loss         0.113539  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.009\n",
      "iter 34300/1000000  loss         0.113526  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.011\n",
      "iter 34301/1000000  loss         0.113526  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.011\n",
      "iter 34400/1000000  loss         0.113512  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.013\n",
      "iter 34401/1000000  loss         0.113512  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.013\n",
      "iter 34500/1000000  loss         0.113498  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.015\n",
      "iter 34501/1000000  loss         0.113498  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.015\n",
      "iter 34600/1000000  loss         0.113485  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.017\n",
      "iter 34601/1000000  loss         0.113485  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.017\n",
      "iter 34700/1000000  loss         0.113471  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.019\n",
      "iter 34701/1000000  loss         0.113471  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.019\n",
      "iter 34800/1000000  loss         0.113458  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.021\n",
      "iter 34801/1000000  loss         0.113458  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.021\n",
      "iter 34900/1000000  loss         0.113445  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.023\n",
      "iter 34901/1000000  loss         0.113444  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.023\n",
      "iter 35000/1000000  loss         0.113431  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.025\n",
      "iter 35001/1000000  loss         0.113431  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.025\n",
      "iter 35100/1000000  loss         0.113418  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.027\n",
      "iter 35101/1000000  loss         0.113418  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.027\n",
      "iter 35200/1000000  loss         0.113405  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.029\n",
      "iter 35201/1000000  loss         0.113405  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.029\n",
      "iter 35300/1000000  loss         0.113392  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.031\n",
      "iter 35301/1000000  loss         0.113392  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.031\n",
      "iter 35400/1000000  loss         0.113380  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.033\n",
      "iter 35401/1000000  loss         0.113379  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.033\n",
      "iter 35500/1000000  loss         0.113367  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.035\n",
      "iter 35501/1000000  loss         0.113367  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.035\n",
      "iter 35600/1000000  loss         0.113354  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.037\n",
      "iter 35601/1000000  loss         0.113354  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.037\n",
      "iter 35700/1000000  loss         0.113341  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.039\n",
      "iter 35701/1000000  loss         0.113341  avg_L1_norm_grad         0.000018  w[0]    0.039 bias    3.039\n",
      "iter 35800/1000000  loss         0.113329  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.041\n",
      "iter 35801/1000000  loss         0.113329  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.041\n",
      "iter 35900/1000000  loss         0.113317  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.043\n",
      "iter 35901/1000000  loss         0.113316  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.043\n",
      "iter 36000/1000000  loss         0.113304  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.045\n",
      "iter 36001/1000000  loss         0.113304  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.045\n",
      "iter 36100/1000000  loss         0.113292  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.047\n",
      "iter 36101/1000000  loss         0.113292  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.047\n",
      "iter 36200/1000000  loss         0.113280  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.049\n",
      "iter 36201/1000000  loss         0.113280  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.049\n",
      "iter 36300/1000000  loss         0.113268  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.051\n",
      "iter 36301/1000000  loss         0.113268  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.051\n",
      "iter 36400/1000000  loss         0.113256  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.053\n",
      "iter 36401/1000000  loss         0.113256  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.053\n",
      "iter 36500/1000000  loss         0.113244  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.055\n",
      "iter 36501/1000000  loss         0.113244  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.055\n",
      "iter 36600/1000000  loss         0.113232  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.057\n",
      "iter 36601/1000000  loss         0.113232  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.057\n",
      "iter 36700/1000000  loss         0.113220  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.059\n",
      "iter 36701/1000000  loss         0.113220  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.059\n",
      "iter 36800/1000000  loss         0.113208  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.061\n",
      "iter 36801/1000000  loss         0.113208  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.061\n",
      "iter 36900/1000000  loss         0.113197  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.063\n",
      "iter 36901/1000000  loss         0.113197  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.063\n",
      "iter 37000/1000000  loss         0.113185  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.065\n",
      "iter 37001/1000000  loss         0.113185  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.065\n",
      "iter 37100/1000000  loss         0.113174  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.067\n",
      "iter 37101/1000000  loss         0.113174  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.067\n",
      "iter 37200/1000000  loss         0.113162  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.068\n",
      "iter 37201/1000000  loss         0.113162  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.068\n",
      "iter 37300/1000000  loss         0.113151  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.070\n",
      "iter 37301/1000000  loss         0.113151  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.070\n",
      "iter 37400/1000000  loss         0.113140  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.072\n",
      "iter 37401/1000000  loss         0.113140  avg_L1_norm_grad         0.000017  w[0]    0.039 bias    3.072\n",
      "iter 37500/1000000  loss         0.113128  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.074\n",
      "iter 37501/1000000  loss         0.113128  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.074\n",
      "iter 37600/1000000  loss         0.113117  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.076\n",
      "iter 37601/1000000  loss         0.113117  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.076\n",
      "iter 37700/1000000  loss         0.113106  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.078\n",
      "iter 37701/1000000  loss         0.113106  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.078\n",
      "iter 37800/1000000  loss         0.113095  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.080\n",
      "iter 37801/1000000  loss         0.113095  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.080\n",
      "iter 37900/1000000  loss         0.113084  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.081\n",
      "iter 37901/1000000  loss         0.113084  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.081\n",
      "iter 38000/1000000  loss         0.113074  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38001/1000000  loss         0.113073  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.083\n",
      "iter 38100/1000000  loss         0.113063  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.085\n",
      "iter 38101/1000000  loss         0.113063  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.085\n",
      "iter 38200/1000000  loss         0.113052  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.087\n",
      "iter 38201/1000000  loss         0.113052  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.087\n",
      "iter 38300/1000000  loss         0.113041  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.089\n",
      "iter 38301/1000000  loss         0.113041  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.089\n",
      "iter 38400/1000000  loss         0.113031  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.091\n",
      "iter 38401/1000000  loss         0.113031  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.091\n",
      "iter 38500/1000000  loss         0.113020  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.092\n",
      "iter 38501/1000000  loss         0.113020  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.092\n",
      "iter 38600/1000000  loss         0.113010  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.094\n",
      "iter 38601/1000000  loss         0.113010  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.094\n",
      "iter 38700/1000000  loss         0.113000  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.096\n",
      "iter 38701/1000000  loss         0.113000  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.096\n",
      "iter 38800/1000000  loss         0.112989  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.098\n",
      "iter 38801/1000000  loss         0.112989  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.098\n",
      "iter 38900/1000000  loss         0.112979  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.100\n",
      "iter 38901/1000000  loss         0.112979  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.100\n",
      "iter 39000/1000000  loss         0.112969  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.101\n",
      "iter 39001/1000000  loss         0.112969  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.101\n",
      "iter 39100/1000000  loss         0.112959  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.103\n",
      "iter 39101/1000000  loss         0.112959  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.103\n",
      "iter 39200/1000000  loss         0.112949  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.105\n",
      "iter 39201/1000000  loss         0.112949  avg_L1_norm_grad         0.000016  w[0]    0.039 bias    3.105\n",
      "iter 39300/1000000  loss         0.112939  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.107\n",
      "iter 39301/1000000  loss         0.112939  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.107\n",
      "iter 39400/1000000  loss         0.112929  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.108\n",
      "iter 39401/1000000  loss         0.112929  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.108\n",
      "iter 39500/1000000  loss         0.112919  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.110\n",
      "iter 39501/1000000  loss         0.112919  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.110\n",
      "iter 39600/1000000  loss         0.112909  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.112\n",
      "iter 39601/1000000  loss         0.112909  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.112\n",
      "iter 39700/1000000  loss         0.112900  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.114\n",
      "iter 39701/1000000  loss         0.112900  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.114\n",
      "iter 39800/1000000  loss         0.112890  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.115\n",
      "iter 39801/1000000  loss         0.112890  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.115\n",
      "iter 39900/1000000  loss         0.112880  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.117\n",
      "iter 39901/1000000  loss         0.112880  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.117\n",
      "iter 40000/1000000  loss         0.112871  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.119\n",
      "iter 40001/1000000  loss         0.112871  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.119\n",
      "iter 40100/1000000  loss         0.112861  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.120\n",
      "iter 40101/1000000  loss         0.112861  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.120\n",
      "iter 40200/1000000  loss         0.112852  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.122\n",
      "iter 40201/1000000  loss         0.112852  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.122\n",
      "iter 40300/1000000  loss         0.112843  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.124\n",
      "iter 40301/1000000  loss         0.112843  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.124\n",
      "iter 40400/1000000  loss         0.112833  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.125\n",
      "iter 40401/1000000  loss         0.112833  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.125\n",
      "iter 40500/1000000  loss         0.112824  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.127\n",
      "iter 40501/1000000  loss         0.112824  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.127\n",
      "iter 40600/1000000  loss         0.112815  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.129\n",
      "iter 40601/1000000  loss         0.112815  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.129\n",
      "iter 40700/1000000  loss         0.112806  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.130\n",
      "iter 40701/1000000  loss         0.112806  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.130\n",
      "iter 40800/1000000  loss         0.112797  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.132\n",
      "iter 40801/1000000  loss         0.112797  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.132\n",
      "iter 40900/1000000  loss         0.112788  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.134\n",
      "iter 40901/1000000  loss         0.112788  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.134\n",
      "iter 41000/1000000  loss         0.112779  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.135\n",
      "iter 41001/1000000  loss         0.112779  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.135\n",
      "iter 41100/1000000  loss         0.112770  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.137\n",
      "iter 41101/1000000  loss         0.112770  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.137\n",
      "iter 41200/1000000  loss         0.112761  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.139\n",
      "iter 41201/1000000  loss         0.112761  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.139\n",
      "iter 41300/1000000  loss         0.112752  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.140\n",
      "iter 41301/1000000  loss         0.112752  avg_L1_norm_grad         0.000015  w[0]    0.039 bias    3.140\n",
      "iter 41400/1000000  loss         0.112743  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.142\n",
      "iter 41401/1000000  loss         0.112743  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.142\n",
      "iter 41500/1000000  loss         0.112735  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.144\n",
      "iter 41501/1000000  loss         0.112735  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.144\n",
      "iter 41600/1000000  loss         0.112726  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.145\n",
      "iter 41601/1000000  loss         0.112726  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.145\n",
      "iter 41700/1000000  loss         0.112718  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.147\n",
      "iter 41701/1000000  loss         0.112718  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.147\n",
      "iter 41800/1000000  loss         0.112709  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.148\n",
      "iter 41801/1000000  loss         0.112709  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.148\n",
      "iter 41900/1000000  loss         0.112701  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 41901/1000000  loss         0.112701  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.150\n",
      "iter 42000/1000000  loss         0.112692  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.152\n",
      "iter 42001/1000000  loss         0.112692  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.152\n",
      "iter 42100/1000000  loss         0.112684  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.153\n",
      "iter 42101/1000000  loss         0.112684  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.153\n",
      "iter 42200/1000000  loss         0.112676  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.155\n",
      "iter 42201/1000000  loss         0.112675  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.155\n",
      "iter 42300/1000000  loss         0.112667  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.156\n",
      "iter 42301/1000000  loss         0.112667  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.156\n",
      "iter 42400/1000000  loss         0.112659  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.158\n",
      "iter 42401/1000000  loss         0.112659  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.158\n",
      "iter 42500/1000000  loss         0.112651  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.159\n",
      "iter 42501/1000000  loss         0.112651  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.160\n",
      "iter 42600/1000000  loss         0.112643  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.161\n",
      "iter 42601/1000000  loss         0.112643  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.161\n",
      "iter 42700/1000000  loss         0.112635  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.163\n",
      "iter 42701/1000000  loss         0.112635  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.163\n",
      "iter 42800/1000000  loss         0.112627  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.164\n",
      "iter 42801/1000000  loss         0.112627  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.164\n",
      "iter 42900/1000000  loss         0.112619  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.166\n",
      "iter 42901/1000000  loss         0.112619  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.166\n",
      "iter 43000/1000000  loss         0.112611  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.167\n",
      "iter 43001/1000000  loss         0.112611  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.167\n",
      "iter 43100/1000000  loss         0.112603  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.169\n",
      "iter 43101/1000000  loss         0.112603  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.169\n",
      "iter 43200/1000000  loss         0.112595  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.170\n",
      "iter 43201/1000000  loss         0.112595  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.170\n",
      "iter 43300/1000000  loss         0.112587  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.172\n",
      "iter 43301/1000000  loss         0.112587  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.172\n",
      "iter 43400/1000000  loss         0.112580  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.173\n",
      "iter 43401/1000000  loss         0.112580  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.173\n",
      "iter 43500/1000000  loss         0.112572  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.175\n",
      "iter 43501/1000000  loss         0.112572  avg_L1_norm_grad         0.000014  w[0]    0.039 bias    3.175\n",
      "iter 43600/1000000  loss         0.112564  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.176\n",
      "iter 43601/1000000  loss         0.112564  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.176\n",
      "iter 43700/1000000  loss         0.112557  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.178\n",
      "iter 43701/1000000  loss         0.112557  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.178\n",
      "iter 43800/1000000  loss         0.112549  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.179\n",
      "iter 43801/1000000  loss         0.112549  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.179\n",
      "iter 43900/1000000  loss         0.112542  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.181\n",
      "iter 43901/1000000  loss         0.112542  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.181\n",
      "iter 44000/1000000  loss         0.112534  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.182\n",
      "iter 44001/1000000  loss         0.112534  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.182\n",
      "iter 44100/1000000  loss         0.112527  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.184\n",
      "iter 44101/1000000  loss         0.112527  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.184\n",
      "iter 44200/1000000  loss         0.112520  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.185\n",
      "iter 44201/1000000  loss         0.112519  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.185\n",
      "iter 44300/1000000  loss         0.112512  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.187\n",
      "iter 44301/1000000  loss         0.112512  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.187\n",
      "iter 44400/1000000  loss         0.112505  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.188\n",
      "iter 44401/1000000  loss         0.112505  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.188\n",
      "iter 44500/1000000  loss         0.112498  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.190\n",
      "iter 44501/1000000  loss         0.112498  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.190\n",
      "iter 44600/1000000  loss         0.112491  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.191\n",
      "iter 44601/1000000  loss         0.112491  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.191\n",
      "iter 44700/1000000  loss         0.112483  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.193\n",
      "iter 44701/1000000  loss         0.112483  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.193\n",
      "iter 44800/1000000  loss         0.112476  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.194\n",
      "iter 44801/1000000  loss         0.112476  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.194\n",
      "iter 44900/1000000  loss         0.112469  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.195\n",
      "iter 44901/1000000  loss         0.112469  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.195\n",
      "iter 45000/1000000  loss         0.112462  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.197\n",
      "iter 45001/1000000  loss         0.112462  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.197\n",
      "iter 45100/1000000  loss         0.112455  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.198\n",
      "iter 45101/1000000  loss         0.112455  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.198\n",
      "iter 45200/1000000  loss         0.112448  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.200\n",
      "iter 45201/1000000  loss         0.112448  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.200\n",
      "iter 45300/1000000  loss         0.112442  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.201\n",
      "iter 45301/1000000  loss         0.112441  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.201\n",
      "iter 45400/1000000  loss         0.112435  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.203\n",
      "iter 45401/1000000  loss         0.112435  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.203\n",
      "iter 45500/1000000  loss         0.112428  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.204\n",
      "iter 45501/1000000  loss         0.112428  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.204\n",
      "iter 45600/1000000  loss         0.112421  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.205\n",
      "iter 45601/1000000  loss         0.112421  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.205\n",
      "iter 45700/1000000  loss         0.112414  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.207\n",
      "iter 45701/1000000  loss         0.112414  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.207\n",
      "iter 45800/1000000  loss         0.112408  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 45801/1000000  loss         0.112408  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.208\n",
      "iter 45900/1000000  loss         0.112401  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.210\n",
      "iter 45901/1000000  loss         0.112401  avg_L1_norm_grad         0.000013  w[0]    0.039 bias    3.210\n",
      "iter 46000/1000000  loss         0.112394  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.211\n",
      "iter 46001/1000000  loss         0.112394  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.211\n",
      "iter 46100/1000000  loss         0.112388  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.212\n",
      "iter 46101/1000000  loss         0.112388  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.212\n",
      "iter 46200/1000000  loss         0.112381  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.214\n",
      "iter 46201/1000000  loss         0.112381  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.214\n",
      "iter 46300/1000000  loss         0.112375  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.215\n",
      "iter 46301/1000000  loss         0.112375  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.215\n",
      "iter 46400/1000000  loss         0.112368  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.216\n",
      "iter 46401/1000000  loss         0.112368  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.216\n",
      "iter 46500/1000000  loss         0.112362  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.218\n",
      "iter 46501/1000000  loss         0.112362  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.218\n",
      "iter 46600/1000000  loss         0.112356  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.219\n",
      "iter 46601/1000000  loss         0.112356  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.219\n",
      "iter 46700/1000000  loss         0.112349  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.221\n",
      "iter 46701/1000000  loss         0.112349  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.221\n",
      "iter 46800/1000000  loss         0.112343  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.222\n",
      "iter 46801/1000000  loss         0.112343  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.222\n",
      "iter 46900/1000000  loss         0.112337  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.223\n",
      "iter 46901/1000000  loss         0.112337  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.223\n",
      "iter 47000/1000000  loss         0.112331  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.225\n",
      "iter 47001/1000000  loss         0.112331  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.225\n",
      "iter 47100/1000000  loss         0.112324  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.226\n",
      "iter 47101/1000000  loss         0.112324  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.226\n",
      "iter 47200/1000000  loss         0.112318  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.227\n",
      "iter 47201/1000000  loss         0.112318  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.227\n",
      "iter 47300/1000000  loss         0.112312  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.229\n",
      "iter 47301/1000000  loss         0.112312  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.229\n",
      "iter 47400/1000000  loss         0.112306  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.230\n",
      "iter 47401/1000000  loss         0.112306  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.230\n",
      "iter 47500/1000000  loss         0.112300  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.231\n",
      "iter 47501/1000000  loss         0.112300  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.231\n",
      "iter 47600/1000000  loss         0.112294  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.232\n",
      "iter 47601/1000000  loss         0.112294  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.232\n",
      "iter 47700/1000000  loss         0.112288  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.234\n",
      "iter 47701/1000000  loss         0.112288  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.234\n",
      "iter 47800/1000000  loss         0.112282  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.235\n",
      "iter 47801/1000000  loss         0.112282  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.235\n",
      "iter 47900/1000000  loss         0.112276  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.236\n",
      "iter 47901/1000000  loss         0.112276  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.236\n",
      "iter 48000/1000000  loss         0.112270  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.238\n",
      "iter 48001/1000000  loss         0.112270  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.238\n",
      "iter 48100/1000000  loss         0.112265  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.239\n",
      "iter 48101/1000000  loss         0.112264  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.239\n",
      "iter 48200/1000000  loss         0.112259  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.240\n",
      "iter 48201/1000000  loss         0.112259  avg_L1_norm_grad         0.000012  w[0]    0.039 bias    3.240\n",
      "iter 48300/1000000  loss         0.112253  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.242\n",
      "iter 48301/1000000  loss         0.112253  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.242\n",
      "iter 48400/1000000  loss         0.112247  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.243\n",
      "iter 48401/1000000  loss         0.112247  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.243\n",
      "iter 48500/1000000  loss         0.112242  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.244\n",
      "iter 48501/1000000  loss         0.112241  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.244\n",
      "iter 48600/1000000  loss         0.112236  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.245\n",
      "iter 48601/1000000  loss         0.112236  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.245\n",
      "iter 48700/1000000  loss         0.112230  avg_L1_norm_grad         0.000012  w[0]    0.038 bias    3.247\n",
      "iter 48701/1000000  loss         0.112230  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.247\n",
      "iter 48800/1000000  loss         0.112225  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.248\n",
      "iter 48801/1000000  loss         0.112225  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.248\n",
      "iter 48900/1000000  loss         0.112219  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.249\n",
      "iter 48901/1000000  loss         0.112219  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.249\n",
      "iter 49000/1000000  loss         0.112214  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.250\n",
      "iter 49001/1000000  loss         0.112214  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.250\n",
      "iter 49100/1000000  loss         0.112208  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.252\n",
      "iter 49101/1000000  loss         0.112208  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.252\n",
      "iter 49200/1000000  loss         0.112203  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.253\n",
      "iter 49201/1000000  loss         0.112203  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.253\n",
      "iter 49300/1000000  loss         0.112197  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.254\n",
      "iter 49301/1000000  loss         0.112197  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.254\n",
      "iter 49400/1000000  loss         0.112192  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.255\n",
      "iter 49401/1000000  loss         0.112192  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.255\n",
      "iter 49500/1000000  loss         0.112186  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.257\n",
      "iter 49501/1000000  loss         0.112186  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.257\n",
      "iter 49600/1000000  loss         0.112181  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.258\n",
      "iter 49601/1000000  loss         0.112181  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.258\n",
      "iter 49700/1000000  loss         0.112176  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 49701/1000000  loss         0.112176  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.259\n",
      "iter 49800/1000000  loss         0.112170  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.260\n",
      "iter 49801/1000000  loss         0.112170  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.260\n",
      "iter 49900/1000000  loss         0.112165  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.261\n",
      "iter 49901/1000000  loss         0.112165  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.261\n",
      "iter 50000/1000000  loss         0.112160  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.263\n",
      "iter 50001/1000000  loss         0.112160  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.263\n",
      "iter 50100/1000000  loss         0.112155  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.264\n",
      "iter 50101/1000000  loss         0.112155  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.264\n",
      "iter 50200/1000000  loss         0.112150  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.265\n",
      "iter 50201/1000000  loss         0.112150  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.265\n",
      "iter 50300/1000000  loss         0.112145  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.266\n",
      "iter 50301/1000000  loss         0.112144  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.266\n",
      "iter 50400/1000000  loss         0.112139  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.268\n",
      "iter 50401/1000000  loss         0.112139  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.268\n",
      "iter 50500/1000000  loss         0.112134  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.269\n",
      "iter 50501/1000000  loss         0.112134  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.269\n",
      "iter 50600/1000000  loss         0.112129  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.270\n",
      "iter 50601/1000000  loss         0.112129  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.270\n",
      "iter 50700/1000000  loss         0.112124  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.271\n",
      "iter 50701/1000000  loss         0.112124  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.271\n",
      "iter 50800/1000000  loss         0.112119  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.272\n",
      "iter 50801/1000000  loss         0.112119  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.272\n",
      "iter 50900/1000000  loss         0.112114  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.273\n",
      "iter 50901/1000000  loss         0.112114  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.273\n",
      "iter 51000/1000000  loss         0.112109  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.275\n",
      "iter 51001/1000000  loss         0.112109  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.275\n",
      "iter 51100/1000000  loss         0.112105  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.276\n",
      "iter 51101/1000000  loss         0.112105  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.276\n",
      "iter 51200/1000000  loss         0.112100  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.277\n",
      "iter 51201/1000000  loss         0.112100  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.277\n",
      "iter 51300/1000000  loss         0.112095  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.278\n",
      "iter 51301/1000000  loss         0.112095  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.278\n",
      "iter 51400/1000000  loss         0.112090  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.279\n",
      "iter 51401/1000000  loss         0.112090  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.279\n",
      "iter 51500/1000000  loss         0.112085  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.280\n",
      "iter 51501/1000000  loss         0.112085  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.280\n",
      "iter 51600/1000000  loss         0.112080  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.282\n",
      "iter 51601/1000000  loss         0.112080  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.282\n",
      "iter 51700/1000000  loss         0.112076  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.283\n",
      "iter 51701/1000000  loss         0.112076  avg_L1_norm_grad         0.000011  w[0]    0.038 bias    3.283\n",
      "iter 51800/1000000  loss         0.112071  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.284\n",
      "iter 51801/1000000  loss         0.112071  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.284\n",
      "iter 51900/1000000  loss         0.112066  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.285\n",
      "iter 51901/1000000  loss         0.112066  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.285\n",
      "iter 52000/1000000  loss         0.112062  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.286\n",
      "iter 52001/1000000  loss         0.112062  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.286\n",
      "iter 52100/1000000  loss         0.112057  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.287\n",
      "iter 52101/1000000  loss         0.112057  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.287\n",
      "iter 52200/1000000  loss         0.112052  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.288\n",
      "iter 52201/1000000  loss         0.112052  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.288\n",
      "iter 52300/1000000  loss         0.112048  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.289\n",
      "iter 52301/1000000  loss         0.112048  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.289\n",
      "iter 52400/1000000  loss         0.112043  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.291\n",
      "iter 52401/1000000  loss         0.112043  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.291\n",
      "iter 52500/1000000  loss         0.112039  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.292\n",
      "iter 52501/1000000  loss         0.112039  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.292\n",
      "iter 52600/1000000  loss         0.112034  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.293\n",
      "iter 52601/1000000  loss         0.112034  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.293\n",
      "iter 52700/1000000  loss         0.112030  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.294\n",
      "iter 52701/1000000  loss         0.112030  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.294\n",
      "iter 52800/1000000  loss         0.112025  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.295\n",
      "iter 52801/1000000  loss         0.112025  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.295\n",
      "iter 52900/1000000  loss         0.112021  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.296\n",
      "iter 52901/1000000  loss         0.112021  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.296\n",
      "iter 53000/1000000  loss         0.112017  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.297\n",
      "iter 53001/1000000  loss         0.112017  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.297\n",
      "iter 53100/1000000  loss         0.112012  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.298\n",
      "iter 53101/1000000  loss         0.112012  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.298\n",
      "iter 53200/1000000  loss         0.112008  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.299\n",
      "iter 53201/1000000  loss         0.112008  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.299\n",
      "iter 53300/1000000  loss         0.112004  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.301\n",
      "iter 53301/1000000  loss         0.112004  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.301\n",
      "iter 53400/1000000  loss         0.111999  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.302\n",
      "iter 53401/1000000  loss         0.111999  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.302\n",
      "iter 53500/1000000  loss         0.111995  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.303\n",
      "iter 53501/1000000  loss         0.111995  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.303\n",
      "iter 53600/1000000  loss         0.111991  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 53601/1000000  loss         0.111991  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.304\n",
      "iter 53700/1000000  loss         0.111987  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.305\n",
      "iter 53701/1000000  loss         0.111987  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.305\n",
      "iter 53800/1000000  loss         0.111982  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.306\n",
      "iter 53801/1000000  loss         0.111982  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.306\n",
      "iter 53900/1000000  loss         0.111978  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.307\n",
      "iter 53901/1000000  loss         0.111978  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.307\n",
      "iter 54000/1000000  loss         0.111974  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.308\n",
      "iter 54001/1000000  loss         0.111974  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.308\n",
      "iter 54100/1000000  loss         0.111970  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.309\n",
      "iter 54101/1000000  loss         0.111970  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.309\n",
      "iter 54200/1000000  loss         0.111966  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.310\n",
      "iter 54201/1000000  loss         0.111966  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.310\n",
      "iter 54300/1000000  loss         0.111962  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.311\n",
      "iter 54301/1000000  loss         0.111962  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.311\n",
      "iter 54400/1000000  loss         0.111958  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.312\n",
      "iter 54401/1000000  loss         0.111958  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.312\n",
      "iter 54500/1000000  loss         0.111954  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.313\n",
      "iter 54501/1000000  loss         0.111954  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.313\n",
      "iter 54600/1000000  loss         0.111950  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.314\n",
      "iter 54601/1000000  loss         0.111950  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.314\n",
      "iter 54700/1000000  loss         0.111946  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.315\n",
      "iter 54701/1000000  loss         0.111946  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.315\n",
      "iter 54800/1000000  loss         0.111942  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.316\n",
      "iter 54801/1000000  loss         0.111942  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.316\n",
      "iter 54900/1000000  loss         0.111938  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.317\n",
      "iter 54901/1000000  loss         0.111938  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.317\n",
      "iter 55000/1000000  loss         0.111934  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.318\n",
      "iter 55001/1000000  loss         0.111934  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.318\n",
      "iter 55100/1000000  loss         0.111930  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.319\n",
      "iter 55101/1000000  loss         0.111930  avg_L1_norm_grad         0.000010  w[0]    0.038 bias    3.319\n",
      "iter 55200/1000000  loss         0.111926  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.320\n",
      "iter 55201/1000000  loss         0.111926  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.321\n",
      "iter 55300/1000000  loss         0.111922  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.322\n",
      "iter 55301/1000000  loss         0.111922  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.322\n",
      "iter 55400/1000000  loss         0.111918  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.323\n",
      "iter 55401/1000000  loss         0.111918  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.323\n",
      "iter 55500/1000000  loss         0.111914  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.324\n",
      "iter 55501/1000000  loss         0.111914  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.324\n",
      "iter 55600/1000000  loss         0.111911  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.325\n",
      "iter 55601/1000000  loss         0.111911  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.325\n",
      "iter 55700/1000000  loss         0.111907  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.326\n",
      "iter 55701/1000000  loss         0.111907  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.326\n",
      "iter 55800/1000000  loss         0.111903  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.327\n",
      "iter 55801/1000000  loss         0.111903  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.327\n",
      "iter 55900/1000000  loss         0.111899  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.328\n",
      "iter 55901/1000000  loss         0.111899  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.328\n",
      "iter 56000/1000000  loss         0.111896  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.329\n",
      "iter 56001/1000000  loss         0.111896  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.329\n",
      "iter 56100/1000000  loss         0.111892  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.330\n",
      "iter 56101/1000000  loss         0.111892  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.330\n",
      "iter 56200/1000000  loss         0.111888  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.331\n",
      "iter 56201/1000000  loss         0.111888  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.331\n",
      "iter 56300/1000000  loss         0.111885  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.332\n",
      "iter 56301/1000000  loss         0.111885  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.332\n",
      "iter 56400/1000000  loss         0.111881  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.333\n",
      "iter 56401/1000000  loss         0.111881  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.333\n",
      "iter 56500/1000000  loss         0.111877  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.333\n",
      "iter 56501/1000000  loss         0.111877  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.334\n",
      "iter 56600/1000000  loss         0.111874  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.334\n",
      "iter 56601/1000000  loss         0.111874  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.334\n",
      "iter 56700/1000000  loss         0.111870  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.335\n",
      "iter 56701/1000000  loss         0.111870  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.335\n",
      "iter 56800/1000000  loss         0.111867  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.336\n",
      "iter 56801/1000000  loss         0.111867  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.336\n",
      "iter 56900/1000000  loss         0.111863  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.337\n",
      "iter 56901/1000000  loss         0.111863  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.337\n",
      "iter 57000/1000000  loss         0.111860  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.338\n",
      "iter 57001/1000000  loss         0.111860  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.338\n",
      "iter 57100/1000000  loss         0.111856  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.339\n",
      "iter 57101/1000000  loss         0.111856  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.339\n",
      "iter 57200/1000000  loss         0.111853  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.340\n",
      "iter 57201/1000000  loss         0.111853  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.340\n",
      "iter 57300/1000000  loss         0.111849  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.341\n",
      "iter 57301/1000000  loss         0.111849  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.341\n",
      "iter 57400/1000000  loss         0.111846  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.342\n",
      "iter 57401/1000000  loss         0.111846  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.342\n",
      "iter 57500/1000000  loss         0.111842  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 57501/1000000  loss         0.111842  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.343\n",
      "iter 57600/1000000  loss         0.111839  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.344\n",
      "iter 57601/1000000  loss         0.111839  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.344\n",
      "iter 57700/1000000  loss         0.111836  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.345\n",
      "iter 57701/1000000  loss         0.111836  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.345\n",
      "iter 57800/1000000  loss         0.111832  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.346\n",
      "iter 57801/1000000  loss         0.111832  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.346\n",
      "iter 57900/1000000  loss         0.111829  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.347\n",
      "iter 57901/1000000  loss         0.111829  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.347\n",
      "iter 58000/1000000  loss         0.111826  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.348\n",
      "iter 58001/1000000  loss         0.111826  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.348\n",
      "iter 58100/1000000  loss         0.111822  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.349\n",
      "iter 58101/1000000  loss         0.111822  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.349\n",
      "iter 58200/1000000  loss         0.111819  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.350\n",
      "iter 58201/1000000  loss         0.111819  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.350\n",
      "iter 58300/1000000  loss         0.111816  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.351\n",
      "iter 58301/1000000  loss         0.111816  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.351\n",
      "iter 58400/1000000  loss         0.111813  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.352\n",
      "iter 58401/1000000  loss         0.111813  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.352\n",
      "iter 58500/1000000  loss         0.111809  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.352\n",
      "iter 58501/1000000  loss         0.111809  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.352\n",
      "iter 58600/1000000  loss         0.111806  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.353\n",
      "iter 58601/1000000  loss         0.111806  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.353\n",
      "iter 58700/1000000  loss         0.111803  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.354\n",
      "iter 58701/1000000  loss         0.111803  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.354\n",
      "iter 58800/1000000  loss         0.111800  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.355\n",
      "iter 58801/1000000  loss         0.111800  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.355\n",
      "iter 58900/1000000  loss         0.111797  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.356\n",
      "iter 58901/1000000  loss         0.111797  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.356\n",
      "iter 59000/1000000  loss         0.111793  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.357\n",
      "iter 59001/1000000  loss         0.111793  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.357\n",
      "iter 59100/1000000  loss         0.111790  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.358\n",
      "iter 59101/1000000  loss         0.111790  avg_L1_norm_grad         0.000009  w[0]    0.038 bias    3.358\n",
      "iter 59200/1000000  loss         0.111787  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.359\n",
      "iter 59201/1000000  loss         0.111787  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.359\n",
      "iter 59300/1000000  loss         0.111784  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.360\n",
      "iter 59301/1000000  loss         0.111784  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.360\n",
      "iter 59400/1000000  loss         0.111781  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.361\n",
      "iter 59401/1000000  loss         0.111781  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.361\n",
      "iter 59500/1000000  loss         0.111778  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.361\n",
      "iter 59501/1000000  loss         0.111778  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.362\n",
      "iter 59600/1000000  loss         0.111775  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.362\n",
      "iter 59601/1000000  loss         0.111775  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.362\n",
      "iter 59700/1000000  loss         0.111772  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.363\n",
      "iter 59701/1000000  loss         0.111772  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.363\n",
      "iter 59800/1000000  loss         0.111769  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.364\n",
      "iter 59801/1000000  loss         0.111769  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.364\n",
      "iter 59900/1000000  loss         0.111766  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.365\n",
      "iter 59901/1000000  loss         0.111766  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.365\n",
      "iter 60000/1000000  loss         0.111763  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.366\n",
      "iter 60001/1000000  loss         0.111763  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.366\n",
      "iter 60100/1000000  loss         0.111760  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.367\n",
      "iter 60101/1000000  loss         0.111760  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.367\n",
      "iter 60200/1000000  loss         0.111757  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.368\n",
      "iter 60201/1000000  loss         0.111757  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.368\n",
      "iter 60300/1000000  loss         0.111754  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.369\n",
      "iter 60301/1000000  loss         0.111754  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.369\n",
      "iter 60400/1000000  loss         0.111751  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.369\n",
      "iter 60401/1000000  loss         0.111751  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.369\n",
      "iter 60500/1000000  loss         0.111748  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.370\n",
      "iter 60501/1000000  loss         0.111748  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.370\n",
      "iter 60600/1000000  loss         0.111745  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.371\n",
      "iter 60601/1000000  loss         0.111745  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.371\n",
      "iter 60700/1000000  loss         0.111743  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.372\n",
      "iter 60701/1000000  loss         0.111743  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.372\n",
      "iter 60800/1000000  loss         0.111740  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.373\n",
      "iter 60801/1000000  loss         0.111740  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.373\n",
      "iter 60900/1000000  loss         0.111737  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.374\n",
      "iter 60901/1000000  loss         0.111737  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.374\n",
      "iter 61000/1000000  loss         0.111734  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.375\n",
      "iter 61001/1000000  loss         0.111734  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.375\n",
      "iter 61100/1000000  loss         0.111731  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.375\n",
      "iter 61101/1000000  loss         0.111731  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.375\n",
      "iter 61200/1000000  loss         0.111729  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.376\n",
      "iter 61201/1000000  loss         0.111728  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.376\n",
      "iter 61300/1000000  loss         0.111726  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.377\n",
      "iter 61301/1000000  loss         0.111726  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.377\n",
      "iter 61400/1000000  loss         0.111723  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 61401/1000000  loss         0.111723  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.378\n",
      "iter 61500/1000000  loss         0.111720  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.379\n",
      "iter 61501/1000000  loss         0.111720  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.379\n",
      "iter 61600/1000000  loss         0.111718  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.380\n",
      "iter 61601/1000000  loss         0.111717  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.380\n",
      "iter 61700/1000000  loss         0.111715  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.380\n",
      "iter 61701/1000000  loss         0.111715  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.380\n",
      "iter 61800/1000000  loss         0.111712  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.381\n",
      "iter 61801/1000000  loss         0.111712  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.381\n",
      "iter 61900/1000000  loss         0.111709  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.382\n",
      "iter 61901/1000000  loss         0.111709  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.382\n",
      "iter 62000/1000000  loss         0.111707  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.383\n",
      "iter 62001/1000000  loss         0.111707  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.383\n",
      "iter 62100/1000000  loss         0.111704  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.384\n",
      "iter 62101/1000000  loss         0.111704  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.384\n",
      "iter 62200/1000000  loss         0.111701  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.385\n",
      "iter 62201/1000000  loss         0.111701  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.385\n",
      "iter 62300/1000000  loss         0.111699  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.385\n",
      "iter 62301/1000000  loss         0.111699  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.385\n",
      "iter 62400/1000000  loss         0.111696  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.386\n",
      "iter 62401/1000000  loss         0.111696  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.386\n",
      "iter 62500/1000000  loss         0.111694  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.387\n",
      "iter 62501/1000000  loss         0.111694  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.387\n",
      "iter 62600/1000000  loss         0.111691  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.388\n",
      "iter 62601/1000000  loss         0.111691  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.388\n",
      "iter 62700/1000000  loss         0.111688  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.389\n",
      "iter 62701/1000000  loss         0.111688  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.389\n",
      "iter 62800/1000000  loss         0.111686  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.389\n",
      "iter 62801/1000000  loss         0.111686  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.389\n",
      "iter 62900/1000000  loss         0.111683  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.390\n",
      "iter 62901/1000000  loss         0.111683  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.390\n",
      "iter 63000/1000000  loss         0.111681  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.391\n",
      "iter 63001/1000000  loss         0.111681  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.391\n",
      "iter 63100/1000000  loss         0.111678  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.392\n",
      "iter 63101/1000000  loss         0.111678  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.392\n",
      "iter 63200/1000000  loss         0.111676  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.393\n",
      "iter 63201/1000000  loss         0.111676  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.393\n",
      "iter 63300/1000000  loss         0.111673  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.393\n",
      "iter 63301/1000000  loss         0.111673  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.393\n",
      "iter 63400/1000000  loss         0.111671  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.394\n",
      "iter 63401/1000000  loss         0.111671  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.394\n",
      "iter 63500/1000000  loss         0.111668  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.395\n",
      "iter 63501/1000000  loss         0.111668  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.395\n",
      "iter 63600/1000000  loss         0.111666  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.396\n",
      "iter 63601/1000000  loss         0.111666  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.396\n",
      "iter 63700/1000000  loss         0.111664  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.397\n",
      "iter 63701/1000000  loss         0.111664  avg_L1_norm_grad         0.000008  w[0]    0.038 bias    3.397\n",
      "iter 63800/1000000  loss         0.111661  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.397\n",
      "iter 63801/1000000  loss         0.111661  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.397\n",
      "iter 63900/1000000  loss         0.111659  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.398\n",
      "iter 63901/1000000  loss         0.111659  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.398\n",
      "iter 64000/1000000  loss         0.111656  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.399\n",
      "iter 64001/1000000  loss         0.111656  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.399\n",
      "iter 64100/1000000  loss         0.111654  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.400\n",
      "iter 64101/1000000  loss         0.111654  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.400\n",
      "iter 64200/1000000  loss         0.111652  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.400\n",
      "iter 64201/1000000  loss         0.111652  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.400\n",
      "iter 64300/1000000  loss         0.111649  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.401\n",
      "iter 64301/1000000  loss         0.111649  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.401\n",
      "iter 64400/1000000  loss         0.111647  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.402\n",
      "iter 64401/1000000  loss         0.111647  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.402\n",
      "iter 64500/1000000  loss         0.111645  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.403\n",
      "iter 64501/1000000  loss         0.111645  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.403\n",
      "iter 64600/1000000  loss         0.111642  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.403\n",
      "iter 64601/1000000  loss         0.111642  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.403\n",
      "iter 64700/1000000  loss         0.111640  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.404\n",
      "iter 64701/1000000  loss         0.111640  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.404\n",
      "iter 64800/1000000  loss         0.111638  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.405\n",
      "iter 64801/1000000  loss         0.111638  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.405\n",
      "iter 64900/1000000  loss         0.111635  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.406\n",
      "iter 64901/1000000  loss         0.111635  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.406\n",
      "iter 65000/1000000  loss         0.111633  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.406\n",
      "iter 65001/1000000  loss         0.111633  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.406\n",
      "iter 65100/1000000  loss         0.111631  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.407\n",
      "iter 65101/1000000  loss         0.111631  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.407\n",
      "iter 65200/1000000  loss         0.111629  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.408\n",
      "iter 65201/1000000  loss         0.111629  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.408\n",
      "iter 65300/1000000  loss         0.111626  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 65301/1000000  loss         0.111626  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.409\n",
      "iter 65400/1000000  loss         0.111624  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.409\n",
      "iter 65401/1000000  loss         0.111624  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.409\n",
      "iter 65500/1000000  loss         0.111622  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.410\n",
      "iter 65501/1000000  loss         0.111622  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.410\n",
      "iter 65600/1000000  loss         0.111620  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.411\n",
      "iter 65601/1000000  loss         0.111620  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.411\n",
      "iter 65700/1000000  loss         0.111618  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.412\n",
      "iter 65701/1000000  loss         0.111617  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.412\n",
      "iter 65800/1000000  loss         0.111615  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.412\n",
      "iter 65801/1000000  loss         0.111615  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.412\n",
      "iter 65900/1000000  loss         0.111613  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.413\n",
      "iter 65901/1000000  loss         0.111613  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.413\n",
      "iter 66000/1000000  loss         0.111611  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.414\n",
      "iter 66001/1000000  loss         0.111611  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.414\n",
      "iter 66100/1000000  loss         0.111609  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.415\n",
      "iter 66101/1000000  loss         0.111609  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.415\n",
      "iter 66200/1000000  loss         0.111607  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.415\n",
      "iter 66201/1000000  loss         0.111607  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.415\n",
      "iter 66300/1000000  loss         0.111605  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.416\n",
      "iter 66301/1000000  loss         0.111605  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.416\n",
      "iter 66400/1000000  loss         0.111603  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.417\n",
      "iter 66401/1000000  loss         0.111603  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.417\n",
      "iter 66500/1000000  loss         0.111600  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.417\n",
      "iter 66501/1000000  loss         0.111600  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.417\n",
      "iter 66600/1000000  loss         0.111598  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.418\n",
      "iter 66601/1000000  loss         0.111598  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.418\n",
      "iter 66700/1000000  loss         0.111596  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.419\n",
      "iter 66701/1000000  loss         0.111596  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.419\n",
      "iter 66800/1000000  loss         0.111594  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.420\n",
      "iter 66801/1000000  loss         0.111594  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.420\n",
      "iter 66900/1000000  loss         0.111592  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.420\n",
      "iter 66901/1000000  loss         0.111592  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.420\n",
      "iter 67000/1000000  loss         0.111590  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.421\n",
      "iter 67001/1000000  loss         0.111590  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.421\n",
      "iter 67100/1000000  loss         0.111588  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.422\n",
      "iter 67101/1000000  loss         0.111588  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.422\n",
      "iter 67200/1000000  loss         0.111586  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.422\n",
      "iter 67201/1000000  loss         0.111586  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.422\n",
      "iter 67300/1000000  loss         0.111584  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.423\n",
      "iter 67301/1000000  loss         0.111584  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.423\n",
      "iter 67400/1000000  loss         0.111582  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.424\n",
      "iter 67401/1000000  loss         0.111582  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.424\n",
      "iter 67500/1000000  loss         0.111580  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.425\n",
      "iter 67501/1000000  loss         0.111580  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.425\n",
      "iter 67600/1000000  loss         0.111578  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.425\n",
      "iter 67601/1000000  loss         0.111578  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.425\n",
      "iter 67700/1000000  loss         0.111576  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.426\n",
      "iter 67701/1000000  loss         0.111576  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.426\n",
      "iter 67800/1000000  loss         0.111574  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.427\n",
      "iter 67801/1000000  loss         0.111574  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.427\n",
      "iter 67900/1000000  loss         0.111572  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.427\n",
      "iter 67901/1000000  loss         0.111572  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.427\n",
      "iter 68000/1000000  loss         0.111570  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.428\n",
      "iter 68001/1000000  loss         0.111570  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.428\n",
      "iter 68100/1000000  loss         0.111568  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.429\n",
      "iter 68101/1000000  loss         0.111568  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.429\n",
      "iter 68200/1000000  loss         0.111566  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.429\n",
      "iter 68201/1000000  loss         0.111566  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.429\n",
      "iter 68300/1000000  loss         0.111565  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.430\n",
      "iter 68301/1000000  loss         0.111565  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.430\n",
      "iter 68400/1000000  loss         0.111563  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.431\n",
      "iter 68401/1000000  loss         0.111563  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.431\n",
      "iter 68500/1000000  loss         0.111561  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.431\n",
      "iter 68501/1000000  loss         0.111561  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.431\n",
      "iter 68600/1000000  loss         0.111559  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.432\n",
      "iter 68601/1000000  loss         0.111559  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.432\n",
      "iter 68700/1000000  loss         0.111557  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.433\n",
      "iter 68701/1000000  loss         0.111557  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.433\n",
      "iter 68800/1000000  loss         0.111555  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.433\n",
      "iter 68801/1000000  loss         0.111555  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.433\n",
      "iter 68900/1000000  loss         0.111553  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.434\n",
      "iter 68901/1000000  loss         0.111553  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.434\n",
      "iter 69000/1000000  loss         0.111551  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.435\n",
      "iter 69001/1000000  loss         0.111551  avg_L1_norm_grad         0.000007  w[0]    0.038 bias    3.435\n",
      "iter 69100/1000000  loss         0.111550  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.435\n",
      "iter 69101/1000000  loss         0.111550  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.435\n",
      "iter 69200/1000000  loss         0.111548  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69201/1000000  loss         0.111548  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.436\n",
      "iter 69300/1000000  loss         0.111546  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.437\n",
      "iter 69301/1000000  loss         0.111546  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.437\n",
      "iter 69400/1000000  loss         0.111544  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.437\n",
      "iter 69401/1000000  loss         0.111544  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.437\n",
      "iter 69500/1000000  loss         0.111542  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.438\n",
      "iter 69501/1000000  loss         0.111542  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.438\n",
      "iter 69600/1000000  loss         0.111541  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.439\n",
      "iter 69601/1000000  loss         0.111541  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.439\n",
      "iter 69700/1000000  loss         0.111539  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.439\n",
      "iter 69701/1000000  loss         0.111539  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.439\n",
      "iter 69800/1000000  loss         0.111537  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.440\n",
      "iter 69801/1000000  loss         0.111537  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.440\n",
      "iter 69900/1000000  loss         0.111535  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.441\n",
      "iter 69901/1000000  loss         0.111535  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.441\n",
      "iter 70000/1000000  loss         0.111534  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.441\n",
      "iter 70001/1000000  loss         0.111534  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.441\n",
      "iter 70100/1000000  loss         0.111532  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.442\n",
      "iter 70101/1000000  loss         0.111532  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.442\n",
      "iter 70200/1000000  loss         0.111530  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.443\n",
      "iter 70201/1000000  loss         0.111530  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.443\n",
      "iter 70300/1000000  loss         0.111528  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.443\n",
      "iter 70301/1000000  loss         0.111528  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.443\n",
      "iter 70400/1000000  loss         0.111527  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.444\n",
      "iter 70401/1000000  loss         0.111527  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.444\n",
      "iter 70500/1000000  loss         0.111525  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.444\n",
      "iter 70501/1000000  loss         0.111525  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.444\n",
      "iter 70600/1000000  loss         0.111523  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.445\n",
      "iter 70601/1000000  loss         0.111523  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.445\n",
      "iter 70700/1000000  loss         0.111522  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.446\n",
      "iter 70701/1000000  loss         0.111522  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.446\n",
      "iter 70800/1000000  loss         0.111520  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.446\n",
      "iter 70801/1000000  loss         0.111520  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.446\n",
      "iter 70900/1000000  loss         0.111518  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.447\n",
      "iter 70901/1000000  loss         0.111518  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.447\n",
      "iter 71000/1000000  loss         0.111517  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.448\n",
      "iter 71001/1000000  loss         0.111517  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.448\n",
      "iter 71100/1000000  loss         0.111515  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.448\n",
      "iter 71101/1000000  loss         0.111515  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.448\n",
      "iter 71200/1000000  loss         0.111513  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.449\n",
      "iter 71201/1000000  loss         0.111513  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.449\n",
      "iter 71300/1000000  loss         0.111512  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.449\n",
      "iter 71301/1000000  loss         0.111512  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.449\n",
      "iter 71400/1000000  loss         0.111510  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.450\n",
      "iter 71401/1000000  loss         0.111510  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.450\n",
      "iter 71500/1000000  loss         0.111509  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.451\n",
      "iter 71501/1000000  loss         0.111509  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.451\n",
      "iter 71600/1000000  loss         0.111507  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.451\n",
      "iter 71601/1000000  loss         0.111507  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.451\n",
      "iter 71700/1000000  loss         0.111505  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.452\n",
      "iter 71701/1000000  loss         0.111505  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.452\n",
      "iter 71800/1000000  loss         0.111504  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.453\n",
      "iter 71801/1000000  loss         0.111504  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.453\n",
      "iter 71900/1000000  loss         0.111502  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.453\n",
      "iter 71901/1000000  loss         0.111502  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.453\n",
      "iter 72000/1000000  loss         0.111501  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.454\n",
      "iter 72001/1000000  loss         0.111501  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.454\n",
      "iter 72100/1000000  loss         0.111499  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.454\n",
      "iter 72101/1000000  loss         0.111499  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.454\n",
      "iter 72200/1000000  loss         0.111498  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.455\n",
      "iter 72201/1000000  loss         0.111498  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.455\n",
      "iter 72300/1000000  loss         0.111496  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.456\n",
      "iter 72301/1000000  loss         0.111496  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.456\n",
      "iter 72400/1000000  loss         0.111494  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.456\n",
      "iter 72401/1000000  loss         0.111494  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.456\n",
      "iter 72500/1000000  loss         0.111493  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.457\n",
      "iter 72501/1000000  loss         0.111493  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.457\n",
      "iter 72600/1000000  loss         0.111491  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.457\n",
      "iter 72601/1000000  loss         0.111491  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.457\n",
      "iter 72700/1000000  loss         0.111490  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.458\n",
      "iter 72701/1000000  loss         0.111490  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.458\n",
      "iter 72800/1000000  loss         0.111488  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.459\n",
      "iter 72801/1000000  loss         0.111488  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.459\n",
      "iter 72900/1000000  loss         0.111487  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.459\n",
      "iter 72901/1000000  loss         0.111487  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.459\n",
      "iter 73000/1000000  loss         0.111485  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.460\n",
      "iter 73001/1000000  loss         0.111485  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.460\n",
      "iter 73100/1000000  loss         0.111484  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 73101/1000000  loss         0.111484  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.460\n",
      "iter 73200/1000000  loss         0.111482  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.461\n",
      "iter 73201/1000000  loss         0.111482  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.461\n",
      "iter 73300/1000000  loss         0.111481  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.462\n",
      "iter 73301/1000000  loss         0.111481  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.462\n",
      "iter 73400/1000000  loss         0.111480  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.462\n",
      "iter 73401/1000000  loss         0.111480  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.462\n",
      "iter 73500/1000000  loss         0.111478  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.463\n",
      "iter 73501/1000000  loss         0.111478  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.463\n",
      "iter 73600/1000000  loss         0.111477  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.463\n",
      "iter 73601/1000000  loss         0.111477  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.463\n",
      "iter 73700/1000000  loss         0.111475  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.464\n",
      "iter 73701/1000000  loss         0.111475  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.464\n",
      "iter 73800/1000000  loss         0.111474  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.464\n",
      "iter 73801/1000000  loss         0.111474  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.464\n",
      "iter 73900/1000000  loss         0.111472  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.465\n",
      "iter 73901/1000000  loss         0.111472  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.465\n",
      "iter 74000/1000000  loss         0.111471  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.466\n",
      "iter 74001/1000000  loss         0.111471  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.466\n",
      "iter 74100/1000000  loss         0.111470  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.466\n",
      "iter 74101/1000000  loss         0.111470  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.466\n",
      "iter 74200/1000000  loss         0.111468  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.467\n",
      "iter 74201/1000000  loss         0.111468  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.467\n",
      "iter 74300/1000000  loss         0.111467  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.467\n",
      "iter 74301/1000000  loss         0.111467  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.467\n",
      "iter 74400/1000000  loss         0.111465  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.468\n",
      "iter 74401/1000000  loss         0.111465  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.468\n",
      "iter 74500/1000000  loss         0.111464  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.468\n",
      "iter 74501/1000000  loss         0.111464  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.468\n",
      "iter 74600/1000000  loss         0.111463  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.469\n",
      "iter 74601/1000000  loss         0.111463  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.469\n",
      "iter 74700/1000000  loss         0.111461  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.470\n",
      "iter 74701/1000000  loss         0.111461  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.470\n",
      "iter 74800/1000000  loss         0.111460  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.470\n",
      "iter 74801/1000000  loss         0.111460  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.470\n",
      "iter 74900/1000000  loss         0.111459  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.471\n",
      "iter 74901/1000000  loss         0.111459  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.471\n",
      "iter 75000/1000000  loss         0.111457  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.471\n",
      "iter 75001/1000000  loss         0.111457  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.471\n",
      "iter 75100/1000000  loss         0.111456  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.472\n",
      "iter 75101/1000000  loss         0.111456  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.472\n",
      "iter 75200/1000000  loss         0.111455  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.472\n",
      "iter 75201/1000000  loss         0.111455  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.472\n",
      "iter 75300/1000000  loss         0.111453  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.473\n",
      "iter 75301/1000000  loss         0.111453  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.473\n",
      "iter 75400/1000000  loss         0.111452  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.473\n",
      "iter 75401/1000000  loss         0.111452  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.473\n",
      "iter 75500/1000000  loss         0.111451  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.474\n",
      "iter 75501/1000000  loss         0.111451  avg_L1_norm_grad         0.000006  w[0]    0.038 bias    3.474\n",
      "iter 75600/1000000  loss         0.111449  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.474\n",
      "iter 75601/1000000  loss         0.111449  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.474\n",
      "iter 75700/1000000  loss         0.111448  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.475\n",
      "iter 75701/1000000  loss         0.111448  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.475\n",
      "iter 75800/1000000  loss         0.111447  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.476\n",
      "iter 75801/1000000  loss         0.111447  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.476\n",
      "iter 75900/1000000  loss         0.111445  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.476\n",
      "iter 75901/1000000  loss         0.111445  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.476\n",
      "iter 76000/1000000  loss         0.111444  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.477\n",
      "iter 76001/1000000  loss         0.111444  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.477\n",
      "iter 76100/1000000  loss         0.111443  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.477\n",
      "iter 76101/1000000  loss         0.111443  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.477\n",
      "iter 76200/1000000  loss         0.111442  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.478\n",
      "iter 76201/1000000  loss         0.111442  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.478\n",
      "iter 76300/1000000  loss         0.111440  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.478\n",
      "iter 76301/1000000  loss         0.111440  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.478\n",
      "iter 76400/1000000  loss         0.111439  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.479\n",
      "iter 76401/1000000  loss         0.111439  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.479\n",
      "iter 76500/1000000  loss         0.111438  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.479\n",
      "iter 76501/1000000  loss         0.111438  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.479\n",
      "iter 76600/1000000  loss         0.111437  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.480\n",
      "iter 76601/1000000  loss         0.111437  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.480\n",
      "iter 76700/1000000  loss         0.111435  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.480\n",
      "iter 76701/1000000  loss         0.111435  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.480\n",
      "iter 76800/1000000  loss         0.111434  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.481\n",
      "iter 76801/1000000  loss         0.111434  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.481\n",
      "iter 76900/1000000  loss         0.111433  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.481\n",
      "iter 76901/1000000  loss         0.111433  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.481\n",
      "iter 77000/1000000  loss         0.111432  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 77001/1000000  loss         0.111432  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.482\n",
      "iter 77100/1000000  loss         0.111431  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.482\n",
      "iter 77101/1000000  loss         0.111431  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.482\n",
      "iter 77200/1000000  loss         0.111429  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.483\n",
      "iter 77201/1000000  loss         0.111429  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.483\n",
      "iter 77300/1000000  loss         0.111428  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.484\n",
      "iter 77301/1000000  loss         0.111428  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.484\n",
      "iter 77400/1000000  loss         0.111427  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.484\n",
      "iter 77401/1000000  loss         0.111427  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.484\n",
      "iter 77500/1000000  loss         0.111426  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.485\n",
      "iter 77501/1000000  loss         0.111426  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.485\n",
      "iter 77600/1000000  loss         0.111425  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.485\n",
      "iter 77601/1000000  loss         0.111425  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.485\n",
      "iter 77700/1000000  loss         0.111423  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.486\n",
      "iter 77701/1000000  loss         0.111423  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.486\n",
      "iter 77800/1000000  loss         0.111422  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.486\n",
      "iter 77801/1000000  loss         0.111422  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.486\n",
      "iter 77900/1000000  loss         0.111421  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.487\n",
      "iter 77901/1000000  loss         0.111421  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.487\n",
      "iter 78000/1000000  loss         0.111420  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.487\n",
      "iter 78001/1000000  loss         0.111420  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.487\n",
      "iter 78100/1000000  loss         0.111419  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.488\n",
      "iter 78101/1000000  loss         0.111419  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.488\n",
      "iter 78200/1000000  loss         0.111418  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.488\n",
      "iter 78201/1000000  loss         0.111418  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.488\n",
      "iter 78300/1000000  loss         0.111417  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.489\n",
      "iter 78301/1000000  loss         0.111417  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.489\n",
      "iter 78400/1000000  loss         0.111415  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.489\n",
      "iter 78401/1000000  loss         0.111415  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.489\n",
      "iter 78500/1000000  loss         0.111414  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.490\n",
      "iter 78501/1000000  loss         0.111414  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.490\n",
      "iter 78600/1000000  loss         0.111413  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.490\n",
      "iter 78601/1000000  loss         0.111413  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.490\n",
      "iter 78700/1000000  loss         0.111412  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.491\n",
      "iter 78701/1000000  loss         0.111412  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.491\n",
      "iter 78800/1000000  loss         0.111411  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.491\n",
      "iter 78801/1000000  loss         0.111411  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.491\n",
      "iter 78900/1000000  loss         0.111410  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.492\n",
      "iter 78901/1000000  loss         0.111410  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.492\n",
      "iter 79000/1000000  loss         0.111409  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.492\n",
      "iter 79001/1000000  loss         0.111409  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.492\n",
      "iter 79100/1000000  loss         0.111408  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.493\n",
      "iter 79101/1000000  loss         0.111408  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.493\n",
      "iter 79200/1000000  loss         0.111407  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.493\n",
      "iter 79201/1000000  loss         0.111407  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.493\n",
      "iter 79300/1000000  loss         0.111406  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.494\n",
      "iter 79301/1000000  loss         0.111406  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.494\n",
      "iter 79400/1000000  loss         0.111404  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.494\n",
      "iter 79401/1000000  loss         0.111404  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.494\n",
      "iter 79500/1000000  loss         0.111403  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.495\n",
      "iter 79501/1000000  loss         0.111403  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.495\n",
      "iter 79600/1000000  loss         0.111402  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.495\n",
      "iter 79601/1000000  loss         0.111402  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.495\n",
      "iter 79700/1000000  loss         0.111401  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.496\n",
      "iter 79701/1000000  loss         0.111401  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.496\n",
      "iter 79800/1000000  loss         0.111400  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.496\n",
      "iter 79801/1000000  loss         0.111400  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.496\n",
      "iter 79900/1000000  loss         0.111399  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.497\n",
      "iter 79901/1000000  loss         0.111399  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.497\n",
      "iter 80000/1000000  loss         0.111398  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.497\n",
      "iter 80001/1000000  loss         0.111398  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.497\n",
      "iter 80100/1000000  loss         0.111397  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.497\n",
      "iter 80101/1000000  loss         0.111397  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.497\n",
      "iter 80200/1000000  loss         0.111396  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.498\n",
      "iter 80201/1000000  loss         0.111396  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.498\n",
      "iter 80300/1000000  loss         0.111395  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.498\n",
      "iter 80301/1000000  loss         0.111395  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.498\n",
      "iter 80400/1000000  loss         0.111394  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.499\n",
      "iter 80401/1000000  loss         0.111394  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.499\n",
      "iter 80500/1000000  loss         0.111393  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.499\n",
      "iter 80501/1000000  loss         0.111393  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.499\n",
      "iter 80600/1000000  loss         0.111392  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.500\n",
      "iter 80601/1000000  loss         0.111392  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.500\n",
      "iter 80700/1000000  loss         0.111391  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.500\n",
      "iter 80701/1000000  loss         0.111391  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.500\n",
      "iter 80800/1000000  loss         0.111390  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.501\n",
      "iter 80801/1000000  loss         0.111390  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.501\n",
      "iter 80900/1000000  loss         0.111389  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 80901/1000000  loss         0.111389  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.501\n",
      "iter 81000/1000000  loss         0.111388  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.502\n",
      "iter 81001/1000000  loss         0.111388  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.502\n",
      "iter 81100/1000000  loss         0.111387  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.502\n",
      "iter 81101/1000000  loss         0.111387  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.502\n",
      "iter 81200/1000000  loss         0.111386  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.503\n",
      "iter 81201/1000000  loss         0.111386  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.503\n",
      "iter 81300/1000000  loss         0.111385  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.503\n",
      "iter 81301/1000000  loss         0.111385  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.503\n",
      "iter 81400/1000000  loss         0.111384  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.504\n",
      "iter 81401/1000000  loss         0.111384  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.504\n",
      "iter 81500/1000000  loss         0.111383  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.504\n",
      "iter 81501/1000000  loss         0.111383  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.504\n",
      "iter 81600/1000000  loss         0.111382  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.504\n",
      "iter 81601/1000000  loss         0.111382  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.504\n",
      "iter 81700/1000000  loss         0.111381  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.505\n",
      "iter 81701/1000000  loss         0.111381  avg_L1_norm_grad         0.000005  w[0]    0.038 bias    3.505\n",
      "Done. Converged after 81760 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "new_lr1.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 36000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Square Loaded!\n",
      "New Accuracy 0.9761111111111084\n"
     ]
    }
   ],
   "source": [
    "y_hat_New=np.asarray(new_lr1.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2a, axes_arr = plt.subplots(nrows=1, ncols=2,figsize=(13,5))\n",
    "ax1=axes_arr[0]\n",
    "ax1.set_title('ROC'); ax1.set_xlabel(\"FPR\"); ax1.set_ylabel(\"TPR\");\n",
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "ax1.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "ax1.plot(fpr3te,tpr3te, label=\"Adding Square and All turn_on of X & Y with Noise\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "ax1.plot(fpr1Tte,tpr1Tte, label=\"No added Feature, No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "ax1.plot(fprTte,tprTte, label=\"Adding Square and All turn_on of X & Y, No Noise\")\n",
    "\n",
    "ax1.set_xlim([-0.0, 1.0]);\n",
    "ax1.set_ylim([-0.0, 1.0]);\n",
    "ax1.legend();\n",
    "\n",
    "ax2=axes_arr[1]\n",
    "ax2.set_title('Part of ROC'); ax2.set_xlabel(\"FPR\"); ax2.set_ylabel(\"TPR\");\n",
    "ax2.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "ax2.plot(fpr3te,tpr3te, label=\"Adding Square and All turn_on of X & Y with Noise\")\n",
    "\n",
    "ax2.plot(fpr1Tte,tpr1Tte, label=\"No added Feature, No Noise\")\n",
    "\n",
    "ax2.plot(fprTte,tprTte, label=\"Adding Square and All turn_on of X & Y, No Noise\")\n",
    "\n",
    "ax2.set_xlim([0.0, 0.2]);\n",
    "ax2.set_ylim([0.8, 1.0]);\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2a, axes_arr = plt.subplots(nrows=1, ncols=2,figsize=(13,5))\n",
    "ax1=axes_arr[0]\n",
    "ax1.set_title('ROC'); ax1.set_xlabel(\"FPR\"); ax1.set_ylabel(\"TPR\");\n",
    "ax1.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "ax1.plot(fpr1Tte,tpr1Tte, label=\"No added Feature, No Noise\")\n",
    "\n",
    "ax1.set_xlim([-0.0, 1.0]);\n",
    "ax1.set_ylim([-0.0, 1.0]);\n",
    "ax1.legend();\n",
    "\n",
    "ax2=axes_arr[1]\n",
    "ax2.set_title('Part of ROC'); ax2.set_xlabel(\"FPR\"); ax2.set_ylabel(\"TPR\");\n",
    "ax2.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "ax2.plot(fpr1Tte,tpr1Tte, label=\"No added Feature, No Noise\")\n",
    "\n",
    "ax2.set_xlim([0.0, 0.2]);\n",
    "ax2.set_ylim([0.8, 1.0]);\n",
    "ax2.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig3b, axes_arr = plt.subplots(nrows=1, ncols=2)\n",
    "w1=orig_lr0.w_G[:-1]\n",
    "ax1=axes_arr[0]; ax1.set_title('No Noise');\n",
    "ax1.imshow(w1.reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "\n",
    "w2=orig_lr22.w_G[:-1]\n",
    "ax2=axes_arr[1]; ax2.set_title('With Noise');\n",
    "ax2.imshow(w1.reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 2000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Square Loaded!\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = orig_lr1.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba18O_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.385 0.993995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded 0.0\n",
      "(1, 2000)\n",
      "TurnOnOnce Loaded\n",
      "TurnOn All Loaded\n",
      "TurnOn Y Loaded 0.0\n",
      "TurnOnOnce Y Loaded\n",
      "Square Loaded!\n"
     ]
    }
   ],
   "source": [
    "#x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr1.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba18N_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0375 0.995576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = orig_lr22.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba18O2_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.375 0.993732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  1.19179450e-06  1.19179450e-06  1.19179450e-06\n",
      "  9.31899857e-04  9.30819066e-04  9.36699488e-04  9.49960817e-04\n",
      "  9.30729740e-04  1.37043790e-03  2.90640360e-03  2.36005823e-03\n",
      "  1.84390756e-02  2.64659726e-02 -3.35344986e-02 -1.39446268e-02\n",
      "  2.35198802e-02  8.80809520e-02  8.28684264e-02  8.34217266e-02\n",
      "  5.48979693e-02  2.20185429e-02  1.89024755e-02  2.12148214e-02\n",
      "  1.24235655e-02  3.67807122e-02  2.82424559e-02  4.40484758e-05\n",
      "  0.00000000e+00  1.19179450e-06  1.19179450e-06  1.19179450e-06\n",
      "  9.30781951e-04  9.33584930e-04  1.16066609e-03  1.02026923e-03\n",
      "  1.05924100e-03  2.09048610e-03  4.05163461e-03  1.28896394e-02\n",
      "  6.94427884e-02  7.27734278e-02  5.35549100e-02  1.11804854e-01\n",
      "  1.31663456e-01  1.00095299e-01  7.91141228e-02  1.08516045e-01\n",
      "  1.11669288e-01  4.03081480e-02  5.56383721e-02  6.68796433e-02\n",
      " -2.10978866e-03  3.03941011e-02  1.62360709e-02 -1.90920961e-02\n",
      "  0.00000000e+00  1.19179450e-06  1.19179450e-06  1.19179450e-06\n",
      "  9.42716667e-04  1.17066451e-03  1.16254882e-03  1.22874058e-03\n",
      "  1.73672845e-03  2.65427400e-03  3.95633504e-03  1.46805429e-02\n",
      "  6.67221775e-02  1.08802515e-01  1.14807111e-01  1.14664462e-01\n",
      "  1.32046507e-01  1.65161290e-01  1.24356541e-01  7.81065699e-02\n",
      "  1.93920945e-01  1.39069357e-01  1.42792966e-01  1.32152519e-01\n",
      "  6.57713676e-02  1.15349285e-02 -2.08404168e-03  9.39668695e-04\n",
      "  2.09245244e-06  3.28424694e-06  3.28424694e-06  4.09241588e-05\n",
      "  1.10493319e-03  9.71039387e-04  1.11774591e-03  1.57014905e-03\n",
      "  2.34473368e-03  5.28503676e-03  8.68030394e-03  2.16141934e-02\n",
      "  7.92838763e-02  1.09591139e-01  1.16040406e-01  5.21727705e-02\n",
      "  7.38605604e-02  1.56507886e-01  1.43976008e-01  1.03077244e-01\n",
      "  9.74742660e-02  1.75273415e-01  1.74743090e-01  1.72009550e-01\n",
      "  1.32409123e-01  3.79566859e-02 -9.74726910e-03  5.36431382e-03\n",
      "  1.46690787e-05  3.28424694e-06  1.45941672e-05  1.87353217e-04\n",
      "  1.10129835e-03  9.74027362e-04  1.07582398e-03  1.44101263e-03\n",
      "  1.94625695e-03  7.98389615e-03  1.64642841e-02  2.73348931e-02\n",
      "  8.37556230e-02  1.56878771e-01  6.20604042e-02  1.20353843e-03\n",
      " -1.23167065e-01 -1.28868787e-01 -1.32000259e-01  9.85061611e-03\n",
      "  8.70336591e-02  1.80400895e-01  1.89361739e-01  1.23031507e-01\n",
      "  7.28011168e-02  1.01493992e-02 -3.64681980e-03 -2.86593372e-03\n",
      " -1.53219595e-04 -1.50399977e-04 -5.25700890e-05 -6.82183049e-06\n",
      "  9.69968063e-04  7.87074716e-04  9.26856779e-04  9.09963829e-04\n",
      "  2.17622782e-03  4.85233155e-03  2.30130987e-02  4.37675621e-02\n",
      "  9.98156203e-02  1.40516975e-01  2.24253337e-02 -4.78074481e-02\n",
      " -1.16098212e-01 -2.02607792e-01 -2.64821975e-01 -3.81744406e-03\n",
      "  2.21973552e-01  2.36695823e-01  2.80954389e-01  2.74455851e-01\n",
      "  2.48125959e-01  8.66669200e-02 -7.98156604e-02 -3.22657071e-02\n",
      " -1.01923421e-04 -4.75109899e-05  5.44069964e-05 -1.70022228e-05\n",
      "  1.06731168e-03  5.64855793e-04  9.00210577e-04  9.94548518e-04\n",
      " -5.95397443e-04  8.81491955e-03  2.34232475e-02  3.57112067e-02\n",
      "  1.26710825e-01  2.14393767e-01  1.61728805e-01  1.02481346e-01\n",
      " -1.54779301e-01 -2.65703367e-01 -3.63340114e-01 -2.07050720e-01\n",
      "  1.85227639e-01  3.03006453e-01  2.64920461e-01  1.67979215e-01\n",
      "  2.39051041e-01  2.10145825e-01 -1.25984942e-01 -2.89721407e-02\n",
      "  4.24424049e-05  5.53451463e-05 -2.56854671e-04 -3.40715985e-04\n",
      "  4.58424569e-04  1.10647676e-03  4.70772856e-04  1.63351781e-03\n",
      " -6.93403179e-03 -4.43168405e-03  9.98049950e-03  6.30131418e-02\n",
      "  2.12251106e-01  3.84250229e-01  5.51452145e-01 -1.00992332e-02\n",
      " -1.23816304e-01 -2.94312358e-01 -4.70093697e-01 -6.68308122e-01\n",
      " -9.83621701e-02  2.62376246e-01  2.09326998e-01  1.06777655e-01\n",
      "  1.45150495e-01  3.49908800e-02 -5.93897347e-03 -2.73646050e-02\n",
      "  2.13594926e-04  3.24091999e-04 -1.46116381e-02 -3.01029975e-03\n",
      "  5.57347558e-03 -2.17516517e-04 -1.07094613e-02  1.87379257e-03\n",
      " -6.84248680e-03 -3.04224619e-02  2.52243943e-02  4.54165312e-02\n",
      "  1.60376206e-01  3.84257862e-01  5.13481070e-01  4.66761309e-01\n",
      " -3.86299887e-01 -5.77048005e-01 -2.97096598e-01  4.20198461e-02\n",
      "  1.85959642e-01  1.77255164e-01  1.40914851e-01  2.29585570e-02\n",
      " -5.11227765e-02  2.16945569e-01  3.41345252e-01  6.48475535e-02\n",
      " -2.21166145e-05 -4.34857352e-03 -1.22537666e-02 -1.71233528e-02\n",
      " -1.04068970e-02 -1.02979714e-02 -1.22631064e-02 -3.88756938e-02\n",
      " -3.54439660e-02  7.68775952e-02  3.79694330e-02 -7.02339964e-03\n",
      "  3.02374928e-01  2.25086736e-01  2.32446555e-01  2.67417980e-01\n",
      " -5.17670578e-01 -8.92202550e-01 -4.72789815e-01  1.69313282e-01\n",
      "  2.85268134e-01  3.70162294e-01  1.21921157e-01 -2.98870437e-01\n",
      " -1.18385950e-01 -1.45621523e-01  1.42027002e-01  1.72870165e-01\n",
      "  1.28979329e-03 -6.75891490e-02 -5.31827761e-02 -2.68933068e-02\n",
      " -4.54445929e-02 -2.71153900e-03  9.30456953e-03  4.37630211e-02\n",
      "  7.70683532e-02  3.28645173e-03  3.54997220e-02 -1.93706956e-01\n",
      " -1.02984141e-01  1.95203546e-01 -2.22056492e-01 -1.86859807e-01\n",
      " -6.32881590e-02 -4.58421963e-01 -1.46213117e-01  1.57110373e-01\n",
      "  3.73756652e-01  3.70367620e-01  9.57253198e-02 -2.31485196e-01\n",
      "  2.50016917e-01 -2.30987159e-01 -3.38843648e-02  3.81246320e-01\n",
      " -1.25140184e-03 -7.29928631e-02 -5.56130994e-02 -6.30323566e-02\n",
      " -5.89545148e-02  4.03476887e-02  2.41265666e-01  3.93315503e-01\n",
      "  2.27454226e-01 -1.43863089e-02 -1.70667759e-01 -5.70210242e-01\n",
      " -1.90478765e-01 -3.53327810e-02 -1.04984950e-02 -1.08563787e-01\n",
      " -8.63218080e-03 -6.95332715e-02 -2.39530502e-01  2.15706615e-01\n",
      "  1.60589202e-01  2.73965761e-01  3.09654179e-02  8.95879639e-02\n",
      "  1.43737655e-01 -2.42654026e-01 -2.77128410e-01  3.76061000e-01\n",
      "  4.90770916e-03 -2.55542538e-02 -8.84966889e-02 -2.08699790e-01\n",
      " -1.32205219e-01  2.05713582e-01  6.44561510e-01  4.83162561e-01\n",
      " -3.01713193e-02 -2.57950222e-01 -2.26444143e-01  1.36367416e-01\n",
      " -1.40909198e-01 -1.00620462e-01 -1.24380714e-01 -2.20299969e-01\n",
      " -1.08784233e-01 -2.69372182e-01 -2.74631840e-01 -2.70863083e-01\n",
      " -6.46373477e-02 -1.37054324e-02 -9.61052740e-02  2.43632604e-02\n",
      "  1.09177175e-01 -1.89117675e-01 -3.92193287e-01 -2.16832624e-02\n",
      "  1.24105940e-01  2.60716053e-03 -2.52238339e-01 -4.62919419e-01\n",
      " -2.67152972e-01  8.51802460e-01  5.72238677e-01  5.30379268e-02\n",
      " -2.21325757e-01 -2.29678125e-01 -4.72689354e-02 -1.58597773e-01\n",
      " -2.61736163e-01 -2.32209053e-01 -2.29417013e-01 -1.88483518e-01\n",
      " -1.73030427e-01 -1.68138388e-01 -1.37142323e-01 -2.21326673e-01\n",
      " -1.61528970e-01 -2.57580082e-01 -2.29470289e-01 -1.60075344e-01\n",
      " -3.29148203e-01 -3.77229664e-01 -7.36494084e-01 -2.81145129e-01\n",
      "  2.77711945e-01 -9.77085552e-02 -4.12049364e-01 -3.98415477e-01\n",
      " -7.76171277e-02  1.32339014e+00  6.68542295e-01  2.69136788e-01\n",
      " -9.76982924e-02  8.47384278e-03 -9.12794661e-02 -5.10048483e-01\n",
      "  6.11805992e-02 -1.94471264e-01 -3.18991762e-01 -8.91803049e-02\n",
      " -3.87626865e-01 -2.12484368e-02  7.15199446e-04 -1.21527508e-01\n",
      "  2.00862910e-01 -6.67409081e-02  1.04050413e-01 -1.65970472e-01\n",
      " -4.75409187e-01 -7.80373080e-01 -6.84329048e-01 -2.81519241e-01\n",
      "  5.61651890e-02 -7.74731290e-02 -4.92141621e-01 -5.75756278e-01\n",
      " -1.89013325e-02  1.10473515e+00  5.90904463e-01  3.40664870e-01\n",
      "  3.32375157e-01 -8.84831834e-03 -3.37723353e-02 -2.47561140e-01\n",
      " -7.57469907e-02  1.73882675e-02 -1.55879933e-01 -2.44614344e-01\n",
      " -1.59376693e-01 -1.13746873e-03 -5.67580215e-02  2.77594137e-01\n",
      "  2.09823643e-01  6.71956876e-02  1.27787697e-01 -8.74427018e-02\n",
      " -4.13757851e-01 -5.69026091e-01 -4.17314676e-01  1.05150276e-01\n",
      " -2.88944021e-01 -2.29318777e-01 -3.62307977e-01 -6.46491588e-01\n",
      "  3.80450469e-02  1.18039141e+00  9.12078740e-01  6.88281081e-01\n",
      "  4.23945917e-01  2.52634617e-02 -2.52250573e-01 -7.94905815e-02\n",
      " -4.33341887e-02 -1.36948142e-01 -4.31259776e-02 -1.50024587e-01\n",
      " -5.38841828e-02 -2.55974891e-01  2.35417208e-02  7.37885567e-02\n",
      "  1.14685058e-01 -5.05911294e-02  3.26017992e-01  1.41913243e-01\n",
      " -1.87490751e-01 -2.02545872e-01 -2.68953380e-02  3.88658913e-04\n",
      " -1.48442393e-01 -6.30155008e-03 -7.61464715e-01 -6.26527057e-01\n",
      "  1.49422872e-01  7.90321286e-01  6.20048952e-01  5.47026706e-01\n",
      "  2.35638526e-01 -1.15022340e-02 -2.19225734e-01 -3.61516995e-01\n",
      " -1.86178411e-01 -1.01263591e-01  2.03296586e-01 -4.98613813e-02\n",
      " -1.54751640e-01 -1.94200411e-01  7.69202451e-02  1.30190033e-01\n",
      " -1.01113574e-01  6.48310658e-03  1.87140873e-01  2.39252285e-01\n",
      "  2.67704506e-01  4.14437407e-01  1.69344257e-01  1.81896532e-01\n",
      " -1.88273451e-01 -3.26219361e-01 -5.70757382e-01 -3.62276325e-01\n",
      "  1.23658643e-01  5.14151488e-01  5.59204550e-01  3.53418650e-01\n",
      "  2.11936231e-01 -6.38074992e-02 -1.62894206e-01 -1.15524185e-01\n",
      " -3.40840129e-02 -1.76147351e-01  4.66544886e-02 -5.86793330e-02\n",
      "  5.03767319e-02  1.44378494e-01 -6.18997069e-02 -1.67574776e-01\n",
      " -1.02209015e-01  2.22504852e-02  2.93772008e-01  6.94145343e-02\n",
      "  7.86974496e-02  5.58397292e-01 -6.89257757e-02 -2.26737866e-01\n",
      "  4.66061138e-02  2.33157457e-02 -1.80058069e-01  4.54325596e-02\n",
      "  1.68731209e-01  8.63082851e-01  4.53820593e-01  2.64861865e-01\n",
      "  8.65251079e-02 -2.17502538e-02  1.17882556e-01  1.23082615e-01\n",
      "  8.23028821e-02 -8.74882915e-02 -3.04317815e-02 -2.78363013e-01\n",
      " -1.57187756e-01 -3.46576924e-01 -1.84537016e-01 -1.80552171e-01\n",
      " -1.32734368e-01 -1.08026745e-01  6.73318185e-03  1.25520441e-01\n",
      "  2.09668638e-01  7.49337507e-01 -1.93529550e-01 -6.82992164e-01\n",
      " -1.47914111e-01 -1.23185060e-01 -9.56343001e-02 -9.56725225e-02\n",
      " -4.53406466e-03  3.90816898e-01  2.99168011e-01  3.57057756e-01\n",
      "  1.50736437e-01  2.25563758e-01  1.03371862e-01  1.62754894e-01\n",
      "  3.11895117e-02  7.21147148e-03 -1.22781998e-01 -1.72256784e-01\n",
      " -2.09805673e-01 -7.95303788e-01 -5.77780081e-01 -4.97431716e-01\n",
      " -1.31465301e-01  1.13715662e-01 -5.06724702e-03  1.43546803e-01\n",
      "  2.17269284e-01  6.39213020e-01 -1.33401971e-01 -5.51392048e-01\n",
      " -8.35663399e-02 -1.35916026e-01 -1.61371107e-01 -8.90045862e-02\n",
      "  1.84492720e-04  1.98586728e-01  3.54659027e-01  3.07662323e-01\n",
      "  2.05415714e-01  1.67588712e-01  2.18660874e-01  1.64785855e-01\n",
      "  2.28868518e-01  1.35964160e-01  5.80422938e-02 -3.79857317e-02\n",
      " -1.87388373e-01 -4.05679590e-01 -5.01678852e-01 -5.73438331e-01\n",
      " -1.74410222e-01  1.56108282e-01  1.41354270e-01  1.81127847e-01\n",
      "  3.17436296e-01  4.39994900e-01 -2.00171419e-01 -2.77566074e-01\n",
      "  8.27653601e-02 -1.99691829e-02 -9.23816766e-02 -8.04480192e-02\n",
      "  7.26584028e-03  1.87635742e-01  2.67804895e-01  3.53440101e-01\n",
      "  2.31326203e-01  1.10386778e-01  9.60963057e-02 -4.21302444e-02\n",
      "  4.85847979e-02  1.17369778e-01 -4.97373177e-02 -1.80183001e-01\n",
      " -3.00913228e-01 -4.41506684e-01 -3.77426092e-01 -2.34322533e-01\n",
      " -6.38014061e-02 -1.82091782e-02  3.95072471e-02  1.80298479e-01\n",
      "  2.51161221e-01  4.36709235e-01 -3.94196631e-02 -7.52655536e-02\n",
      "  3.31933949e-02  1.41849802e-01  5.58926104e-02  1.17387758e-02\n",
      "  7.17619814e-02  1.31965045e-01  1.56920116e-01  2.13294154e-01\n",
      "  1.66653489e-01  1.46906100e-01  2.09564640e-01  1.93566299e-01\n",
      "  1.07495852e-01  6.25319849e-02  8.23845425e-02  5.72800321e-02\n",
      " -9.00235482e-02 -1.64204456e-01  2.41405538e-02 -6.35238783e-02\n",
      " -3.74107153e-02 -2.10692483e-02  9.65811602e-02  2.08480037e-01\n",
      "  2.80401731e-01  4.04127425e-01  7.25371818e-02 -1.31737324e-02\n",
      " -2.38735408e-02  1.91207061e-02  1.09783632e-01  1.49412839e-01\n",
      "  1.40283387e-01  1.94210031e-01  1.82695916e-01  2.27985675e-01\n",
      "  2.34332238e-01  2.47413771e-01  1.85434230e-01  1.95066912e-01\n",
      "  6.99329332e-02  7.65215235e-02  1.25727453e-01 -4.40172143e-04\n",
      " -1.44177350e-01 -1.28980644e-01 -9.39862969e-02 -9.14805295e-02\n",
      " -3.74477822e-02  1.25196511e-02  6.39736045e-02  9.90995194e-02\n",
      "  2.01161924e-01  2.31075156e-01  8.98209768e-02  8.03555945e-03\n",
      "  4.08974876e-02  4.22616978e-02  2.44241517e-02  3.52160336e-02\n",
      "  6.13992087e-02  9.93965250e-02  1.31739261e-01  1.80413452e-01\n",
      "  2.12949094e-01  2.32011654e-01  2.63796538e-01  1.89852119e-01\n",
      "  1.58326199e-01  1.68690181e-01  1.39627963e-01 -1.56014988e-02\n",
      " -1.00167828e-01 -6.77015069e-02 -8.91246509e-03  3.88951167e-02\n",
      "  2.40834876e-02  6.69045313e-02  8.56486964e-02  8.36308801e-02\n",
      "  7.33879363e-02  1.14401951e-01  7.47113343e-02  1.70879484e-03\n",
      "  1.63291738e-02  5.18802746e-02  7.76734882e-02  9.32030966e-02\n",
      "  1.22177249e-01  1.51045947e-01  1.45116919e-01  2.08756720e-01\n",
      "  2.43021263e-01  2.40700246e-01  2.41312978e-01  2.19184537e-01\n",
      "  2.06421986e-01  2.58285121e-01  1.14338307e-01  1.81194182e-02\n",
      "  3.29790847e-02  8.19883955e-02  9.70013942e-02  9.55803098e-02\n",
      "  1.20122942e-01  9.38991326e-02  1.10427646e-01  9.55545293e-02\n",
      "  1.12996798e-01  1.17382836e-01  6.39996911e-02  6.20443758e-03\n",
      "  1.02939107e-03  3.93048179e-03  9.12791011e-03  1.64057278e-02\n",
      "  3.16845678e-02  5.38435954e-02  6.15776145e-02  9.31802962e-02\n",
      "  1.28551009e-01  1.55798561e-01  1.86323924e-01  1.87139526e-01\n",
      "  1.68947872e-01  9.65505650e-02  3.45302035e-02  2.24993986e-02\n",
      "  7.51971009e-03  4.22582056e-02  7.32490957e-02  6.05115713e-02\n",
      "  6.88187522e-02  5.10283314e-02  5.92021608e-02  4.21659414e-02\n",
      "  4.02970946e-02  2.84904006e-02  2.68447664e-02  2.02332320e-03\n",
      "  2.19963826e+00]\n"
     ]
    }
   ],
   "source": [
    "#Save Weights:\n",
    "W00=orig_lr0.w_G\n",
    "print(W00)\n",
    "W01=orig_lr1.w_G\n",
    "np.savetxt('trained_weights/Orig_No_Noise.txt', W00)\n",
    "np.savetxt('trained_weights/Orig_With_Noise.txt', W01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
