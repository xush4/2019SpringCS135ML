{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent as LRGD\n",
    "from LRGradientDescentWithFeatureTransform import LRGDWithFeatureTransform as LRGDF\n",
    "from show_images import show_images\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigm #sigmoid function\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.special import expit as sigm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= genfromtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',')[1:]\n",
    "#xbias_NG = lr.insert_final_col_of_all_ones(x_all)\n",
    "y= genfromtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TP_TN_FP_FN(ytrue_N, yhat_N):\n",
    "    ''' Compute counts of four possible outcomes of a binary classifier for evaluation.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array of floats\n",
    "        Each entry represents the binary value (0 or 1) of 'true' label of one example\n",
    "        One entry per example in current dataset\n",
    "    yhat_N : 1D array of floats\n",
    "        Each entry represents a predicted binary value (either 0 or 1).\n",
    "        One entry per example in current dataset.\n",
    "        Needs to be same size as ytrue_N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TP : float\n",
    "        Number of true positives\n",
    "    TN : float\n",
    "        Number of true negatives\n",
    "    FP : float\n",
    "        Number of false positives\n",
    "    FN : float\n",
    "        Number of false negatives\n",
    "    '''\n",
    "    TP = 0.0\n",
    "    TN = 0.0\n",
    "    FP = 0.0\n",
    "    FN = 0.0\n",
    "    FP_id=[]\n",
    "    FN_id=[]\n",
    "    l=ytrue_N.size\n",
    "    for i in range(0,l):\n",
    "        if (yhat_N[i]==1):\n",
    "            if (ytrue_N[i]==1):\n",
    "                TP=TP+1.0\n",
    "            else:\n",
    "                FP=FP+1.0\n",
    "                #FP_id.append(i)\n",
    "        else:\n",
    "            if (ytrue_N[i]==0):\n",
    "                TN=TN+1.0\n",
    "            else:\n",
    "                FN=FN+1.0\n",
    "                #FN_id.append(i)      \n",
    "    return TP, TN, FP, FN #, FP_id, FN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise(x,y):\n",
    "    N=int(x[0,:].size)\n",
    "    #print(N)\n",
    "    x_all=x;\n",
    "    y_all=y;\n",
    "    x_on=0;\n",
    "    for j in range(9):\n",
    "        x_j=x\n",
    "        for i in range(y.size):\n",
    "            for k in range(randint(1,10)):\n",
    "                pos=randint(0,N)\n",
    "                x_j[i, pos]=1-x[i,pos]\n",
    "        x_all=np.concatenate((x_all, x_j), axis=0)\n",
    "        y_all=np.concatenate((y_all, y), axis=0)\n",
    "        #print(x_all.shape)\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n=make_noise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 784) (1, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(x_n.shape, np.matrix(y_n).shape)\n",
    "#all_D=np.column_stack((x_n, np.matrix(y_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle:\n",
    "Data=np.concatenate((x_n, np.matrix(y_n).T), axis=1)\n",
    "np.random.shuffle(Data)\n",
    "x_n=Data[:,:-1]\n",
    "y_n=np.asarray(Data[:,-1]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_rate=0.3\n",
    "x_va=x_n[:int(np.ceil(va_rate*y_n.shape[0])),]\n",
    "y_va=y_n[:int(np.ceil(va_rate*y_n.shape[0]))]\n",
    "x_te=x_n[int(np.ceil(va_rate*y_n.shape[0])):,]\n",
    "y_te=y_n[int(np.ceil(va_rate*y_n.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 784) (84000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print (x_te.shape, y_te.shape)\n",
    "print(y_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030126  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.910796  avg_L1_norm_grad         0.028347  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.846762  avg_L1_norm_grad         0.019586  w[0]    0.001 bias    0.021\n",
      "iter    3/1000000  loss         0.802430  avg_L1_norm_grad         0.018423  w[0]    0.001 bias    0.025\n",
      "iter    4/1000000  loss         0.768905  avg_L1_norm_grad         0.014005  w[0]    0.001 bias    0.040\n",
      "iter    5/1000000  loss         0.743009  avg_L1_norm_grad         0.013412  w[0]    0.001 bias    0.047\n",
      "iter    6/1000000  loss         0.721829  avg_L1_norm_grad         0.011474  w[0]    0.001 bias    0.059\n",
      "iter    7/1000000  loss         0.703856  avg_L1_norm_grad         0.010973  w[0]    0.002 bias    0.068\n",
      "iter    8/1000000  loss         0.688114  avg_L1_norm_grad         0.010060  w[0]    0.002 bias    0.078\n",
      "iter    9/1000000  loss         0.674022  avg_L1_norm_grad         0.009597  w[0]    0.002 bias    0.087\n",
      "iter   10/1000000  loss         0.661206  avg_L1_norm_grad         0.009077  w[0]    0.002 bias    0.096\n",
      "iter   11/1000000  loss         0.649414  avg_L1_norm_grad         0.008688  w[0]    0.002 bias    0.105\n",
      "iter   12/1000000  loss         0.638467  avg_L1_norm_grad         0.008324  w[0]    0.003 bias    0.114\n",
      "iter   13/1000000  loss         0.628234  avg_L1_norm_grad         0.008011  w[0]    0.003 bias    0.122\n",
      "iter   14/1000000  loss         0.618616  avg_L1_norm_grad         0.007730  w[0]    0.003 bias    0.131\n",
      "iter   15/1000000  loss         0.609537  avg_L1_norm_grad         0.007478  w[0]    0.003 bias    0.139\n",
      "iter   16/1000000  loss         0.600937  avg_L1_norm_grad         0.007249  w[0]    0.003 bias    0.147\n",
      "iter   17/1000000  loss         0.592767  avg_L1_norm_grad         0.007038  w[0]    0.004 bias    0.155\n",
      "iter   18/1000000  loss         0.584987  avg_L1_norm_grad         0.006845  w[0]    0.004 bias    0.163\n",
      "iter   19/1000000  loss         0.577562  avg_L1_norm_grad         0.006666  w[0]    0.004 bias    0.171\n",
      "iter  100/1000000  loss         0.360135  avg_L1_norm_grad         0.002337  w[0]    0.010 bias    0.589\n",
      "iter  101/1000000  loss         0.359088  avg_L1_norm_grad         0.002320  w[0]    0.010 bias    0.593\n",
      "iter  200/1000000  loss         0.296817  avg_L1_norm_grad         0.001385  w[0]    0.013 bias    0.887\n",
      "iter  201/1000000  loss         0.296429  avg_L1_norm_grad         0.001380  w[0]    0.013 bias    0.889\n",
      "iter  300/1000000  loss         0.268103  avg_L1_norm_grad         0.001025  w[0]    0.015 bias    1.097\n",
      "iter  301/1000000  loss         0.267890  avg_L1_norm_grad         0.001022  w[0]    0.015 bias    1.099\n",
      "iter  400/1000000  loss         0.250951  avg_L1_norm_grad         0.000832  w[0]    0.015 bias    1.263\n",
      "iter  401/1000000  loss         0.250812  avg_L1_norm_grad         0.000831  w[0]    0.015 bias    1.264\n",
      "iter  500/1000000  loss         0.239263  avg_L1_norm_grad         0.000711  w[0]    0.016 bias    1.399\n",
      "iter  501/1000000  loss         0.239164  avg_L1_norm_grad         0.000710  w[0]    0.016 bias    1.401\n",
      "iter  600/1000000  loss         0.230647  avg_L1_norm_grad         0.000627  w[0]    0.016 bias    1.516\n",
      "iter  601/1000000  loss         0.230572  avg_L1_norm_grad         0.000627  w[0]    0.016 bias    1.517\n",
      "iter  700/1000000  loss         0.223956  avg_L1_norm_grad         0.000566  w[0]    0.016 bias    1.618\n",
      "iter  701/1000000  loss         0.223896  avg_L1_norm_grad         0.000566  w[0]    0.016 bias    1.619\n",
      "iter  800/1000000  loss         0.218562  avg_L1_norm_grad         0.000518  w[0]    0.016 bias    1.709\n",
      "iter  801/1000000  loss         0.218513  avg_L1_norm_grad         0.000518  w[0]    0.016 bias    1.709\n",
      "iter  900/1000000  loss         0.214093  avg_L1_norm_grad         0.000480  w[0]    0.015 bias    1.790\n",
      "iter  901/1000000  loss         0.214052  avg_L1_norm_grad         0.000479  w[0]    0.015 bias    1.790\n",
      "iter 1000/1000000  loss         0.210310  avg_L1_norm_grad         0.000448  w[0]    0.015 bias    1.863\n",
      "iter 1001/1000000  loss         0.210275  avg_L1_norm_grad         0.000447  w[0]    0.015 bias    1.864\n",
      "iter 1100/1000000  loss         0.207055  avg_L1_norm_grad         0.000420  w[0]    0.015 bias    1.931\n",
      "iter 1101/1000000  loss         0.207024  avg_L1_norm_grad         0.000420  w[0]    0.015 bias    1.931\n",
      "iter 1200/1000000  loss         0.204215  avg_L1_norm_grad         0.000396  w[0]    0.015 bias    1.993\n",
      "iter 1201/1000000  loss         0.204188  avg_L1_norm_grad         0.000396  w[0]    0.015 bias    1.993\n",
      "iter 1300/1000000  loss         0.201709  avg_L1_norm_grad         0.000375  w[0]    0.015 bias    2.050\n",
      "iter 1301/1000000  loss         0.201686  avg_L1_norm_grad         0.000375  w[0]    0.015 bias    2.050\n",
      "iter 1400/1000000  loss         0.199479  avg_L1_norm_grad         0.000357  w[0]    0.015 bias    2.103\n",
      "iter 1401/1000000  loss         0.199458  avg_L1_norm_grad         0.000357  w[0]    0.015 bias    2.103\n",
      "iter 1500/1000000  loss         0.197477  avg_L1_norm_grad         0.000340  w[0]    0.015 bias    2.153\n",
      "iter 1501/1000000  loss         0.197458  avg_L1_norm_grad         0.000340  w[0]    0.015 bias    2.153\n",
      "iter 1600/1000000  loss         0.195669  avg_L1_norm_grad         0.000325  w[0]    0.015 bias    2.199\n",
      "iter 1601/1000000  loss         0.195651  avg_L1_norm_grad         0.000325  w[0]    0.015 bias    2.200\n",
      "iter 1700/1000000  loss         0.194025  avg_L1_norm_grad         0.000312  w[0]    0.015 bias    2.243\n",
      "iter 1701/1000000  loss         0.194009  avg_L1_norm_grad         0.000312  w[0]    0.015 bias    2.243\n",
      "iter 1800/1000000  loss         0.192524  avg_L1_norm_grad         0.000300  w[0]    0.015 bias    2.284\n",
      "iter 1801/1000000  loss         0.192509  avg_L1_norm_grad         0.000300  w[0]    0.015 bias    2.284\n",
      "iter 1900/1000000  loss         0.191146  avg_L1_norm_grad         0.000289  w[0]    0.015 bias    2.323\n",
      "iter 1901/1000000  loss         0.191133  avg_L1_norm_grad         0.000288  w[0]    0.015 bias    2.323\n",
      "iter 2000/1000000  loss         0.189877  avg_L1_norm_grad         0.000278  w[0]    0.015 bias    2.360\n",
      "iter 2001/1000000  loss         0.189865  avg_L1_norm_grad         0.000278  w[0]    0.015 bias    2.360\n",
      "iter 2100/1000000  loss         0.188704  avg_L1_norm_grad         0.000269  w[0]    0.015 bias    2.395\n",
      "iter 2101/1000000  loss         0.188692  avg_L1_norm_grad         0.000269  w[0]    0.015 bias    2.395\n",
      "iter 2200/1000000  loss         0.187615  avg_L1_norm_grad         0.000260  w[0]    0.015 bias    2.428\n",
      "iter 2201/1000000  loss         0.187605  avg_L1_norm_grad         0.000260  w[0]    0.015 bias    2.428\n",
      "iter 2300/1000000  loss         0.186602  avg_L1_norm_grad         0.000252  w[0]    0.015 bias    2.459\n",
      "iter 2301/1000000  loss         0.186593  avg_L1_norm_grad         0.000251  w[0]    0.015 bias    2.460\n",
      "iter 2400/1000000  loss         0.185658  avg_L1_norm_grad         0.000244  w[0]    0.015 bias    2.489\n",
      "iter 2401/1000000  loss         0.185648  avg_L1_norm_grad         0.000244  w[0]    0.015 bias    2.490\n",
      "iter 2500/1000000  loss         0.184774  avg_L1_norm_grad         0.000237  w[0]    0.015 bias    2.518\n",
      "iter 2501/1000000  loss         0.184766  avg_L1_norm_grad         0.000237  w[0]    0.015 bias    2.519\n",
      "iter 2600/1000000  loss         0.183946  avg_L1_norm_grad         0.000230  w[0]    0.015 bias    2.546\n",
      "iter 2601/1000000  loss         0.183938  avg_L1_norm_grad         0.000230  w[0]    0.015 bias    2.546\n",
      "iter 2700/1000000  loss         0.183168  avg_L1_norm_grad         0.000224  w[0]    0.016 bias    2.572\n",
      "iter 2701/1000000  loss         0.183161  avg_L1_norm_grad         0.000224  w[0]    0.016 bias    2.572\n",
      "iter 2800/1000000  loss         0.182436  avg_L1_norm_grad         0.000218  w[0]    0.016 bias    2.597\n",
      "iter 2801/1000000  loss         0.182429  avg_L1_norm_grad         0.000218  w[0]    0.016 bias    2.598\n",
      "iter 2900/1000000  loss         0.181746  avg_L1_norm_grad         0.000212  w[0]    0.016 bias    2.622\n",
      "iter 2901/1000000  loss         0.181740  avg_L1_norm_grad         0.000212  w[0]    0.016 bias    2.622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.181095  avg_L1_norm_grad         0.000206  w[0]    0.016 bias    2.645\n",
      "iter 3001/1000000  loss         0.181089  avg_L1_norm_grad         0.000206  w[0]    0.016 bias    2.645\n",
      "iter 3100/1000000  loss         0.180479  avg_L1_norm_grad         0.000201  w[0]    0.017 bias    2.667\n",
      "iter 3101/1000000  loss         0.180473  avg_L1_norm_grad         0.000201  w[0]    0.017 bias    2.667\n",
      "iter 3200/1000000  loss         0.179895  avg_L1_norm_grad         0.000196  w[0]    0.017 bias    2.689\n",
      "iter 3201/1000000  loss         0.179890  avg_L1_norm_grad         0.000196  w[0]    0.017 bias    2.689\n",
      "iter 3300/1000000  loss         0.179342  avg_L1_norm_grad         0.000192  w[0]    0.017 bias    2.709\n",
      "iter 3301/1000000  loss         0.179337  avg_L1_norm_grad         0.000192  w[0]    0.017 bias    2.710\n",
      "iter 3400/1000000  loss         0.178817  avg_L1_norm_grad         0.000187  w[0]    0.017 bias    2.729\n",
      "iter 3401/1000000  loss         0.178812  avg_L1_norm_grad         0.000187  w[0]    0.017 bias    2.730\n",
      "iter 3500/1000000  loss         0.178317  avg_L1_norm_grad         0.000183  w[0]    0.018 bias    2.749\n",
      "iter 3501/1000000  loss         0.178312  avg_L1_norm_grad         0.000183  w[0]    0.018 bias    2.749\n",
      "iter 3600/1000000  loss         0.177842  avg_L1_norm_grad         0.000179  w[0]    0.018 bias    2.767\n",
      "iter 3601/1000000  loss         0.177837  avg_L1_norm_grad         0.000179  w[0]    0.018 bias    2.767\n",
      "iter 3700/1000000  loss         0.177389  avg_L1_norm_grad         0.000175  w[0]    0.018 bias    2.785\n",
      "iter 3701/1000000  loss         0.177384  avg_L1_norm_grad         0.000175  w[0]    0.018 bias    2.785\n",
      "iter 3800/1000000  loss         0.176957  avg_L1_norm_grad         0.000171  w[0]    0.019 bias    2.803\n",
      "iter 3801/1000000  loss         0.176953  avg_L1_norm_grad         0.000171  w[0]    0.019 bias    2.803\n",
      "iter 3900/1000000  loss         0.176544  avg_L1_norm_grad         0.000168  w[0]    0.019 bias    2.819\n",
      "iter 3901/1000000  loss         0.176540  avg_L1_norm_grad         0.000168  w[0]    0.019 bias    2.820\n",
      "iter 4000/1000000  loss         0.176150  avg_L1_norm_grad         0.000164  w[0]    0.020 bias    2.836\n",
      "iter 4001/1000000  loss         0.176146  avg_L1_norm_grad         0.000164  w[0]    0.020 bias    2.836\n",
      "iter 4100/1000000  loss         0.175774  avg_L1_norm_grad         0.000161  w[0]    0.020 bias    2.851\n",
      "iter 4101/1000000  loss         0.175770  avg_L1_norm_grad         0.000161  w[0]    0.020 bias    2.852\n",
      "iter 4200/1000000  loss         0.175413  avg_L1_norm_grad         0.000158  w[0]    0.020 bias    2.867\n",
      "iter 4201/1000000  loss         0.175409  avg_L1_norm_grad         0.000158  w[0]    0.020 bias    2.867\n",
      "iter 4300/1000000  loss         0.175067  avg_L1_norm_grad         0.000155  w[0]    0.021 bias    2.881\n",
      "iter 4301/1000000  loss         0.175064  avg_L1_norm_grad         0.000155  w[0]    0.021 bias    2.882\n",
      "iter 4400/1000000  loss         0.174736  avg_L1_norm_grad         0.000152  w[0]    0.021 bias    2.896\n",
      "iter 4401/1000000  loss         0.174733  avg_L1_norm_grad         0.000152  w[0]    0.021 bias    2.896\n",
      "iter 4500/1000000  loss         0.174419  avg_L1_norm_grad         0.000149  w[0]    0.021 bias    2.910\n",
      "iter 4501/1000000  loss         0.174416  avg_L1_norm_grad         0.000149  w[0]    0.021 bias    2.910\n",
      "iter 4600/1000000  loss         0.174114  avg_L1_norm_grad         0.000146  w[0]    0.022 bias    2.923\n",
      "iter 4601/1000000  loss         0.174111  avg_L1_norm_grad         0.000146  w[0]    0.022 bias    2.923\n",
      "iter 4700/1000000  loss         0.173821  avg_L1_norm_grad         0.000143  w[0]    0.022 bias    2.936\n",
      "iter 4701/1000000  loss         0.173818  avg_L1_norm_grad         0.000143  w[0]    0.022 bias    2.937\n",
      "iter 4800/1000000  loss         0.173540  avg_L1_norm_grad         0.000141  w[0]    0.023 bias    2.949\n",
      "iter 4801/1000000  loss         0.173537  avg_L1_norm_grad         0.000141  w[0]    0.023 bias    2.949\n",
      "iter 4900/1000000  loss         0.173269  avg_L1_norm_grad         0.000138  w[0]    0.023 bias    2.962\n",
      "iter 4901/1000000  loss         0.173266  avg_L1_norm_grad         0.000138  w[0]    0.023 bias    2.962\n",
      "iter 5000/1000000  loss         0.173009  avg_L1_norm_grad         0.000136  w[0]    0.023 bias    2.974\n",
      "iter 5001/1000000  loss         0.173006  avg_L1_norm_grad         0.000136  w[0]    0.023 bias    2.974\n",
      "iter 5100/1000000  loss         0.172758  avg_L1_norm_grad         0.000133  w[0]    0.024 bias    2.985\n",
      "iter 5101/1000000  loss         0.172756  avg_L1_norm_grad         0.000133  w[0]    0.024 bias    2.986\n",
      "iter 5200/1000000  loss         0.172517  avg_L1_norm_grad         0.000131  w[0]    0.024 bias    2.997\n",
      "iter 5201/1000000  loss         0.172514  avg_L1_norm_grad         0.000131  w[0]    0.024 bias    2.997\n",
      "iter 5300/1000000  loss         0.172284  avg_L1_norm_grad         0.000129  w[0]    0.025 bias    3.008\n",
      "iter 5301/1000000  loss         0.172282  avg_L1_norm_grad         0.000129  w[0]    0.025 bias    3.008\n",
      "iter 5400/1000000  loss         0.172060  avg_L1_norm_grad         0.000127  w[0]    0.025 bias    3.019\n",
      "iter 5401/1000000  loss         0.172058  avg_L1_norm_grad         0.000127  w[0]    0.025 bias    3.019\n",
      "iter 5500/1000000  loss         0.171843  avg_L1_norm_grad         0.000124  w[0]    0.026 bias    3.030\n",
      "iter 5501/1000000  loss         0.171841  avg_L1_norm_grad         0.000124  w[0]    0.026 bias    3.030\n",
      "iter 5600/1000000  loss         0.171635  avg_L1_norm_grad         0.000122  w[0]    0.026 bias    3.040\n",
      "iter 5601/1000000  loss         0.171633  avg_L1_norm_grad         0.000122  w[0]    0.026 bias    3.040\n",
      "iter 5700/1000000  loss         0.171433  avg_L1_norm_grad         0.000120  w[0]    0.026 bias    3.050\n",
      "iter 5701/1000000  loss         0.171431  avg_L1_norm_grad         0.000120  w[0]    0.026 bias    3.050\n",
      "iter 5800/1000000  loss         0.171238  avg_L1_norm_grad         0.000118  w[0]    0.027 bias    3.060\n",
      "iter 5801/1000000  loss         0.171237  avg_L1_norm_grad         0.000118  w[0]    0.027 bias    3.060\n",
      "iter 5900/1000000  loss         0.171050  avg_L1_norm_grad         0.000117  w[0]    0.027 bias    3.069\n",
      "iter 5901/1000000  loss         0.171048  avg_L1_norm_grad         0.000117  w[0]    0.027 bias    3.069\n",
      "iter 6000/1000000  loss         0.170869  avg_L1_norm_grad         0.000115  w[0]    0.028 bias    3.079\n",
      "iter 6001/1000000  loss         0.170867  avg_L1_norm_grad         0.000115  w[0]    0.028 bias    3.079\n",
      "iter 6100/1000000  loss         0.170693  avg_L1_norm_grad         0.000113  w[0]    0.028 bias    3.088\n",
      "iter 6101/1000000  loss         0.170691  avg_L1_norm_grad         0.000113  w[0]    0.028 bias    3.088\n",
      "iter 6200/1000000  loss         0.170523  avg_L1_norm_grad         0.000111  w[0]    0.029 bias    3.097\n",
      "iter 6201/1000000  loss         0.170521  avg_L1_norm_grad         0.000111  w[0]    0.029 bias    3.097\n",
      "iter 6300/1000000  loss         0.170358  avg_L1_norm_grad         0.000109  w[0]    0.029 bias    3.106\n",
      "iter 6301/1000000  loss         0.170357  avg_L1_norm_grad         0.000109  w[0]    0.029 bias    3.106\n",
      "iter 6400/1000000  loss         0.170199  avg_L1_norm_grad         0.000108  w[0]    0.029 bias    3.114\n",
      "iter 6401/1000000  loss         0.170197  avg_L1_norm_grad         0.000108  w[0]    0.029 bias    3.114\n",
      "iter 6500/1000000  loss         0.170045  avg_L1_norm_grad         0.000106  w[0]    0.030 bias    3.122\n",
      "iter 6501/1000000  loss         0.170043  avg_L1_norm_grad         0.000106  w[0]    0.030 bias    3.122\n",
      "iter 6600/1000000  loss         0.169895  avg_L1_norm_grad         0.000104  w[0]    0.030 bias    3.131\n",
      "iter 6601/1000000  loss         0.169894  avg_L1_norm_grad         0.000104  w[0]    0.030 bias    3.131\n",
      "iter 6700/1000000  loss         0.169751  avg_L1_norm_grad         0.000103  w[0]    0.031 bias    3.139\n",
      "iter 6701/1000000  loss         0.169749  avg_L1_norm_grad         0.000103  w[0]    0.031 bias    3.139\n",
      "iter 6800/1000000  loss         0.169610  avg_L1_norm_grad         0.000101  w[0]    0.031 bias    3.146\n",
      "iter 6801/1000000  loss         0.169609  avg_L1_norm_grad         0.000101  w[0]    0.031 bias    3.146\n",
      "iter 6900/1000000  loss         0.169474  avg_L1_norm_grad         0.000100  w[0]    0.032 bias    3.154\n",
      "iter 6901/1000000  loss         0.169473  avg_L1_norm_grad         0.000100  w[0]    0.032 bias    3.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.169343  avg_L1_norm_grad         0.000098  w[0]    0.032 bias    3.161\n",
      "iter 7001/1000000  loss         0.169341  avg_L1_norm_grad         0.000098  w[0]    0.032 bias    3.162\n",
      "iter 7100/1000000  loss         0.169215  avg_L1_norm_grad         0.000097  w[0]    0.032 bias    3.169\n",
      "iter 7101/1000000  loss         0.169214  avg_L1_norm_grad         0.000097  w[0]    0.032 bias    3.169\n",
      "iter 7200/1000000  loss         0.169091  avg_L1_norm_grad         0.000095  w[0]    0.033 bias    3.176\n",
      "iter 7201/1000000  loss         0.169089  avg_L1_norm_grad         0.000095  w[0]    0.033 bias    3.176\n",
      "iter 7300/1000000  loss         0.168970  avg_L1_norm_grad         0.000094  w[0]    0.033 bias    3.183\n",
      "iter 7301/1000000  loss         0.168969  avg_L1_norm_grad         0.000094  w[0]    0.033 bias    3.183\n",
      "iter 7400/1000000  loss         0.168853  avg_L1_norm_grad         0.000093  w[0]    0.034 bias    3.190\n",
      "iter 7401/1000000  loss         0.168852  avg_L1_norm_grad         0.000093  w[0]    0.034 bias    3.190\n",
      "iter 7500/1000000  loss         0.168740  avg_L1_norm_grad         0.000091  w[0]    0.034 bias    3.197\n",
      "iter 7501/1000000  loss         0.168739  avg_L1_norm_grad         0.000091  w[0]    0.034 bias    3.197\n",
      "iter 7600/1000000  loss         0.168630  avg_L1_norm_grad         0.000090  w[0]    0.034 bias    3.203\n",
      "iter 7601/1000000  loss         0.168629  avg_L1_norm_grad         0.000090  w[0]    0.034 bias    3.203\n",
      "iter 7700/1000000  loss         0.168523  avg_L1_norm_grad         0.000089  w[0]    0.035 bias    3.210\n",
      "iter 7701/1000000  loss         0.168522  avg_L1_norm_grad         0.000089  w[0]    0.035 bias    3.210\n",
      "iter 7800/1000000  loss         0.168419  avg_L1_norm_grad         0.000088  w[0]    0.035 bias    3.216\n",
      "iter 7801/1000000  loss         0.168418  avg_L1_norm_grad         0.000088  w[0]    0.035 bias    3.216\n",
      "iter 7900/1000000  loss         0.168318  avg_L1_norm_grad         0.000086  w[0]    0.036 bias    3.222\n",
      "iter 7901/1000000  loss         0.168317  avg_L1_norm_grad         0.000086  w[0]    0.036 bias    3.222\n",
      "iter 8000/1000000  loss         0.168219  avg_L1_norm_grad         0.000085  w[0]    0.036 bias    3.228\n",
      "iter 8001/1000000  loss         0.168218  avg_L1_norm_grad         0.000085  w[0]    0.036 bias    3.229\n",
      "iter 8100/1000000  loss         0.168124  avg_L1_norm_grad         0.000084  w[0]    0.037 bias    3.234\n",
      "iter 8101/1000000  loss         0.168123  avg_L1_norm_grad         0.000084  w[0]    0.037 bias    3.235\n",
      "iter 8200/1000000  loss         0.168031  avg_L1_norm_grad         0.000083  w[0]    0.037 bias    3.240\n",
      "iter 8201/1000000  loss         0.168030  avg_L1_norm_grad         0.000083  w[0]    0.037 bias    3.240\n",
      "iter 8300/1000000  loss         0.167940  avg_L1_norm_grad         0.000082  w[0]    0.037 bias    3.246\n",
      "iter 8301/1000000  loss         0.167940  avg_L1_norm_grad         0.000082  w[0]    0.037 bias    3.246\n",
      "iter 8400/1000000  loss         0.167853  avg_L1_norm_grad         0.000081  w[0]    0.038 bias    3.252\n",
      "iter 8401/1000000  loss         0.167852  avg_L1_norm_grad         0.000081  w[0]    0.038 bias    3.252\n",
      "iter 8500/1000000  loss         0.167767  avg_L1_norm_grad         0.000080  w[0]    0.038 bias    3.257\n",
      "iter 8501/1000000  loss         0.167766  avg_L1_norm_grad         0.000080  w[0]    0.038 bias    3.257\n",
      "iter 8600/1000000  loss         0.167684  avg_L1_norm_grad         0.000079  w[0]    0.039 bias    3.263\n",
      "iter 8601/1000000  loss         0.167683  avg_L1_norm_grad         0.000079  w[0]    0.039 bias    3.263\n",
      "iter 8700/1000000  loss         0.167603  avg_L1_norm_grad         0.000078  w[0]    0.039 bias    3.268\n",
      "iter 8701/1000000  loss         0.167602  avg_L1_norm_grad         0.000078  w[0]    0.039 bias    3.268\n",
      "iter 8800/1000000  loss         0.167524  avg_L1_norm_grad         0.000077  w[0]    0.039 bias    3.274\n",
      "iter 8801/1000000  loss         0.167523  avg_L1_norm_grad         0.000077  w[0]    0.039 bias    3.274\n",
      "iter 8900/1000000  loss         0.167447  avg_L1_norm_grad         0.000076  w[0]    0.040 bias    3.279\n",
      "iter 8901/1000000  loss         0.167446  avg_L1_norm_grad         0.000076  w[0]    0.040 bias    3.279\n",
      "iter 9000/1000000  loss         0.167372  avg_L1_norm_grad         0.000075  w[0]    0.040 bias    3.284\n",
      "iter 9001/1000000  loss         0.167371  avg_L1_norm_grad         0.000075  w[0]    0.040 bias    3.284\n",
      "iter 9100/1000000  loss         0.167299  avg_L1_norm_grad         0.000074  w[0]    0.041 bias    3.289\n",
      "iter 9101/1000000  loss         0.167299  avg_L1_norm_grad         0.000074  w[0]    0.041 bias    3.289\n",
      "iter 9200/1000000  loss         0.167228  avg_L1_norm_grad         0.000073  w[0]    0.041 bias    3.294\n",
      "iter 9201/1000000  loss         0.167228  avg_L1_norm_grad         0.000073  w[0]    0.041 bias    3.294\n",
      "iter 9300/1000000  loss         0.167159  avg_L1_norm_grad         0.000072  w[0]    0.041 bias    3.299\n",
      "iter 9301/1000000  loss         0.167159  avg_L1_norm_grad         0.000072  w[0]    0.041 bias    3.299\n",
      "iter 9400/1000000  loss         0.167092  avg_L1_norm_grad         0.000071  w[0]    0.042 bias    3.303\n",
      "iter 9401/1000000  loss         0.167091  avg_L1_norm_grad         0.000071  w[0]    0.042 bias    3.303\n",
      "iter 9500/1000000  loss         0.167026  avg_L1_norm_grad         0.000070  w[0]    0.042 bias    3.308\n",
      "iter 9501/1000000  loss         0.167026  avg_L1_norm_grad         0.000070  w[0]    0.042 bias    3.308\n",
      "iter 9600/1000000  loss         0.166962  avg_L1_norm_grad         0.000069  w[0]    0.042 bias    3.313\n",
      "iter 9601/1000000  loss         0.166962  avg_L1_norm_grad         0.000069  w[0]    0.042 bias    3.313\n",
      "iter 9700/1000000  loss         0.166900  avg_L1_norm_grad         0.000068  w[0]    0.043 bias    3.317\n",
      "iter 9701/1000000  loss         0.166899  avg_L1_norm_grad         0.000068  w[0]    0.043 bias    3.317\n",
      "iter 9800/1000000  loss         0.166839  avg_L1_norm_grad         0.000067  w[0]    0.043 bias    3.322\n",
      "iter 9801/1000000  loss         0.166838  avg_L1_norm_grad         0.000067  w[0]    0.043 bias    3.322\n",
      "iter 9900/1000000  loss         0.166780  avg_L1_norm_grad         0.000067  w[0]    0.044 bias    3.326\n",
      "iter 9901/1000000  loss         0.166779  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    3.326\n",
      "iter 10000/1000000  loss         0.166722  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    3.330\n",
      "iter 10001/1000000  loss         0.166721  avg_L1_norm_grad         0.000066  w[0]    0.044 bias    3.331\n",
      "iter 10100/1000000  loss         0.166665  avg_L1_norm_grad         0.000065  w[0]    0.044 bias    3.335\n",
      "iter 10101/1000000  loss         0.166665  avg_L1_norm_grad         0.000065  w[0]    0.044 bias    3.335\n",
      "iter 10200/1000000  loss         0.166610  avg_L1_norm_grad         0.000064  w[0]    0.045 bias    3.339\n",
      "iter 10201/1000000  loss         0.166610  avg_L1_norm_grad         0.000064  w[0]    0.045 bias    3.339\n",
      "iter 10300/1000000  loss         0.166556  avg_L1_norm_grad         0.000063  w[0]    0.045 bias    3.343\n",
      "iter 10301/1000000  loss         0.166556  avg_L1_norm_grad         0.000063  w[0]    0.045 bias    3.343\n",
      "iter 10400/1000000  loss         0.166504  avg_L1_norm_grad         0.000063  w[0]    0.045 bias    3.347\n",
      "iter 10401/1000000  loss         0.166503  avg_L1_norm_grad         0.000063  w[0]    0.045 bias    3.347\n",
      "iter 10500/1000000  loss         0.166453  avg_L1_norm_grad         0.000062  w[0]    0.046 bias    3.351\n",
      "iter 10501/1000000  loss         0.166452  avg_L1_norm_grad         0.000062  w[0]    0.046 bias    3.351\n",
      "iter 10600/1000000  loss         0.166403  avg_L1_norm_grad         0.000061  w[0]    0.046 bias    3.355\n",
      "iter 10601/1000000  loss         0.166402  avg_L1_norm_grad         0.000061  w[0]    0.046 bias    3.355\n",
      "iter 10700/1000000  loss         0.166354  avg_L1_norm_grad         0.000060  w[0]    0.047 bias    3.359\n",
      "iter 10701/1000000  loss         0.166354  avg_L1_norm_grad         0.000060  w[0]    0.047 bias    3.359\n",
      "iter 10800/1000000  loss         0.166306  avg_L1_norm_grad         0.000060  w[0]    0.047 bias    3.363\n",
      "iter 10801/1000000  loss         0.166306  avg_L1_norm_grad         0.000060  w[0]    0.047 bias    3.363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.166260  avg_L1_norm_grad         0.000059  w[0]    0.047 bias    3.366\n",
      "iter 10901/1000000  loss         0.166259  avg_L1_norm_grad         0.000059  w[0]    0.047 bias    3.367\n",
      "iter 11000/1000000  loss         0.166215  avg_L1_norm_grad         0.000058  w[0]    0.048 bias    3.370\n",
      "iter 11001/1000000  loss         0.166214  avg_L1_norm_grad         0.000058  w[0]    0.048 bias    3.370\n",
      "iter 11100/1000000  loss         0.166170  avg_L1_norm_grad         0.000058  w[0]    0.048 bias    3.374\n",
      "iter 11101/1000000  loss         0.166170  avg_L1_norm_grad         0.000058  w[0]    0.048 bias    3.374\n",
      "iter 11200/1000000  loss         0.166127  avg_L1_norm_grad         0.000057  w[0]    0.048 bias    3.377\n",
      "iter 11201/1000000  loss         0.166126  avg_L1_norm_grad         0.000057  w[0]    0.048 bias    3.377\n",
      "iter 11300/1000000  loss         0.166085  avg_L1_norm_grad         0.000056  w[0]    0.049 bias    3.381\n",
      "iter 11301/1000000  loss         0.166084  avg_L1_norm_grad         0.000056  w[0]    0.049 bias    3.381\n",
      "iter 11400/1000000  loss         0.166043  avg_L1_norm_grad         0.000056  w[0]    0.049 bias    3.384\n",
      "iter 11401/1000000  loss         0.166043  avg_L1_norm_grad         0.000056  w[0]    0.049 bias    3.385\n",
      "iter 11500/1000000  loss         0.166003  avg_L1_norm_grad         0.000055  w[0]    0.049 bias    3.388\n",
      "iter 11501/1000000  loss         0.166003  avg_L1_norm_grad         0.000055  w[0]    0.049 bias    3.388\n",
      "iter 11600/1000000  loss         0.165964  avg_L1_norm_grad         0.000054  w[0]    0.050 bias    3.391\n",
      "iter 11601/1000000  loss         0.165963  avg_L1_norm_grad         0.000054  w[0]    0.050 bias    3.391\n",
      "iter 11700/1000000  loss         0.165925  avg_L1_norm_grad         0.000054  w[0]    0.050 bias    3.395\n",
      "iter 11701/1000000  loss         0.165925  avg_L1_norm_grad         0.000054  w[0]    0.050 bias    3.395\n",
      "iter 11800/1000000  loss         0.165887  avg_L1_norm_grad         0.000053  w[0]    0.050 bias    3.398\n",
      "iter 11801/1000000  loss         0.165887  avg_L1_norm_grad         0.000053  w[0]    0.050 bias    3.398\n",
      "iter 11900/1000000  loss         0.165851  avg_L1_norm_grad         0.000053  w[0]    0.051 bias    3.401\n",
      "iter 11901/1000000  loss         0.165850  avg_L1_norm_grad         0.000053  w[0]    0.051 bias    3.401\n",
      "iter 12000/1000000  loss         0.165815  avg_L1_norm_grad         0.000052  w[0]    0.051 bias    3.404\n",
      "iter 12001/1000000  loss         0.165814  avg_L1_norm_grad         0.000052  w[0]    0.051 bias    3.404\n",
      "iter 12100/1000000  loss         0.165779  avg_L1_norm_grad         0.000051  w[0]    0.051 bias    3.408\n",
      "iter 12101/1000000  loss         0.165779  avg_L1_norm_grad         0.000051  w[0]    0.051 bias    3.408\n",
      "iter 12200/1000000  loss         0.165745  avg_L1_norm_grad         0.000051  w[0]    0.052 bias    3.411\n",
      "iter 12201/1000000  loss         0.165745  avg_L1_norm_grad         0.000051  w[0]    0.052 bias    3.411\n",
      "iter 12300/1000000  loss         0.165711  avg_L1_norm_grad         0.000050  w[0]    0.052 bias    3.414\n",
      "iter 12301/1000000  loss         0.165711  avg_L1_norm_grad         0.000050  w[0]    0.052 bias    3.414\n",
      "iter 12400/1000000  loss         0.165678  avg_L1_norm_grad         0.000050  w[0]    0.052 bias    3.417\n",
      "iter 12401/1000000  loss         0.165678  avg_L1_norm_grad         0.000050  w[0]    0.052 bias    3.417\n",
      "iter 12500/1000000  loss         0.165646  avg_L1_norm_grad         0.000049  w[0]    0.053 bias    3.420\n",
      "iter 12501/1000000  loss         0.165646  avg_L1_norm_grad         0.000049  w[0]    0.053 bias    3.420\n",
      "iter 12600/1000000  loss         0.165615  avg_L1_norm_grad         0.000049  w[0]    0.053 bias    3.423\n",
      "iter 12601/1000000  loss         0.165615  avg_L1_norm_grad         0.000049  w[0]    0.053 bias    3.423\n",
      "iter 12700/1000000  loss         0.165584  avg_L1_norm_grad         0.000048  w[0]    0.053 bias    3.426\n",
      "iter 12701/1000000  loss         0.165584  avg_L1_norm_grad         0.000048  w[0]    0.053 bias    3.426\n",
      "iter 12800/1000000  loss         0.165554  avg_L1_norm_grad         0.000047  w[0]    0.053 bias    3.429\n",
      "iter 12801/1000000  loss         0.165554  avg_L1_norm_grad         0.000047  w[0]    0.053 bias    3.429\n",
      "iter 12900/1000000  loss         0.165525  avg_L1_norm_grad         0.000047  w[0]    0.054 bias    3.431\n",
      "iter 12901/1000000  loss         0.165524  avg_L1_norm_grad         0.000047  w[0]    0.054 bias    3.431\n",
      "iter 13000/1000000  loss         0.165496  avg_L1_norm_grad         0.000046  w[0]    0.054 bias    3.434\n",
      "iter 13001/1000000  loss         0.165496  avg_L1_norm_grad         0.000046  w[0]    0.054 bias    3.434\n",
      "iter 13100/1000000  loss         0.165468  avg_L1_norm_grad         0.000046  w[0]    0.054 bias    3.437\n",
      "iter 13101/1000000  loss         0.165467  avg_L1_norm_grad         0.000046  w[0]    0.054 bias    3.437\n",
      "iter 13200/1000000  loss         0.165440  avg_L1_norm_grad         0.000045  w[0]    0.055 bias    3.440\n",
      "iter 13201/1000000  loss         0.165440  avg_L1_norm_grad         0.000045  w[0]    0.055 bias    3.440\n",
      "iter 13300/1000000  loss         0.165413  avg_L1_norm_grad         0.000045  w[0]    0.055 bias    3.442\n",
      "iter 13301/1000000  loss         0.165413  avg_L1_norm_grad         0.000045  w[0]    0.055 bias    3.442\n",
      "iter 13400/1000000  loss         0.165387  avg_L1_norm_grad         0.000045  w[0]    0.055 bias    3.445\n",
      "iter 13401/1000000  loss         0.165386  avg_L1_norm_grad         0.000044  w[0]    0.055 bias    3.445\n",
      "iter 13500/1000000  loss         0.165361  avg_L1_norm_grad         0.000044  w[0]    0.056 bias    3.448\n",
      "iter 13501/1000000  loss         0.165361  avg_L1_norm_grad         0.000044  w[0]    0.056 bias    3.448\n",
      "iter 13600/1000000  loss         0.165336  avg_L1_norm_grad         0.000044  w[0]    0.056 bias    3.450\n",
      "iter 13601/1000000  loss         0.165335  avg_L1_norm_grad         0.000044  w[0]    0.056 bias    3.450\n",
      "iter 13700/1000000  loss         0.165311  avg_L1_norm_grad         0.000043  w[0]    0.056 bias    3.453\n",
      "iter 13701/1000000  loss         0.165311  avg_L1_norm_grad         0.000043  w[0]    0.056 bias    3.453\n",
      "iter 13800/1000000  loss         0.165287  avg_L1_norm_grad         0.000043  w[0]    0.056 bias    3.455\n",
      "iter 13801/1000000  loss         0.165286  avg_L1_norm_grad         0.000043  w[0]    0.056 bias    3.455\n",
      "iter 13900/1000000  loss         0.165263  avg_L1_norm_grad         0.000042  w[0]    0.057 bias    3.458\n",
      "iter 13901/1000000  loss         0.165263  avg_L1_norm_grad         0.000042  w[0]    0.057 bias    3.458\n",
      "iter 14000/1000000  loss         0.165240  avg_L1_norm_grad         0.000042  w[0]    0.057 bias    3.460\n",
      "iter 14001/1000000  loss         0.165240  avg_L1_norm_grad         0.000042  w[0]    0.057 bias    3.460\n",
      "iter 14100/1000000  loss         0.165217  avg_L1_norm_grad         0.000041  w[0]    0.057 bias    3.463\n",
      "iter 14101/1000000  loss         0.165217  avg_L1_norm_grad         0.000041  w[0]    0.057 bias    3.463\n",
      "iter 14200/1000000  loss         0.165195  avg_L1_norm_grad         0.000041  w[0]    0.058 bias    3.465\n",
      "iter 14201/1000000  loss         0.165195  avg_L1_norm_grad         0.000041  w[0]    0.058 bias    3.465\n",
      "iter 14300/1000000  loss         0.165173  avg_L1_norm_grad         0.000040  w[0]    0.058 bias    3.468\n",
      "iter 14301/1000000  loss         0.165173  avg_L1_norm_grad         0.000040  w[0]    0.058 bias    3.468\n",
      "iter 14400/1000000  loss         0.165152  avg_L1_norm_grad         0.000040  w[0]    0.058 bias    3.470\n",
      "iter 14401/1000000  loss         0.165151  avg_L1_norm_grad         0.000040  w[0]    0.058 bias    3.470\n",
      "iter 14500/1000000  loss         0.165131  avg_L1_norm_grad         0.000040  w[0]    0.058 bias    3.472\n",
      "iter 14501/1000000  loss         0.165131  avg_L1_norm_grad         0.000040  w[0]    0.058 bias    3.472\n",
      "iter 14600/1000000  loss         0.165110  avg_L1_norm_grad         0.000039  w[0]    0.059 bias    3.474\n",
      "iter 14601/1000000  loss         0.165110  avg_L1_norm_grad         0.000039  w[0]    0.059 bias    3.475\n",
      "iter 14700/1000000  loss         0.165090  avg_L1_norm_grad         0.000039  w[0]    0.059 bias    3.477\n",
      "iter 14701/1000000  loss         0.165090  avg_L1_norm_grad         0.000039  w[0]    0.059 bias    3.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14800/1000000  loss         0.165071  avg_L1_norm_grad         0.000038  w[0]    0.059 bias    3.479\n",
      "iter 14801/1000000  loss         0.165070  avg_L1_norm_grad         0.000038  w[0]    0.059 bias    3.479\n",
      "iter 14900/1000000  loss         0.165051  avg_L1_norm_grad         0.000038  w[0]    0.059 bias    3.481\n",
      "iter 14901/1000000  loss         0.165051  avg_L1_norm_grad         0.000038  w[0]    0.059 bias    3.481\n",
      "iter 15000/1000000  loss         0.165033  avg_L1_norm_grad         0.000038  w[0]    0.060 bias    3.483\n",
      "iter 15001/1000000  loss         0.165032  avg_L1_norm_grad         0.000038  w[0]    0.060 bias    3.483\n",
      "iter 15100/1000000  loss         0.165014  avg_L1_norm_grad         0.000037  w[0]    0.060 bias    3.486\n",
      "iter 15101/1000000  loss         0.165014  avg_L1_norm_grad         0.000037  w[0]    0.060 bias    3.486\n",
      "iter 15200/1000000  loss         0.164996  avg_L1_norm_grad         0.000037  w[0]    0.060 bias    3.488\n",
      "iter 15201/1000000  loss         0.164996  avg_L1_norm_grad         0.000037  w[0]    0.060 bias    3.488\n",
      "iter 15300/1000000  loss         0.164978  avg_L1_norm_grad         0.000036  w[0]    0.060 bias    3.490\n",
      "iter 15301/1000000  loss         0.164978  avg_L1_norm_grad         0.000036  w[0]    0.060 bias    3.490\n",
      "iter 15400/1000000  loss         0.164961  avg_L1_norm_grad         0.000036  w[0]    0.061 bias    3.492\n",
      "iter 15401/1000000  loss         0.164961  avg_L1_norm_grad         0.000036  w[0]    0.061 bias    3.492\n",
      "iter 15500/1000000  loss         0.164944  avg_L1_norm_grad         0.000036  w[0]    0.061 bias    3.494\n",
      "iter 15501/1000000  loss         0.164944  avg_L1_norm_grad         0.000036  w[0]    0.061 bias    3.494\n",
      "iter 15600/1000000  loss         0.164927  avg_L1_norm_grad         0.000035  w[0]    0.061 bias    3.496\n",
      "iter 15601/1000000  loss         0.164927  avg_L1_norm_grad         0.000035  w[0]    0.061 bias    3.496\n",
      "iter 15700/1000000  loss         0.164911  avg_L1_norm_grad         0.000035  w[0]    0.061 bias    3.498\n",
      "iter 15701/1000000  loss         0.164911  avg_L1_norm_grad         0.000035  w[0]    0.061 bias    3.498\n",
      "iter 15800/1000000  loss         0.164895  avg_L1_norm_grad         0.000035  w[0]    0.062 bias    3.500\n",
      "iter 15801/1000000  loss         0.164895  avg_L1_norm_grad         0.000035  w[0]    0.062 bias    3.500\n",
      "iter 15900/1000000  loss         0.164879  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    3.502\n",
      "iter 15901/1000000  loss         0.164879  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    3.502\n",
      "iter 16000/1000000  loss         0.164864  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    3.504\n",
      "iter 16001/1000000  loss         0.164864  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    3.504\n",
      "iter 16100/1000000  loss         0.164849  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    3.506\n",
      "iter 16101/1000000  loss         0.164849  avg_L1_norm_grad         0.000034  w[0]    0.062 bias    3.506\n",
      "iter 16200/1000000  loss         0.164834  avg_L1_norm_grad         0.000033  w[0]    0.063 bias    3.508\n",
      "iter 16201/1000000  loss         0.164834  avg_L1_norm_grad         0.000033  w[0]    0.063 bias    3.508\n",
      "iter 16300/1000000  loss         0.164820  avg_L1_norm_grad         0.000033  w[0]    0.063 bias    3.509\n",
      "iter 16301/1000000  loss         0.164819  avg_L1_norm_grad         0.000033  w[0]    0.063 bias    3.509\n",
      "iter 16400/1000000  loss         0.164805  avg_L1_norm_grad         0.000033  w[0]    0.063 bias    3.511\n",
      "iter 16401/1000000  loss         0.164805  avg_L1_norm_grad         0.000033  w[0]    0.063 bias    3.511\n",
      "iter 16500/1000000  loss         0.164791  avg_L1_norm_grad         0.000032  w[0]    0.063 bias    3.513\n",
      "iter 16501/1000000  loss         0.164791  avg_L1_norm_grad         0.000032  w[0]    0.063 bias    3.513\n",
      "iter 16600/1000000  loss         0.164778  avg_L1_norm_grad         0.000032  w[0]    0.064 bias    3.515\n",
      "iter 16601/1000000  loss         0.164778  avg_L1_norm_grad         0.000032  w[0]    0.064 bias    3.515\n",
      "iter 16700/1000000  loss         0.164764  avg_L1_norm_grad         0.000032  w[0]    0.064 bias    3.517\n",
      "iter 16701/1000000  loss         0.164764  avg_L1_norm_grad         0.000032  w[0]    0.064 bias    3.517\n",
      "iter 16800/1000000  loss         0.164751  avg_L1_norm_grad         0.000031  w[0]    0.064 bias    3.518\n",
      "iter 16801/1000000  loss         0.164751  avg_L1_norm_grad         0.000031  w[0]    0.064 bias    3.518\n",
      "iter 16900/1000000  loss         0.164738  avg_L1_norm_grad         0.000031  w[0]    0.064 bias    3.520\n",
      "iter 16901/1000000  loss         0.164738  avg_L1_norm_grad         0.000031  w[0]    0.064 bias    3.520\n",
      "iter 17000/1000000  loss         0.164726  avg_L1_norm_grad         0.000031  w[0]    0.065 bias    3.522\n",
      "iter 17001/1000000  loss         0.164726  avg_L1_norm_grad         0.000031  w[0]    0.065 bias    3.522\n",
      "iter 17100/1000000  loss         0.164713  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.524\n",
      "iter 17101/1000000  loss         0.164713  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.524\n",
      "iter 17200/1000000  loss         0.164701  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.525\n",
      "iter 17201/1000000  loss         0.164701  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.525\n",
      "iter 17300/1000000  loss         0.164689  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.527\n",
      "iter 17301/1000000  loss         0.164689  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.527\n",
      "iter 17400/1000000  loss         0.164678  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.529\n",
      "iter 17401/1000000  loss         0.164678  avg_L1_norm_grad         0.000030  w[0]    0.065 bias    3.529\n",
      "iter 17500/1000000  loss         0.164666  avg_L1_norm_grad         0.000029  w[0]    0.066 bias    3.530\n",
      "iter 17501/1000000  loss         0.164666  avg_L1_norm_grad         0.000029  w[0]    0.066 bias    3.530\n",
      "iter 17600/1000000  loss         0.164655  avg_L1_norm_grad         0.000029  w[0]    0.066 bias    3.532\n",
      "iter 17601/1000000  loss         0.164655  avg_L1_norm_grad         0.000029  w[0]    0.066 bias    3.532\n",
      "iter 17700/1000000  loss         0.164644  avg_L1_norm_grad         0.000029  w[0]    0.066 bias    3.533\n",
      "iter 17701/1000000  loss         0.164644  avg_L1_norm_grad         0.000029  w[0]    0.066 bias    3.533\n",
      "iter 17800/1000000  loss         0.164633  avg_L1_norm_grad         0.000028  w[0]    0.066 bias    3.535\n",
      "iter 17801/1000000  loss         0.164633  avg_L1_norm_grad         0.000028  w[0]    0.066 bias    3.535\n",
      "iter 17900/1000000  loss         0.164623  avg_L1_norm_grad         0.000028  w[0]    0.066 bias    3.537\n",
      "iter 17901/1000000  loss         0.164622  avg_L1_norm_grad         0.000028  w[0]    0.066 bias    3.537\n",
      "iter 18000/1000000  loss         0.164612  avg_L1_norm_grad         0.000028  w[0]    0.067 bias    3.538\n",
      "iter 18001/1000000  loss         0.164612  avg_L1_norm_grad         0.000028  w[0]    0.067 bias    3.538\n",
      "iter 18100/1000000  loss         0.164602  avg_L1_norm_grad         0.000028  w[0]    0.067 bias    3.540\n",
      "iter 18101/1000000  loss         0.164602  avg_L1_norm_grad         0.000028  w[0]    0.067 bias    3.540\n",
      "iter 18200/1000000  loss         0.164592  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.541\n",
      "iter 18201/1000000  loss         0.164592  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.541\n",
      "iter 18300/1000000  loss         0.164582  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.543\n",
      "iter 18301/1000000  loss         0.164582  avg_L1_norm_grad         0.000027  w[0]    0.067 bias    3.543\n",
      "iter 18400/1000000  loss         0.164572  avg_L1_norm_grad         0.000027  w[0]    0.068 bias    3.544\n",
      "iter 18401/1000000  loss         0.164572  avg_L1_norm_grad         0.000027  w[0]    0.068 bias    3.544\n",
      "iter 18500/1000000  loss         0.164563  avg_L1_norm_grad         0.000027  w[0]    0.068 bias    3.546\n",
      "iter 18501/1000000  loss         0.164563  avg_L1_norm_grad         0.000027  w[0]    0.068 bias    3.546\n",
      "iter 18600/1000000  loss         0.164554  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.547\n",
      "iter 18601/1000000  loss         0.164554  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18700/1000000  loss         0.164545  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.548\n",
      "iter 18701/1000000  loss         0.164545  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.548\n",
      "iter 18800/1000000  loss         0.164536  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.550\n",
      "iter 18801/1000000  loss         0.164536  avg_L1_norm_grad         0.000026  w[0]    0.068 bias    3.550\n",
      "iter 18900/1000000  loss         0.164527  avg_L1_norm_grad         0.000026  w[0]    0.069 bias    3.551\n",
      "iter 18901/1000000  loss         0.164527  avg_L1_norm_grad         0.000026  w[0]    0.069 bias    3.551\n",
      "iter 19000/1000000  loss         0.164518  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.553\n",
      "iter 19001/1000000  loss         0.164518  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.553\n",
      "iter 19100/1000000  loss         0.164510  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.554\n",
      "iter 19101/1000000  loss         0.164510  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.554\n",
      "iter 19200/1000000  loss         0.164502  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.555\n",
      "iter 19201/1000000  loss         0.164502  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.555\n",
      "iter 19300/1000000  loss         0.164493  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.557\n",
      "iter 19301/1000000  loss         0.164493  avg_L1_norm_grad         0.000025  w[0]    0.069 bias    3.557\n",
      "iter 19400/1000000  loss         0.164486  avg_L1_norm_grad         0.000024  w[0]    0.069 bias    3.558\n",
      "iter 19401/1000000  loss         0.164485  avg_L1_norm_grad         0.000024  w[0]    0.069 bias    3.558\n",
      "iter 19500/1000000  loss         0.164478  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.559\n",
      "iter 19501/1000000  loss         0.164478  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.559\n",
      "iter 19600/1000000  loss         0.164470  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.561\n",
      "iter 19601/1000000  loss         0.164470  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.561\n",
      "iter 19700/1000000  loss         0.164462  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.562\n",
      "iter 19701/1000000  loss         0.164462  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.562\n",
      "iter 19800/1000000  loss         0.164455  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.563\n",
      "iter 19801/1000000  loss         0.164455  avg_L1_norm_grad         0.000024  w[0]    0.070 bias    3.563\n",
      "iter 19900/1000000  loss         0.164448  avg_L1_norm_grad         0.000023  w[0]    0.070 bias    3.564\n",
      "iter 19901/1000000  loss         0.164448  avg_L1_norm_grad         0.000023  w[0]    0.070 bias    3.564\n",
      "iter 20000/1000000  loss         0.164441  avg_L1_norm_grad         0.000023  w[0]    0.071 bias    3.566\n",
      "iter 20001/1000000  loss         0.164441  avg_L1_norm_grad         0.000023  w[0]    0.071 bias    3.566\n",
      "iter 20100/1000000  loss         0.164434  avg_L1_norm_grad         0.000023  w[0]    0.071 bias    3.567\n",
      "iter 20101/1000000  loss         0.164434  avg_L1_norm_grad         0.000023  w[0]    0.071 bias    3.567\n",
      "iter 20200/1000000  loss         0.164427  avg_L1_norm_grad         0.000023  w[0]    0.071 bias    3.568\n",
      "iter 20201/1000000  loss         0.164427  avg_L1_norm_grad         0.000023  w[0]    0.071 bias    3.568\n",
      "iter 20300/1000000  loss         0.164420  avg_L1_norm_grad         0.000022  w[0]    0.071 bias    3.569\n",
      "iter 20301/1000000  loss         0.164420  avg_L1_norm_grad         0.000022  w[0]    0.071 bias    3.569\n",
      "iter 20400/1000000  loss         0.164413  avg_L1_norm_grad         0.000022  w[0]    0.071 bias    3.571\n",
      "iter 20401/1000000  loss         0.164413  avg_L1_norm_grad         0.000022  w[0]    0.071 bias    3.571\n",
      "iter 20500/1000000  loss         0.164407  avg_L1_norm_grad         0.000022  w[0]    0.071 bias    3.572\n",
      "iter 20501/1000000  loss         0.164407  avg_L1_norm_grad         0.000022  w[0]    0.071 bias    3.572\n",
      "iter 20600/1000000  loss         0.164400  avg_L1_norm_grad         0.000022  w[0]    0.072 bias    3.573\n",
      "iter 20601/1000000  loss         0.164400  avg_L1_norm_grad         0.000022  w[0]    0.072 bias    3.573\n",
      "iter 20700/1000000  loss         0.164394  avg_L1_norm_grad         0.000022  w[0]    0.072 bias    3.574\n",
      "iter 20701/1000000  loss         0.164394  avg_L1_norm_grad         0.000022  w[0]    0.072 bias    3.574\n",
      "iter 20800/1000000  loss         0.164388  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.575\n",
      "iter 20801/1000000  loss         0.164388  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.575\n",
      "iter 20900/1000000  loss         0.164382  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.576\n",
      "iter 20901/1000000  loss         0.164382  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.576\n",
      "iter 21000/1000000  loss         0.164376  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.577\n",
      "iter 21001/1000000  loss         0.164376  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.578\n",
      "iter 21100/1000000  loss         0.164370  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.579\n",
      "iter 21101/1000000  loss         0.164370  avg_L1_norm_grad         0.000021  w[0]    0.072 bias    3.579\n",
      "iter 21200/1000000  loss         0.164364  avg_L1_norm_grad         0.000021  w[0]    0.073 bias    3.580\n",
      "iter 21201/1000000  loss         0.164364  avg_L1_norm_grad         0.000021  w[0]    0.073 bias    3.580\n",
      "iter 21300/1000000  loss         0.164359  avg_L1_norm_grad         0.000021  w[0]    0.073 bias    3.581\n",
      "iter 21301/1000000  loss         0.164359  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.581\n",
      "iter 21400/1000000  loss         0.164353  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.582\n",
      "iter 21401/1000000  loss         0.164353  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.582\n",
      "iter 21500/1000000  loss         0.164348  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.583\n",
      "iter 21501/1000000  loss         0.164348  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.583\n",
      "iter 21600/1000000  loss         0.164342  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.584\n",
      "iter 21601/1000000  loss         0.164342  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.584\n",
      "iter 21700/1000000  loss         0.164337  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.585\n",
      "iter 21701/1000000  loss         0.164337  avg_L1_norm_grad         0.000020  w[0]    0.073 bias    3.585\n",
      "iter 21800/1000000  loss         0.164332  avg_L1_norm_grad         0.000020  w[0]    0.074 bias    3.586\n",
      "iter 21801/1000000  loss         0.164332  avg_L1_norm_grad         0.000020  w[0]    0.074 bias    3.586\n",
      "iter 21900/1000000  loss         0.164327  avg_L1_norm_grad         0.000019  w[0]    0.074 bias    3.587\n",
      "iter 21901/1000000  loss         0.164327  avg_L1_norm_grad         0.000019  w[0]    0.074 bias    3.587\n",
      "iter 22000/1000000  loss         0.164322  avg_L1_norm_grad         0.000019  w[0]    0.074 bias    3.588\n",
      "iter 22001/1000000  loss         0.164322  avg_L1_norm_grad         0.000019  w[0]    0.074 bias    3.588\n",
      "Done. Converged after 22045 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on original features!\n",
    "orig_lr = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "TurnOn Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 787 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.030089  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.910876  avg_L1_norm_grad         0.028369  w[0]   -0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.846841  avg_L1_norm_grad         0.019586  w[0]    0.001 bias    0.021\n",
      "iter    3/1000000  loss         0.802489  avg_L1_norm_grad         0.018452  w[0]    0.001 bias    0.025\n",
      "iter    4/1000000  loss         0.768928  avg_L1_norm_grad         0.013985  w[0]    0.001 bias    0.040\n",
      "iter    5/1000000  loss         0.743012  avg_L1_norm_grad         0.013410  w[0]    0.001 bias    0.047\n",
      "iter    6/1000000  loss         0.721819  avg_L1_norm_grad         0.011447  w[0]    0.001 bias    0.059\n",
      "iter    7/1000000  loss         0.703842  avg_L1_norm_grad         0.010956  w[0]    0.002 bias    0.068\n",
      "iter    8/1000000  loss         0.688099  avg_L1_norm_grad         0.010037  w[0]    0.002 bias    0.078\n",
      "iter    9/1000000  loss         0.674008  avg_L1_norm_grad         0.009578  w[0]    0.002 bias    0.087\n",
      "iter   10/1000000  loss         0.661192  avg_L1_norm_grad         0.009058  w[0]    0.002 bias    0.096\n",
      "iter   11/1000000  loss         0.649400  avg_L1_norm_grad         0.008671  w[0]    0.002 bias    0.105\n",
      "iter   12/1000000  loss         0.638453  avg_L1_norm_grad         0.008307  w[0]    0.003 bias    0.113\n",
      "iter   13/1000000  loss         0.628220  avg_L1_norm_grad         0.007995  w[0]    0.003 bias    0.122\n",
      "iter   14/1000000  loss         0.618603  avg_L1_norm_grad         0.007715  w[0]    0.003 bias    0.131\n",
      "iter   15/1000000  loss         0.609524  avg_L1_norm_grad         0.007464  w[0]    0.003 bias    0.139\n",
      "iter   16/1000000  loss         0.600924  avg_L1_norm_grad         0.007235  w[0]    0.003 bias    0.147\n",
      "iter   17/1000000  loss         0.592753  avg_L1_norm_grad         0.007025  w[0]    0.004 bias    0.155\n",
      "iter   18/1000000  loss         0.584973  avg_L1_norm_grad         0.006833  w[0]    0.004 bias    0.163\n",
      "iter   19/1000000  loss         0.577548  avg_L1_norm_grad         0.006654  w[0]    0.004 bias    0.171\n",
      "iter  100/1000000  loss         0.360084  avg_L1_norm_grad         0.002335  w[0]    0.010 bias    0.589\n",
      "iter  101/1000000  loss         0.359038  avg_L1_norm_grad         0.002317  w[0]    0.010 bias    0.593\n",
      "iter  200/1000000  loss         0.296719  avg_L1_norm_grad         0.001384  w[0]    0.013 bias    0.887\n",
      "iter  201/1000000  loss         0.296331  avg_L1_norm_grad         0.001379  w[0]    0.013 bias    0.889\n",
      "iter  300/1000000  loss         0.267965  avg_L1_norm_grad         0.001024  w[0]    0.015 bias    1.098\n",
      "iter  301/1000000  loss         0.267751  avg_L1_norm_grad         0.001022  w[0]    0.015 bias    1.100\n",
      "iter  400/1000000  loss         0.250775  avg_L1_norm_grad         0.000832  w[0]    0.015 bias    1.264\n",
      "iter  401/1000000  loss         0.250637  avg_L1_norm_grad         0.000831  w[0]    0.015 bias    1.265\n",
      "iter  500/1000000  loss         0.239053  avg_L1_norm_grad         0.000711  w[0]    0.016 bias    1.401\n",
      "iter  501/1000000  loss         0.238954  avg_L1_norm_grad         0.000710  w[0]    0.016 bias    1.402\n",
      "iter  600/1000000  loss         0.230405  avg_L1_norm_grad         0.000628  w[0]    0.016 bias    1.518\n",
      "iter  601/1000000  loss         0.230330  avg_L1_norm_grad         0.000627  w[0]    0.016 bias    1.519\n",
      "iter  700/1000000  loss         0.223683  avg_L1_norm_grad         0.000566  w[0]    0.016 bias    1.621\n",
      "iter  701/1000000  loss         0.223623  avg_L1_norm_grad         0.000566  w[0]    0.016 bias    1.622\n",
      "iter  800/1000000  loss         0.218260  avg_L1_norm_grad         0.000519  w[0]    0.016 bias    1.712\n",
      "iter  801/1000000  loss         0.218211  avg_L1_norm_grad         0.000518  w[0]    0.016 bias    1.713\n",
      "iter  900/1000000  loss         0.213762  avg_L1_norm_grad         0.000480  w[0]    0.016 bias    1.794\n",
      "iter  901/1000000  loss         0.213720  avg_L1_norm_grad         0.000480  w[0]    0.016 bias    1.794\n",
      "iter 1000/1000000  loss         0.209951  avg_L1_norm_grad         0.000448  w[0]    0.015 bias    1.868\n",
      "iter 1001/1000000  loss         0.209916  avg_L1_norm_grad         0.000448  w[0]    0.015 bias    1.869\n",
      "iter 1100/1000000  loss         0.206668  avg_L1_norm_grad         0.000420  w[0]    0.015 bias    1.936\n",
      "iter 1101/1000000  loss         0.206638  avg_L1_norm_grad         0.000420  w[0]    0.015 bias    1.936\n",
      "iter 1200/1000000  loss         0.203801  avg_L1_norm_grad         0.000397  w[0]    0.015 bias    1.998\n",
      "iter 1201/1000000  loss         0.203774  avg_L1_norm_grad         0.000397  w[0]    0.015 bias    1.999\n",
      "iter 1300/1000000  loss         0.201270  avg_L1_norm_grad         0.000376  w[0]    0.015 bias    2.056\n",
      "iter 1301/1000000  loss         0.201246  avg_L1_norm_grad         0.000376  w[0]    0.015 bias    2.057\n",
      "iter 1400/1000000  loss         0.199014  avg_L1_norm_grad         0.000357  w[0]    0.015 bias    2.110\n",
      "iter 1401/1000000  loss         0.198992  avg_L1_norm_grad         0.000357  w[0]    0.015 bias    2.111\n",
      "iter 1500/1000000  loss         0.196987  avg_L1_norm_grad         0.000341  w[0]    0.015 bias    2.160\n",
      "iter 1501/1000000  loss         0.196968  avg_L1_norm_grad         0.000341  w[0]    0.015 bias    2.161\n",
      "iter 1600/1000000  loss         0.195154  avg_L1_norm_grad         0.000326  w[0]    0.015 bias    2.208\n",
      "iter 1601/1000000  loss         0.195136  avg_L1_norm_grad         0.000326  w[0]    0.015 bias    2.208\n",
      "iter 1700/1000000  loss         0.193486  avg_L1_norm_grad         0.000312  w[0]    0.015 bias    2.252\n",
      "iter 1701/1000000  loss         0.193470  avg_L1_norm_grad         0.000312  w[0]    0.015 bias    2.252\n",
      "iter 1800/1000000  loss         0.191961  avg_L1_norm_grad         0.000300  w[0]    0.015 bias    2.294\n",
      "iter 1801/1000000  loss         0.191946  avg_L1_norm_grad         0.000300  w[0]    0.015 bias    2.294\n",
      "iter 1900/1000000  loss         0.190560  avg_L1_norm_grad         0.000289  w[0]    0.015 bias    2.334\n",
      "iter 1901/1000000  loss         0.190546  avg_L1_norm_grad         0.000289  w[0]    0.015 bias    2.334\n",
      "iter 2000/1000000  loss         0.189267  avg_L1_norm_grad         0.000279  w[0]    0.015 bias    2.371\n",
      "iter 2001/1000000  loss         0.189255  avg_L1_norm_grad         0.000279  w[0]    0.015 bias    2.371\n",
      "iter 2100/1000000  loss         0.188071  avg_L1_norm_grad         0.000269  w[0]    0.015 bias    2.407\n",
      "iter 2101/1000000  loss         0.188059  avg_L1_norm_grad         0.000269  w[0]    0.015 bias    2.407\n",
      "iter 2200/1000000  loss         0.186960  avg_L1_norm_grad         0.000260  w[0]    0.015 bias    2.441\n",
      "iter 2201/1000000  loss         0.186949  avg_L1_norm_grad         0.000260  w[0]    0.015 bias    2.441\n",
      "iter 2300/1000000  loss         0.185925  avg_L1_norm_grad         0.000252  w[0]    0.015 bias    2.473\n",
      "iter 2301/1000000  loss         0.185915  avg_L1_norm_grad         0.000252  w[0]    0.015 bias    2.473\n",
      "iter 2400/1000000  loss         0.184958  avg_L1_norm_grad         0.000244  w[0]    0.015 bias    2.504\n",
      "iter 2401/1000000  loss         0.184949  avg_L1_norm_grad         0.000244  w[0]    0.015 bias    2.504\n",
      "iter 2500/1000000  loss         0.184053  avg_L1_norm_grad         0.000237  w[0]    0.016 bias    2.533\n",
      "iter 2501/1000000  loss         0.184044  avg_L1_norm_grad         0.000237  w[0]    0.016 bias    2.534\n",
      "iter 2600/1000000  loss         0.183204  avg_L1_norm_grad         0.000231  w[0]    0.016 bias    2.562\n",
      "iter 2601/1000000  loss         0.183195  avg_L1_norm_grad         0.000230  w[0]    0.016 bias    2.562\n",
      "iter 2700/1000000  loss         0.182405  avg_L1_norm_grad         0.000224  w[0]    0.016 bias    2.589\n",
      "iter 2701/1000000  loss         0.182397  avg_L1_norm_grad         0.000224  w[0]    0.016 bias    2.589\n",
      "iter 2800/1000000  loss         0.181652  avg_L1_norm_grad         0.000218  w[0]    0.016 bias    2.615\n",
      "iter 2801/1000000  loss         0.181645  avg_L1_norm_grad         0.000218  w[0]    0.016 bias    2.615\n",
      "iter 2900/1000000  loss         0.180941  avg_L1_norm_grad         0.000213  w[0]    0.016 bias    2.640\n",
      "iter 2901/1000000  loss         0.180934  avg_L1_norm_grad         0.000212  w[0]    0.016 bias    2.640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.180269  avg_L1_norm_grad         0.000207  w[0]    0.017 bias    2.664\n",
      "iter 3001/1000000  loss         0.180263  avg_L1_norm_grad         0.000207  w[0]    0.017 bias    2.664\n",
      "iter 3100/1000000  loss         0.179633  avg_L1_norm_grad         0.000202  w[0]    0.017 bias    2.687\n",
      "iter 3101/1000000  loss         0.179627  avg_L1_norm_grad         0.000202  w[0]    0.017 bias    2.687\n",
      "iter 3200/1000000  loss         0.179030  avg_L1_norm_grad         0.000197  w[0]    0.017 bias    2.709\n",
      "iter 3201/1000000  loss         0.179024  avg_L1_norm_grad         0.000197  w[0]    0.017 bias    2.710\n",
      "iter 3300/1000000  loss         0.178456  avg_L1_norm_grad         0.000192  w[0]    0.018 bias    2.731\n",
      "iter 3301/1000000  loss         0.178451  avg_L1_norm_grad         0.000192  w[0]    0.018 bias    2.731\n",
      "iter 3400/1000000  loss         0.177911  avg_L1_norm_grad         0.000188  w[0]    0.018 bias    2.752\n",
      "iter 3401/1000000  loss         0.177906  avg_L1_norm_grad         0.000188  w[0]    0.018 bias    2.752\n",
      "iter 3500/1000000  loss         0.177393  avg_L1_norm_grad         0.000183  w[0]    0.018 bias    2.772\n",
      "iter 3501/1000000  loss         0.177387  avg_L1_norm_grad         0.000183  w[0]    0.018 bias    2.772\n",
      "iter 3600/1000000  loss         0.176898  avg_L1_norm_grad         0.000179  w[0]    0.019 bias    2.791\n",
      "iter 3601/1000000  loss         0.176893  avg_L1_norm_grad         0.000179  w[0]    0.019 bias    2.791\n",
      "iter 3700/1000000  loss         0.176426  avg_L1_norm_grad         0.000175  w[0]    0.019 bias    2.810\n",
      "iter 3701/1000000  loss         0.176421  avg_L1_norm_grad         0.000175  w[0]    0.019 bias    2.810\n",
      "iter 3800/1000000  loss         0.175975  avg_L1_norm_grad         0.000172  w[0]    0.019 bias    2.828\n",
      "iter 3801/1000000  loss         0.175971  avg_L1_norm_grad         0.000172  w[0]    0.019 bias    2.828\n",
      "iter 3900/1000000  loss         0.175544  avg_L1_norm_grad         0.000168  w[0]    0.020 bias    2.846\n",
      "iter 3901/1000000  loss         0.175540  avg_L1_norm_grad         0.000168  w[0]    0.020 bias    2.846\n",
      "iter 4000/1000000  loss         0.175131  avg_L1_norm_grad         0.000165  w[0]    0.020 bias    2.863\n",
      "iter 4001/1000000  loss         0.175127  avg_L1_norm_grad         0.000165  w[0]    0.020 bias    2.863\n",
      "iter 4100/1000000  loss         0.174736  avg_L1_norm_grad         0.000161  w[0]    0.020 bias    2.879\n",
      "iter 4101/1000000  loss         0.174732  avg_L1_norm_grad         0.000161  w[0]    0.020 bias    2.880\n",
      "iter 4200/1000000  loss         0.174357  avg_L1_norm_grad         0.000158  w[0]    0.021 bias    2.896\n",
      "iter 4201/1000000  loss         0.174354  avg_L1_norm_grad         0.000158  w[0]    0.021 bias    2.896\n",
      "iter 4300/1000000  loss         0.173994  avg_L1_norm_grad         0.000155  w[0]    0.021 bias    2.911\n",
      "iter 4301/1000000  loss         0.173990  avg_L1_norm_grad         0.000155  w[0]    0.021 bias    2.911\n",
      "iter 4400/1000000  loss         0.173645  avg_L1_norm_grad         0.000152  w[0]    0.022 bias    2.926\n",
      "iter 4401/1000000  loss         0.173642  avg_L1_norm_grad         0.000152  w[0]    0.022 bias    2.927\n",
      "iter 4500/1000000  loss         0.173310  avg_L1_norm_grad         0.000149  w[0]    0.022 bias    2.941\n",
      "iter 4501/1000000  loss         0.173306  avg_L1_norm_grad         0.000149  w[0]    0.022 bias    2.941\n",
      "iter 4600/1000000  loss         0.172987  avg_L1_norm_grad         0.000147  w[0]    0.023 bias    2.956\n",
      "iter 4601/1000000  loss         0.172984  avg_L1_norm_grad         0.000147  w[0]    0.023 bias    2.956\n",
      "iter 4700/1000000  loss         0.172677  avg_L1_norm_grad         0.000144  w[0]    0.023 bias    2.970\n",
      "iter 4701/1000000  loss         0.172674  avg_L1_norm_grad         0.000144  w[0]    0.023 bias    2.970\n",
      "iter 4800/1000000  loss         0.172378  avg_L1_norm_grad         0.000141  w[0]    0.023 bias    2.983\n",
      "iter 4801/1000000  loss         0.172375  avg_L1_norm_grad         0.000141  w[0]    0.023 bias    2.983\n",
      "iter 4900/1000000  loss         0.172091  avg_L1_norm_grad         0.000139  w[0]    0.024 bias    2.996\n",
      "iter 4901/1000000  loss         0.172088  avg_L1_norm_grad         0.000139  w[0]    0.024 bias    2.997\n",
      "iter 5000/1000000  loss         0.171813  avg_L1_norm_grad         0.000136  w[0]    0.024 bias    3.009\n",
      "iter 5001/1000000  loss         0.171810  avg_L1_norm_grad         0.000136  w[0]    0.024 bias    3.009\n",
      "iter 5100/1000000  loss         0.171546  avg_L1_norm_grad         0.000134  w[0]    0.025 bias    3.022\n",
      "iter 5101/1000000  loss         0.171543  avg_L1_norm_grad         0.000134  w[0]    0.025 bias    3.022\n",
      "iter 5200/1000000  loss         0.171287  avg_L1_norm_grad         0.000132  w[0]    0.025 bias    3.034\n",
      "iter 5201/1000000  loss         0.171285  avg_L1_norm_grad         0.000132  w[0]    0.025 bias    3.034\n",
      "iter 5300/1000000  loss         0.171038  avg_L1_norm_grad         0.000129  w[0]    0.026 bias    3.046\n",
      "iter 5301/1000000  loss         0.171036  avg_L1_norm_grad         0.000129  w[0]    0.026 bias    3.046\n",
      "iter 5400/1000000  loss         0.170797  avg_L1_norm_grad         0.000127  w[0]    0.026 bias    3.058\n",
      "iter 5401/1000000  loss         0.170795  avg_L1_norm_grad         0.000127  w[0]    0.026 bias    3.058\n",
      "iter 5500/1000000  loss         0.170565  avg_L1_norm_grad         0.000125  w[0]    0.026 bias    3.069\n",
      "iter 5501/1000000  loss         0.170562  avg_L1_norm_grad         0.000125  w[0]    0.026 bias    3.069\n",
      "iter 5600/1000000  loss         0.170340  avg_L1_norm_grad         0.000123  w[0]    0.027 bias    3.080\n",
      "iter 5601/1000000  loss         0.170337  avg_L1_norm_grad         0.000123  w[0]    0.027 bias    3.081\n",
      "iter 5700/1000000  loss         0.170122  avg_L1_norm_grad         0.000121  w[0]    0.027 bias    3.091\n",
      "iter 5701/1000000  loss         0.170120  avg_L1_norm_grad         0.000121  w[0]    0.027 bias    3.091\n",
      "iter 5800/1000000  loss         0.169911  avg_L1_norm_grad         0.000119  w[0]    0.028 bias    3.102\n",
      "iter 5801/1000000  loss         0.169909  avg_L1_norm_grad         0.000119  w[0]    0.028 bias    3.102\n",
      "iter 5900/1000000  loss         0.169707  avg_L1_norm_grad         0.000117  w[0]    0.028 bias    3.112\n",
      "iter 5901/1000000  loss         0.169705  avg_L1_norm_grad         0.000117  w[0]    0.028 bias    3.113\n",
      "iter 6000/1000000  loss         0.169510  avg_L1_norm_grad         0.000115  w[0]    0.029 bias    3.123\n",
      "iter 6001/1000000  loss         0.169508  avg_L1_norm_grad         0.000115  w[0]    0.029 bias    3.123\n",
      "iter 6100/1000000  loss         0.169318  avg_L1_norm_grad         0.000113  w[0]    0.029 bias    3.133\n",
      "iter 6101/1000000  loss         0.169316  avg_L1_norm_grad         0.000113  w[0]    0.029 bias    3.133\n",
      "iter 6200/1000000  loss         0.169133  avg_L1_norm_grad         0.000112  w[0]    0.030 bias    3.142\n",
      "iter 6201/1000000  loss         0.169131  avg_L1_norm_grad         0.000112  w[0]    0.030 bias    3.143\n",
      "iter 6300/1000000  loss         0.168953  avg_L1_norm_grad         0.000110  w[0]    0.030 bias    3.152\n",
      "iter 6301/1000000  loss         0.168951  avg_L1_norm_grad         0.000110  w[0]    0.030 bias    3.152\n",
      "iter 6400/1000000  loss         0.168778  avg_L1_norm_grad         0.000108  w[0]    0.030 bias    3.161\n",
      "iter 6401/1000000  loss         0.168776  avg_L1_norm_grad         0.000108  w[0]    0.030 bias    3.161\n",
      "iter 6500/1000000  loss         0.168608  avg_L1_norm_grad         0.000107  w[0]    0.031 bias    3.171\n",
      "iter 6501/1000000  loss         0.168607  avg_L1_norm_grad         0.000107  w[0]    0.031 bias    3.171\n",
      "iter 6600/1000000  loss         0.168444  avg_L1_norm_grad         0.000105  w[0]    0.031 bias    3.180\n",
      "iter 6601/1000000  loss         0.168442  avg_L1_norm_grad         0.000105  w[0]    0.031 bias    3.180\n",
      "iter 6700/1000000  loss         0.168284  avg_L1_norm_grad         0.000103  w[0]    0.032 bias    3.188\n",
      "iter 6701/1000000  loss         0.168283  avg_L1_norm_grad         0.000103  w[0]    0.032 bias    3.188\n",
      "iter 6800/1000000  loss         0.168129  avg_L1_norm_grad         0.000102  w[0]    0.032 bias    3.197\n",
      "iter 6801/1000000  loss         0.168127  avg_L1_norm_grad         0.000102  w[0]    0.032 bias    3.197\n",
      "iter 6900/1000000  loss         0.167978  avg_L1_norm_grad         0.000100  w[0]    0.033 bias    3.205\n",
      "iter 6901/1000000  loss         0.167977  avg_L1_norm_grad         0.000100  w[0]    0.033 bias    3.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.167832  avg_L1_norm_grad         0.000099  w[0]    0.033 bias    3.214\n",
      "iter 7001/1000000  loss         0.167830  avg_L1_norm_grad         0.000099  w[0]    0.033 bias    3.214\n",
      "iter 7100/1000000  loss         0.167689  avg_L1_norm_grad         0.000097  w[0]    0.034 bias    3.222\n",
      "iter 7101/1000000  loss         0.167688  avg_L1_norm_grad         0.000097  w[0]    0.034 bias    3.222\n",
      "iter 7200/1000000  loss         0.167550  avg_L1_norm_grad         0.000096  w[0]    0.034 bias    3.230\n",
      "iter 7201/1000000  loss         0.167549  avg_L1_norm_grad         0.000096  w[0]    0.034 bias    3.230\n",
      "iter 7300/1000000  loss         0.167415  avg_L1_norm_grad         0.000095  w[0]    0.034 bias    3.238\n",
      "iter 7301/1000000  loss         0.167414  avg_L1_norm_grad         0.000095  w[0]    0.034 bias    3.238\n",
      "iter 7400/1000000  loss         0.167284  avg_L1_norm_grad         0.000093  w[0]    0.035 bias    3.245\n",
      "iter 7401/1000000  loss         0.167283  avg_L1_norm_grad         0.000093  w[0]    0.035 bias    3.246\n",
      "iter 7500/1000000  loss         0.167156  avg_L1_norm_grad         0.000092  w[0]    0.035 bias    3.253\n",
      "iter 7501/1000000  loss         0.167155  avg_L1_norm_grad         0.000092  w[0]    0.035 bias    3.253\n",
      "iter 7600/1000000  loss         0.167032  avg_L1_norm_grad         0.000091  w[0]    0.036 bias    3.260\n",
      "iter 7601/1000000  loss         0.167031  avg_L1_norm_grad         0.000091  w[0]    0.036 bias    3.261\n",
      "iter 7700/1000000  loss         0.166911  avg_L1_norm_grad         0.000089  w[0]    0.036 bias    3.268\n",
      "iter 7701/1000000  loss         0.166909  avg_L1_norm_grad         0.000089  w[0]    0.036 bias    3.268\n",
      "iter 7800/1000000  loss         0.166793  avg_L1_norm_grad         0.000088  w[0]    0.037 bias    3.275\n",
      "iter 7801/1000000  loss         0.166791  avg_L1_norm_grad         0.000088  w[0]    0.037 bias    3.275\n",
      "iter 7900/1000000  loss         0.166678  avg_L1_norm_grad         0.000087  w[0]    0.037 bias    3.282\n",
      "iter 7901/1000000  loss         0.166676  avg_L1_norm_grad         0.000087  w[0]    0.037 bias    3.282\n",
      "iter 8000/1000000  loss         0.166565  avg_L1_norm_grad         0.000086  w[0]    0.038 bias    3.289\n",
      "iter 8001/1000000  loss         0.166564  avg_L1_norm_grad         0.000086  w[0]    0.038 bias    3.289\n",
      "iter 8100/1000000  loss         0.166456  avg_L1_norm_grad         0.000085  w[0]    0.038 bias    3.296\n",
      "iter 8101/1000000  loss         0.166455  avg_L1_norm_grad         0.000085  w[0]    0.038 bias    3.296\n",
      "iter 8200/1000000  loss         0.166349  avg_L1_norm_grad         0.000083  w[0]    0.038 bias    3.303\n",
      "iter 8201/1000000  loss         0.166348  avg_L1_norm_grad         0.000083  w[0]    0.038 bias    3.303\n",
      "iter 8300/1000000  loss         0.166245  avg_L1_norm_grad         0.000082  w[0]    0.039 bias    3.309\n",
      "iter 8301/1000000  loss         0.166244  avg_L1_norm_grad         0.000082  w[0]    0.039 bias    3.309\n",
      "iter 8400/1000000  loss         0.166144  avg_L1_norm_grad         0.000081  w[0]    0.039 bias    3.316\n",
      "iter 8401/1000000  loss         0.166143  avg_L1_norm_grad         0.000081  w[0]    0.039 bias    3.316\n",
      "iter 8500/1000000  loss         0.166045  avg_L1_norm_grad         0.000080  w[0]    0.040 bias    3.322\n",
      "iter 8501/1000000  loss         0.166044  avg_L1_norm_grad         0.000080  w[0]    0.040 bias    3.322\n",
      "iter 8600/1000000  loss         0.165948  avg_L1_norm_grad         0.000079  w[0]    0.040 bias    3.328\n",
      "iter 8601/1000000  loss         0.165947  avg_L1_norm_grad         0.000079  w[0]    0.040 bias    3.328\n",
      "iter 8700/1000000  loss         0.165854  avg_L1_norm_grad         0.000078  w[0]    0.040 bias    3.334\n",
      "iter 8701/1000000  loss         0.165853  avg_L1_norm_grad         0.000078  w[0]    0.040 bias    3.335\n",
      "iter 8800/1000000  loss         0.165762  avg_L1_norm_grad         0.000077  w[0]    0.041 bias    3.341\n",
      "iter 8801/1000000  loss         0.165761  avg_L1_norm_grad         0.000077  w[0]    0.041 bias    3.341\n",
      "iter 8900/1000000  loss         0.165672  avg_L1_norm_grad         0.000076  w[0]    0.041 bias    3.347\n",
      "iter 8901/1000000  loss         0.165671  avg_L1_norm_grad         0.000076  w[0]    0.041 bias    3.347\n",
      "iter 9000/1000000  loss         0.165584  avg_L1_norm_grad         0.000075  w[0]    0.042 bias    3.353\n",
      "iter 9001/1000000  loss         0.165583  avg_L1_norm_grad         0.000075  w[0]    0.042 bias    3.353\n",
      "iter 9100/1000000  loss         0.165498  avg_L1_norm_grad         0.000074  w[0]    0.042 bias    3.358\n",
      "iter 9101/1000000  loss         0.165497  avg_L1_norm_grad         0.000074  w[0]    0.042 bias    3.358\n",
      "iter 9200/1000000  loss         0.165414  avg_L1_norm_grad         0.000073  w[0]    0.043 bias    3.364\n",
      "iter 9201/1000000  loss         0.165413  avg_L1_norm_grad         0.000073  w[0]    0.043 bias    3.364\n",
      "iter 9300/1000000  loss         0.165332  avg_L1_norm_grad         0.000072  w[0]    0.043 bias    3.370\n",
      "iter 9301/1000000  loss         0.165332  avg_L1_norm_grad         0.000072  w[0]    0.043 bias    3.370\n",
      "iter 9400/1000000  loss         0.165252  avg_L1_norm_grad         0.000071  w[0]    0.043 bias    3.375\n",
      "iter 9401/1000000  loss         0.165251  avg_L1_norm_grad         0.000071  w[0]    0.043 bias    3.375\n",
      "iter 9500/1000000  loss         0.165174  avg_L1_norm_grad         0.000070  w[0]    0.044 bias    3.381\n",
      "iter 9501/1000000  loss         0.165173  avg_L1_norm_grad         0.000070  w[0]    0.044 bias    3.381\n",
      "iter 9600/1000000  loss         0.165097  avg_L1_norm_grad         0.000069  w[0]    0.044 bias    3.386\n",
      "iter 9601/1000000  loss         0.165096  avg_L1_norm_grad         0.000069  w[0]    0.044 bias    3.386\n",
      "iter 9700/1000000  loss         0.165022  avg_L1_norm_grad         0.000069  w[0]    0.045 bias    3.392\n",
      "iter 9701/1000000  loss         0.165021  avg_L1_norm_grad         0.000069  w[0]    0.045 bias    3.392\n",
      "iter 9800/1000000  loss         0.164949  avg_L1_norm_grad         0.000068  w[0]    0.045 bias    3.397\n",
      "iter 9801/1000000  loss         0.164948  avg_L1_norm_grad         0.000068  w[0]    0.045 bias    3.397\n",
      "iter 9900/1000000  loss         0.164877  avg_L1_norm_grad         0.000067  w[0]    0.045 bias    3.402\n",
      "iter 9901/1000000  loss         0.164876  avg_L1_norm_grad         0.000067  w[0]    0.045 bias    3.402\n",
      "iter 10000/1000000  loss         0.164807  avg_L1_norm_grad         0.000066  w[0]    0.046 bias    3.407\n",
      "iter 10001/1000000  loss         0.164806  avg_L1_norm_grad         0.000066  w[0]    0.046 bias    3.407\n",
      "iter 10100/1000000  loss         0.164738  avg_L1_norm_grad         0.000065  w[0]    0.046 bias    3.412\n",
      "iter 10101/1000000  loss         0.164737  avg_L1_norm_grad         0.000065  w[0]    0.046 bias    3.412\n",
      "iter 10200/1000000  loss         0.164671  avg_L1_norm_grad         0.000065  w[0]    0.047 bias    3.417\n",
      "iter 10201/1000000  loss         0.164670  avg_L1_norm_grad         0.000065  w[0]    0.047 bias    3.417\n",
      "iter 10300/1000000  loss         0.164605  avg_L1_norm_grad         0.000064  w[0]    0.047 bias    3.422\n",
      "iter 10301/1000000  loss         0.164604  avg_L1_norm_grad         0.000064  w[0]    0.047 bias    3.422\n",
      "iter 10400/1000000  loss         0.164540  avg_L1_norm_grad         0.000063  w[0]    0.047 bias    3.427\n",
      "iter 10401/1000000  loss         0.164540  avg_L1_norm_grad         0.000063  w[0]    0.047 bias    3.427\n",
      "iter 10500/1000000  loss         0.164477  avg_L1_norm_grad         0.000062  w[0]    0.048 bias    3.432\n",
      "iter 10501/1000000  loss         0.164476  avg_L1_norm_grad         0.000062  w[0]    0.048 bias    3.432\n",
      "iter 10600/1000000  loss         0.164415  avg_L1_norm_grad         0.000062  w[0]    0.048 bias    3.436\n",
      "iter 10601/1000000  loss         0.164415  avg_L1_norm_grad         0.000062  w[0]    0.048 bias    3.436\n",
      "iter 10700/1000000  loss         0.164355  avg_L1_norm_grad         0.000061  w[0]    0.048 bias    3.441\n",
      "iter 10701/1000000  loss         0.164354  avg_L1_norm_grad         0.000061  w[0]    0.048 bias    3.441\n",
      "iter 10800/1000000  loss         0.164295  avg_L1_norm_grad         0.000060  w[0]    0.049 bias    3.446\n",
      "iter 10801/1000000  loss         0.164295  avg_L1_norm_grad         0.000060  w[0]    0.049 bias    3.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.164237  avg_L1_norm_grad         0.000059  w[0]    0.049 bias    3.450\n",
      "iter 10901/1000000  loss         0.164236  avg_L1_norm_grad         0.000059  w[0]    0.049 bias    3.450\n",
      "iter 11000/1000000  loss         0.164180  avg_L1_norm_grad         0.000059  w[0]    0.050 bias    3.455\n",
      "iter 11001/1000000  loss         0.164179  avg_L1_norm_grad         0.000059  w[0]    0.050 bias    3.455\n",
      "iter 11100/1000000  loss         0.164124  avg_L1_norm_grad         0.000058  w[0]    0.050 bias    3.459\n",
      "iter 11101/1000000  loss         0.164123  avg_L1_norm_grad         0.000058  w[0]    0.050 bias    3.459\n",
      "iter 11200/1000000  loss         0.164069  avg_L1_norm_grad         0.000057  w[0]    0.050 bias    3.463\n",
      "iter 11201/1000000  loss         0.164068  avg_L1_norm_grad         0.000057  w[0]    0.050 bias    3.463\n",
      "iter 11300/1000000  loss         0.164015  avg_L1_norm_grad         0.000057  w[0]    0.051 bias    3.468\n",
      "iter 11301/1000000  loss         0.164015  avg_L1_norm_grad         0.000057  w[0]    0.051 bias    3.468\n",
      "iter 11400/1000000  loss         0.163962  avg_L1_norm_grad         0.000056  w[0]    0.051 bias    3.472\n",
      "iter 11401/1000000  loss         0.163962  avg_L1_norm_grad         0.000056  w[0]    0.051 bias    3.472\n",
      "iter 11500/1000000  loss         0.163911  avg_L1_norm_grad         0.000055  w[0]    0.051 bias    3.476\n",
      "iter 11501/1000000  loss         0.163910  avg_L1_norm_grad         0.000055  w[0]    0.051 bias    3.476\n",
      "iter 11600/1000000  loss         0.163860  avg_L1_norm_grad         0.000055  w[0]    0.052 bias    3.480\n",
      "iter 11601/1000000  loss         0.163859  avg_L1_norm_grad         0.000055  w[0]    0.052 bias    3.480\n",
      "iter 11700/1000000  loss         0.163810  avg_L1_norm_grad         0.000054  w[0]    0.052 bias    3.484\n",
      "iter 11701/1000000  loss         0.163810  avg_L1_norm_grad         0.000054  w[0]    0.052 bias    3.485\n",
      "iter 11800/1000000  loss         0.163761  avg_L1_norm_grad         0.000054  w[0]    0.052 bias    3.489\n",
      "iter 11801/1000000  loss         0.163761  avg_L1_norm_grad         0.000054  w[0]    0.052 bias    3.489\n",
      "iter 11900/1000000  loss         0.163713  avg_L1_norm_grad         0.000053  w[0]    0.053 bias    3.493\n",
      "iter 11901/1000000  loss         0.163713  avg_L1_norm_grad         0.000053  w[0]    0.053 bias    3.493\n",
      "iter 12000/1000000  loss         0.163666  avg_L1_norm_grad         0.000052  w[0]    0.053 bias    3.497\n",
      "iter 12001/1000000  loss         0.163666  avg_L1_norm_grad         0.000052  w[0]    0.053 bias    3.497\n",
      "iter 12100/1000000  loss         0.163620  avg_L1_norm_grad         0.000052  w[0]    0.054 bias    3.500\n",
      "iter 12101/1000000  loss         0.163620  avg_L1_norm_grad         0.000052  w[0]    0.054 bias    3.500\n",
      "iter 12200/1000000  loss         0.163575  avg_L1_norm_grad         0.000051  w[0]    0.054 bias    3.504\n",
      "iter 12201/1000000  loss         0.163574  avg_L1_norm_grad         0.000051  w[0]    0.054 bias    3.504\n",
      "iter 12300/1000000  loss         0.163530  avg_L1_norm_grad         0.000051  w[0]    0.054 bias    3.508\n",
      "iter 12301/1000000  loss         0.163530  avg_L1_norm_grad         0.000051  w[0]    0.054 bias    3.508\n",
      "iter 12400/1000000  loss         0.163486  avg_L1_norm_grad         0.000050  w[0]    0.055 bias    3.512\n",
      "iter 12401/1000000  loss         0.163486  avg_L1_norm_grad         0.000050  w[0]    0.055 bias    3.512\n",
      "iter 12500/1000000  loss         0.163443  avg_L1_norm_grad         0.000050  w[0]    0.055 bias    3.516\n",
      "iter 12501/1000000  loss         0.163443  avg_L1_norm_grad         0.000050  w[0]    0.055 bias    3.516\n",
      "iter 12600/1000000  loss         0.163401  avg_L1_norm_grad         0.000049  w[0]    0.055 bias    3.519\n",
      "iter 12601/1000000  loss         0.163401  avg_L1_norm_grad         0.000049  w[0]    0.055 bias    3.519\n",
      "iter 12700/1000000  loss         0.163360  avg_L1_norm_grad         0.000048  w[0]    0.056 bias    3.523\n",
      "iter 12701/1000000  loss         0.163359  avg_L1_norm_grad         0.000048  w[0]    0.056 bias    3.523\n",
      "iter 12800/1000000  loss         0.163319  avg_L1_norm_grad         0.000048  w[0]    0.056 bias    3.527\n",
      "iter 12801/1000000  loss         0.163319  avg_L1_norm_grad         0.000048  w[0]    0.056 bias    3.527\n",
      "iter 12900/1000000  loss         0.163279  avg_L1_norm_grad         0.000047  w[0]    0.056 bias    3.530\n",
      "iter 12901/1000000  loss         0.163279  avg_L1_norm_grad         0.000047  w[0]    0.056 bias    3.530\n",
      "iter 13000/1000000  loss         0.163240  avg_L1_norm_grad         0.000047  w[0]    0.057 bias    3.534\n",
      "iter 13001/1000000  loss         0.163239  avg_L1_norm_grad         0.000047  w[0]    0.057 bias    3.534\n",
      "iter 13100/1000000  loss         0.163201  avg_L1_norm_grad         0.000046  w[0]    0.057 bias    3.537\n",
      "iter 13101/1000000  loss         0.163201  avg_L1_norm_grad         0.000046  w[0]    0.057 bias    3.537\n",
      "iter 13200/1000000  loss         0.163163  avg_L1_norm_grad         0.000046  w[0]    0.057 bias    3.541\n",
      "iter 13201/1000000  loss         0.163163  avg_L1_norm_grad         0.000046  w[0]    0.057 bias    3.541\n",
      "iter 13300/1000000  loss         0.163126  avg_L1_norm_grad         0.000045  w[0]    0.057 bias    3.544\n",
      "iter 13301/1000000  loss         0.163125  avg_L1_norm_grad         0.000045  w[0]    0.057 bias    3.544\n",
      "iter 13400/1000000  loss         0.163089  avg_L1_norm_grad         0.000045  w[0]    0.058 bias    3.548\n",
      "iter 13401/1000000  loss         0.163089  avg_L1_norm_grad         0.000045  w[0]    0.058 bias    3.548\n",
      "iter 13500/1000000  loss         0.163053  avg_L1_norm_grad         0.000044  w[0]    0.058 bias    3.551\n",
      "iter 13501/1000000  loss         0.163053  avg_L1_norm_grad         0.000044  w[0]    0.058 bias    3.551\n",
      "iter 13600/1000000  loss         0.163018  avg_L1_norm_grad         0.000044  w[0]    0.058 bias    3.554\n",
      "iter 13601/1000000  loss         0.163017  avg_L1_norm_grad         0.000044  w[0]    0.058 bias    3.554\n",
      "iter 13700/1000000  loss         0.162983  avg_L1_norm_grad         0.000044  w[0]    0.059 bias    3.558\n",
      "iter 13701/1000000  loss         0.162982  avg_L1_norm_grad         0.000044  w[0]    0.059 bias    3.558\n",
      "iter 13800/1000000  loss         0.162948  avg_L1_norm_grad         0.000043  w[0]    0.059 bias    3.561\n",
      "iter 13801/1000000  loss         0.162948  avg_L1_norm_grad         0.000043  w[0]    0.059 bias    3.561\n",
      "iter 13900/1000000  loss         0.162915  avg_L1_norm_grad         0.000043  w[0]    0.059 bias    3.564\n",
      "iter 13901/1000000  loss         0.162914  avg_L1_norm_grad         0.000043  w[0]    0.059 bias    3.564\n",
      "iter 14000/1000000  loss         0.162881  avg_L1_norm_grad         0.000042  w[0]    0.060 bias    3.567\n",
      "iter 14001/1000000  loss         0.162881  avg_L1_norm_grad         0.000042  w[0]    0.060 bias    3.567\n",
      "iter 14100/1000000  loss         0.162849  avg_L1_norm_grad         0.000042  w[0]    0.060 bias    3.570\n",
      "iter 14101/1000000  loss         0.162848  avg_L1_norm_grad         0.000042  w[0]    0.060 bias    3.570\n",
      "iter 14200/1000000  loss         0.162817  avg_L1_norm_grad         0.000041  w[0]    0.060 bias    3.573\n",
      "iter 14201/1000000  loss         0.162816  avg_L1_norm_grad         0.000041  w[0]    0.060 bias    3.574\n",
      "iter 14300/1000000  loss         0.162785  avg_L1_norm_grad         0.000041  w[0]    0.061 bias    3.577\n",
      "iter 14301/1000000  loss         0.162785  avg_L1_norm_grad         0.000041  w[0]    0.061 bias    3.577\n",
      "iter 14400/1000000  loss         0.162754  avg_L1_norm_grad         0.000040  w[0]    0.061 bias    3.580\n",
      "iter 14401/1000000  loss         0.162753  avg_L1_norm_grad         0.000040  w[0]    0.061 bias    3.580\n",
      "iter 14500/1000000  loss         0.162723  avg_L1_norm_grad         0.000040  w[0]    0.061 bias    3.583\n",
      "iter 14501/1000000  loss         0.162723  avg_L1_norm_grad         0.000040  w[0]    0.061 bias    3.583\n",
      "iter 14600/1000000  loss         0.162693  avg_L1_norm_grad         0.000040  w[0]    0.061 bias    3.586\n",
      "iter 14601/1000000  loss         0.162693  avg_L1_norm_grad         0.000040  w[0]    0.061 bias    3.586\n",
      "iter 14700/1000000  loss         0.162663  avg_L1_norm_grad         0.000039  w[0]    0.062 bias    3.589\n",
      "iter 14701/1000000  loss         0.162663  avg_L1_norm_grad         0.000039  w[0]    0.062 bias    3.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14800/1000000  loss         0.162634  avg_L1_norm_grad         0.000039  w[0]    0.062 bias    3.592\n",
      "iter 14801/1000000  loss         0.162634  avg_L1_norm_grad         0.000039  w[0]    0.062 bias    3.592\n",
      "iter 14900/1000000  loss         0.162605  avg_L1_norm_grad         0.000038  w[0]    0.062 bias    3.594\n",
      "iter 14901/1000000  loss         0.162605  avg_L1_norm_grad         0.000038  w[0]    0.062 bias    3.595\n",
      "iter 15000/1000000  loss         0.162577  avg_L1_norm_grad         0.000038  w[0]    0.063 bias    3.597\n",
      "iter 15001/1000000  loss         0.162577  avg_L1_norm_grad         0.000038  w[0]    0.063 bias    3.597\n",
      "iter 15100/1000000  loss         0.162549  avg_L1_norm_grad         0.000038  w[0]    0.063 bias    3.600\n",
      "iter 15101/1000000  loss         0.162549  avg_L1_norm_grad         0.000038  w[0]    0.063 bias    3.600\n",
      "iter 15200/1000000  loss         0.162521  avg_L1_norm_grad         0.000037  w[0]    0.063 bias    3.603\n",
      "iter 15201/1000000  loss         0.162521  avg_L1_norm_grad         0.000037  w[0]    0.063 bias    3.603\n",
      "iter 15300/1000000  loss         0.162494  avg_L1_norm_grad         0.000037  w[0]    0.063 bias    3.606\n",
      "iter 15301/1000000  loss         0.162494  avg_L1_norm_grad         0.000037  w[0]    0.063 bias    3.606\n",
      "iter 15400/1000000  loss         0.162468  avg_L1_norm_grad         0.000037  w[0]    0.064 bias    3.609\n",
      "iter 15401/1000000  loss         0.162467  avg_L1_norm_grad         0.000037  w[0]    0.064 bias    3.609\n",
      "iter 15500/1000000  loss         0.162441  avg_L1_norm_grad         0.000036  w[0]    0.064 bias    3.611\n",
      "iter 15501/1000000  loss         0.162441  avg_L1_norm_grad         0.000036  w[0]    0.064 bias    3.611\n",
      "iter 15600/1000000  loss         0.162416  avg_L1_norm_grad         0.000036  w[0]    0.064 bias    3.614\n",
      "iter 15601/1000000  loss         0.162415  avg_L1_norm_grad         0.000036  w[0]    0.064 bias    3.614\n",
      "iter 15700/1000000  loss         0.162390  avg_L1_norm_grad         0.000035  w[0]    0.065 bias    3.617\n",
      "iter 15701/1000000  loss         0.162390  avg_L1_norm_grad         0.000035  w[0]    0.065 bias    3.617\n",
      "iter 15800/1000000  loss         0.162365  avg_L1_norm_grad         0.000035  w[0]    0.065 bias    3.619\n",
      "iter 15801/1000000  loss         0.162365  avg_L1_norm_grad         0.000035  w[0]    0.065 bias    3.620\n",
      "iter 15900/1000000  loss         0.162340  avg_L1_norm_grad         0.000035  w[0]    0.065 bias    3.622\n",
      "iter 15901/1000000  loss         0.162340  avg_L1_norm_grad         0.000035  w[0]    0.065 bias    3.622\n",
      "iter 16000/1000000  loss         0.162316  avg_L1_norm_grad         0.000034  w[0]    0.065 bias    3.625\n",
      "iter 16001/1000000  loss         0.162315  avg_L1_norm_grad         0.000034  w[0]    0.065 bias    3.625\n",
      "iter 16100/1000000  loss         0.162292  avg_L1_norm_grad         0.000034  w[0]    0.066 bias    3.627\n",
      "iter 16101/1000000  loss         0.162291  avg_L1_norm_grad         0.000034  w[0]    0.066 bias    3.627\n",
      "iter 16200/1000000  loss         0.162268  avg_L1_norm_grad         0.000034  w[0]    0.066 bias    3.630\n",
      "iter 16201/1000000  loss         0.162268  avg_L1_norm_grad         0.000034  w[0]    0.066 bias    3.630\n",
      "iter 16300/1000000  loss         0.162244  avg_L1_norm_grad         0.000033  w[0]    0.066 bias    3.632\n",
      "iter 16301/1000000  loss         0.162244  avg_L1_norm_grad         0.000033  w[0]    0.066 bias    3.633\n",
      "iter 16400/1000000  loss         0.162221  avg_L1_norm_grad         0.000033  w[0]    0.066 bias    3.635\n",
      "iter 16401/1000000  loss         0.162221  avg_L1_norm_grad         0.000033  w[0]    0.066 bias    3.635\n",
      "iter 16500/1000000  loss         0.162199  avg_L1_norm_grad         0.000033  w[0]    0.067 bias    3.638\n",
      "iter 16501/1000000  loss         0.162198  avg_L1_norm_grad         0.000033  w[0]    0.067 bias    3.638\n",
      "iter 16600/1000000  loss         0.162176  avg_L1_norm_grad         0.000032  w[0]    0.067 bias    3.640\n",
      "iter 16601/1000000  loss         0.162176  avg_L1_norm_grad         0.000032  w[0]    0.067 bias    3.640\n",
      "iter 16700/1000000  loss         0.162154  avg_L1_norm_grad         0.000032  w[0]    0.067 bias    3.642\n",
      "iter 16701/1000000  loss         0.162154  avg_L1_norm_grad         0.000032  w[0]    0.067 bias    3.643\n",
      "iter 16800/1000000  loss         0.162132  avg_L1_norm_grad         0.000032  w[0]    0.067 bias    3.645\n",
      "iter 16801/1000000  loss         0.162132  avg_L1_norm_grad         0.000032  w[0]    0.067 bias    3.645\n",
      "iter 16900/1000000  loss         0.162111  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.647\n",
      "iter 16901/1000000  loss         0.162110  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.647\n",
      "iter 17000/1000000  loss         0.162089  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.650\n",
      "iter 17001/1000000  loss         0.162089  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.650\n",
      "iter 17100/1000000  loss         0.162068  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.652\n",
      "iter 17101/1000000  loss         0.162068  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.652\n",
      "iter 17200/1000000  loss         0.162048  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.654\n",
      "iter 17201/1000000  loss         0.162048  avg_L1_norm_grad         0.000031  w[0]    0.068 bias    3.655\n",
      "iter 17300/1000000  loss         0.162027  avg_L1_norm_grad         0.000030  w[0]    0.069 bias    3.657\n",
      "iter 17301/1000000  loss         0.162027  avg_L1_norm_grad         0.000030  w[0]    0.069 bias    3.657\n",
      "iter 17400/1000000  loss         0.162007  avg_L1_norm_grad         0.000030  w[0]    0.069 bias    3.659\n",
      "iter 17401/1000000  loss         0.162007  avg_L1_norm_grad         0.000030  w[0]    0.069 bias    3.659\n",
      "iter 17500/1000000  loss         0.161987  avg_L1_norm_grad         0.000030  w[0]    0.069 bias    3.661\n",
      "iter 17501/1000000  loss         0.161987  avg_L1_norm_grad         0.000030  w[0]    0.069 bias    3.661\n",
      "iter 17600/1000000  loss         0.161968  avg_L1_norm_grad         0.000029  w[0]    0.069 bias    3.664\n",
      "iter 17601/1000000  loss         0.161968  avg_L1_norm_grad         0.000029  w[0]    0.069 bias    3.664\n",
      "iter 17700/1000000  loss         0.161948  avg_L1_norm_grad         0.000029  w[0]    0.070 bias    3.666\n",
      "iter 17701/1000000  loss         0.161948  avg_L1_norm_grad         0.000029  w[0]    0.070 bias    3.666\n",
      "iter 17800/1000000  loss         0.161929  avg_L1_norm_grad         0.000029  w[0]    0.070 bias    3.668\n",
      "iter 17801/1000000  loss         0.161929  avg_L1_norm_grad         0.000029  w[0]    0.070 bias    3.668\n",
      "iter 17900/1000000  loss         0.161911  avg_L1_norm_grad         0.000029  w[0]    0.070 bias    3.670\n",
      "iter 17901/1000000  loss         0.161910  avg_L1_norm_grad         0.000029  w[0]    0.070 bias    3.670\n",
      "iter 18000/1000000  loss         0.161892  avg_L1_norm_grad         0.000028  w[0]    0.070 bias    3.673\n",
      "iter 18001/1000000  loss         0.161892  avg_L1_norm_grad         0.000028  w[0]    0.070 bias    3.673\n",
      "iter 18100/1000000  loss         0.161874  avg_L1_norm_grad         0.000028  w[0]    0.071 bias    3.675\n",
      "iter 18101/1000000  loss         0.161873  avg_L1_norm_grad         0.000028  w[0]    0.071 bias    3.675\n",
      "iter 18200/1000000  loss         0.161855  avg_L1_norm_grad         0.000028  w[0]    0.071 bias    3.677\n",
      "iter 18201/1000000  loss         0.161855  avg_L1_norm_grad         0.000028  w[0]    0.071 bias    3.677\n",
      "iter 18300/1000000  loss         0.161838  avg_L1_norm_grad         0.000028  w[0]    0.071 bias    3.679\n",
      "iter 18301/1000000  loss         0.161837  avg_L1_norm_grad         0.000028  w[0]    0.071 bias    3.679\n",
      "iter 18400/1000000  loss         0.161820  avg_L1_norm_grad         0.000027  w[0]    0.071 bias    3.681\n",
      "iter 18401/1000000  loss         0.161820  avg_L1_norm_grad         0.000027  w[0]    0.071 bias    3.681\n",
      "iter 18500/1000000  loss         0.161802  avg_L1_norm_grad         0.000027  w[0]    0.071 bias    3.683\n",
      "iter 18501/1000000  loss         0.161802  avg_L1_norm_grad         0.000027  w[0]    0.071 bias    3.683\n",
      "iter 18600/1000000  loss         0.161785  avg_L1_norm_grad         0.000027  w[0]    0.072 bias    3.685\n",
      "iter 18601/1000000  loss         0.161785  avg_L1_norm_grad         0.000027  w[0]    0.072 bias    3.686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18700/1000000  loss         0.161768  avg_L1_norm_grad         0.000027  w[0]    0.072 bias    3.688\n",
      "iter 18701/1000000  loss         0.161768  avg_L1_norm_grad         0.000027  w[0]    0.072 bias    3.688\n",
      "iter 18800/1000000  loss         0.161751  avg_L1_norm_grad         0.000026  w[0]    0.072 bias    3.690\n",
      "iter 18801/1000000  loss         0.161751  avg_L1_norm_grad         0.000026  w[0]    0.072 bias    3.690\n",
      "iter 18900/1000000  loss         0.161735  avg_L1_norm_grad         0.000026  w[0]    0.072 bias    3.692\n",
      "iter 18901/1000000  loss         0.161734  avg_L1_norm_grad         0.000026  w[0]    0.072 bias    3.692\n",
      "iter 19000/1000000  loss         0.161718  avg_L1_norm_grad         0.000026  w[0]    0.073 bias    3.694\n",
      "iter 19001/1000000  loss         0.161718  avg_L1_norm_grad         0.000026  w[0]    0.073 bias    3.694\n",
      "iter 19100/1000000  loss         0.161702  avg_L1_norm_grad         0.000026  w[0]    0.073 bias    3.696\n",
      "iter 19101/1000000  loss         0.161702  avg_L1_norm_grad         0.000026  w[0]    0.073 bias    3.696\n",
      "iter 19200/1000000  loss         0.161686  avg_L1_norm_grad         0.000025  w[0]    0.073 bias    3.698\n",
      "iter 19201/1000000  loss         0.161686  avg_L1_norm_grad         0.000025  w[0]    0.073 bias    3.698\n",
      "iter 19300/1000000  loss         0.161670  avg_L1_norm_grad         0.000025  w[0]    0.073 bias    3.700\n",
      "iter 19301/1000000  loss         0.161670  avg_L1_norm_grad         0.000025  w[0]    0.073 bias    3.700\n",
      "iter 19400/1000000  loss         0.161654  avg_L1_norm_grad         0.000025  w[0]    0.073 bias    3.702\n",
      "iter 19401/1000000  loss         0.161654  avg_L1_norm_grad         0.000025  w[0]    0.073 bias    3.702\n",
      "iter 19500/1000000  loss         0.161639  avg_L1_norm_grad         0.000025  w[0]    0.074 bias    3.704\n",
      "iter 19501/1000000  loss         0.161639  avg_L1_norm_grad         0.000025  w[0]    0.074 bias    3.704\n",
      "iter 19600/1000000  loss         0.161624  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.706\n",
      "iter 19601/1000000  loss         0.161624  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.706\n",
      "iter 19700/1000000  loss         0.161609  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.707\n",
      "iter 19701/1000000  loss         0.161608  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.707\n",
      "iter 19800/1000000  loss         0.161594  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.709\n",
      "iter 19801/1000000  loss         0.161593  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.709\n",
      "iter 19900/1000000  loss         0.161579  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.711\n",
      "iter 19901/1000000  loss         0.161579  avg_L1_norm_grad         0.000024  w[0]    0.074 bias    3.711\n",
      "iter 20000/1000000  loss         0.161564  avg_L1_norm_grad         0.000024  w[0]    0.075 bias    3.713\n",
      "iter 20001/1000000  loss         0.161564  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.713\n",
      "iter 20100/1000000  loss         0.161550  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.715\n",
      "iter 20101/1000000  loss         0.161550  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.715\n",
      "iter 20200/1000000  loss         0.161536  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.717\n",
      "iter 20201/1000000  loss         0.161535  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.717\n",
      "iter 20300/1000000  loss         0.161521  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.719\n",
      "iter 20301/1000000  loss         0.161521  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.719\n",
      "iter 20400/1000000  loss         0.161507  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.720\n",
      "iter 20401/1000000  loss         0.161507  avg_L1_norm_grad         0.000023  w[0]    0.075 bias    3.721\n",
      "iter 20500/1000000  loss         0.161494  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.722\n",
      "iter 20501/1000000  loss         0.161494  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.722\n",
      "iter 20600/1000000  loss         0.161480  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.724\n",
      "iter 20601/1000000  loss         0.161480  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.724\n",
      "iter 20700/1000000  loss         0.161467  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.726\n",
      "iter 20701/1000000  loss         0.161466  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.726\n",
      "iter 20800/1000000  loss         0.161453  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.728\n",
      "iter 20801/1000000  loss         0.161453  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.728\n",
      "iter 20900/1000000  loss         0.161440  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.729\n",
      "iter 20901/1000000  loss         0.161440  avg_L1_norm_grad         0.000022  w[0]    0.076 bias    3.729\n",
      "iter 21000/1000000  loss         0.161427  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.731\n",
      "iter 21001/1000000  loss         0.161427  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.731\n",
      "iter 21100/1000000  loss         0.161414  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.733\n",
      "iter 21101/1000000  loss         0.161414  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.733\n",
      "iter 21200/1000000  loss         0.161401  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.735\n",
      "iter 21201/1000000  loss         0.161401  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.735\n",
      "iter 21300/1000000  loss         0.161389  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.736\n",
      "iter 21301/1000000  loss         0.161388  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.736\n",
      "iter 21400/1000000  loss         0.161376  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.738\n",
      "iter 21401/1000000  loss         0.161376  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.738\n",
      "iter 21500/1000000  loss         0.161364  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.740\n",
      "iter 21501/1000000  loss         0.161364  avg_L1_norm_grad         0.000021  w[0]    0.077 bias    3.740\n",
      "iter 21600/1000000  loss         0.161351  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.741\n",
      "iter 21601/1000000  loss         0.161351  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.741\n",
      "iter 21700/1000000  loss         0.161339  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.743\n",
      "iter 21701/1000000  loss         0.161339  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.743\n",
      "iter 21800/1000000  loss         0.161327  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.745\n",
      "iter 21801/1000000  loss         0.161327  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.745\n",
      "iter 21900/1000000  loss         0.161316  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.746\n",
      "iter 21901/1000000  loss         0.161315  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.746\n",
      "iter 22000/1000000  loss         0.161304  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.748\n",
      "iter 22001/1000000  loss         0.161304  avg_L1_norm_grad         0.000020  w[0]    0.078 bias    3.748\n",
      "iter 22100/1000000  loss         0.161292  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.749\n",
      "iter 22101/1000000  loss         0.161292  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.749\n",
      "iter 22200/1000000  loss         0.161281  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.751\n",
      "iter 22201/1000000  loss         0.161281  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.751\n",
      "iter 22300/1000000  loss         0.161269  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.753\n",
      "iter 22301/1000000  loss         0.161269  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.753\n",
      "iter 22400/1000000  loss         0.161258  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.754\n",
      "iter 22401/1000000  loss         0.161258  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.754\n",
      "iter 22500/1000000  loss         0.161247  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.756\n",
      "iter 22501/1000000  loss         0.161247  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22600/1000000  loss         0.161236  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.757\n",
      "iter 22601/1000000  loss         0.161236  avg_L1_norm_grad         0.000019  w[0]    0.079 bias    3.757\n",
      "iter 22700/1000000  loss         0.161225  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.759\n",
      "iter 22701/1000000  loss         0.161225  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.759\n",
      "iter 22800/1000000  loss         0.161214  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.760\n",
      "iter 22801/1000000  loss         0.161214  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.760\n",
      "iter 22900/1000000  loss         0.161203  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.762\n",
      "iter 22901/1000000  loss         0.161203  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.762\n",
      "iter 23000/1000000  loss         0.161193  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.763\n",
      "iter 23001/1000000  loss         0.161192  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.763\n",
      "iter 23100/1000000  loss         0.161182  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.765\n",
      "iter 23101/1000000  loss         0.161182  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.765\n",
      "iter 23200/1000000  loss         0.161172  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.766\n",
      "iter 23201/1000000  loss         0.161172  avg_L1_norm_grad         0.000018  w[0]    0.080 bias    3.766\n",
      "iter 23300/1000000  loss         0.161161  avg_L1_norm_grad         0.000018  w[0]    0.081 bias    3.768\n",
      "iter 23301/1000000  loss         0.161161  avg_L1_norm_grad         0.000018  w[0]    0.081 bias    3.768\n",
      "iter 23400/1000000  loss         0.161151  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.769\n",
      "iter 23401/1000000  loss         0.161151  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.769\n",
      "iter 23500/1000000  loss         0.161141  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.771\n",
      "iter 23501/1000000  loss         0.161141  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.771\n",
      "iter 23600/1000000  loss         0.161131  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.772\n",
      "iter 23601/1000000  loss         0.161131  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.772\n",
      "iter 23700/1000000  loss         0.161121  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.774\n",
      "iter 23701/1000000  loss         0.161121  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.774\n",
      "iter 23800/1000000  loss         0.161111  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.775\n",
      "iter 23801/1000000  loss         0.161111  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.775\n",
      "iter 23900/1000000  loss         0.161101  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.777\n",
      "iter 23901/1000000  loss         0.161101  avg_L1_norm_grad         0.000017  w[0]    0.081 bias    3.777\n",
      "iter 24000/1000000  loss         0.161092  avg_L1_norm_grad         0.000017  w[0]    0.082 bias    3.778\n",
      "iter 24001/1000000  loss         0.161092  avg_L1_norm_grad         0.000017  w[0]    0.082 bias    3.778\n",
      "iter 24100/1000000  loss         0.161082  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.779\n",
      "iter 24101/1000000  loss         0.161082  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.779\n",
      "iter 24200/1000000  loss         0.161073  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.781\n",
      "iter 24201/1000000  loss         0.161073  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.781\n",
      "iter 24300/1000000  loss         0.161063  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.782\n",
      "iter 24301/1000000  loss         0.161063  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.782\n",
      "iter 24400/1000000  loss         0.161054  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.783\n",
      "iter 24401/1000000  loss         0.161054  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.783\n",
      "iter 24500/1000000  loss         0.161045  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.785\n",
      "iter 24501/1000000  loss         0.161045  avg_L1_norm_grad         0.000016  w[0]    0.082 bias    3.785\n",
      "iter 24600/1000000  loss         0.161036  avg_L1_norm_grad         0.000016  w[0]    0.083 bias    3.786\n",
      "iter 24601/1000000  loss         0.161036  avg_L1_norm_grad         0.000016  w[0]    0.083 bias    3.786\n",
      "iter 24700/1000000  loss         0.161027  avg_L1_norm_grad         0.000016  w[0]    0.083 bias    3.788\n",
      "iter 24701/1000000  loss         0.161026  avg_L1_norm_grad         0.000016  w[0]    0.083 bias    3.788\n",
      "iter 24800/1000000  loss         0.161018  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.789\n",
      "iter 24801/1000000  loss         0.161017  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.789\n",
      "iter 24900/1000000  loss         0.161009  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.790\n",
      "iter 24901/1000000  loss         0.161009  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.790\n",
      "iter 25000/1000000  loss         0.161000  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.791\n",
      "iter 25001/1000000  loss         0.161000  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.792\n",
      "iter 25100/1000000  loss         0.160991  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.793\n",
      "iter 25101/1000000  loss         0.160991  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.793\n",
      "iter 25200/1000000  loss         0.160982  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.794\n",
      "iter 25201/1000000  loss         0.160982  avg_L1_norm_grad         0.000015  w[0]    0.083 bias    3.794\n",
      "iter 25300/1000000  loss         0.160974  avg_L1_norm_grad         0.000015  w[0]    0.084 bias    3.795\n",
      "iter 25301/1000000  loss         0.160974  avg_L1_norm_grad         0.000015  w[0]    0.084 bias    3.795\n",
      "iter 25400/1000000  loss         0.160965  avg_L1_norm_grad         0.000015  w[0]    0.084 bias    3.797\n",
      "iter 25401/1000000  loss         0.160965  avg_L1_norm_grad         0.000015  w[0]    0.084 bias    3.797\n",
      "iter 25500/1000000  loss         0.160957  avg_L1_norm_grad         0.000015  w[0]    0.084 bias    3.798\n",
      "iter 25501/1000000  loss         0.160957  avg_L1_norm_grad         0.000015  w[0]    0.084 bias    3.798\n",
      "iter 25600/1000000  loss         0.160949  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.799\n",
      "iter 25601/1000000  loss         0.160948  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.799\n",
      "iter 25700/1000000  loss         0.160940  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.800\n",
      "iter 25701/1000000  loss         0.160940  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.801\n",
      "iter 25800/1000000  loss         0.160932  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.802\n",
      "iter 25801/1000000  loss         0.160932  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.802\n",
      "iter 25900/1000000  loss         0.160924  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.803\n",
      "iter 25901/1000000  loss         0.160924  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.803\n",
      "iter 26000/1000000  loss         0.160916  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.804\n",
      "iter 26001/1000000  loss         0.160916  avg_L1_norm_grad         0.000014  w[0]    0.084 bias    3.804\n",
      "iter 26100/1000000  loss         0.160908  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.805\n",
      "iter 26101/1000000  loss         0.160908  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.805\n",
      "iter 26200/1000000  loss         0.160900  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.807\n",
      "iter 26201/1000000  loss         0.160900  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.807\n",
      "iter 26300/1000000  loss         0.160892  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.808\n",
      "iter 26301/1000000  loss         0.160892  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.808\n",
      "iter 26400/1000000  loss         0.160884  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.809\n",
      "iter 26401/1000000  loss         0.160884  avg_L1_norm_grad         0.000014  w[0]    0.085 bias    3.809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26500/1000000  loss         0.160876  avg_L1_norm_grad         0.000013  w[0]    0.085 bias    3.810\n",
      "iter 26501/1000000  loss         0.160876  avg_L1_norm_grad         0.000013  w[0]    0.085 bias    3.810\n",
      "iter 26600/1000000  loss         0.160869  avg_L1_norm_grad         0.000013  w[0]    0.085 bias    3.811\n",
      "iter 26601/1000000  loss         0.160868  avg_L1_norm_grad         0.000013  w[0]    0.085 bias    3.811\n",
      "iter 26700/1000000  loss         0.160861  avg_L1_norm_grad         0.000013  w[0]    0.085 bias    3.813\n",
      "iter 26701/1000000  loss         0.160861  avg_L1_norm_grad         0.000013  w[0]    0.085 bias    3.813\n",
      "iter 26800/1000000  loss         0.160853  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.814\n",
      "iter 26801/1000000  loss         0.160853  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.814\n",
      "iter 26900/1000000  loss         0.160846  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.815\n",
      "iter 26901/1000000  loss         0.160846  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.815\n",
      "iter 27000/1000000  loss         0.160838  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.816\n",
      "iter 27001/1000000  loss         0.160838  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.816\n",
      "iter 27100/1000000  loss         0.160831  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.817\n",
      "iter 27101/1000000  loss         0.160831  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.817\n",
      "iter 27200/1000000  loss         0.160824  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.818\n",
      "iter 27201/1000000  loss         0.160824  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.818\n",
      "iter 27300/1000000  loss         0.160816  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.820\n",
      "iter 27301/1000000  loss         0.160816  avg_L1_norm_grad         0.000013  w[0]    0.086 bias    3.820\n",
      "iter 27400/1000000  loss         0.160809  avg_L1_norm_grad         0.000012  w[0]    0.086 bias    3.821\n",
      "iter 27401/1000000  loss         0.160809  avg_L1_norm_grad         0.000012  w[0]    0.086 bias    3.821\n",
      "iter 27500/1000000  loss         0.160802  avg_L1_norm_grad         0.000012  w[0]    0.086 bias    3.822\n",
      "iter 27501/1000000  loss         0.160802  avg_L1_norm_grad         0.000012  w[0]    0.086 bias    3.822\n",
      "iter 27600/1000000  loss         0.160795  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.823\n",
      "iter 27601/1000000  loss         0.160795  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.823\n",
      "iter 27700/1000000  loss         0.160788  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.824\n",
      "iter 27701/1000000  loss         0.160788  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.824\n",
      "iter 27800/1000000  loss         0.160781  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.825\n",
      "iter 27801/1000000  loss         0.160781  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.825\n",
      "iter 27900/1000000  loss         0.160774  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.826\n",
      "iter 27901/1000000  loss         0.160774  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.826\n",
      "iter 28000/1000000  loss         0.160767  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.827\n",
      "iter 28001/1000000  loss         0.160767  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.827\n",
      "iter 28100/1000000  loss         0.160760  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.829\n",
      "iter 28101/1000000  loss         0.160760  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.829\n",
      "iter 28200/1000000  loss         0.160753  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.830\n",
      "iter 28201/1000000  loss         0.160753  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.830\n",
      "iter 28300/1000000  loss         0.160747  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.831\n",
      "iter 28301/1000000  loss         0.160747  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.831\n",
      "iter 28400/1000000  loss         0.160740  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.832\n",
      "iter 28401/1000000  loss         0.160740  avg_L1_norm_grad         0.000012  w[0]    0.087 bias    3.832\n",
      "iter 28500/1000000  loss         0.160733  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.833\n",
      "iter 28501/1000000  loss         0.160733  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.833\n",
      "iter 28600/1000000  loss         0.160727  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.834\n",
      "iter 28601/1000000  loss         0.160727  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.834\n",
      "iter 28700/1000000  loss         0.160720  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.835\n",
      "iter 28701/1000000  loss         0.160720  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.835\n",
      "iter 28800/1000000  loss         0.160714  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.836\n",
      "iter 28801/1000000  loss         0.160714  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.836\n",
      "iter 28900/1000000  loss         0.160707  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.837\n",
      "iter 28901/1000000  loss         0.160707  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.837\n",
      "iter 29000/1000000  loss         0.160701  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.838\n",
      "iter 29001/1000000  loss         0.160701  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.838\n",
      "iter 29100/1000000  loss         0.160695  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.839\n",
      "iter 29101/1000000  loss         0.160695  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.839\n",
      "iter 29200/1000000  loss         0.160688  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.840\n",
      "iter 29201/1000000  loss         0.160688  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.840\n",
      "iter 29300/1000000  loss         0.160682  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.841\n",
      "iter 29301/1000000  loss         0.160682  avg_L1_norm_grad         0.000011  w[0]    0.088 bias    3.841\n",
      "iter 29400/1000000  loss         0.160676  avg_L1_norm_grad         0.000011  w[0]    0.089 bias    3.842\n",
      "iter 29401/1000000  loss         0.160676  avg_L1_norm_grad         0.000011  w[0]    0.089 bias    3.842\n",
      "iter 29500/1000000  loss         0.160670  avg_L1_norm_grad         0.000011  w[0]    0.089 bias    3.843\n",
      "iter 29501/1000000  loss         0.160670  avg_L1_norm_grad         0.000011  w[0]    0.089 bias    3.843\n",
      "iter 29600/1000000  loss         0.160664  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.844\n",
      "iter 29601/1000000  loss         0.160664  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.844\n",
      "iter 29700/1000000  loss         0.160658  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.845\n",
      "iter 29701/1000000  loss         0.160658  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.845\n",
      "iter 29800/1000000  loss         0.160652  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.846\n",
      "iter 29801/1000000  loss         0.160652  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.846\n",
      "iter 29900/1000000  loss         0.160646  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.847\n",
      "iter 29901/1000000  loss         0.160646  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.847\n",
      "iter 30000/1000000  loss         0.160640  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.848\n",
      "iter 30001/1000000  loss         0.160640  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.848\n",
      "iter 30100/1000000  loss         0.160634  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.849\n",
      "iter 30101/1000000  loss         0.160634  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.849\n",
      "iter 30200/1000000  loss         0.160628  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.850\n",
      "iter 30201/1000000  loss         0.160628  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.850\n",
      "iter 30300/1000000  loss         0.160622  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.851\n",
      "iter 30301/1000000  loss         0.160622  avg_L1_norm_grad         0.000010  w[0]    0.089 bias    3.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30400/1000000  loss         0.160616  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.852\n",
      "iter 30401/1000000  loss         0.160616  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.852\n",
      "iter 30500/1000000  loss         0.160611  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.853\n",
      "iter 30501/1000000  loss         0.160611  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.853\n",
      "iter 30600/1000000  loss         0.160605  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.854\n",
      "iter 30601/1000000  loss         0.160605  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.854\n",
      "iter 30700/1000000  loss         0.160599  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.855\n",
      "iter 30701/1000000  loss         0.160599  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.855\n",
      "iter 30800/1000000  loss         0.160594  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.856\n",
      "iter 30801/1000000  loss         0.160594  avg_L1_norm_grad         0.000010  w[0]    0.090 bias    3.856\n",
      "iter 30900/1000000  loss         0.160588  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.857\n",
      "iter 30901/1000000  loss         0.160588  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.857\n",
      "iter 31000/1000000  loss         0.160583  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.857\n",
      "iter 31001/1000000  loss         0.160583  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.857\n",
      "iter 31100/1000000  loss         0.160577  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.858\n",
      "iter 31101/1000000  loss         0.160577  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.858\n",
      "iter 31200/1000000  loss         0.160572  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.859\n",
      "iter 31201/1000000  loss         0.160572  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.859\n",
      "iter 31300/1000000  loss         0.160566  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.860\n",
      "iter 31301/1000000  loss         0.160566  avg_L1_norm_grad         0.000009  w[0]    0.090 bias    3.860\n",
      "iter 31400/1000000  loss         0.160561  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.861\n",
      "iter 31401/1000000  loss         0.160561  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.861\n",
      "iter 31500/1000000  loss         0.160556  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.862\n",
      "iter 31501/1000000  loss         0.160556  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.862\n",
      "iter 31600/1000000  loss         0.160550  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.863\n",
      "iter 31601/1000000  loss         0.160550  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.863\n",
      "iter 31700/1000000  loss         0.160545  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.864\n",
      "iter 31701/1000000  loss         0.160545  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.864\n",
      "iter 31800/1000000  loss         0.160540  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.865\n",
      "iter 31801/1000000  loss         0.160540  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.865\n",
      "iter 31900/1000000  loss         0.160535  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.866\n",
      "iter 31901/1000000  loss         0.160535  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.866\n",
      "iter 32000/1000000  loss         0.160529  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.866\n",
      "iter 32001/1000000  loss         0.160529  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.866\n",
      "iter 32100/1000000  loss         0.160524  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.867\n",
      "iter 32101/1000000  loss         0.160524  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.867\n",
      "iter 32200/1000000  loss         0.160519  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.868\n",
      "iter 32201/1000000  loss         0.160519  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.868\n",
      "iter 32300/1000000  loss         0.160514  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.869\n",
      "iter 32301/1000000  loss         0.160514  avg_L1_norm_grad         0.000009  w[0]    0.091 bias    3.869\n",
      "iter 32400/1000000  loss         0.160509  avg_L1_norm_grad         0.000008  w[0]    0.091 bias    3.870\n",
      "iter 32401/1000000  loss         0.160509  avg_L1_norm_grad         0.000008  w[0]    0.091 bias    3.870\n",
      "iter 32500/1000000  loss         0.160504  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.871\n",
      "iter 32501/1000000  loss         0.160504  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.871\n",
      "iter 32600/1000000  loss         0.160499  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.872\n",
      "iter 32601/1000000  loss         0.160499  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.872\n",
      "iter 32700/1000000  loss         0.160494  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.872\n",
      "iter 32701/1000000  loss         0.160494  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.872\n",
      "iter 32800/1000000  loss         0.160489  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.873\n",
      "iter 32801/1000000  loss         0.160489  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.873\n",
      "iter 32900/1000000  loss         0.160484  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.874\n",
      "iter 32901/1000000  loss         0.160484  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.874\n",
      "iter 33000/1000000  loss         0.160480  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.875\n",
      "iter 33001/1000000  loss         0.160480  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.875\n",
      "iter 33100/1000000  loss         0.160475  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.876\n",
      "iter 33101/1000000  loss         0.160475  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.876\n",
      "iter 33200/1000000  loss         0.160470  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.877\n",
      "iter 33201/1000000  loss         0.160470  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.877\n",
      "iter 33300/1000000  loss         0.160465  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.877\n",
      "iter 33301/1000000  loss         0.160465  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.877\n",
      "iter 33400/1000000  loss         0.160461  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.878\n",
      "iter 33401/1000000  loss         0.160461  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.878\n",
      "iter 33500/1000000  loss         0.160456  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.879\n",
      "iter 33501/1000000  loss         0.160456  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.879\n",
      "iter 33600/1000000  loss         0.160451  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.880\n",
      "iter 33601/1000000  loss         0.160451  avg_L1_norm_grad         0.000008  w[0]    0.092 bias    3.880\n",
      "iter 33700/1000000  loss         0.160447  avg_L1_norm_grad         0.000008  w[0]    0.093 bias    3.881\n",
      "iter 33701/1000000  loss         0.160447  avg_L1_norm_grad         0.000008  w[0]    0.093 bias    3.881\n",
      "iter 33800/1000000  loss         0.160442  avg_L1_norm_grad         0.000008  w[0]    0.093 bias    3.881\n",
      "iter 33801/1000000  loss         0.160442  avg_L1_norm_grad         0.000008  w[0]    0.093 bias    3.881\n",
      "iter 33900/1000000  loss         0.160437  avg_L1_norm_grad         0.000008  w[0]    0.093 bias    3.882\n",
      "iter 33901/1000000  loss         0.160437  avg_L1_norm_grad         0.000008  w[0]    0.093 bias    3.882\n",
      "iter 34000/1000000  loss         0.160433  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.883\n",
      "iter 34001/1000000  loss         0.160433  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.883\n",
      "iter 34100/1000000  loss         0.160428  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.884\n",
      "iter 34101/1000000  loss         0.160428  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.884\n",
      "iter 34200/1000000  loss         0.160424  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.885\n",
      "iter 34201/1000000  loss         0.160424  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34300/1000000  loss         0.160419  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.885\n",
      "iter 34301/1000000  loss         0.160419  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.885\n",
      "iter 34400/1000000  loss         0.160415  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.886\n",
      "iter 34401/1000000  loss         0.160415  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.886\n",
      "iter 34500/1000000  loss         0.160411  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.887\n",
      "iter 34501/1000000  loss         0.160411  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.887\n",
      "iter 34600/1000000  loss         0.160406  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.888\n",
      "iter 34601/1000000  loss         0.160406  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.888\n",
      "iter 34700/1000000  loss         0.160402  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.888\n",
      "iter 34701/1000000  loss         0.160402  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.888\n",
      "iter 34800/1000000  loss         0.160398  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.889\n",
      "iter 34801/1000000  loss         0.160398  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.889\n",
      "iter 34900/1000000  loss         0.160393  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.890\n",
      "iter 34901/1000000  loss         0.160393  avg_L1_norm_grad         0.000007  w[0]    0.093 bias    3.890\n",
      "iter 35000/1000000  loss         0.160389  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.891\n",
      "iter 35001/1000000  loss         0.160389  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.891\n",
      "iter 35100/1000000  loss         0.160385  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.891\n",
      "iter 35101/1000000  loss         0.160385  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.891\n",
      "iter 35200/1000000  loss         0.160381  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.892\n",
      "iter 35201/1000000  loss         0.160381  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.892\n",
      "iter 35300/1000000  loss         0.160376  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.893\n",
      "iter 35301/1000000  loss         0.160376  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.893\n",
      "iter 35400/1000000  loss         0.160372  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.894\n",
      "iter 35401/1000000  loss         0.160372  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.894\n",
      "iter 35500/1000000  loss         0.160368  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.894\n",
      "iter 35501/1000000  loss         0.160368  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.894\n",
      "iter 35600/1000000  loss         0.160364  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.895\n",
      "iter 35601/1000000  loss         0.160364  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.895\n",
      "iter 35700/1000000  loss         0.160360  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.896\n",
      "iter 35701/1000000  loss         0.160360  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.896\n",
      "iter 35800/1000000  loss         0.160356  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.897\n",
      "iter 35801/1000000  loss         0.160356  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.897\n",
      "iter 35900/1000000  loss         0.160352  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.897\n",
      "iter 35901/1000000  loss         0.160352  avg_L1_norm_grad         0.000007  w[0]    0.094 bias    3.897\n",
      "iter 36000/1000000  loss         0.160348  avg_L1_norm_grad         0.000006  w[0]    0.094 bias    3.898\n",
      "iter 36001/1000000  loss         0.160348  avg_L1_norm_grad         0.000006  w[0]    0.094 bias    3.898\n",
      "iter 36100/1000000  loss         0.160344  avg_L1_norm_grad         0.000006  w[0]    0.094 bias    3.899\n",
      "iter 36101/1000000  loss         0.160344  avg_L1_norm_grad         0.000006  w[0]    0.094 bias    3.899\n",
      "iter 36200/1000000  loss         0.160340  avg_L1_norm_grad         0.000006  w[0]    0.094 bias    3.899\n",
      "iter 36201/1000000  loss         0.160340  avg_L1_norm_grad         0.000006  w[0]    0.094 bias    3.899\n",
      "iter 36300/1000000  loss         0.160336  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.900\n",
      "iter 36301/1000000  loss         0.160336  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.900\n",
      "iter 36400/1000000  loss         0.160332  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.901\n",
      "iter 36401/1000000  loss         0.160332  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.901\n",
      "iter 36500/1000000  loss         0.160328  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.902\n",
      "iter 36501/1000000  loss         0.160328  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.902\n",
      "iter 36600/1000000  loss         0.160324  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.902\n",
      "iter 36601/1000000  loss         0.160324  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.902\n",
      "iter 36700/1000000  loss         0.160320  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.903\n",
      "iter 36701/1000000  loss         0.160320  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.903\n",
      "iter 36800/1000000  loss         0.160317  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.904\n",
      "iter 36801/1000000  loss         0.160316  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.904\n",
      "iter 36900/1000000  loss         0.160313  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.904\n",
      "iter 36901/1000000  loss         0.160313  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.904\n",
      "iter 37000/1000000  loss         0.160309  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.905\n",
      "iter 37001/1000000  loss         0.160309  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.905\n",
      "iter 37100/1000000  loss         0.160305  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.906\n",
      "iter 37101/1000000  loss         0.160305  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.906\n",
      "iter 37200/1000000  loss         0.160301  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.906\n",
      "iter 37201/1000000  loss         0.160301  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.906\n",
      "iter 37300/1000000  loss         0.160298  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.907\n",
      "iter 37301/1000000  loss         0.160298  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.907\n",
      "iter 37400/1000000  loss         0.160294  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.908\n",
      "iter 37401/1000000  loss         0.160294  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.908\n",
      "iter 37500/1000000  loss         0.160290  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.908\n",
      "iter 37501/1000000  loss         0.160290  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.908\n",
      "iter 37600/1000000  loss         0.160287  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.909\n",
      "iter 37601/1000000  loss         0.160287  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.909\n",
      "iter 37700/1000000  loss         0.160283  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.910\n",
      "iter 37701/1000000  loss         0.160283  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.910\n",
      "iter 37800/1000000  loss         0.160279  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.910\n",
      "iter 37801/1000000  loss         0.160279  avg_L1_norm_grad         0.000006  w[0]    0.095 bias    3.910\n",
      "iter 37900/1000000  loss         0.160276  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.911\n",
      "iter 37901/1000000  loss         0.160276  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.911\n",
      "iter 38000/1000000  loss         0.160272  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.912\n",
      "iter 38001/1000000  loss         0.160272  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.912\n",
      "iter 38100/1000000  loss         0.160269  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.912\n",
      "iter 38101/1000000  loss         0.160269  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38200/1000000  loss         0.160265  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.913\n",
      "iter 38201/1000000  loss         0.160265  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.913\n",
      "iter 38300/1000000  loss         0.160262  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.914\n",
      "iter 38301/1000000  loss         0.160262  avg_L1_norm_grad         0.000006  w[0]    0.096 bias    3.914\n",
      "iter 38400/1000000  loss         0.160258  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.914\n",
      "iter 38401/1000000  loss         0.160258  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.914\n",
      "iter 38500/1000000  loss         0.160255  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.915\n",
      "iter 38501/1000000  loss         0.160255  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.915\n",
      "iter 38600/1000000  loss         0.160251  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.916\n",
      "iter 38601/1000000  loss         0.160251  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.916\n",
      "iter 38700/1000000  loss         0.160248  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.916\n",
      "iter 38701/1000000  loss         0.160248  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.916\n",
      "iter 38800/1000000  loss         0.160244  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.917\n",
      "iter 38801/1000000  loss         0.160244  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.917\n",
      "iter 38900/1000000  loss         0.160241  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.917\n",
      "iter 38901/1000000  loss         0.160241  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.917\n",
      "iter 39000/1000000  loss         0.160238  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.918\n",
      "iter 39001/1000000  loss         0.160238  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.918\n",
      "iter 39100/1000000  loss         0.160234  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.919\n",
      "iter 39101/1000000  loss         0.160234  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.919\n",
      "iter 39200/1000000  loss         0.160231  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.919\n",
      "iter 39201/1000000  loss         0.160231  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.919\n",
      "iter 39300/1000000  loss         0.160228  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.920\n",
      "iter 39301/1000000  loss         0.160228  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.920\n",
      "iter 39400/1000000  loss         0.160224  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.921\n",
      "iter 39401/1000000  loss         0.160224  avg_L1_norm_grad         0.000005  w[0]    0.096 bias    3.921\n",
      "iter 39500/1000000  loss         0.160221  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.921\n",
      "iter 39501/1000000  loss         0.160221  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.921\n",
      "iter 39600/1000000  loss         0.160218  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.922\n",
      "iter 39601/1000000  loss         0.160218  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.922\n",
      "iter 39700/1000000  loss         0.160214  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.922\n",
      "iter 39701/1000000  loss         0.160214  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.922\n",
      "iter 39800/1000000  loss         0.160211  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.923\n",
      "iter 39801/1000000  loss         0.160211  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.923\n",
      "iter 39900/1000000  loss         0.160208  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.924\n",
      "iter 39901/1000000  loss         0.160208  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.924\n",
      "iter 40000/1000000  loss         0.160205  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.924\n",
      "iter 40001/1000000  loss         0.160205  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.924\n",
      "iter 40100/1000000  loss         0.160202  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.925\n",
      "iter 40101/1000000  loss         0.160202  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.925\n",
      "iter 40200/1000000  loss         0.160198  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.925\n",
      "iter 40201/1000000  loss         0.160198  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.925\n",
      "iter 40300/1000000  loss         0.160195  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.926\n",
      "iter 40301/1000000  loss         0.160195  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.926\n",
      "iter 40400/1000000  loss         0.160192  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.927\n",
      "iter 40401/1000000  loss         0.160192  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.927\n",
      "iter 40500/1000000  loss         0.160189  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.927\n",
      "iter 40501/1000000  loss         0.160189  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.927\n",
      "iter 40600/1000000  loss         0.160186  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.928\n",
      "iter 40601/1000000  loss         0.160186  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.928\n",
      "iter 40700/1000000  loss         0.160183  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.928\n",
      "iter 40701/1000000  loss         0.160183  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.928\n",
      "iter 40800/1000000  loss         0.160180  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.929\n",
      "iter 40801/1000000  loss         0.160180  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.929\n",
      "iter 40900/1000000  loss         0.160177  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.929\n",
      "iter 40901/1000000  loss         0.160177  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.929\n",
      "iter 41000/1000000  loss         0.160174  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.930\n",
      "iter 41001/1000000  loss         0.160174  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.930\n",
      "iter 41100/1000000  loss         0.160171  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.931\n",
      "iter 41101/1000000  loss         0.160171  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.931\n",
      "iter 41200/1000000  loss         0.160168  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.931\n",
      "iter 41201/1000000  loss         0.160168  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.931\n",
      "iter 41300/1000000  loss         0.160165  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.932\n",
      "iter 41301/1000000  loss         0.160165  avg_L1_norm_grad         0.000005  w[0]    0.097 bias    3.932\n",
      "iter 41400/1000000  loss         0.160162  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.932\n",
      "iter 41401/1000000  loss         0.160162  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.932\n",
      "iter 41500/1000000  loss         0.160159  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.933\n",
      "iter 41501/1000000  loss         0.160159  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.933\n",
      "iter 41600/1000000  loss         0.160156  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.933\n",
      "iter 41601/1000000  loss         0.160156  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.933\n",
      "iter 41700/1000000  loss         0.160153  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.934\n",
      "iter 41701/1000000  loss         0.160153  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.934\n",
      "iter 41800/1000000  loss         0.160150  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.934\n",
      "iter 41801/1000000  loss         0.160150  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.934\n",
      "iter 41900/1000000  loss         0.160147  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.935\n",
      "iter 41901/1000000  loss         0.160147  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.935\n",
      "iter 42000/1000000  loss         0.160144  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.936\n",
      "iter 42001/1000000  loss         0.160144  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42100/1000000  loss         0.160142  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.936\n",
      "iter 42101/1000000  loss         0.160141  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.936\n",
      "iter 42200/1000000  loss         0.160139  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.937\n",
      "iter 42201/1000000  loss         0.160139  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.937\n",
      "iter 42300/1000000  loss         0.160136  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.937\n",
      "iter 42301/1000000  loss         0.160136  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.937\n",
      "iter 42400/1000000  loss         0.160133  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.938\n",
      "iter 42401/1000000  loss         0.160133  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.938\n",
      "iter 42500/1000000  loss         0.160130  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.938\n",
      "iter 42501/1000000  loss         0.160130  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.938\n",
      "iter 42600/1000000  loss         0.160127  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.939\n",
      "iter 42601/1000000  loss         0.160127  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.939\n",
      "iter 42700/1000000  loss         0.160125  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.939\n",
      "iter 42701/1000000  loss         0.160125  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.939\n",
      "iter 42800/1000000  loss         0.160122  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.940\n",
      "iter 42801/1000000  loss         0.160122  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.940\n",
      "iter 42900/1000000  loss         0.160119  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.940\n",
      "iter 42901/1000000  loss         0.160119  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.940\n",
      "iter 43000/1000000  loss         0.160117  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.941\n",
      "iter 43001/1000000  loss         0.160116  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.941\n",
      "iter 43100/1000000  loss         0.160114  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.941\n",
      "iter 43101/1000000  loss         0.160114  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.941\n",
      "iter 43200/1000000  loss         0.160111  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.942\n",
      "iter 43201/1000000  loss         0.160111  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.942\n",
      "iter 43300/1000000  loss         0.160108  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.943\n",
      "iter 43301/1000000  loss         0.160108  avg_L1_norm_grad         0.000004  w[0]    0.098 bias    3.943\n",
      "iter 43400/1000000  loss         0.160106  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.943\n",
      "iter 43401/1000000  loss         0.160106  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.943\n",
      "iter 43500/1000000  loss         0.160103  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.944\n",
      "iter 43501/1000000  loss         0.160103  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.944\n",
      "iter 43600/1000000  loss         0.160100  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.944\n",
      "iter 43601/1000000  loss         0.160100  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.944\n",
      "iter 43700/1000000  loss         0.160098  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.945\n",
      "iter 43701/1000000  loss         0.160098  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.945\n",
      "iter 43800/1000000  loss         0.160095  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.945\n",
      "iter 43801/1000000  loss         0.160095  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.945\n",
      "iter 43900/1000000  loss         0.160093  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.946\n",
      "iter 43901/1000000  loss         0.160093  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.946\n",
      "iter 44000/1000000  loss         0.160090  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.946\n",
      "iter 44001/1000000  loss         0.160090  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.946\n",
      "iter 44100/1000000  loss         0.160087  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.947\n",
      "iter 44101/1000000  loss         0.160087  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.947\n",
      "iter 44200/1000000  loss         0.160085  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.947\n",
      "iter 44201/1000000  loss         0.160085  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.947\n",
      "iter 44300/1000000  loss         0.160082  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.948\n",
      "iter 44301/1000000  loss         0.160082  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.948\n",
      "iter 44400/1000000  loss         0.160080  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.948\n",
      "iter 44401/1000000  loss         0.160080  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.948\n",
      "iter 44500/1000000  loss         0.160077  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.949\n",
      "iter 44501/1000000  loss         0.160077  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.949\n",
      "iter 44600/1000000  loss         0.160075  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.949\n",
      "iter 44601/1000000  loss         0.160075  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.949\n",
      "iter 44700/1000000  loss         0.160072  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.950\n",
      "iter 44701/1000000  loss         0.160072  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.950\n",
      "iter 44800/1000000  loss         0.160070  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.950\n",
      "iter 44801/1000000  loss         0.160070  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.950\n",
      "iter 44900/1000000  loss         0.160067  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.951\n",
      "iter 44901/1000000  loss         0.160067  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.951\n",
      "iter 45000/1000000  loss         0.160065  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.951\n",
      "iter 45001/1000000  loss         0.160065  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.951\n",
      "iter 45100/1000000  loss         0.160062  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.952\n",
      "iter 45101/1000000  loss         0.160062  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.952\n",
      "iter 45200/1000000  loss         0.160060  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.952\n",
      "iter 45201/1000000  loss         0.160060  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.952\n",
      "iter 45300/1000000  loss         0.160058  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.953\n",
      "iter 45301/1000000  loss         0.160058  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.953\n",
      "iter 45400/1000000  loss         0.160055  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.953\n",
      "iter 45401/1000000  loss         0.160055  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.953\n",
      "iter 45500/1000000  loss         0.160053  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.953\n",
      "iter 45501/1000000  loss         0.160053  avg_L1_norm_grad         0.000004  w[0]    0.099 bias    3.953\n",
      "iter 45600/1000000  loss         0.160050  avg_L1_norm_grad         0.000003  w[0]    0.099 bias    3.954\n",
      "iter 45601/1000000  loss         0.160050  avg_L1_norm_grad         0.000003  w[0]    0.099 bias    3.954\n",
      "iter 45700/1000000  loss         0.160048  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.954\n",
      "iter 45701/1000000  loss         0.160048  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.954\n",
      "iter 45800/1000000  loss         0.160046  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.955\n",
      "iter 45801/1000000  loss         0.160046  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.955\n",
      "iter 45900/1000000  loss         0.160043  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.955\n",
      "iter 45901/1000000  loss         0.160043  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 46000/1000000  loss         0.160041  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.956\n",
      "iter 46001/1000000  loss         0.160041  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.956\n",
      "iter 46100/1000000  loss         0.160039  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.956\n",
      "iter 46101/1000000  loss         0.160039  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.956\n",
      "iter 46200/1000000  loss         0.160036  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.957\n",
      "iter 46201/1000000  loss         0.160036  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.957\n",
      "iter 46300/1000000  loss         0.160034  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.957\n",
      "iter 46301/1000000  loss         0.160034  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.957\n",
      "iter 46400/1000000  loss         0.160032  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.958\n",
      "iter 46401/1000000  loss         0.160032  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.958\n",
      "iter 46500/1000000  loss         0.160030  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.958\n",
      "iter 46501/1000000  loss         0.160030  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.958\n",
      "iter 46600/1000000  loss         0.160027  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.959\n",
      "iter 46601/1000000  loss         0.160027  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.959\n",
      "iter 46700/1000000  loss         0.160025  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.959\n",
      "iter 46701/1000000  loss         0.160025  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.959\n",
      "iter 46800/1000000  loss         0.160023  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.960\n",
      "iter 46801/1000000  loss         0.160023  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.960\n",
      "iter 46900/1000000  loss         0.160021  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.960\n",
      "iter 46901/1000000  loss         0.160021  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.960\n",
      "iter 47000/1000000  loss         0.160018  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.960\n",
      "iter 47001/1000000  loss         0.160018  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.960\n",
      "iter 47100/1000000  loss         0.160016  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.961\n",
      "iter 47101/1000000  loss         0.160016  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.961\n",
      "iter 47200/1000000  loss         0.160014  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.961\n",
      "iter 47201/1000000  loss         0.160014  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.961\n",
      "iter 47300/1000000  loss         0.160012  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.962\n",
      "iter 47301/1000000  loss         0.160012  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.962\n",
      "iter 47400/1000000  loss         0.160010  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.962\n",
      "iter 47401/1000000  loss         0.160010  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.962\n",
      "iter 47500/1000000  loss         0.160007  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.963\n",
      "iter 47501/1000000  loss         0.160007  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.963\n",
      "iter 47600/1000000  loss         0.160005  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.963\n",
      "iter 47601/1000000  loss         0.160005  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.963\n",
      "iter 47700/1000000  loss         0.160003  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.964\n",
      "iter 47701/1000000  loss         0.160003  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.964\n",
      "iter 47800/1000000  loss         0.160001  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.964\n",
      "iter 47801/1000000  loss         0.160001  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.964\n",
      "iter 47900/1000000  loss         0.159999  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.964\n",
      "iter 47901/1000000  loss         0.159999  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.964\n",
      "iter 48000/1000000  loss         0.159997  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.965\n",
      "iter 48001/1000000  loss         0.159997  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.965\n",
      "iter 48100/1000000  loss         0.159995  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.965\n",
      "iter 48101/1000000  loss         0.159995  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.965\n",
      "iter 48200/1000000  loss         0.159993  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.966\n",
      "iter 48201/1000000  loss         0.159993  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.966\n",
      "iter 48300/1000000  loss         0.159991  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.966\n",
      "iter 48301/1000000  loss         0.159990  avg_L1_norm_grad         0.000003  w[0]    0.100 bias    3.966\n",
      "iter 48400/1000000  loss         0.159988  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.967\n",
      "iter 48401/1000000  loss         0.159988  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.967\n",
      "iter 48500/1000000  loss         0.159986  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.967\n",
      "iter 48501/1000000  loss         0.159986  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.967\n",
      "iter 48600/1000000  loss         0.159984  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.967\n",
      "iter 48601/1000000  loss         0.159984  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.967\n",
      "iter 48700/1000000  loss         0.159982  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.968\n",
      "iter 48701/1000000  loss         0.159982  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.968\n",
      "iter 48800/1000000  loss         0.159980  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.968\n",
      "iter 48801/1000000  loss         0.159980  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.968\n",
      "iter 48900/1000000  loss         0.159978  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.969\n",
      "iter 48901/1000000  loss         0.159978  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.969\n",
      "iter 49000/1000000  loss         0.159976  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.969\n",
      "iter 49001/1000000  loss         0.159976  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.969\n",
      "iter 49100/1000000  loss         0.159974  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.970\n",
      "iter 49101/1000000  loss         0.159974  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.970\n",
      "iter 49200/1000000  loss         0.159972  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.970\n",
      "iter 49201/1000000  loss         0.159972  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.970\n",
      "iter 49300/1000000  loss         0.159970  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.970\n",
      "iter 49301/1000000  loss         0.159970  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.970\n",
      "iter 49400/1000000  loss         0.159968  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.971\n",
      "iter 49401/1000000  loss         0.159968  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.971\n",
      "iter 49500/1000000  loss         0.159966  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.971\n",
      "iter 49501/1000000  loss         0.159966  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.971\n",
      "iter 49600/1000000  loss         0.159964  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.972\n",
      "iter 49601/1000000  loss         0.159964  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.972\n",
      "iter 49700/1000000  loss         0.159962  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.972\n",
      "iter 49701/1000000  loss         0.159962  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.972\n",
      "iter 49800/1000000  loss         0.159961  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.972\n",
      "iter 49801/1000000  loss         0.159960  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 49900/1000000  loss         0.159959  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.973\n",
      "iter 49901/1000000  loss         0.159959  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.973\n",
      "iter 50000/1000000  loss         0.159957  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.973\n",
      "iter 50001/1000000  loss         0.159957  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.973\n",
      "iter 50100/1000000  loss         0.159955  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.974\n",
      "iter 50101/1000000  loss         0.159955  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.974\n",
      "iter 50200/1000000  loss         0.159953  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.974\n",
      "iter 50201/1000000  loss         0.159953  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.974\n",
      "iter 50300/1000000  loss         0.159951  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.974\n",
      "iter 50301/1000000  loss         0.159951  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.974\n",
      "iter 50400/1000000  loss         0.159949  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.975\n",
      "iter 50401/1000000  loss         0.159949  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.975\n",
      "iter 50500/1000000  loss         0.159947  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.975\n",
      "iter 50501/1000000  loss         0.159947  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.975\n",
      "iter 50600/1000000  loss         0.159945  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.976\n",
      "iter 50601/1000000  loss         0.159945  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.976\n",
      "iter 50700/1000000  loss         0.159944  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.976\n",
      "iter 50701/1000000  loss         0.159944  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.976\n",
      "iter 50800/1000000  loss         0.159942  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.976\n",
      "iter 50801/1000000  loss         0.159942  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.976\n",
      "iter 50900/1000000  loss         0.159940  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.977\n",
      "iter 50901/1000000  loss         0.159940  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.977\n",
      "iter 51000/1000000  loss         0.159938  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.977\n",
      "iter 51001/1000000  loss         0.159938  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.977\n",
      "iter 51100/1000000  loss         0.159936  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.978\n",
      "iter 51101/1000000  loss         0.159936  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.978\n",
      "iter 51200/1000000  loss         0.159934  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.978\n",
      "iter 51201/1000000  loss         0.159934  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.978\n",
      "iter 51300/1000000  loss         0.159933  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.978\n",
      "iter 51301/1000000  loss         0.159933  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.978\n",
      "iter 51400/1000000  loss         0.159931  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.979\n",
      "iter 51401/1000000  loss         0.159931  avg_L1_norm_grad         0.000003  w[0]    0.101 bias    3.979\n",
      "iter 51500/1000000  loss         0.159929  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.979\n",
      "iter 51501/1000000  loss         0.159929  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.979\n",
      "iter 51600/1000000  loss         0.159927  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.980\n",
      "iter 51601/1000000  loss         0.159927  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.980\n",
      "iter 51700/1000000  loss         0.159926  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.980\n",
      "iter 51701/1000000  loss         0.159926  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.980\n",
      "iter 51800/1000000  loss         0.159924  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.980\n",
      "iter 51801/1000000  loss         0.159924  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.980\n",
      "iter 51900/1000000  loss         0.159922  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.981\n",
      "iter 51901/1000000  loss         0.159922  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.981\n",
      "iter 52000/1000000  loss         0.159920  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.981\n",
      "iter 52001/1000000  loss         0.159920  avg_L1_norm_grad         0.000003  w[0]    0.102 bias    3.981\n",
      "iter 52100/1000000  loss         0.159919  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.981\n",
      "iter 52101/1000000  loss         0.159919  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.981\n",
      "iter 52200/1000000  loss         0.159917  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.982\n",
      "iter 52201/1000000  loss         0.159917  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.982\n",
      "iter 52300/1000000  loss         0.159915  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.982\n",
      "iter 52301/1000000  loss         0.159915  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.982\n",
      "iter 52400/1000000  loss         0.159913  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.983\n",
      "iter 52401/1000000  loss         0.159913  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.983\n",
      "iter 52500/1000000  loss         0.159912  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.983\n",
      "iter 52501/1000000  loss         0.159912  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.983\n",
      "iter 52600/1000000  loss         0.159910  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.983\n",
      "iter 52601/1000000  loss         0.159910  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.983\n",
      "iter 52700/1000000  loss         0.159908  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.984\n",
      "iter 52701/1000000  loss         0.159908  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.984\n",
      "iter 52800/1000000  loss         0.159907  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.984\n",
      "iter 52801/1000000  loss         0.159907  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.984\n",
      "iter 52900/1000000  loss         0.159905  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.984\n",
      "iter 52901/1000000  loss         0.159905  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.984\n",
      "iter 53000/1000000  loss         0.159903  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.985\n",
      "iter 53001/1000000  loss         0.159903  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.985\n",
      "iter 53100/1000000  loss         0.159902  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.985\n",
      "iter 53101/1000000  loss         0.159902  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.985\n",
      "iter 53200/1000000  loss         0.159900  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.986\n",
      "iter 53201/1000000  loss         0.159900  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.986\n",
      "iter 53300/1000000  loss         0.159899  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.986\n",
      "iter 53301/1000000  loss         0.159899  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.986\n",
      "iter 53400/1000000  loss         0.159897  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.986\n",
      "iter 53401/1000000  loss         0.159897  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.986\n",
      "iter 53500/1000000  loss         0.159895  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.987\n",
      "iter 53501/1000000  loss         0.159895  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.987\n",
      "iter 53600/1000000  loss         0.159894  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.987\n",
      "iter 53601/1000000  loss         0.159894  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.987\n",
      "iter 53700/1000000  loss         0.159892  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.987\n",
      "iter 53701/1000000  loss         0.159892  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 53800/1000000  loss         0.159891  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.988\n",
      "iter 53801/1000000  loss         0.159890  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.988\n",
      "iter 53900/1000000  loss         0.159889  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.988\n",
      "iter 53901/1000000  loss         0.159889  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.988\n",
      "iter 54000/1000000  loss         0.159887  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.988\n",
      "iter 54001/1000000  loss         0.159887  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.988\n",
      "iter 54100/1000000  loss         0.159886  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.989\n",
      "iter 54101/1000000  loss         0.159886  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.989\n",
      "iter 54200/1000000  loss         0.159884  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.989\n",
      "iter 54201/1000000  loss         0.159884  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.989\n",
      "iter 54300/1000000  loss         0.159883  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.989\n",
      "iter 54301/1000000  loss         0.159883  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.990\n",
      "iter 54400/1000000  loss         0.159881  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.990\n",
      "iter 54401/1000000  loss         0.159881  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.990\n",
      "iter 54500/1000000  loss         0.159880  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.990\n",
      "iter 54501/1000000  loss         0.159880  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.990\n",
      "iter 54600/1000000  loss         0.159878  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.991\n",
      "iter 54601/1000000  loss         0.159878  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.991\n",
      "iter 54700/1000000  loss         0.159877  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.991\n",
      "iter 54701/1000000  loss         0.159877  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.991\n",
      "iter 54800/1000000  loss         0.159875  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.991\n",
      "iter 54801/1000000  loss         0.159875  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.991\n",
      "iter 54900/1000000  loss         0.159874  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.992\n",
      "iter 54901/1000000  loss         0.159873  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.992\n",
      "iter 55000/1000000  loss         0.159872  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.992\n",
      "iter 55001/1000000  loss         0.159872  avg_L1_norm_grad         0.000002  w[0]    0.102 bias    3.992\n",
      "iter 55100/1000000  loss         0.159871  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.992\n",
      "iter 55101/1000000  loss         0.159870  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.992\n",
      "iter 55200/1000000  loss         0.159869  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.993\n",
      "iter 55201/1000000  loss         0.159869  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.993\n",
      "iter 55300/1000000  loss         0.159868  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.993\n",
      "iter 55301/1000000  loss         0.159868  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.993\n",
      "iter 55400/1000000  loss         0.159866  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.993\n",
      "iter 55401/1000000  loss         0.159866  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.993\n",
      "iter 55500/1000000  loss         0.159865  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.994\n",
      "iter 55501/1000000  loss         0.159865  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.994\n",
      "iter 55600/1000000  loss         0.159863  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.994\n",
      "iter 55601/1000000  loss         0.159863  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.994\n",
      "iter 55700/1000000  loss         0.159862  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.994\n",
      "iter 55701/1000000  loss         0.159862  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.994\n",
      "iter 55800/1000000  loss         0.159860  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.995\n",
      "iter 55801/1000000  loss         0.159860  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.995\n",
      "iter 55900/1000000  loss         0.159859  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.995\n",
      "iter 55901/1000000  loss         0.159859  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.995\n",
      "iter 56000/1000000  loss         0.159857  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.995\n",
      "iter 56001/1000000  loss         0.159857  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.995\n",
      "iter 56100/1000000  loss         0.159856  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.996\n",
      "iter 56101/1000000  loss         0.159856  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.996\n",
      "iter 56200/1000000  loss         0.159855  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.996\n",
      "iter 56201/1000000  loss         0.159855  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.996\n",
      "iter 56300/1000000  loss         0.159853  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.996\n",
      "iter 56301/1000000  loss         0.159853  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.996\n",
      "iter 56400/1000000  loss         0.159852  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.997\n",
      "iter 56401/1000000  loss         0.159852  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.997\n",
      "iter 56500/1000000  loss         0.159850  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.997\n",
      "iter 56501/1000000  loss         0.159850  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.997\n",
      "iter 56600/1000000  loss         0.159849  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.997\n",
      "iter 56601/1000000  loss         0.159849  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.997\n",
      "iter 56700/1000000  loss         0.159848  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.998\n",
      "iter 56701/1000000  loss         0.159848  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.998\n",
      "iter 56800/1000000  loss         0.159846  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.998\n",
      "iter 56801/1000000  loss         0.159846  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.998\n",
      "iter 56900/1000000  loss         0.159845  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.998\n",
      "iter 56901/1000000  loss         0.159845  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.998\n",
      "iter 57000/1000000  loss         0.159843  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.999\n",
      "iter 57001/1000000  loss         0.159843  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.999\n",
      "iter 57100/1000000  loss         0.159842  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.999\n",
      "iter 57101/1000000  loss         0.159842  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.999\n",
      "iter 57200/1000000  loss         0.159841  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.999\n",
      "iter 57201/1000000  loss         0.159841  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    3.999\n",
      "iter 57300/1000000  loss         0.159839  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.000\n",
      "iter 57301/1000000  loss         0.159839  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.000\n",
      "iter 57400/1000000  loss         0.159838  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.000\n",
      "iter 57401/1000000  loss         0.159838  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.000\n",
      "iter 57500/1000000  loss         0.159837  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.000\n",
      "iter 57501/1000000  loss         0.159837  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.000\n",
      "iter 57600/1000000  loss         0.159835  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 57601/1000000  loss         0.159835  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 57700/1000000  loss         0.159834  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 57701/1000000  loss         0.159834  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 57800/1000000  loss         0.159833  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 57801/1000000  loss         0.159833  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 57900/1000000  loss         0.159831  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 57901/1000000  loss         0.159831  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.001\n",
      "iter 58000/1000000  loss         0.159830  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.002\n",
      "iter 58001/1000000  loss         0.159830  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.002\n",
      "iter 58100/1000000  loss         0.159829  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.002\n",
      "iter 58101/1000000  loss         0.159829  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.002\n",
      "iter 58200/1000000  loss         0.159828  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.002\n",
      "iter 58201/1000000  loss         0.159828  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.002\n",
      "iter 58300/1000000  loss         0.159826  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.003\n",
      "iter 58301/1000000  loss         0.159826  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.003\n",
      "iter 58400/1000000  loss         0.159825  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.003\n",
      "iter 58401/1000000  loss         0.159825  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.003\n",
      "iter 58500/1000000  loss         0.159824  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.003\n",
      "iter 58501/1000000  loss         0.159824  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.003\n",
      "iter 58600/1000000  loss         0.159822  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.004\n",
      "iter 58601/1000000  loss         0.159822  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.004\n",
      "iter 58700/1000000  loss         0.159821  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.004\n",
      "iter 58701/1000000  loss         0.159821  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.004\n",
      "iter 58800/1000000  loss         0.159820  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.004\n",
      "iter 58801/1000000  loss         0.159820  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.004\n",
      "iter 58900/1000000  loss         0.159819  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 58901/1000000  loss         0.159819  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59000/1000000  loss         0.159817  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59001/1000000  loss         0.159817  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59100/1000000  loss         0.159816  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59101/1000000  loss         0.159816  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59200/1000000  loss         0.159815  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59201/1000000  loss         0.159815  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.005\n",
      "iter 59300/1000000  loss         0.159814  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.006\n",
      "iter 59301/1000000  loss         0.159814  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.006\n",
      "iter 59400/1000000  loss         0.159813  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.006\n",
      "iter 59401/1000000  loss         0.159812  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.006\n",
      "iter 59500/1000000  loss         0.159811  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.006\n",
      "iter 59501/1000000  loss         0.159811  avg_L1_norm_grad         0.000002  w[0]    0.103 bias    4.006\n",
      "iter 59600/1000000  loss         0.159810  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.007\n",
      "iter 59601/1000000  loss         0.159810  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.007\n",
      "iter 59700/1000000  loss         0.159809  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.007\n",
      "iter 59701/1000000  loss         0.159809  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.007\n",
      "iter 59800/1000000  loss         0.159808  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.007\n",
      "iter 59801/1000000  loss         0.159808  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.007\n",
      "iter 59900/1000000  loss         0.159806  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 59901/1000000  loss         0.159806  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60000/1000000  loss         0.159805  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60001/1000000  loss         0.159805  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60100/1000000  loss         0.159804  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60101/1000000  loss         0.159804  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60200/1000000  loss         0.159803  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60201/1000000  loss         0.159803  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.008\n",
      "iter 60300/1000000  loss         0.159802  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.009\n",
      "iter 60301/1000000  loss         0.159802  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.009\n",
      "iter 60400/1000000  loss         0.159801  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.009\n",
      "iter 60401/1000000  loss         0.159801  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.009\n",
      "iter 60500/1000000  loss         0.159799  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.009\n",
      "iter 60501/1000000  loss         0.159799  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.009\n",
      "iter 60600/1000000  loss         0.159798  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60601/1000000  loss         0.159798  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60700/1000000  loss         0.159797  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60701/1000000  loss         0.159797  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60800/1000000  loss         0.159796  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60801/1000000  loss         0.159796  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60900/1000000  loss         0.159795  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 60901/1000000  loss         0.159795  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.010\n",
      "iter 61000/1000000  loss         0.159794  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.011\n",
      "iter 61001/1000000  loss         0.159794  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.011\n",
      "iter 61100/1000000  loss         0.159793  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.011\n",
      "iter 61101/1000000  loss         0.159793  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.011\n",
      "iter 61200/1000000  loss         0.159792  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.011\n",
      "iter 61201/1000000  loss         0.159791  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.011\n",
      "iter 61300/1000000  loss         0.159790  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61301/1000000  loss         0.159790  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61400/1000000  loss         0.159789  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61401/1000000  loss         0.159789  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61500/1000000  loss         0.159788  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61501/1000000  loss         0.159788  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 61600/1000000  loss         0.159787  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61601/1000000  loss         0.159787  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.012\n",
      "iter 61700/1000000  loss         0.159786  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.013\n",
      "iter 61701/1000000  loss         0.159786  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.013\n",
      "iter 61800/1000000  loss         0.159785  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.013\n",
      "iter 61801/1000000  loss         0.159785  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.013\n",
      "iter 61900/1000000  loss         0.159784  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.013\n",
      "iter 61901/1000000  loss         0.159784  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.013\n",
      "iter 62000/1000000  loss         0.159783  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62001/1000000  loss         0.159783  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62100/1000000  loss         0.159782  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62101/1000000  loss         0.159782  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62200/1000000  loss         0.159781  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62201/1000000  loss         0.159781  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62300/1000000  loss         0.159780  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62301/1000000  loss         0.159780  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.014\n",
      "iter 62400/1000000  loss         0.159778  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62401/1000000  loss         0.159778  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62500/1000000  loss         0.159777  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62501/1000000  loss         0.159777  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62600/1000000  loss         0.159776  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62601/1000000  loss         0.159776  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62700/1000000  loss         0.159775  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62701/1000000  loss         0.159775  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.015\n",
      "iter 62800/1000000  loss         0.159774  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.016\n",
      "iter 62801/1000000  loss         0.159774  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.016\n",
      "iter 62900/1000000  loss         0.159773  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.016\n",
      "iter 62901/1000000  loss         0.159773  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.016\n",
      "iter 63000/1000000  loss         0.159772  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.016\n",
      "iter 63001/1000000  loss         0.159772  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.016\n",
      "iter 63100/1000000  loss         0.159771  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63101/1000000  loss         0.159771  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63200/1000000  loss         0.159770  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63201/1000000  loss         0.159770  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63300/1000000  loss         0.159769  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63301/1000000  loss         0.159769  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63400/1000000  loss         0.159768  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63401/1000000  loss         0.159768  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.017\n",
      "iter 63500/1000000  loss         0.159767  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63501/1000000  loss         0.159767  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63600/1000000  loss         0.159766  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63601/1000000  loss         0.159766  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63700/1000000  loss         0.159765  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63701/1000000  loss         0.159765  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63800/1000000  loss         0.159764  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63801/1000000  loss         0.159764  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.018\n",
      "iter 63900/1000000  loss         0.159763  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 63901/1000000  loss         0.159763  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64000/1000000  loss         0.159762  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64001/1000000  loss         0.159762  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64100/1000000  loss         0.159761  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64101/1000000  loss         0.159761  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64200/1000000  loss         0.159760  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64201/1000000  loss         0.159760  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.019\n",
      "iter 64300/1000000  loss         0.159759  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.020\n",
      "iter 64301/1000000  loss         0.159759  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.020\n",
      "iter 64400/1000000  loss         0.159758  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.020\n",
      "iter 64401/1000000  loss         0.159758  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.020\n",
      "iter 64500/1000000  loss         0.159757  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.020\n",
      "iter 64501/1000000  loss         0.159757  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.020\n",
      "iter 64600/1000000  loss         0.159756  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64601/1000000  loss         0.159756  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64700/1000000  loss         0.159755  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64701/1000000  loss         0.159755  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64800/1000000  loss         0.159754  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64801/1000000  loss         0.159754  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64900/1000000  loss         0.159754  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 64901/1000000  loss         0.159754  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.021\n",
      "iter 65000/1000000  loss         0.159753  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.022\n",
      "iter 65001/1000000  loss         0.159753  avg_L1_norm_grad         0.000002  w[0]    0.104 bias    4.022\n",
      "iter 65100/1000000  loss         0.159752  avg_L1_norm_grad         0.000002  w[0]    0.105 bias    4.022\n",
      "iter 65101/1000000  loss         0.159752  avg_L1_norm_grad         0.000002  w[0]    0.105 bias    4.022\n",
      "iter 65200/1000000  loss         0.159751  avg_L1_norm_grad         0.000002  w[0]    0.105 bias    4.022\n",
      "iter 65201/1000000  loss         0.159751  avg_L1_norm_grad         0.000002  w[0]    0.105 bias    4.022\n",
      "iter 65300/1000000  loss         0.159750  avg_L1_norm_grad         0.000002  w[0]    0.105 bias    4.022\n",
      "iter 65301/1000000  loss         0.159750  avg_L1_norm_grad         0.000002  w[0]    0.105 bias    4.022\n",
      "iter 65400/1000000  loss         0.159749  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65401/1000000  loss         0.159749  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 65500/1000000  loss         0.159748  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65501/1000000  loss         0.159748  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65600/1000000  loss         0.159747  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65601/1000000  loss         0.159747  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65700/1000000  loss         0.159746  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65701/1000000  loss         0.159746  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.023\n",
      "iter 65800/1000000  loss         0.159745  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 65801/1000000  loss         0.159745  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 65900/1000000  loss         0.159744  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 65901/1000000  loss         0.159744  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 66000/1000000  loss         0.159743  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 66001/1000000  loss         0.159743  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 66100/1000000  loss         0.159743  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 66101/1000000  loss         0.159743  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.024\n",
      "iter 66200/1000000  loss         0.159742  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66201/1000000  loss         0.159742  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66300/1000000  loss         0.159741  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66301/1000000  loss         0.159741  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66400/1000000  loss         0.159740  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66401/1000000  loss         0.159740  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66500/1000000  loss         0.159739  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66501/1000000  loss         0.159739  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.025\n",
      "iter 66600/1000000  loss         0.159738  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66601/1000000  loss         0.159738  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66700/1000000  loss         0.159737  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66701/1000000  loss         0.159737  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66800/1000000  loss         0.159736  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66801/1000000  loss         0.159736  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66900/1000000  loss         0.159736  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 66901/1000000  loss         0.159736  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 67000/1000000  loss         0.159735  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 67001/1000000  loss         0.159735  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.026\n",
      "iter 67100/1000000  loss         0.159734  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67101/1000000  loss         0.159734  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67200/1000000  loss         0.159733  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67201/1000000  loss         0.159733  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67300/1000000  loss         0.159732  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67301/1000000  loss         0.159732  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67400/1000000  loss         0.159731  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67401/1000000  loss         0.159731  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.027\n",
      "iter 67500/1000000  loss         0.159730  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67501/1000000  loss         0.159730  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67600/1000000  loss         0.159730  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67601/1000000  loss         0.159730  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67700/1000000  loss         0.159729  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67701/1000000  loss         0.159729  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67800/1000000  loss         0.159728  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67801/1000000  loss         0.159728  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.028\n",
      "iter 67900/1000000  loss         0.159727  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 67901/1000000  loss         0.159727  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68000/1000000  loss         0.159726  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68001/1000000  loss         0.159726  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68100/1000000  loss         0.159726  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68101/1000000  loss         0.159726  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68200/1000000  loss         0.159725  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68201/1000000  loss         0.159725  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.029\n",
      "iter 68300/1000000  loss         0.159724  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68301/1000000  loss         0.159724  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68400/1000000  loss         0.159723  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68401/1000000  loss         0.159723  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68500/1000000  loss         0.159722  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68501/1000000  loss         0.159722  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68600/1000000  loss         0.159722  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68601/1000000  loss         0.159722  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.030\n",
      "iter 68700/1000000  loss         0.159721  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 68701/1000000  loss         0.159721  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 68800/1000000  loss         0.159720  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 68801/1000000  loss         0.159720  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 68900/1000000  loss         0.159719  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 68901/1000000  loss         0.159719  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 69000/1000000  loss         0.159718  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 69001/1000000  loss         0.159718  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 69100/1000000  loss         0.159718  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 69101/1000000  loss         0.159718  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.031\n",
      "iter 69200/1000000  loss         0.159717  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69201/1000000  loss         0.159717  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69300/1000000  loss         0.159716  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69301/1000000  loss         0.159716  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69400/1000000  loss         0.159715  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69401/1000000  loss         0.159715  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69500/1000000  loss         0.159715  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69501/1000000  loss         0.159715  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.032\n",
      "iter 69600/1000000  loss         0.159714  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69601/1000000  loss         0.159714  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69700/1000000  loss         0.159713  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69701/1000000  loss         0.159713  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69800/1000000  loss         0.159712  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69801/1000000  loss         0.159712  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69900/1000000  loss         0.159712  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 69901/1000000  loss         0.159712  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 70000/1000000  loss         0.159711  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 70001/1000000  loss         0.159711  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.033\n",
      "iter 70100/1000000  loss         0.159710  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70101/1000000  loss         0.159710  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70200/1000000  loss         0.159709  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70201/1000000  loss         0.159709  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70300/1000000  loss         0.159709  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70301/1000000  loss         0.159709  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70400/1000000  loss         0.159708  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70401/1000000  loss         0.159708  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.034\n",
      "iter 70500/1000000  loss         0.159707  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70501/1000000  loss         0.159707  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70600/1000000  loss         0.159706  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70601/1000000  loss         0.159706  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70700/1000000  loss         0.159706  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70701/1000000  loss         0.159706  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70800/1000000  loss         0.159705  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70801/1000000  loss         0.159705  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70900/1000000  loss         0.159704  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 70901/1000000  loss         0.159704  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.035\n",
      "iter 71000/1000000  loss         0.159704  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71001/1000000  loss         0.159704  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71100/1000000  loss         0.159703  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71101/1000000  loss         0.159703  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71200/1000000  loss         0.159702  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71201/1000000  loss         0.159702  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71300/1000000  loss         0.159701  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71301/1000000  loss         0.159701  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.036\n",
      "iter 71400/1000000  loss         0.159701  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71401/1000000  loss         0.159701  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71500/1000000  loss         0.159700  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71501/1000000  loss         0.159700  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71600/1000000  loss         0.159699  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71601/1000000  loss         0.159699  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71700/1000000  loss         0.159699  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71701/1000000  loss         0.159699  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71800/1000000  loss         0.159698  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71801/1000000  loss         0.159698  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.037\n",
      "iter 71900/1000000  loss         0.159697  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 71901/1000000  loss         0.159697  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72000/1000000  loss         0.159697  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72001/1000000  loss         0.159697  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72100/1000000  loss         0.159696  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72101/1000000  loss         0.159696  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72200/1000000  loss         0.159695  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72201/1000000  loss         0.159695  avg_L1_norm_grad         0.000001  w[0]    0.105 bias    4.038\n",
      "iter 72300/1000000  loss         0.159695  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.038\n",
      "iter 72301/1000000  loss         0.159695  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.038\n",
      "iter 72400/1000000  loss         0.159694  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72401/1000000  loss         0.159694  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72500/1000000  loss         0.159693  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72501/1000000  loss         0.159693  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72600/1000000  loss         0.159693  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72601/1000000  loss         0.159693  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72700/1000000  loss         0.159692  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72701/1000000  loss         0.159692  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72800/1000000  loss         0.159691  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72801/1000000  loss         0.159691  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.039\n",
      "iter 72900/1000000  loss         0.159691  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 72901/1000000  loss         0.159691  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73000/1000000  loss         0.159690  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73001/1000000  loss         0.159690  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73100/1000000  loss         0.159689  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73101/1000000  loss         0.159689  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73200/1000000  loss         0.159689  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73201/1000000  loss         0.159689  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 73300/1000000  loss         0.159688  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73301/1000000  loss         0.159688  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.040\n",
      "iter 73400/1000000  loss         0.159687  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73401/1000000  loss         0.159687  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73500/1000000  loss         0.159687  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73501/1000000  loss         0.159687  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73600/1000000  loss         0.159686  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73601/1000000  loss         0.159686  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73700/1000000  loss         0.159686  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73701/1000000  loss         0.159686  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73800/1000000  loss         0.159685  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73801/1000000  loss         0.159685  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.041\n",
      "iter 73900/1000000  loss         0.159684  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 73901/1000000  loss         0.159684  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74000/1000000  loss         0.159684  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74001/1000000  loss         0.159684  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74100/1000000  loss         0.159683  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74101/1000000  loss         0.159683  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74200/1000000  loss         0.159682  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74201/1000000  loss         0.159682  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74300/1000000  loss         0.159682  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74301/1000000  loss         0.159682  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.042\n",
      "iter 74400/1000000  loss         0.159681  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74401/1000000  loss         0.159681  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74500/1000000  loss         0.159681  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74501/1000000  loss         0.159681  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74600/1000000  loss         0.159680  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74601/1000000  loss         0.159680  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74700/1000000  loss         0.159679  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74701/1000000  loss         0.159679  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74800/1000000  loss         0.159679  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74801/1000000  loss         0.159679  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.043\n",
      "iter 74900/1000000  loss         0.159678  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 74901/1000000  loss         0.159678  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75000/1000000  loss         0.159678  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75001/1000000  loss         0.159678  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75100/1000000  loss         0.159677  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75101/1000000  loss         0.159677  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75200/1000000  loss         0.159676  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75201/1000000  loss         0.159676  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75300/1000000  loss         0.159676  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75301/1000000  loss         0.159676  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.044\n",
      "iter 75400/1000000  loss         0.159675  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75401/1000000  loss         0.159675  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75500/1000000  loss         0.159675  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75501/1000000  loss         0.159675  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75600/1000000  loss         0.159674  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75601/1000000  loss         0.159674  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75700/1000000  loss         0.159674  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75701/1000000  loss         0.159674  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75800/1000000  loss         0.159673  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75801/1000000  loss         0.159673  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.045\n",
      "iter 75900/1000000  loss         0.159672  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 75901/1000000  loss         0.159672  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76000/1000000  loss         0.159672  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76001/1000000  loss         0.159672  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76100/1000000  loss         0.159671  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76101/1000000  loss         0.159671  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76200/1000000  loss         0.159671  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76201/1000000  loss         0.159671  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76300/1000000  loss         0.159670  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76301/1000000  loss         0.159670  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.046\n",
      "iter 76400/1000000  loss         0.159670  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76401/1000000  loss         0.159670  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76500/1000000  loss         0.159669  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76501/1000000  loss         0.159669  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76600/1000000  loss         0.159669  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76601/1000000  loss         0.159669  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76700/1000000  loss         0.159668  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76701/1000000  loss         0.159668  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76800/1000000  loss         0.159667  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76801/1000000  loss         0.159667  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76900/1000000  loss         0.159667  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 76901/1000000  loss         0.159667  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.047\n",
      "iter 77000/1000000  loss         0.159666  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77001/1000000  loss         0.159666  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77100/1000000  loss         0.159666  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77101/1000000  loss         0.159666  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 77200/1000000  loss         0.159665  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77201/1000000  loss         0.159665  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77300/1000000  loss         0.159665  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77301/1000000  loss         0.159665  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77400/1000000  loss         0.159664  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77401/1000000  loss         0.159664  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.048\n",
      "iter 77500/1000000  loss         0.159664  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77501/1000000  loss         0.159664  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77600/1000000  loss         0.159663  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77601/1000000  loss         0.159663  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77700/1000000  loss         0.159663  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77701/1000000  loss         0.159663  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77800/1000000  loss         0.159662  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77801/1000000  loss         0.159662  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77900/1000000  loss         0.159662  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 77901/1000000  loss         0.159662  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.049\n",
      "iter 78000/1000000  loss         0.159661  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78001/1000000  loss         0.159661  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78100/1000000  loss         0.159661  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78101/1000000  loss         0.159661  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78200/1000000  loss         0.159660  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78201/1000000  loss         0.159660  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78300/1000000  loss         0.159660  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78301/1000000  loss         0.159660  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78400/1000000  loss         0.159659  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78401/1000000  loss         0.159659  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78500/1000000  loss         0.159659  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78501/1000000  loss         0.159659  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.050\n",
      "iter 78600/1000000  loss         0.159658  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78601/1000000  loss         0.159658  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78700/1000000  loss         0.159658  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78701/1000000  loss         0.159658  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78800/1000000  loss         0.159657  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78801/1000000  loss         0.159657  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78900/1000000  loss         0.159657  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 78901/1000000  loss         0.159657  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 79000/1000000  loss         0.159656  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 79001/1000000  loss         0.159656  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 79100/1000000  loss         0.159656  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 79101/1000000  loss         0.159656  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.051\n",
      "iter 79200/1000000  loss         0.159655  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79201/1000000  loss         0.159655  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79300/1000000  loss         0.159655  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79301/1000000  loss         0.159655  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79400/1000000  loss         0.159654  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79401/1000000  loss         0.159654  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79500/1000000  loss         0.159654  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79501/1000000  loss         0.159654  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79600/1000000  loss         0.159653  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79601/1000000  loss         0.159653  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.052\n",
      "iter 79700/1000000  loss         0.159653  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 79701/1000000  loss         0.159653  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 79800/1000000  loss         0.159652  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 79801/1000000  loss         0.159652  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 79900/1000000  loss         0.159652  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 79901/1000000  loss         0.159652  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80000/1000000  loss         0.159651  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80001/1000000  loss         0.159651  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80100/1000000  loss         0.159651  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80101/1000000  loss         0.159651  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80200/1000000  loss         0.159650  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80201/1000000  loss         0.159650  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.053\n",
      "iter 80300/1000000  loss         0.159650  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80301/1000000  loss         0.159650  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80400/1000000  loss         0.159649  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80401/1000000  loss         0.159649  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80500/1000000  loss         0.159649  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80501/1000000  loss         0.159649  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80600/1000000  loss         0.159649  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80601/1000000  loss         0.159649  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80700/1000000  loss         0.159648  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80701/1000000  loss         0.159648  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80800/1000000  loss         0.159648  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80801/1000000  loss         0.159648  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.054\n",
      "iter 80900/1000000  loss         0.159647  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 80901/1000000  loss         0.159647  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81000/1000000  loss         0.159647  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81001/1000000  loss         0.159647  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 81100/1000000  loss         0.159646  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81101/1000000  loss         0.159646  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81200/1000000  loss         0.159646  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81201/1000000  loss         0.159646  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81300/1000000  loss         0.159645  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81301/1000000  loss         0.159645  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81400/1000000  loss         0.159645  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81401/1000000  loss         0.159645  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.055\n",
      "iter 81500/1000000  loss         0.159645  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81501/1000000  loss         0.159645  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81600/1000000  loss         0.159644  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81601/1000000  loss         0.159644  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81700/1000000  loss         0.159644  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81701/1000000  loss         0.159644  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81800/1000000  loss         0.159643  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81801/1000000  loss         0.159643  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81900/1000000  loss         0.159643  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 81901/1000000  loss         0.159643  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 82000/1000000  loss         0.159642  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 82001/1000000  loss         0.159642  avg_L1_norm_grad         0.000001  w[0]    0.106 bias    4.056\n",
      "iter 82100/1000000  loss         0.159642  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82101/1000000  loss         0.159642  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82200/1000000  loss         0.159642  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82201/1000000  loss         0.159642  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82300/1000000  loss         0.159641  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82301/1000000  loss         0.159641  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82400/1000000  loss         0.159641  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82401/1000000  loss         0.159641  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82500/1000000  loss         0.159640  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82501/1000000  loss         0.159640  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82600/1000000  loss         0.159640  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82601/1000000  loss         0.159640  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.057\n",
      "iter 82700/1000000  loss         0.159639  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 82701/1000000  loss         0.159639  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 82800/1000000  loss         0.159639  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 82801/1000000  loss         0.159639  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 82900/1000000  loss         0.159639  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 82901/1000000  loss         0.159639  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83000/1000000  loss         0.159638  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83001/1000000  loss         0.159638  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83100/1000000  loss         0.159638  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83101/1000000  loss         0.159638  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83200/1000000  loss         0.159637  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83201/1000000  loss         0.159637  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83300/1000000  loss         0.159637  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83301/1000000  loss         0.159637  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.058\n",
      "iter 83400/1000000  loss         0.159637  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83401/1000000  loss         0.159637  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83500/1000000  loss         0.159636  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83501/1000000  loss         0.159636  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83600/1000000  loss         0.159636  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83601/1000000  loss         0.159636  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83700/1000000  loss         0.159635  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83701/1000000  loss         0.159635  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83800/1000000  loss         0.159635  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83801/1000000  loss         0.159635  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83900/1000000  loss         0.159635  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 83901/1000000  loss         0.159635  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.059\n",
      "iter 84000/1000000  loss         0.159634  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84001/1000000  loss         0.159634  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84100/1000000  loss         0.159634  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84101/1000000  loss         0.159634  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84200/1000000  loss         0.159633  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84201/1000000  loss         0.159633  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84300/1000000  loss         0.159633  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84301/1000000  loss         0.159633  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84400/1000000  loss         0.159633  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84401/1000000  loss         0.159633  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84500/1000000  loss         0.159632  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84501/1000000  loss         0.159632  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.060\n",
      "iter 84600/1000000  loss         0.159632  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84601/1000000  loss         0.159632  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84700/1000000  loss         0.159632  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84701/1000000  loss         0.159632  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84800/1000000  loss         0.159631  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84801/1000000  loss         0.159631  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84900/1000000  loss         0.159631  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 84901/1000000  loss         0.159631  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 85000/1000000  loss         0.159630  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 85001/1000000  loss         0.159630  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 85100/1000000  loss         0.159630  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 85101/1000000  loss         0.159630  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 85200/1000000  loss         0.159630  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 85201/1000000  loss         0.159630  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.061\n",
      "iter 85300/1000000  loss         0.159629  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85301/1000000  loss         0.159629  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85400/1000000  loss         0.159629  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85401/1000000  loss         0.159629  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85500/1000000  loss         0.159629  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85501/1000000  loss         0.159629  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85600/1000000  loss         0.159628  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85601/1000000  loss         0.159628  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85700/1000000  loss         0.159628  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85701/1000000  loss         0.159628  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85800/1000000  loss         0.159627  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85801/1000000  loss         0.159627  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85900/1000000  loss         0.159627  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 85901/1000000  loss         0.159627  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.062\n",
      "iter 86000/1000000  loss         0.159627  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86001/1000000  loss         0.159627  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86100/1000000  loss         0.159626  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86101/1000000  loss         0.159626  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86200/1000000  loss         0.159626  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86201/1000000  loss         0.159626  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86300/1000000  loss         0.159626  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86301/1000000  loss         0.159626  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86400/1000000  loss         0.159625  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86401/1000000  loss         0.159625  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86500/1000000  loss         0.159625  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86501/1000000  loss         0.159625  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.063\n",
      "iter 86600/1000000  loss         0.159625  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86601/1000000  loss         0.159625  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86700/1000000  loss         0.159624  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86701/1000000  loss         0.159624  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86800/1000000  loss         0.159624  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86801/1000000  loss         0.159624  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86900/1000000  loss         0.159624  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 86901/1000000  loss         0.159624  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87000/1000000  loss         0.159623  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87001/1000000  loss         0.159623  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87100/1000000  loss         0.159623  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87101/1000000  loss         0.159623  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87200/1000000  loss         0.159623  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87201/1000000  loss         0.159623  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.064\n",
      "iter 87300/1000000  loss         0.159622  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87301/1000000  loss         0.159622  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87400/1000000  loss         0.159622  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87401/1000000  loss         0.159622  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87500/1000000  loss         0.159622  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87501/1000000  loss         0.159622  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87600/1000000  loss         0.159621  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87601/1000000  loss         0.159621  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87700/1000000  loss         0.159621  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87701/1000000  loss         0.159621  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87800/1000000  loss         0.159621  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87801/1000000  loss         0.159621  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87900/1000000  loss         0.159620  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 87901/1000000  loss         0.159620  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.065\n",
      "iter 88000/1000000  loss         0.159620  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88001/1000000  loss         0.159620  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88100/1000000  loss         0.159620  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88101/1000000  loss         0.159620  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88200/1000000  loss         0.159619  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88201/1000000  loss         0.159619  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88300/1000000  loss         0.159619  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88301/1000000  loss         0.159619  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88400/1000000  loss         0.159619  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88401/1000000  loss         0.159619  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88500/1000000  loss         0.159618  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88501/1000000  loss         0.159618  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88600/1000000  loss         0.159618  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88601/1000000  loss         0.159618  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.066\n",
      "iter 88700/1000000  loss         0.159618  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 88701/1000000  loss         0.159618  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 88800/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 88801/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 88900/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 88901/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89000/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89001/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89100/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89101/1000000  loss         0.159617  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89200/1000000  loss         0.159616  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89201/1000000  loss         0.159616  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89300/1000000  loss         0.159616  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89301/1000000  loss         0.159616  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89400/1000000  loss         0.159616  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89401/1000000  loss         0.159616  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.067\n",
      "iter 89500/1000000  loss         0.159615  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89501/1000000  loss         0.159615  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89600/1000000  loss         0.159615  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89601/1000000  loss         0.159615  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89700/1000000  loss         0.159615  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89701/1000000  loss         0.159615  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89800/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89801/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89900/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 89901/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 90000/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 90001/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 90100/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 90101/1000000  loss         0.159614  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.068\n",
      "iter 90200/1000000  loss         0.159613  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90201/1000000  loss         0.159613  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90300/1000000  loss         0.159613  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90301/1000000  loss         0.159613  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90400/1000000  loss         0.159613  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90401/1000000  loss         0.159613  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90500/1000000  loss         0.159612  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90501/1000000  loss         0.159612  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90600/1000000  loss         0.159612  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90601/1000000  loss         0.159612  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90700/1000000  loss         0.159612  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90701/1000000  loss         0.159612  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90800/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90801/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.069\n",
      "iter 90900/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 90901/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91000/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91001/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91100/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91101/1000000  loss         0.159611  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91200/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91201/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91300/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91301/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91400/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91401/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91500/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91501/1000000  loss         0.159610  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91600/1000000  loss         0.159609  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91601/1000000  loss         0.159609  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.070\n",
      "iter 91700/1000000  loss         0.159609  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 91701/1000000  loss         0.159609  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 91800/1000000  loss         0.159609  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 91801/1000000  loss         0.159609  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 91900/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 91901/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92000/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92001/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92100/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92101/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92200/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92201/1000000  loss         0.159608  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92300/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92301/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92400/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92401/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.071\n",
      "iter 92500/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92501/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92600/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92601/1000000  loss         0.159607  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92700/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92701/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 92800/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92801/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92900/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 92901/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93000/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93001/1000000  loss         0.159606  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93100/1000000  loss         0.159605  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93101/1000000  loss         0.159605  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93200/1000000  loss         0.159605  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93201/1000000  loss         0.159605  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.072\n",
      "iter 93300/1000000  loss         0.159605  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93301/1000000  loss         0.159605  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93400/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93401/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93500/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93501/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93600/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93601/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93700/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93701/1000000  loss         0.159604  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93800/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93801/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93900/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 93901/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 94000/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 94001/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.073\n",
      "iter 94100/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94101/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94200/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94201/1000000  loss         0.159603  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94300/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94301/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94400/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94401/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94500/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94501/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94600/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94601/1000000  loss         0.159602  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94700/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94701/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94800/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94801/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.074\n",
      "iter 94900/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 94901/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95000/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95001/1000000  loss         0.159601  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95100/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95101/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95200/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95201/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95300/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95301/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95400/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95401/1000000  loss         0.159600  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95500/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95501/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95600/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95601/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95700/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95701/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.075\n",
      "iter 95800/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 95801/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 95900/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 95901/1000000  loss         0.159599  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96000/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96001/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96100/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96101/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96200/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96201/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96300/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96301/1000000  loss         0.159598  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96400/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96401/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96500/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96501/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.107 bias    4.076\n",
      "iter 96600/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 96601/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 96700/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 96701/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 96800/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 96801/1000000  loss         0.159597  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 96900/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 96901/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97000/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97001/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97100/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97101/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97200/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97201/1000000  loss         0.159596  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97300/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97301/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97400/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97401/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.077\n",
      "iter 97500/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97501/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97600/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97601/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97700/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97701/1000000  loss         0.159595  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97800/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97801/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97900/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 97901/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98000/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98001/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98100/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98101/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98200/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98201/1000000  loss         0.159594  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98300/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98301/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.078\n",
      "iter 98400/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98401/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98500/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98501/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98600/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98601/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98700/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98701/1000000  loss         0.159593  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98800/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98801/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98900/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 98901/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99000/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99001/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99100/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99101/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99200/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99201/1000000  loss         0.159592  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.079\n",
      "iter 99300/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99301/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99400/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99401/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99500/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99501/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99600/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99601/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99700/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99701/1000000  loss         0.159591  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99800/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99801/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99900/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 99901/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 100000/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 100001/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 100100/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 100101/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.080\n",
      "iter 100200/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100201/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100300/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100301/1000000  loss         0.159590  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100400/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100401/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100500/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100501/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100600/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100601/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100700/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100701/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100800/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100801/1000000  loss         0.159589  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100900/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 100901/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 101000/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 101001/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 101100/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 101101/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.081\n",
      "iter 101200/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101201/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101300/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101301/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101400/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101401/1000000  loss         0.159588  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101500/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101501/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101600/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101601/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101700/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101701/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101800/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101801/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101900/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 101901/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 102000/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 102001/1000000  loss         0.159587  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 102100/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 102101/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.082\n",
      "iter 102200/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102201/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102300/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102301/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102400/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102401/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102500/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102501/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102600/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102601/1000000  loss         0.159586  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102700/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102701/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102800/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102801/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102900/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 102901/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 103000/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 103001/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 103100/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 103101/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.083\n",
      "iter 103200/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103201/1000000  loss         0.159585  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103300/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103301/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103400/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103401/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103500/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103501/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103600/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103601/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103700/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103701/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103800/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103801/1000000  loss         0.159584  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103900/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 103901/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 104000/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 104001/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 104100/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 104101/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.084\n",
      "iter 104200/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104201/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104300/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104301/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104400/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104401/1000000  loss         0.159583  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 104500/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104501/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104600/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104601/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104700/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104701/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104800/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104801/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104900/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 104901/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 105000/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 105001/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 105100/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 105101/1000000  loss         0.159582  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.085\n",
      "iter 105200/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105201/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105300/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105301/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105400/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105401/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105500/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105501/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105600/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105601/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105700/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105701/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105800/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105801/1000000  loss         0.159581  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105900/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 105901/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106000/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106001/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106100/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106101/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106200/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106201/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.086\n",
      "iter 106300/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106301/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106400/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106401/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106500/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106501/1000000  loss         0.159580  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106600/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106601/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106700/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106701/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106800/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106801/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106900/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 106901/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107000/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107001/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107100/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107101/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107200/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107201/1000000  loss         0.159579  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107300/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107301/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.087\n",
      "iter 107400/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107401/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107500/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107501/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107600/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107601/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107700/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107701/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107800/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107801/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107900/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 107901/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108000/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108001/1000000  loss         0.159578  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108100/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108101/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108200/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108201/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108300/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108301/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 108400/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108401/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.088\n",
      "iter 108500/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108501/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108600/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108601/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108700/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108701/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108800/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108801/1000000  loss         0.159577  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108900/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 108901/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109000/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109001/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109100/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109101/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109200/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109201/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109300/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109301/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109400/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109401/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109500/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109501/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109600/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109601/1000000  loss         0.159576  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.089\n",
      "iter 109700/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 109701/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 109800/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 109801/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 109900/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 109901/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110000/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110001/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110100/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110101/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110200/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110201/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110300/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110301/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110400/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110401/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110500/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110501/1000000  loss         0.159575  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110600/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110601/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110700/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110701/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110800/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110801/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.090\n",
      "iter 110900/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 110901/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111000/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111001/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111100/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111101/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111200/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111201/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111300/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111301/1000000  loss         0.159574  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111400/1000000  loss         0.159573  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111401/1000000  loss         0.159573  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111500/1000000  loss         0.159573  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111501/1000000  loss         0.159573  avg_L1_norm_grad         0.000001  w[0]    0.108 bias    4.091\n",
      "iter 111600/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111601/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111700/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111701/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111800/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111801/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111900/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 111901/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 112000/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 112001/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.091\n",
      "iter 112100/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112101/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112200/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112201/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 112300/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112301/1000000  loss         0.159573  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112400/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112401/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112500/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112501/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112600/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112601/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112700/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "iter 112701/1000000  loss         0.159572  avg_L1_norm_grad         0.000000  w[0]    0.108 bias    4.092\n",
      "Done. Converged after 112776 iterations.\n"
     ]
    }
   ],
   "source": [
    "## Run LR on transformed features!\n",
    "new_lr = LRGDF(alpha=10.0, step_size=0.1)\n",
    "new_lr.fit(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Turn_On and Average!\n",
      "TurnOn Loaded\n",
      "Ave Loaded\n",
      "Initializing w_G with 787 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028528  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.918064  avg_L1_norm_grad         0.029994  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.856437  avg_L1_norm_grad         0.020531  w[0]    0.001 bias    0.019\n",
      "iter    3/1000000  loss         0.811329  avg_L1_norm_grad         0.021532  w[0]    0.002 bias    0.022\n",
      "iter    4/1000000  loss         0.775614  avg_L1_norm_grad         0.015158  w[0]    0.003 bias    0.037\n",
      "iter    5/1000000  loss         0.747649  avg_L1_norm_grad         0.015655  w[0]    0.003 bias    0.041\n",
      "iter    6/1000000  loss         0.724615  avg_L1_norm_grad         0.012310  w[0]    0.004 bias    0.053\n",
      "iter    7/1000000  loss         0.705275  avg_L1_norm_grad         0.012378  w[0]    0.005 bias    0.060\n",
      "iter    8/1000000  loss         0.688490  avg_L1_norm_grad         0.010832  w[0]    0.005 bias    0.069\n",
      "iter    9/1000000  loss         0.673598  avg_L1_norm_grad         0.010593  w[0]    0.006 bias    0.077\n",
      "iter   10/1000000  loss         0.660147  avg_L1_norm_grad         0.009839  w[0]    0.006 bias    0.085\n",
      "iter   11/1000000  loss         0.647838  avg_L1_norm_grad         0.009505  w[0]    0.007 bias    0.093\n",
      "iter   12/1000000  loss         0.636465  avg_L1_norm_grad         0.009068  w[0]    0.007 bias    0.100\n",
      "iter   13/1000000  loss         0.625880  avg_L1_norm_grad         0.008756  w[0]    0.008 bias    0.108\n",
      "iter   14/1000000  loss         0.615969  avg_L1_norm_grad         0.008442  w[0]    0.009 bias    0.115\n",
      "iter   15/1000000  loss         0.606647  avg_L1_norm_grad         0.008172  w[0]    0.009 bias    0.122\n",
      "iter   16/1000000  loss         0.597844  avg_L1_norm_grad         0.007920  w[0]    0.010 bias    0.129\n",
      "iter   17/1000000  loss         0.589504  avg_L1_norm_grad         0.007692  w[0]    0.010 bias    0.136\n",
      "iter   18/1000000  loss         0.581582  avg_L1_norm_grad         0.007480  w[0]    0.011 bias    0.143\n",
      "iter   19/1000000  loss         0.574039  avg_L1_norm_grad         0.007282  w[0]    0.011 bias    0.149\n",
      "iter  100/1000000  loss         0.359980  avg_L1_norm_grad         0.002544  w[0]    0.037 bias    0.478\n",
      "iter  101/1000000  loss         0.358999  avg_L1_norm_grad         0.002524  w[0]    0.038 bias    0.480\n",
      "iter  200/1000000  loss         0.302133  avg_L1_norm_grad         0.001480  w[0]    0.056 bias    0.682\n",
      "iter  201/1000000  loss         0.301789  avg_L1_norm_grad         0.001474  w[0]    0.056 bias    0.683\n",
      "iter  300/1000000  loss         0.277149  avg_L1_norm_grad         0.001070  w[0]    0.070 bias    0.814\n",
      "iter  301/1000000  loss         0.276967  avg_L1_norm_grad         0.001067  w[0]    0.070 bias    0.815\n",
      "iter  400/1000000  loss         0.262771  avg_L1_norm_grad         0.000852  w[0]    0.081 bias    0.911\n",
      "iter  401/1000000  loss         0.262658  avg_L1_norm_grad         0.000851  w[0]    0.081 bias    0.912\n",
      "iter  500/1000000  loss         0.253310  avg_L1_norm_grad         0.000714  w[0]    0.090 bias    0.987\n",
      "iter  501/1000000  loss         0.253231  avg_L1_norm_grad         0.000712  w[0]    0.090 bias    0.987\n",
      "iter  600/1000000  loss         0.246570  avg_L1_norm_grad         0.000617  w[0]    0.098 bias    1.049\n",
      "iter  601/1000000  loss         0.246512  avg_L1_norm_grad         0.000616  w[0]    0.098 bias    1.049\n",
      "iter  700/1000000  loss         0.241508  avg_L1_norm_grad         0.000545  w[0]    0.104 bias    1.101\n",
      "iter  701/1000000  loss         0.241464  avg_L1_norm_grad         0.000545  w[0]    0.105 bias    1.102\n",
      "iter  800/1000000  loss         0.237562  avg_L1_norm_grad         0.000488  w[0]    0.111 bias    1.146\n",
      "iter  801/1000000  loss         0.237527  avg_L1_norm_grad         0.000488  w[0]    0.111 bias    1.146\n",
      "iter  900/1000000  loss         0.234399  avg_L1_norm_grad         0.000442  w[0]    0.116 bias    1.185\n",
      "iter  901/1000000  loss         0.234370  avg_L1_norm_grad         0.000441  w[0]    0.116 bias    1.186\n",
      "iter 1000/1000000  loss         0.231808  avg_L1_norm_grad         0.000403  w[0]    0.121 bias    1.220\n",
      "iter 1001/1000000  loss         0.231784  avg_L1_norm_grad         0.000403  w[0]    0.121 bias    1.221\n",
      "iter 1100/1000000  loss         0.229650  avg_L1_norm_grad         0.000371  w[0]    0.125 bias    1.252\n",
      "iter 1101/1000000  loss         0.229630  avg_L1_norm_grad         0.000370  w[0]    0.125 bias    1.252\n",
      "iter 1200/1000000  loss         0.227827  avg_L1_norm_grad         0.000343  w[0]    0.129 bias    1.280\n",
      "iter 1201/1000000  loss         0.227810  avg_L1_norm_grad         0.000343  w[0]    0.129 bias    1.280\n",
      "iter 1300/1000000  loss         0.226271  avg_L1_norm_grad         0.000319  w[0]    0.133 bias    1.306\n",
      "iter 1301/1000000  loss         0.226256  avg_L1_norm_grad         0.000319  w[0]    0.133 bias    1.306\n",
      "iter 1400/1000000  loss         0.224929  avg_L1_norm_grad         0.000298  w[0]    0.136 bias    1.330\n",
      "iter 1401/1000000  loss         0.224917  avg_L1_norm_grad         0.000297  w[0]    0.136 bias    1.330\n",
      "iter 1500/1000000  loss         0.223764  avg_L1_norm_grad         0.000279  w[0]    0.139 bias    1.352\n",
      "iter 1501/1000000  loss         0.223753  avg_L1_norm_grad         0.000279  w[0]    0.139 bias    1.352\n",
      "iter 1600/1000000  loss         0.222745  avg_L1_norm_grad         0.000262  w[0]    0.142 bias    1.373\n",
      "iter 1601/1000000  loss         0.222735  avg_L1_norm_grad         0.000262  w[0]    0.142 bias    1.373\n",
      "iter 1700/1000000  loss         0.221848  avg_L1_norm_grad         0.000247  w[0]    0.144 bias    1.392\n",
      "iter 1701/1000000  loss         0.221840  avg_L1_norm_grad         0.000247  w[0]    0.144 bias    1.392\n",
      "iter 1800/1000000  loss         0.221056  avg_L1_norm_grad         0.000233  w[0]    0.146 bias    1.410\n",
      "iter 1801/1000000  loss         0.221048  avg_L1_norm_grad         0.000233  w[0]    0.146 bias    1.410\n",
      "iter 1900/1000000  loss         0.220352  avg_L1_norm_grad         0.000220  w[0]    0.148 bias    1.427\n",
      "iter 1901/1000000  loss         0.220346  avg_L1_norm_grad         0.000220  w[0]    0.148 bias    1.427\n",
      "iter 2000/1000000  loss         0.219725  avg_L1_norm_grad         0.000208  w[0]    0.150 bias    1.443\n",
      "iter 2001/1000000  loss         0.219719  avg_L1_norm_grad         0.000208  w[0]    0.150 bias    1.443\n",
      "iter 2100/1000000  loss         0.219164  avg_L1_norm_grad         0.000197  w[0]    0.152 bias    1.458\n",
      "iter 2101/1000000  loss         0.219159  avg_L1_norm_grad         0.000197  w[0]    0.152 bias    1.458\n",
      "iter 2200/1000000  loss         0.218661  avg_L1_norm_grad         0.000187  w[0]    0.154 bias    1.473\n",
      "iter 2201/1000000  loss         0.218657  avg_L1_norm_grad         0.000187  w[0]    0.154 bias    1.473\n",
      "iter 2300/1000000  loss         0.218209  avg_L1_norm_grad         0.000178  w[0]    0.155 bias    1.486\n",
      "iter 2301/1000000  loss         0.218204  avg_L1_norm_grad         0.000178  w[0]    0.155 bias    1.486\n",
      "iter 2400/1000000  loss         0.217800  avg_L1_norm_grad         0.000169  w[0]    0.156 bias    1.499\n",
      "iter 2401/1000000  loss         0.217796  avg_L1_norm_grad         0.000169  w[0]    0.156 bias    1.499\n",
      "iter 2500/1000000  loss         0.217431  avg_L1_norm_grad         0.000161  w[0]    0.158 bias    1.512\n",
      "iter 2501/1000000  loss         0.217428  avg_L1_norm_grad         0.000161  w[0]    0.158 bias    1.512\n",
      "iter 2600/1000000  loss         0.217097  avg_L1_norm_grad         0.000153  w[0]    0.159 bias    1.523\n",
      "iter 2601/1000000  loss         0.217093  avg_L1_norm_grad         0.000153  w[0]    0.159 bias    1.524\n",
      "iter 2700/1000000  loss         0.216793  avg_L1_norm_grad         0.000146  w[0]    0.160 bias    1.535\n",
      "iter 2701/1000000  loss         0.216790  avg_L1_norm_grad         0.000146  w[0]    0.160 bias    1.535\n",
      "iter 2800/1000000  loss         0.216517  avg_L1_norm_grad         0.000139  w[0]    0.161 bias    1.546\n",
      "iter 2801/1000000  loss         0.216514  avg_L1_norm_grad         0.000139  w[0]    0.161 bias    1.546\n",
      "iter 2900/1000000  loss         0.216265  avg_L1_norm_grad         0.000133  w[0]    0.162 bias    1.556\n",
      "iter 2901/1000000  loss         0.216263  avg_L1_norm_grad         0.000132  w[0]    0.162 bias    1.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000/1000000  loss         0.216036  avg_L1_norm_grad         0.000127  w[0]    0.163 bias    1.566\n",
      "iter 3001/1000000  loss         0.216034  avg_L1_norm_grad         0.000127  w[0]    0.163 bias    1.566\n",
      "iter 3100/1000000  loss         0.215826  avg_L1_norm_grad         0.000121  w[0]    0.163 bias    1.576\n",
      "iter 3101/1000000  loss         0.215824  avg_L1_norm_grad         0.000121  w[0]    0.163 bias    1.576\n",
      "iter 3200/1000000  loss         0.215635  avg_L1_norm_grad         0.000116  w[0]    0.164 bias    1.585\n",
      "iter 3201/1000000  loss         0.215633  avg_L1_norm_grad         0.000116  w[0]    0.164 bias    1.585\n",
      "iter 3300/1000000  loss         0.215459  avg_L1_norm_grad         0.000111  w[0]    0.165 bias    1.594\n",
      "iter 3301/1000000  loss         0.215457  avg_L1_norm_grad         0.000111  w[0]    0.165 bias    1.594\n",
      "iter 3400/1000000  loss         0.215298  avg_L1_norm_grad         0.000106  w[0]    0.165 bias    1.602\n",
      "iter 3401/1000000  loss         0.215296  avg_L1_norm_grad         0.000106  w[0]    0.165 bias    1.602\n",
      "iter 3500/1000000  loss         0.215150  avg_L1_norm_grad         0.000101  w[0]    0.166 bias    1.610\n",
      "iter 3501/1000000  loss         0.215149  avg_L1_norm_grad         0.000101  w[0]    0.166 bias    1.610\n",
      "iter 3600/1000000  loss         0.215014  avg_L1_norm_grad         0.000097  w[0]    0.167 bias    1.618\n",
      "iter 3601/1000000  loss         0.215013  avg_L1_norm_grad         0.000097  w[0]    0.167 bias    1.618\n",
      "iter 3700/1000000  loss         0.214889  avg_L1_norm_grad         0.000093  w[0]    0.167 bias    1.626\n",
      "iter 3701/1000000  loss         0.214888  avg_L1_norm_grad         0.000093  w[0]    0.167 bias    1.626\n",
      "iter 3800/1000000  loss         0.214774  avg_L1_norm_grad         0.000089  w[0]    0.168 bias    1.633\n",
      "iter 3801/1000000  loss         0.214773  avg_L1_norm_grad         0.000089  w[0]    0.168 bias    1.633\n",
      "iter 3900/1000000  loss         0.214668  avg_L1_norm_grad         0.000086  w[0]    0.168 bias    1.640\n",
      "iter 3901/1000000  loss         0.214666  avg_L1_norm_grad         0.000086  w[0]    0.168 bias    1.640\n",
      "iter 4000/1000000  loss         0.214570  avg_L1_norm_grad         0.000082  w[0]    0.168 bias    1.647\n",
      "iter 4001/1000000  loss         0.214569  avg_L1_norm_grad         0.000082  w[0]    0.168 bias    1.647\n",
      "iter 4100/1000000  loss         0.214479  avg_L1_norm_grad         0.000079  w[0]    0.169 bias    1.653\n",
      "iter 4101/1000000  loss         0.214478  avg_L1_norm_grad         0.000079  w[0]    0.169 bias    1.653\n",
      "iter 4200/1000000  loss         0.214395  avg_L1_norm_grad         0.000076  w[0]    0.169 bias    1.660\n",
      "iter 4201/1000000  loss         0.214395  avg_L1_norm_grad         0.000076  w[0]    0.169 bias    1.660\n",
      "iter 4300/1000000  loss         0.214318  avg_L1_norm_grad         0.000073  w[0]    0.169 bias    1.666\n",
      "iter 4301/1000000  loss         0.214317  avg_L1_norm_grad         0.000073  w[0]    0.169 bias    1.666\n",
      "iter 4400/1000000  loss         0.214247  avg_L1_norm_grad         0.000070  w[0]    0.170 bias    1.671\n",
      "iter 4401/1000000  loss         0.214246  avg_L1_norm_grad         0.000070  w[0]    0.170 bias    1.672\n",
      "iter 4500/1000000  loss         0.214180  avg_L1_norm_grad         0.000067  w[0]    0.170 bias    1.677\n",
      "iter 4501/1000000  loss         0.214180  avg_L1_norm_grad         0.000067  w[0]    0.170 bias    1.677\n",
      "iter 4600/1000000  loss         0.214119  avg_L1_norm_grad         0.000064  w[0]    0.170 bias    1.683\n",
      "iter 4601/1000000  loss         0.214118  avg_L1_norm_grad         0.000064  w[0]    0.170 bias    1.683\n",
      "iter 4700/1000000  loss         0.214062  avg_L1_norm_grad         0.000062  w[0]    0.171 bias    1.688\n",
      "iter 4701/1000000  loss         0.214061  avg_L1_norm_grad         0.000062  w[0]    0.171 bias    1.688\n",
      "iter 4800/1000000  loss         0.214009  avg_L1_norm_grad         0.000060  w[0]    0.171 bias    1.693\n",
      "iter 4801/1000000  loss         0.214009  avg_L1_norm_grad         0.000060  w[0]    0.171 bias    1.693\n",
      "iter 4900/1000000  loss         0.213960  avg_L1_norm_grad         0.000057  w[0]    0.171 bias    1.698\n",
      "iter 4901/1000000  loss         0.213960  avg_L1_norm_grad         0.000057  w[0]    0.171 bias    1.698\n",
      "iter 5000/1000000  loss         0.213914  avg_L1_norm_grad         0.000055  w[0]    0.171 bias    1.703\n",
      "iter 5001/1000000  loss         0.213914  avg_L1_norm_grad         0.000055  w[0]    0.171 bias    1.703\n",
      "iter 5100/1000000  loss         0.213872  avg_L1_norm_grad         0.000053  w[0]    0.171 bias    1.707\n",
      "iter 5101/1000000  loss         0.213872  avg_L1_norm_grad         0.000053  w[0]    0.171 bias    1.707\n",
      "iter 5200/1000000  loss         0.213833  avg_L1_norm_grad         0.000051  w[0]    0.172 bias    1.712\n",
      "iter 5201/1000000  loss         0.213832  avg_L1_norm_grad         0.000051  w[0]    0.172 bias    1.712\n",
      "iter 5300/1000000  loss         0.213796  avg_L1_norm_grad         0.000049  w[0]    0.172 bias    1.716\n",
      "iter 5301/1000000  loss         0.213796  avg_L1_norm_grad         0.000049  w[0]    0.172 bias    1.716\n",
      "iter 5400/1000000  loss         0.213762  avg_L1_norm_grad         0.000047  w[0]    0.172 bias    1.720\n",
      "iter 5401/1000000  loss         0.213761  avg_L1_norm_grad         0.000047  w[0]    0.172 bias    1.720\n",
      "iter 5500/1000000  loss         0.213730  avg_L1_norm_grad         0.000046  w[0]    0.172 bias    1.724\n",
      "iter 5501/1000000  loss         0.213730  avg_L1_norm_grad         0.000046  w[0]    0.172 bias    1.724\n",
      "iter 5600/1000000  loss         0.213700  avg_L1_norm_grad         0.000044  w[0]    0.172 bias    1.728\n",
      "iter 5601/1000000  loss         0.213700  avg_L1_norm_grad         0.000044  w[0]    0.172 bias    1.728\n",
      "iter 5700/1000000  loss         0.213673  avg_L1_norm_grad         0.000042  w[0]    0.172 bias    1.732\n",
      "iter 5701/1000000  loss         0.213672  avg_L1_norm_grad         0.000042  w[0]    0.172 bias    1.732\n",
      "iter 5800/1000000  loss         0.213647  avg_L1_norm_grad         0.000041  w[0]    0.172 bias    1.736\n",
      "iter 5801/1000000  loss         0.213647  avg_L1_norm_grad         0.000041  w[0]    0.172 bias    1.736\n",
      "iter 5900/1000000  loss         0.213623  avg_L1_norm_grad         0.000039  w[0]    0.173 bias    1.739\n",
      "iter 5901/1000000  loss         0.213622  avg_L1_norm_grad         0.000039  w[0]    0.173 bias    1.739\n",
      "iter 6000/1000000  loss         0.213600  avg_L1_norm_grad         0.000038  w[0]    0.173 bias    1.742\n",
      "iter 6001/1000000  loss         0.213600  avg_L1_norm_grad         0.000038  w[0]    0.173 bias    1.743\n",
      "iter 6100/1000000  loss         0.213579  avg_L1_norm_grad         0.000037  w[0]    0.173 bias    1.746\n",
      "iter 6101/1000000  loss         0.213579  avg_L1_norm_grad         0.000037  w[0]    0.173 bias    1.746\n",
      "iter 6200/1000000  loss         0.213560  avg_L1_norm_grad         0.000035  w[0]    0.173 bias    1.749\n",
      "iter 6201/1000000  loss         0.213559  avg_L1_norm_grad         0.000035  w[0]    0.173 bias    1.749\n",
      "iter 6300/1000000  loss         0.213541  avg_L1_norm_grad         0.000034  w[0]    0.173 bias    1.752\n",
      "iter 6301/1000000  loss         0.213541  avg_L1_norm_grad         0.000034  w[0]    0.173 bias    1.752\n",
      "iter 6400/1000000  loss         0.213524  avg_L1_norm_grad         0.000033  w[0]    0.173 bias    1.755\n",
      "iter 6401/1000000  loss         0.213524  avg_L1_norm_grad         0.000033  w[0]    0.173 bias    1.755\n",
      "iter 6500/1000000  loss         0.213508  avg_L1_norm_grad         0.000032  w[0]    0.173 bias    1.758\n",
      "iter 6501/1000000  loss         0.213508  avg_L1_norm_grad         0.000032  w[0]    0.173 bias    1.758\n",
      "iter 6600/1000000  loss         0.213493  avg_L1_norm_grad         0.000031  w[0]    0.173 bias    1.761\n",
      "iter 6601/1000000  loss         0.213493  avg_L1_norm_grad         0.000031  w[0]    0.173 bias    1.761\n",
      "iter 6700/1000000  loss         0.213479  avg_L1_norm_grad         0.000030  w[0]    0.173 bias    1.764\n",
      "iter 6701/1000000  loss         0.213479  avg_L1_norm_grad         0.000030  w[0]    0.173 bias    1.764\n",
      "iter 6800/1000000  loss         0.213466  avg_L1_norm_grad         0.000029  w[0]    0.173 bias    1.766\n",
      "iter 6801/1000000  loss         0.213466  avg_L1_norm_grad         0.000028  w[0]    0.173 bias    1.766\n",
      "iter 6900/1000000  loss         0.213453  avg_L1_norm_grad         0.000028  w[0]    0.173 bias    1.769\n",
      "iter 6901/1000000  loss         0.213453  avg_L1_norm_grad         0.000028  w[0]    0.173 bias    1.769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000/1000000  loss         0.213442  avg_L1_norm_grad         0.000027  w[0]    0.174 bias    1.771\n",
      "iter 7001/1000000  loss         0.213442  avg_L1_norm_grad         0.000027  w[0]    0.174 bias    1.771\n",
      "iter 7100/1000000  loss         0.213431  avg_L1_norm_grad         0.000026  w[0]    0.174 bias    1.774\n",
      "iter 7101/1000000  loss         0.213431  avg_L1_norm_grad         0.000026  w[0]    0.174 bias    1.774\n",
      "iter 7200/1000000  loss         0.213421  avg_L1_norm_grad         0.000025  w[0]    0.174 bias    1.776\n",
      "iter 7201/1000000  loss         0.213421  avg_L1_norm_grad         0.000025  w[0]    0.174 bias    1.776\n",
      "iter 7300/1000000  loss         0.213411  avg_L1_norm_grad         0.000024  w[0]    0.174 bias    1.778\n",
      "iter 7301/1000000  loss         0.213411  avg_L1_norm_grad         0.000024  w[0]    0.174 bias    1.778\n",
      "iter 7400/1000000  loss         0.213402  avg_L1_norm_grad         0.000023  w[0]    0.174 bias    1.780\n",
      "iter 7401/1000000  loss         0.213402  avg_L1_norm_grad         0.000023  w[0]    0.174 bias    1.780\n",
      "iter 7500/1000000  loss         0.213394  avg_L1_norm_grad         0.000022  w[0]    0.174 bias    1.782\n",
      "iter 7501/1000000  loss         0.213394  avg_L1_norm_grad         0.000022  w[0]    0.174 bias    1.782\n",
      "iter 7600/1000000  loss         0.213386  avg_L1_norm_grad         0.000022  w[0]    0.174 bias    1.785\n",
      "iter 7601/1000000  loss         0.213386  avg_L1_norm_grad         0.000022  w[0]    0.174 bias    1.785\n",
      "iter 7700/1000000  loss         0.213378  avg_L1_norm_grad         0.000021  w[0]    0.174 bias    1.787\n",
      "iter 7701/1000000  loss         0.213378  avg_L1_norm_grad         0.000021  w[0]    0.174 bias    1.787\n",
      "iter 7800/1000000  loss         0.213371  avg_L1_norm_grad         0.000020  w[0]    0.174 bias    1.788\n",
      "iter 7801/1000000  loss         0.213371  avg_L1_norm_grad         0.000020  w[0]    0.174 bias    1.788\n",
      "iter 7900/1000000  loss         0.213365  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.790\n",
      "iter 7901/1000000  loss         0.213365  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.790\n",
      "iter 8000/1000000  loss         0.213358  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.792\n",
      "iter 8001/1000000  loss         0.213358  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.792\n",
      "iter 8100/1000000  loss         0.213353  avg_L1_norm_grad         0.000018  w[0]    0.174 bias    1.794\n",
      "iter 8101/1000000  loss         0.213353  avg_L1_norm_grad         0.000018  w[0]    0.174 bias    1.794\n",
      "iter 8200/1000000  loss         0.213347  avg_L1_norm_grad         0.000018  w[0]    0.174 bias    1.796\n",
      "iter 8201/1000000  loss         0.213347  avg_L1_norm_grad         0.000018  w[0]    0.174 bias    1.796\n",
      "iter 8300/1000000  loss         0.213342  avg_L1_norm_grad         0.000017  w[0]    0.174 bias    1.797\n",
      "iter 8301/1000000  loss         0.213342  avg_L1_norm_grad         0.000017  w[0]    0.174 bias    1.797\n",
      "iter 8400/1000000  loss         0.213337  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.799\n",
      "iter 8401/1000000  loss         0.213337  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.799\n",
      "iter 8500/1000000  loss         0.213333  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.800\n",
      "iter 8501/1000000  loss         0.213333  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.800\n",
      "iter 8600/1000000  loss         0.213328  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.802\n",
      "iter 8601/1000000  loss         0.213328  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.802\n",
      "iter 8700/1000000  loss         0.213324  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.803\n",
      "iter 8701/1000000  loss         0.213324  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.803\n",
      "iter 8800/1000000  loss         0.213320  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.805\n",
      "iter 8801/1000000  loss         0.213320  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.805\n",
      "iter 8900/1000000  loss         0.213317  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.806\n",
      "iter 8901/1000000  loss         0.213317  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.806\n",
      "iter 9000/1000000  loss         0.213313  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.808\n",
      "iter 9001/1000000  loss         0.213313  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.808\n",
      "iter 9100/1000000  loss         0.213310  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.809\n",
      "iter 9101/1000000  loss         0.213310  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.809\n",
      "iter 9200/1000000  loss         0.213307  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.810\n",
      "iter 9201/1000000  loss         0.213307  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.810\n",
      "iter 9300/1000000  loss         0.213304  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.811\n",
      "iter 9301/1000000  loss         0.213304  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.811\n",
      "iter 9400/1000000  loss         0.213301  avg_L1_norm_grad         0.000012  w[0]    0.175 bias    1.812\n",
      "iter 9401/1000000  loss         0.213301  avg_L1_norm_grad         0.000012  w[0]    0.175 bias    1.813\n",
      "iter 9500/1000000  loss         0.213299  avg_L1_norm_grad         0.000011  w[0]    0.175 bias    1.814\n",
      "iter 9501/1000000  loss         0.213299  avg_L1_norm_grad         0.000011  w[0]    0.175 bias    1.814\n",
      "iter 9600/1000000  loss         0.213296  avg_L1_norm_grad         0.000011  w[0]    0.175 bias    1.815\n",
      "iter 9601/1000000  loss         0.213296  avg_L1_norm_grad         0.000011  w[0]    0.175 bias    1.815\n",
      "iter 9700/1000000  loss         0.213294  avg_L1_norm_grad         0.000011  w[0]    0.175 bias    1.816\n",
      "iter 9701/1000000  loss         0.213294  avg_L1_norm_grad         0.000011  w[0]    0.175 bias    1.816\n",
      "iter 9800/1000000  loss         0.213292  avg_L1_norm_grad         0.000010  w[0]    0.175 bias    1.817\n",
      "iter 9801/1000000  loss         0.213292  avg_L1_norm_grad         0.000010  w[0]    0.175 bias    1.817\n",
      "iter 9900/1000000  loss         0.213290  avg_L1_norm_grad         0.000010  w[0]    0.175 bias    1.818\n",
      "iter 9901/1000000  loss         0.213290  avg_L1_norm_grad         0.000010  w[0]    0.175 bias    1.818\n",
      "iter 10000/1000000  loss         0.213288  avg_L1_norm_grad         0.000010  w[0]    0.175 bias    1.819\n",
      "iter 10001/1000000  loss         0.213288  avg_L1_norm_grad         0.000010  w[0]    0.175 bias    1.819\n",
      "iter 10100/1000000  loss         0.213286  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.820\n",
      "iter 10101/1000000  loss         0.213286  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.820\n",
      "iter 10200/1000000  loss         0.213284  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.821\n",
      "iter 10201/1000000  loss         0.213284  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.821\n",
      "iter 10300/1000000  loss         0.213283  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.822\n",
      "iter 10301/1000000  loss         0.213283  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.822\n",
      "iter 10400/1000000  loss         0.213281  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.823\n",
      "iter 10401/1000000  loss         0.213281  avg_L1_norm_grad         0.000009  w[0]    0.175 bias    1.823\n",
      "iter 10500/1000000  loss         0.213280  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.823\n",
      "iter 10501/1000000  loss         0.213280  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.823\n",
      "iter 10600/1000000  loss         0.213278  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.824\n",
      "iter 10601/1000000  loss         0.213278  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.824\n",
      "iter 10700/1000000  loss         0.213277  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.825\n",
      "iter 10701/1000000  loss         0.213277  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.825\n",
      "iter 10800/1000000  loss         0.213276  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.826\n",
      "iter 10801/1000000  loss         0.213276  avg_L1_norm_grad         0.000008  w[0]    0.175 bias    1.826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10900/1000000  loss         0.213275  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.827\n",
      "iter 10901/1000000  loss         0.213274  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.827\n",
      "iter 11000/1000000  loss         0.213273  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.827\n",
      "iter 11001/1000000  loss         0.213273  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.827\n",
      "iter 11100/1000000  loss         0.213272  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.828\n",
      "iter 11101/1000000  loss         0.213272  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.828\n",
      "iter 11200/1000000  loss         0.213271  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.829\n",
      "iter 11201/1000000  loss         0.213271  avg_L1_norm_grad         0.000007  w[0]    0.175 bias    1.829\n",
      "iter 11300/1000000  loss         0.213270  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.830\n",
      "iter 11301/1000000  loss         0.213270  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.830\n",
      "iter 11400/1000000  loss         0.213269  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.830\n",
      "iter 11401/1000000  loss         0.213269  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.830\n",
      "iter 11500/1000000  loss         0.213269  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.831\n",
      "iter 11501/1000000  loss         0.213269  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.831\n",
      "iter 11600/1000000  loss         0.213268  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.831\n",
      "iter 11601/1000000  loss         0.213268  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.831\n",
      "iter 11700/1000000  loss         0.213267  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.832\n",
      "iter 11701/1000000  loss         0.213267  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.832\n",
      "iter 11800/1000000  loss         0.213266  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.833\n",
      "iter 11801/1000000  loss         0.213266  avg_L1_norm_grad         0.000006  w[0]    0.175 bias    1.833\n",
      "iter 11900/1000000  loss         0.213265  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.833\n",
      "iter 11901/1000000  loss         0.213265  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.833\n",
      "iter 12000/1000000  loss         0.213265  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.834\n",
      "iter 12001/1000000  loss         0.213265  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.834\n",
      "iter 12100/1000000  loss         0.213264  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.834\n",
      "iter 12101/1000000  loss         0.213264  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.834\n",
      "iter 12200/1000000  loss         0.213264  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.835\n",
      "iter 12201/1000000  loss         0.213263  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.835\n",
      "iter 12300/1000000  loss         0.213263  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.835\n",
      "iter 12301/1000000  loss         0.213263  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.835\n",
      "iter 12400/1000000  loss         0.213262  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.836\n",
      "iter 12401/1000000  loss         0.213262  avg_L1_norm_grad         0.000005  w[0]    0.175 bias    1.836\n",
      "iter 12500/1000000  loss         0.213262  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.836\n",
      "iter 12501/1000000  loss         0.213262  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.836\n",
      "iter 12600/1000000  loss         0.213261  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.837\n",
      "iter 12601/1000000  loss         0.213261  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.837\n",
      "iter 12700/1000000  loss         0.213261  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.837\n",
      "iter 12701/1000000  loss         0.213261  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.837\n",
      "iter 12800/1000000  loss         0.213260  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.838\n",
      "iter 12801/1000000  loss         0.213260  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.838\n",
      "iter 12900/1000000  loss         0.213260  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.838\n",
      "iter 12901/1000000  loss         0.213260  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.838\n",
      "iter 13000/1000000  loss         0.213260  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.839\n",
      "iter 13001/1000000  loss         0.213260  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.839\n",
      "iter 13100/1000000  loss         0.213259  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.839\n",
      "iter 13101/1000000  loss         0.213259  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.839\n",
      "iter 13200/1000000  loss         0.213259  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.839\n",
      "iter 13201/1000000  loss         0.213259  avg_L1_norm_grad         0.000004  w[0]    0.175 bias    1.839\n",
      "iter 13300/1000000  loss         0.213258  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.840\n",
      "iter 13301/1000000  loss         0.213258  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.840\n",
      "iter 13400/1000000  loss         0.213258  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.840\n",
      "iter 13401/1000000  loss         0.213258  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.840\n",
      "iter 13500/1000000  loss         0.213258  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.841\n",
      "iter 13501/1000000  loss         0.213258  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.841\n",
      "iter 13600/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.841\n",
      "iter 13601/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.841\n",
      "iter 13700/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.841\n",
      "iter 13701/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.841\n",
      "iter 13800/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.842\n",
      "iter 13801/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.842\n",
      "iter 13900/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.842\n",
      "iter 13901/1000000  loss         0.213257  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.842\n",
      "iter 14000/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.842\n",
      "iter 14001/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.842\n",
      "iter 14100/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14101/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14200/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14201/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14300/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14301/1000000  loss         0.213256  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14400/1000000  loss         0.213255  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14401/1000000  loss         0.213255  avg_L1_norm_grad         0.000003  w[0]    0.175 bias    1.843\n",
      "iter 14500/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.844\n",
      "iter 14501/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.844\n",
      "iter 14600/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.844\n",
      "iter 14601/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.844\n",
      "iter 14700/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.844\n",
      "iter 14701/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14800/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 14801/1000000  loss         0.213255  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 14900/1000000  loss         0.213254  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 14901/1000000  loss         0.213254  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 15000/1000000  loss         0.213254  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 15001/1000000  loss         0.213254  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 15100/1000000  loss         0.213254  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "iter 15101/1000000  loss         0.213254  avg_L1_norm_grad         0.000002  w[0]    0.175 bias    1.845\n",
      "Done. Converged after 15155 iterations.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 1000000 iters of gradient descent with step_size 0.1\n",
      "iter    0/1000000  loss         1.000000  avg_L1_norm_grad         0.028561  w[0]    0.000 bias    0.000\n",
      "iter    1/1000000  loss         0.917990  avg_L1_norm_grad         0.029976  w[0]    0.000 bias    0.000\n",
      "iter    2/1000000  loss         0.856338  avg_L1_norm_grad         0.020520  w[0]    0.001 bias    0.019\n",
      "iter    3/1000000  loss         0.811230  avg_L1_norm_grad         0.021475  w[0]    0.002 bias    0.022\n",
      "iter    4/1000000  loss         0.775555  avg_L1_norm_grad         0.015156  w[0]    0.003 bias    0.037\n",
      "iter    5/1000000  loss         0.747618  avg_L1_norm_grad         0.015628  w[0]    0.003 bias    0.042\n",
      "iter    6/1000000  loss         0.724613  avg_L1_norm_grad         0.012332  w[0]    0.004 bias    0.053\n",
      "iter    7/1000000  loss         0.705288  avg_L1_norm_grad         0.012381  w[0]    0.005 bias    0.060\n",
      "iter    8/1000000  loss         0.688511  avg_L1_norm_grad         0.010856  w[0]    0.005 bias    0.069\n",
      "iter    9/1000000  loss         0.673624  avg_L1_norm_grad         0.010606  w[0]    0.006 bias    0.077\n",
      "iter   10/1000000  loss         0.660174  avg_L1_norm_grad         0.009858  w[0]    0.006 bias    0.085\n",
      "iter   11/1000000  loss         0.647866  avg_L1_norm_grad         0.009520  w[0]    0.007 bias    0.093\n",
      "iter   12/1000000  loss         0.636494  avg_L1_norm_grad         0.009084  w[0]    0.007 bias    0.100\n",
      "iter   13/1000000  loss         0.625910  avg_L1_norm_grad         0.008770  w[0]    0.008 bias    0.108\n",
      "iter   14/1000000  loss         0.616001  avg_L1_norm_grad         0.008456  w[0]    0.009 bias    0.115\n",
      "iter   15/1000000  loss         0.606679  avg_L1_norm_grad         0.008185  w[0]    0.009 bias    0.122\n",
      "iter   16/1000000  loss         0.597877  avg_L1_norm_grad         0.007932  w[0]    0.010 bias    0.129\n",
      "iter   17/1000000  loss         0.589538  avg_L1_norm_grad         0.007704  w[0]    0.010 bias    0.136\n",
      "iter   18/1000000  loss         0.581617  avg_L1_norm_grad         0.007491  w[0]    0.011 bias    0.143\n",
      "iter   19/1000000  loss         0.574076  avg_L1_norm_grad         0.007292  w[0]    0.011 bias    0.149\n",
      "iter  100/1000000  loss         0.360107  avg_L1_norm_grad         0.002545  w[0]    0.037 bias    0.478\n",
      "iter  101/1000000  loss         0.359127  avg_L1_norm_grad         0.002526  w[0]    0.038 bias    0.480\n",
      "iter  200/1000000  loss         0.302341  avg_L1_norm_grad         0.001479  w[0]    0.056 bias    0.681\n",
      "iter  201/1000000  loss         0.301998  avg_L1_norm_grad         0.001474  w[0]    0.056 bias    0.683\n",
      "iter  300/1000000  loss         0.277418  avg_L1_norm_grad         0.001069  w[0]    0.070 bias    0.813\n",
      "iter  301/1000000  loss         0.277237  avg_L1_norm_grad         0.001067  w[0]    0.070 bias    0.814\n",
      "iter  400/1000000  loss         0.263090  avg_L1_norm_grad         0.000852  w[0]    0.080 bias    0.910\n",
      "iter  401/1000000  loss         0.262976  avg_L1_norm_grad         0.000850  w[0]    0.081 bias    0.911\n",
      "iter  500/1000000  loss         0.253670  avg_L1_norm_grad         0.000713  w[0]    0.090 bias    0.985\n",
      "iter  501/1000000  loss         0.253592  avg_L1_norm_grad         0.000712  w[0]    0.090 bias    0.986\n",
      "iter  600/1000000  loss         0.246966  avg_L1_norm_grad         0.000617  w[0]    0.097 bias    1.047\n",
      "iter  601/1000000  loss         0.246909  avg_L1_norm_grad         0.000616  w[0]    0.098 bias    1.048\n",
      "iter  700/1000000  loss         0.241938  avg_L1_norm_grad         0.000545  w[0]    0.104 bias    1.099\n",
      "iter  701/1000000  loss         0.241894  avg_L1_norm_grad         0.000544  w[0]    0.104 bias    1.099\n",
      "iter  800/1000000  loss         0.238021  avg_L1_norm_grad         0.000488  w[0]    0.110 bias    1.143\n",
      "iter  801/1000000  loss         0.237987  avg_L1_norm_grad         0.000488  w[0]    0.110 bias    1.144\n",
      "iter  900/1000000  loss         0.234885  avg_L1_norm_grad         0.000441  w[0]    0.116 bias    1.182\n",
      "iter  901/1000000  loss         0.234857  avg_L1_norm_grad         0.000441  w[0]    0.116 bias    1.183\n",
      "iter 1000/1000000  loss         0.232320  avg_L1_norm_grad         0.000403  w[0]    0.121 bias    1.217\n",
      "iter 1001/1000000  loss         0.232296  avg_L1_norm_grad         0.000402  w[0]    0.121 bias    1.217\n",
      "iter 1100/1000000  loss         0.230185  avg_L1_norm_grad         0.000370  w[0]    0.125 bias    1.248\n",
      "iter 1101/1000000  loss         0.230165  avg_L1_norm_grad         0.000370  w[0]    0.125 bias    1.248\n",
      "iter 1200/1000000  loss         0.228384  avg_L1_norm_grad         0.000343  w[0]    0.129 bias    1.276\n",
      "iter 1201/1000000  loss         0.228367  avg_L1_norm_grad         0.000342  w[0]    0.129 bias    1.276\n",
      "iter 1300/1000000  loss         0.226848  avg_L1_norm_grad         0.000319  w[0]    0.133 bias    1.301\n",
      "iter 1301/1000000  loss         0.226834  avg_L1_norm_grad         0.000318  w[0]    0.133 bias    1.302\n",
      "iter 1400/1000000  loss         0.225526  avg_L1_norm_grad         0.000297  w[0]    0.136 bias    1.325\n",
      "iter 1401/1000000  loss         0.225514  avg_L1_norm_grad         0.000297  w[0]    0.136 bias    1.325\n",
      "iter 1500/1000000  loss         0.224379  avg_L1_norm_grad         0.000279  w[0]    0.139 bias    1.347\n",
      "iter 1501/1000000  loss         0.224368  avg_L1_norm_grad         0.000278  w[0]    0.139 bias    1.347\n",
      "iter 1600/1000000  loss         0.223377  avg_L1_norm_grad         0.000262  w[0]    0.141 bias    1.367\n",
      "iter 1601/1000000  loss         0.223368  avg_L1_norm_grad         0.000262  w[0]    0.141 bias    1.367\n",
      "iter 1700/1000000  loss         0.222497  avg_L1_norm_grad         0.000246  w[0]    0.144 bias    1.386\n",
      "iter 1701/1000000  loss         0.222489  avg_L1_norm_grad         0.000246  w[0]    0.144 bias    1.386\n",
      "iter 1800/1000000  loss         0.221720  avg_L1_norm_grad         0.000233  w[0]    0.146 bias    1.403\n",
      "iter 1801/1000000  loss         0.221713  avg_L1_norm_grad         0.000232  w[0]    0.146 bias    1.403\n",
      "iter 1900/1000000  loss         0.221031  avg_L1_norm_grad         0.000220  w[0]    0.148 bias    1.420\n",
      "iter 1901/1000000  loss         0.221025  avg_L1_norm_grad         0.000220  w[0]    0.148 bias    1.420\n",
      "iter 2000/1000000  loss         0.220418  avg_L1_norm_grad         0.000208  w[0]    0.150 bias    1.436\n",
      "iter 2001/1000000  loss         0.220412  avg_L1_norm_grad         0.000208  w[0]    0.150 bias    1.436\n",
      "iter 2100/1000000  loss         0.219871  avg_L1_norm_grad         0.000197  w[0]    0.152 bias    1.450\n",
      "iter 2101/1000000  loss         0.219865  avg_L1_norm_grad         0.000197  w[0]    0.152 bias    1.450\n",
      "iter 2200/1000000  loss         0.219380  avg_L1_norm_grad         0.000187  w[0]    0.153 bias    1.464\n",
      "iter 2201/1000000  loss         0.219376  avg_L1_norm_grad         0.000187  w[0]    0.153 bias    1.464\n",
      "iter 2300/1000000  loss         0.218940  avg_L1_norm_grad         0.000177  w[0]    0.155 bias    1.478\n",
      "iter 2301/1000000  loss         0.218935  avg_L1_norm_grad         0.000177  w[0]    0.155 bias    1.478\n",
      "iter 2400/1000000  loss         0.218543  avg_L1_norm_grad         0.000169  w[0]    0.156 bias    1.490\n",
      "iter 2401/1000000  loss         0.218539  avg_L1_norm_grad         0.000168  w[0]    0.156 bias    1.490\n",
      "iter 2500/1000000  loss         0.218185  avg_L1_norm_grad         0.000160  w[0]    0.157 bias    1.502\n",
      "iter 2501/1000000  loss         0.218181  avg_L1_norm_grad         0.000160  w[0]    0.157 bias    1.502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2600/1000000  loss         0.217861  avg_L1_norm_grad         0.000153  w[0]    0.159 bias    1.514\n",
      "iter 2601/1000000  loss         0.217858  avg_L1_norm_grad         0.000153  w[0]    0.159 bias    1.514\n",
      "iter 2700/1000000  loss         0.217568  avg_L1_norm_grad         0.000145  w[0]    0.160 bias    1.525\n",
      "iter 2701/1000000  loss         0.217565  avg_L1_norm_grad         0.000145  w[0]    0.160 bias    1.525\n",
      "iter 2800/1000000  loss         0.217301  avg_L1_norm_grad         0.000139  w[0]    0.161 bias    1.535\n",
      "iter 2801/1000000  loss         0.217299  avg_L1_norm_grad         0.000139  w[0]    0.161 bias    1.535\n",
      "iter 2900/1000000  loss         0.217059  avg_L1_norm_grad         0.000132  w[0]    0.162 bias    1.545\n",
      "iter 2901/1000000  loss         0.217057  avg_L1_norm_grad         0.000132  w[0]    0.162 bias    1.545\n",
      "iter 3000/1000000  loss         0.216839  avg_L1_norm_grad         0.000126  w[0]    0.162 bias    1.555\n",
      "iter 3001/1000000  loss         0.216837  avg_L1_norm_grad         0.000126  w[0]    0.162 bias    1.555\n",
      "iter 3100/1000000  loss         0.216638  avg_L1_norm_grad         0.000121  w[0]    0.163 bias    1.564\n",
      "iter 3101/1000000  loss         0.216636  avg_L1_norm_grad         0.000121  w[0]    0.163 bias    1.564\n",
      "iter 3200/1000000  loss         0.216454  avg_L1_norm_grad         0.000115  w[0]    0.164 bias    1.573\n",
      "iter 3201/1000000  loss         0.216452  avg_L1_norm_grad         0.000115  w[0]    0.164 bias    1.573\n",
      "iter 3300/1000000  loss         0.216286  avg_L1_norm_grad         0.000110  w[0]    0.165 bias    1.581\n",
      "iter 3301/1000000  loss         0.216285  avg_L1_norm_grad         0.000110  w[0]    0.165 bias    1.582\n",
      "iter 3400/1000000  loss         0.216133  avg_L1_norm_grad         0.000106  w[0]    0.165 bias    1.590\n",
      "iter 3401/1000000  loss         0.216131  avg_L1_norm_grad         0.000106  w[0]    0.165 bias    1.590\n",
      "iter 3500/1000000  loss         0.215992  avg_L1_norm_grad         0.000101  w[0]    0.166 bias    1.597\n",
      "iter 3501/1000000  loss         0.215991  avg_L1_norm_grad         0.000101  w[0]    0.166 bias    1.598\n",
      "iter 3600/1000000  loss         0.215863  avg_L1_norm_grad         0.000097  w[0]    0.166 bias    1.605\n",
      "iter 3601/1000000  loss         0.215862  avg_L1_norm_grad         0.000097  w[0]    0.166 bias    1.605\n",
      "iter 3700/1000000  loss         0.215745  avg_L1_norm_grad         0.000093  w[0]    0.167 bias    1.612\n",
      "iter 3701/1000000  loss         0.215744  avg_L1_norm_grad         0.000093  w[0]    0.167 bias    1.612\n",
      "iter 3800/1000000  loss         0.215636  avg_L1_norm_grad         0.000089  w[0]    0.167 bias    1.619\n",
      "iter 3801/1000000  loss         0.215635  avg_L1_norm_grad         0.000089  w[0]    0.167 bias    1.619\n",
      "iter 3900/1000000  loss         0.215536  avg_L1_norm_grad         0.000085  w[0]    0.168 bias    1.626\n",
      "iter 3901/1000000  loss         0.215535  avg_L1_norm_grad         0.000085  w[0]    0.168 bias    1.626\n",
      "iter 4000/1000000  loss         0.215444  avg_L1_norm_grad         0.000082  w[0]    0.168 bias    1.632\n",
      "iter 4001/1000000  loss         0.215443  avg_L1_norm_grad         0.000082  w[0]    0.168 bias    1.632\n",
      "iter 4100/1000000  loss         0.215359  avg_L1_norm_grad         0.000079  w[0]    0.169 bias    1.639\n",
      "iter 4101/1000000  loss         0.215359  avg_L1_norm_grad         0.000078  w[0]    0.169 bias    1.639\n",
      "iter 4200/1000000  loss         0.215281  avg_L1_norm_grad         0.000075  w[0]    0.169 bias    1.645\n",
      "iter 4201/1000000  loss         0.215281  avg_L1_norm_grad         0.000075  w[0]    0.169 bias    1.645\n",
      "iter 4300/1000000  loss         0.215209  avg_L1_norm_grad         0.000072  w[0]    0.169 bias    1.650\n",
      "iter 4301/1000000  loss         0.215209  avg_L1_norm_grad         0.000072  w[0]    0.169 bias    1.650\n",
      "iter 4400/1000000  loss         0.215143  avg_L1_norm_grad         0.000070  w[0]    0.170 bias    1.656\n",
      "iter 4401/1000000  loss         0.215142  avg_L1_norm_grad         0.000070  w[0]    0.170 bias    1.656\n",
      "iter 4500/1000000  loss         0.215081  avg_L1_norm_grad         0.000067  w[0]    0.170 bias    1.661\n",
      "iter 4501/1000000  loss         0.215081  avg_L1_norm_grad         0.000067  w[0]    0.170 bias    1.661\n",
      "iter 4600/1000000  loss         0.215025  avg_L1_norm_grad         0.000064  w[0]    0.170 bias    1.667\n",
      "iter 4601/1000000  loss         0.215024  avg_L1_norm_grad         0.000064  w[0]    0.170 bias    1.667\n",
      "iter 4700/1000000  loss         0.214972  avg_L1_norm_grad         0.000062  w[0]    0.170 bias    1.672\n",
      "iter 4701/1000000  loss         0.214972  avg_L1_norm_grad         0.000062  w[0]    0.170 bias    1.672\n",
      "iter 4800/1000000  loss         0.214924  avg_L1_norm_grad         0.000059  w[0]    0.171 bias    1.676\n",
      "iter 4801/1000000  loss         0.214923  avg_L1_norm_grad         0.000059  w[0]    0.171 bias    1.676\n",
      "iter 4900/1000000  loss         0.214879  avg_L1_norm_grad         0.000057  w[0]    0.171 bias    1.681\n",
      "iter 4901/1000000  loss         0.214879  avg_L1_norm_grad         0.000057  w[0]    0.171 bias    1.681\n",
      "iter 5000/1000000  loss         0.214838  avg_L1_norm_grad         0.000055  w[0]    0.171 bias    1.686\n",
      "iter 5001/1000000  loss         0.214837  avg_L1_norm_grad         0.000055  w[0]    0.171 bias    1.686\n",
      "iter 5100/1000000  loss         0.214799  avg_L1_norm_grad         0.000053  w[0]    0.171 bias    1.690\n",
      "iter 5101/1000000  loss         0.214799  avg_L1_norm_grad         0.000053  w[0]    0.171 bias    1.690\n",
      "iter 5200/1000000  loss         0.214763  avg_L1_norm_grad         0.000051  w[0]    0.171 bias    1.694\n",
      "iter 5201/1000000  loss         0.214763  avg_L1_norm_grad         0.000051  w[0]    0.171 bias    1.694\n",
      "iter 5300/1000000  loss         0.214730  avg_L1_norm_grad         0.000049  w[0]    0.172 bias    1.698\n",
      "iter 5301/1000000  loss         0.214730  avg_L1_norm_grad         0.000049  w[0]    0.172 bias    1.698\n",
      "iter 5400/1000000  loss         0.214700  avg_L1_norm_grad         0.000047  w[0]    0.172 bias    1.702\n",
      "iter 5401/1000000  loss         0.214699  avg_L1_norm_grad         0.000047  w[0]    0.172 bias    1.702\n",
      "iter 5500/1000000  loss         0.214671  avg_L1_norm_grad         0.000045  w[0]    0.172 bias    1.706\n",
      "iter 5501/1000000  loss         0.214671  avg_L1_norm_grad         0.000045  w[0]    0.172 bias    1.706\n",
      "iter 5600/1000000  loss         0.214645  avg_L1_norm_grad         0.000044  w[0]    0.172 bias    1.709\n",
      "iter 5601/1000000  loss         0.214645  avg_L1_norm_grad         0.000044  w[0]    0.172 bias    1.709\n",
      "iter 5700/1000000  loss         0.214620  avg_L1_norm_grad         0.000042  w[0]    0.172 bias    1.713\n",
      "iter 5701/1000000  loss         0.214620  avg_L1_norm_grad         0.000042  w[0]    0.172 bias    1.713\n",
      "iter 5800/1000000  loss         0.214597  avg_L1_norm_grad         0.000041  w[0]    0.172 bias    1.716\n",
      "iter 5801/1000000  loss         0.214597  avg_L1_norm_grad         0.000041  w[0]    0.172 bias    1.716\n",
      "iter 5900/1000000  loss         0.214576  avg_L1_norm_grad         0.000039  w[0]    0.172 bias    1.720\n",
      "iter 5901/1000000  loss         0.214576  avg_L1_norm_grad         0.000039  w[0]    0.172 bias    1.720\n",
      "iter 6000/1000000  loss         0.214557  avg_L1_norm_grad         0.000038  w[0]    0.173 bias    1.723\n",
      "iter 6001/1000000  loss         0.214556  avg_L1_norm_grad         0.000038  w[0]    0.173 bias    1.723\n",
      "iter 6100/1000000  loss         0.214538  avg_L1_norm_grad         0.000036  w[0]    0.173 bias    1.726\n",
      "iter 6101/1000000  loss         0.214538  avg_L1_norm_grad         0.000036  w[0]    0.173 bias    1.726\n",
      "iter 6200/1000000  loss         0.214521  avg_L1_norm_grad         0.000035  w[0]    0.173 bias    1.729\n",
      "iter 6201/1000000  loss         0.214521  avg_L1_norm_grad         0.000035  w[0]    0.173 bias    1.729\n",
      "iter 6300/1000000  loss         0.214505  avg_L1_norm_grad         0.000034  w[0]    0.173 bias    1.732\n",
      "iter 6301/1000000  loss         0.214505  avg_L1_norm_grad         0.000034  w[0]    0.173 bias    1.732\n",
      "iter 6400/1000000  loss         0.214491  avg_L1_norm_grad         0.000033  w[0]    0.173 bias    1.735\n",
      "iter 6401/1000000  loss         0.214491  avg_L1_norm_grad         0.000033  w[0]    0.173 bias    1.735\n",
      "iter 6500/1000000  loss         0.214477  avg_L1_norm_grad         0.000032  w[0]    0.173 bias    1.737\n",
      "iter 6501/1000000  loss         0.214477  avg_L1_norm_grad         0.000032  w[0]    0.173 bias    1.737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6600/1000000  loss         0.214464  avg_L1_norm_grad         0.000030  w[0]    0.173 bias    1.740\n",
      "iter 6601/1000000  loss         0.214464  avg_L1_norm_grad         0.000030  w[0]    0.173 bias    1.740\n",
      "iter 6700/1000000  loss         0.214452  avg_L1_norm_grad         0.000029  w[0]    0.173 bias    1.742\n",
      "iter 6701/1000000  loss         0.214452  avg_L1_norm_grad         0.000029  w[0]    0.173 bias    1.742\n",
      "iter 6800/1000000  loss         0.214441  avg_L1_norm_grad         0.000028  w[0]    0.173 bias    1.745\n",
      "iter 6801/1000000  loss         0.214441  avg_L1_norm_grad         0.000028  w[0]    0.173 bias    1.745\n",
      "iter 6900/1000000  loss         0.214431  avg_L1_norm_grad         0.000027  w[0]    0.173 bias    1.747\n",
      "iter 6901/1000000  loss         0.214431  avg_L1_norm_grad         0.000027  w[0]    0.173 bias    1.747\n",
      "iter 7000/1000000  loss         0.214421  avg_L1_norm_grad         0.000026  w[0]    0.173 bias    1.749\n",
      "iter 7001/1000000  loss         0.214421  avg_L1_norm_grad         0.000026  w[0]    0.173 bias    1.750\n",
      "iter 7100/1000000  loss         0.214412  avg_L1_norm_grad         0.000025  w[0]    0.173 bias    1.752\n",
      "iter 7101/1000000  loss         0.214412  avg_L1_norm_grad         0.000025  w[0]    0.173 bias    1.752\n",
      "iter 7200/1000000  loss         0.214404  avg_L1_norm_grad         0.000025  w[0]    0.174 bias    1.754\n",
      "iter 7201/1000000  loss         0.214404  avg_L1_norm_grad         0.000025  w[0]    0.174 bias    1.754\n",
      "iter 7300/1000000  loss         0.214396  avg_L1_norm_grad         0.000024  w[0]    0.174 bias    1.756\n",
      "iter 7301/1000000  loss         0.214396  avg_L1_norm_grad         0.000024  w[0]    0.174 bias    1.756\n",
      "iter 7400/1000000  loss         0.214389  avg_L1_norm_grad         0.000023  w[0]    0.174 bias    1.758\n",
      "iter 7401/1000000  loss         0.214388  avg_L1_norm_grad         0.000023  w[0]    0.174 bias    1.758\n",
      "iter 7500/1000000  loss         0.214382  avg_L1_norm_grad         0.000022  w[0]    0.174 bias    1.760\n",
      "iter 7501/1000000  loss         0.214382  avg_L1_norm_grad         0.000022  w[0]    0.174 bias    1.760\n",
      "iter 7600/1000000  loss         0.214375  avg_L1_norm_grad         0.000021  w[0]    0.174 bias    1.762\n",
      "iter 7601/1000000  loss         0.214375  avg_L1_norm_grad         0.000021  w[0]    0.174 bias    1.762\n",
      "iter 7700/1000000  loss         0.214369  avg_L1_norm_grad         0.000021  w[0]    0.174 bias    1.764\n",
      "iter 7701/1000000  loss         0.214369  avg_L1_norm_grad         0.000021  w[0]    0.174 bias    1.764\n",
      "iter 7800/1000000  loss         0.214364  avg_L1_norm_grad         0.000020  w[0]    0.174 bias    1.765\n",
      "iter 7801/1000000  loss         0.214364  avg_L1_norm_grad         0.000020  w[0]    0.174 bias    1.765\n",
      "iter 7900/1000000  loss         0.214359  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.767\n",
      "iter 7901/1000000  loss         0.214359  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.767\n",
      "iter 8000/1000000  loss         0.214354  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.769\n",
      "iter 8001/1000000  loss         0.214354  avg_L1_norm_grad         0.000019  w[0]    0.174 bias    1.769\n",
      "iter 8100/1000000  loss         0.214349  avg_L1_norm_grad         0.000018  w[0]    0.174 bias    1.770\n",
      "iter 8101/1000000  loss         0.214349  avg_L1_norm_grad         0.000018  w[0]    0.174 bias    1.770\n",
      "iter 8200/1000000  loss         0.214345  avg_L1_norm_grad         0.000017  w[0]    0.174 bias    1.772\n",
      "iter 8201/1000000  loss         0.214345  avg_L1_norm_grad         0.000017  w[0]    0.174 bias    1.772\n",
      "iter 8300/1000000  loss         0.214341  avg_L1_norm_grad         0.000017  w[0]    0.174 bias    1.773\n",
      "iter 8301/1000000  loss         0.214341  avg_L1_norm_grad         0.000017  w[0]    0.174 bias    1.773\n",
      "iter 8400/1000000  loss         0.214337  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.775\n",
      "iter 8401/1000000  loss         0.214337  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.775\n",
      "iter 8500/1000000  loss         0.214334  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.776\n",
      "iter 8501/1000000  loss         0.214334  avg_L1_norm_grad         0.000016  w[0]    0.174 bias    1.776\n",
      "iter 8600/1000000  loss         0.214331  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.777\n",
      "iter 8601/1000000  loss         0.214331  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.777\n",
      "iter 8700/1000000  loss         0.214328  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.779\n",
      "iter 8701/1000000  loss         0.214328  avg_L1_norm_grad         0.000015  w[0]    0.174 bias    1.779\n",
      "iter 8800/1000000  loss         0.214325  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.780\n",
      "iter 8801/1000000  loss         0.214325  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.780\n",
      "iter 8900/1000000  loss         0.214322  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.781\n",
      "iter 8901/1000000  loss         0.214322  avg_L1_norm_grad         0.000014  w[0]    0.174 bias    1.781\n",
      "iter 9000/1000000  loss         0.214320  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.783\n",
      "iter 9001/1000000  loss         0.214320  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.783\n",
      "iter 9100/1000000  loss         0.214317  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.784\n",
      "iter 9101/1000000  loss         0.214317  avg_L1_norm_grad         0.000013  w[0]    0.174 bias    1.784\n",
      "iter 9200/1000000  loss         0.214315  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.785\n",
      "iter 9201/1000000  loss         0.214315  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.785\n",
      "iter 9300/1000000  loss         0.214313  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.786\n",
      "iter 9301/1000000  loss         0.214313  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.786\n",
      "iter 9400/1000000  loss         0.214311  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.787\n",
      "iter 9401/1000000  loss         0.214311  avg_L1_norm_grad         0.000012  w[0]    0.174 bias    1.787\n",
      "iter 9500/1000000  loss         0.214309  avg_L1_norm_grad         0.000011  w[0]    0.174 bias    1.788\n",
      "iter 9501/1000000  loss         0.214309  avg_L1_norm_grad         0.000011  w[0]    0.174 bias    1.788\n",
      "Done. Converged after 9538 iterations.\n"
     ]
    }
   ],
   "source": [
    "va_rate=0.3\n",
    "x_va0=x[:int(np.ceil(va_rate*y.shape[0])),]\n",
    "y_va0=y[:int(np.ceil(va_rate*y.shape[0]))]\n",
    "x_te0=x[int(np.ceil(va_rate*y.shape[0])):,]\n",
    "y_te0=y[int(np.ceil(va_rate*y.shape[0])):]\n",
    "orig_lr1 = LRGDF(alpha=10.0, step_size=0.1)\n",
    "orig_lr1.fit(x_te0, y_te0)\n",
    "\n",
    "orig_lr0 = LRGD(alpha=10.0, step_size=0.1)\n",
    "orig_lr0.fit(x_te0, y_te0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Accuracy 0.9558611111111084\n",
      "TurnOn Loaded\n",
      "Ave Loaded\n",
      "New Accuracy 0.9578055555555528\n",
      "No Noise Ori 0.9336111111110852\n",
      "TurnOn Loaded\n",
      "Ave Loaded\n",
      "No Noise New 0.9338888888888629\n"
     ]
    }
   ],
   "source": [
    "y_hat_Origin=np.asarray(orig_lr.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_Origin>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"Origin Accuracy\", acc)\n",
    "\n",
    "y_hat_New=np.asarray(new_lr.predict_proba(x_va)[:,1]).reshape(-1)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va, y_hat_New>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"New Accuracy\", acc)\n",
    "\n",
    "y_hat0=np.asarray(orig_lr0.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat0>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise Ori\", acc)\n",
    "\n",
    "y_hat1=np.asarray(orig_lr1.predict_proba(x_va0)[:,1]).reshape(-1)\n",
    "#print(y_hat0)\n",
    "tp, tn, fp, fn=calc_TP_TN_FP_FN(y_va0, y_hat1>=0.5)\n",
    "acc=(tp + tn) / float(tp + tn + fp + fn + 1e-10)\n",
    "print(\"No Noise New\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr2te, tpr2te, thr2te = sklearn.metrics.roc_curve(y_va, y_hat_Origin)\n",
    "plt.plot(fpr2te,tpr2te, label=\"No added Feature with Noise\")\n",
    "\n",
    "fpr3te, tpr3te, thr3te = sklearn.metrics.roc_curve(y_va, y_hat_New)\n",
    "plt.plot(fpr3te,tpr3te, label=\"Adding noise, ave and counting bright block with 2 bright NB\")\n",
    "\n",
    "fpr1Tte, tpr1Tte, thr1Tte = sklearn.metrics.roc_curve(y_va0, y_hat0)\n",
    "plt.plot(fpr1Tte,tpr1Tte, label=\"No added Feature No Noise\")\n",
    "\n",
    "fprTte, tprTte, thrTte = sklearn.metrics.roc_curve(y_va0, y_hat1)\n",
    "plt.plot(fprTte,tprTte, label=\"Adding ave and counting bright block with 2 bright NB\")\n",
    "\n",
    "plt.xlim([-0.0, 1.0]);\n",
    "plt.ylim([-0.0, 1.0]);\n",
    "plt.legend();\n",
    "plt.xlabel(\"FPR\");\n",
    "plt.ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnOn Loaded\n",
      "Ave Loaded\n"
     ]
    }
   ],
   "source": [
    "x_test_NF=genfromtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',')[1:]\n",
    "yproba1_test_N = new_lr.predict_proba(x_test_NF)[:, 1]\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXV0\n",
    "Error Rate: 0.045 0.993339"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
